[
  {
    "code": "def approx(expected, rel=None, abs=None, nan_ok: bool=False) -> ApproxBase:\n    __tracebackhide__ = True\n    if isinstance(expected, Decimal):\n        cls: Type[ApproxBase] = ApproxDecimal\n    elif isinstance(expected, Mapping):\n        cls = ApproxMapping\n    elif _is_numpy_array(expected):\n        expected = _as_numpy_array(expected)\n        cls = ApproxNumpy\n    elif hasattr(expected, '__getitem__') and isinstance(expected, Sized) and (not isinstance(expected, STRING_TYPES)):\n        cls = ApproxSequenceLike\n    elif isinstance(expected, Collection) and (not isinstance(expected, STRING_TYPES)):\n        msg = f'pytest.approx() only supports ordered sequences, but got: {repr(expected)}'\n        raise TypeError(msg)\n    else:\n        cls = ApproxScalar\n    return cls(expected, rel, abs, nan_ok)",
    "label": true
  },
  {
    "code": "def get_constraints(expr: _NameNodes, frame: nodes.LocalsDictNodeNG) -> dict[nodes.If, set[Constraint]]:\n    current_node: nodes.NodeNG | None = expr\n    constraints_mapping: dict[nodes.If, set[Constraint]] = {}\n    while current_node is not None and current_node is not frame:\n        parent = current_node.parent\n        if isinstance(parent, nodes.If):\n            branch, _ = parent.locate_child(current_node)\n            constraints: set[Constraint] | None = None\n            if branch == 'body':\n                constraints = set(_match_constraint(expr, parent.test))\n            elif branch == 'orelse':\n                constraints = set(_match_constraint(expr, parent.test, invert=True))\n            if constraints:\n                constraints_mapping[parent] = constraints\n        current_node = parent\n    return constraints_mapping",
    "label": true
  },
  {
    "code": "def show_sys_implementation() -> None:\n    logger.info('sys.implementation:')\n    implementation_name = sys.implementation.name\n    with indent_log():\n        show_value('name', implementation_name)",
    "label": true
  },
  {
    "code": "def encode_multipart_formdata(fields: _TYPE_FIELDS, boundary: str | None=None) -> tuple[bytes, str]:\n    body = BytesIO()\n    if boundary is None:\n        boundary = choose_boundary()\n    for field in iter_field_objects(fields):\n        body.write(f'--{boundary}\\r\\n'.encode('latin-1'))\n        writer(body).write(field.render_headers())\n        data = field.data\n        if isinstance(data, int):\n            data = str(data)\n        if isinstance(data, str):\n            writer(body).write(data)\n        else:\n            body.write(data)\n        body.write(b'\\r\\n')\n    body.write(f'--{boundary}--\\r\\n'.encode('latin-1'))\n    content_type = f'multipart/form-data; boundary={boundary}'\n    return (body.getvalue(), content_type)",
    "label": true
  },
  {
    "code": "def _type_check(type1, type2) -> bool:\n    if not all(map(has_known_bases, (type1, type2))):\n        raise _NonDeducibleTypeHierarchy\n    if not all([type1.newstyle, type2.newstyle]):\n        return False\n    try:\n        return type1 in type2.mro()[:-1]\n    except MroError as e:\n        raise _NonDeducibleTypeHierarchy from e",
    "label": true
  },
  {
    "code": "def _get_quote_delimiter(string_token: str) -> str:\n    match = QUOTE_DELIMITER_REGEX.match(string_token)\n    if not match:\n        raise ValueError(f'string token {string_token} is not a well-formed string')\n    return match.group(2)",
    "label": true
  },
  {
    "code": "def parse_args(argv: Optional[Sequence[str]]=None) -> Dict[str, Any]:\n    argv = sys.argv[1:] if argv is None else list(argv)\n    remapped_deprecated_args = []\n    for index, arg in enumerate(argv):\n        if arg in DEPRECATED_SINGLE_DASH_ARGS:\n            remapped_deprecated_args.append(arg)\n            argv[index] = f'-{arg}'\n    parser = _build_arg_parser()\n    arguments = {key: value for key, value in vars(parser.parse_args(argv)).items() if value}\n    if remapped_deprecated_args:\n        arguments['remapped_deprecated_args'] = remapped_deprecated_args\n    if 'dont_order_by_type' in arguments:\n        arguments['order_by_type'] = False\n        del arguments['dont_order_by_type']\n    if 'dont_follow_links' in arguments:\n        arguments['follow_links'] = False\n        del arguments['dont_follow_links']\n    if 'dont_float_to_top' in arguments:\n        del arguments['dont_float_to_top']\n        if arguments.get('float_to_top', False):\n            sys.exit(\"Can't set both --float-to-top and --dont-float-to-top.\")\n        else:\n            arguments['float_to_top'] = False\n    multi_line_output = arguments.get('multi_line_output', None)\n    if multi_line_output:\n        if multi_line_output.isdigit():\n            arguments['multi_line_output'] = WrapModes(int(multi_line_output))\n        else:\n            arguments['multi_line_output'] = WrapModes[multi_line_output]\n    return arguments",
    "label": true
  },
  {
    "code": "def product_star(integer_queue: Queue) -> int:\n    bhavya = 1\n    skibid_peter = Queue()\n    while not integer_queue.is_empty():\n        skibid_peter.enqueue(integer_queue.dequeue())\n    while not skibid_peter.is_empty():\n        a = skibid_peter.dequeue()\n        bhavya *= a\n        integer_queue.enqueue(a)\n    return bhavya",
    "label": true
  },
  {
    "code": "def _check_link_requires_python(link: Link, version_info: Tuple[int, int, int], ignore_requires_python: bool=False) -> bool:\n    try:\n        is_compatible = check_requires_python(link.requires_python, version_info=version_info)\n    except specifiers.InvalidSpecifier:\n        logger.debug('Ignoring invalid Requires-Python (%r) for link: %s', link.requires_python, link)\n    else:\n        if not is_compatible:\n            version = '.'.join(map(str, version_info))\n            if not ignore_requires_python:\n                logger.verbose('Link requires a different Python (%s not in: %r): %s', version, link.requires_python, link)\n                return False\n            logger.debug('Ignoring failed Requires-Python check (%s not in: %r) for link: %s', version, link.requires_python, link)\n    return True",
    "label": true
  },
  {
    "code": "def _msvc14_find_vc2017():\n    root = environ.get('ProgramFiles(x86)') or environ.get('ProgramFiles')\n    if not root:\n        return (None, None)\n    suitable_components = ('Microsoft.VisualStudio.Component.VC.Tools.x86.x64', 'Microsoft.VisualStudio.Workload.WDExpress')\n    for component in suitable_components:\n        with contextlib.suppress(CalledProcessError, OSError, UnicodeDecodeError):\n            path = subprocess.check_output([join(root, 'Microsoft Visual Studio', 'Installer', 'vswhere.exe'), '-latest', '-prerelease', '-requires', component, '-property', 'installationPath', '-products', '*']).decode(encoding='mbcs', errors='strict').strip()\n            path = join(path, 'VC', 'Auxiliary', 'Build')\n            if isdir(path):\n                return (15, path)\n    return (None, None)",
    "label": true
  },
  {
    "code": "def find_ordinal(pos_num: int) -> str:\n    if pos_num == 0:\n        return 'th'\n    elif pos_num == 1:\n        return 'st'\n    elif pos_num == 2:\n        return 'nd'\n    elif pos_num == 3:\n        return 'rd'\n    elif 4 <= pos_num <= 20:\n        return 'th'\n    else:\n        return find_ordinal(pos_num % 10)",
    "label": true
  },
  {
    "code": "def _collect_type_vars(types, typevar_types=None):\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if isinstance(t, typevar_types) and t not in tvars and (not _is_unpack(t)):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)",
    "label": true
  },
  {
    "code": "def _encode_auth(auth):\n    auth_s = urllib.parse.unquote(auth)\n    auth_bytes = auth_s.encode()\n    encoded_bytes = base64.b64encode(auth_bytes)\n    encoded = encoded_bytes.decode()\n    return encoded.replace('\\n', '')",
    "label": true
  },
  {
    "code": "def _has_option(options: Values, reqs: List[InstallRequirement], option: str) -> bool:\n    if getattr(options, option, None):\n        return True\n    for req in reqs:\n        if getattr(req, option, None):\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _tree_to_bytes_helper(tree: HuffmanTree) -> list:\n    data = []\n    left_leaf = 0\n    left_info = tree.left.symbol\n    if not tree.left.is_leaf():\n        left_leaf = 1\n        left_info = tree.left.number\n        data.extend(_tree_to_bytes_helper(tree.left))\n    right_leaf = 0\n    right_info = tree.right.symbol\n    if not tree.right.is_leaf():\n        right_leaf = 1\n        right_info = tree.right.number\n        data.extend(_tree_to_bytes_helper(tree.right))\n    data.extend([left_leaf, left_info, right_leaf, right_info])\n    return data",
    "label": true
  },
  {
    "code": "def merge_cookies(cookiejar, cookies):\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError('You can only merge into CookieJar')\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n    return cookiejar",
    "label": true
  },
  {
    "code": "def _prepare_download(resp: Response, link: Link, progress_bar: str) -> Iterable[bytes]:\n    total_length = _get_http_response_size(resp)\n    if link.netloc == PyPI.file_storage_domain:\n        url = link.show_url\n    else:\n        url = link.url_without_fragment\n    logged_url = redact_auth_from_url(url)\n    if total_length:\n        logged_url = '{} ({})'.format(logged_url, format_size(total_length))\n    if is_from_cache(resp):\n        logger.info('Using cached %s', logged_url)\n    else:\n        logger.info('Downloading %s', logged_url)\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif is_from_cache(resp):\n        show_progress = False\n    elif not total_length:\n        show_progress = True\n    elif total_length > 40 * 1000:\n        show_progress = True\n    else:\n        show_progress = False\n    chunks = response_chunks(resp, CONTENT_CHUNK_SIZE)\n    if not show_progress:\n        return chunks\n    renderer = get_download_progress_renderer(bar_type=progress_bar, size=total_length)\n    return renderer(chunks)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e306(msg, _node, source_lines=None):\n    line = msg.line - 1\n    yield from render_context(line - 1, line + 1, source_lines)\n    body = source_lines[line]\n    indentation = len(body) - len(body.lstrip())\n    yield (None, slice(None, None), LineType.ERROR, body[:indentation] + NEW_BLANK_LINE_MESSAGE + ' ' * indentation)\n    yield from render_context(msg.line, msg.line + 2, source_lines)",
    "label": true
  },
  {
    "code": "def get_text_stdin(encoding: t.Optional[str]=None, errors: t.Optional[str]=None) -> t.TextIO:\n    rv = _get_windows_console_stream(sys.stdin, encoding, errors)\n    if rv is not None:\n        return rv\n    return _force_correct_text_reader(sys.stdin, encoding, errors, force_readable=True)",
    "label": true
  },
  {
    "code": "def get_attrs(obj):\n    if type(obj) in builtins_types or (type(obj) is type and obj in builtins_types):\n        return\n    return getattr(obj, '__dict__', None)",
    "label": true
  },
  {
    "code": "def patch_messages():\n    old_add_message = PyLinter.add_message\n\n    def new_add_message(self, msg_id, line=None, node=None, args=None, confidence=UNDEFINED, col_offset=None, end_lineno=None, end_col_offset=None):\n        old_add_message(self, msg_id, line, node, args, confidence, col_offset, end_lineno, end_col_offset)\n        msg_info = self.msgs_store.get_message_definitions(msg_id)[0]\n        if hasattr(self.reporter, 'handle_node'):\n            self.reporter.handle_node(msg_info, node)\n    PyLinter.add_message = new_add_message",
    "label": true
  },
  {
    "code": "def _is_cert(item):\n    expected = Security.SecCertificateGetTypeID()\n    return CoreFoundation.CFGetTypeID(item) == expected",
    "label": true
  },
  {
    "code": "def _call_aside(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f",
    "label": true
  },
  {
    "code": "def pick_bool(*values: Optional[bool]) -> bool:\n    assert values, '1 or more values required'\n    for value in values:\n        if value is not None:\n            return value\n    return bool(value)",
    "label": true
  },
  {
    "code": "def file_size(path: str) -> Union[int, float]:\n    if os.path.islink(path):\n        return 0\n    return os.path.getsize(path)",
    "label": true
  },
  {
    "code": "def glob_to_re(pattern):\n    pattern_re = fnmatch.translate(pattern)\n    sep = os.sep\n    if os.sep == '\\\\':\n        sep = '\\\\\\\\\\\\\\\\'\n    escaped = '\\\\1[^%s]' % sep\n    pattern_re = re.sub('((?<!\\\\\\\\)(\\\\\\\\\\\\\\\\)*)\\\\.', escaped, pattern_re)\n    return pattern_re",
    "label": true
  },
  {
    "code": "def glob0(dirname, basename):\n    if not basename:\n        if os.path.isdir(dirname):\n            return [basename]\n    elif os.path.lexists(os.path.join(dirname, basename)):\n        return [basename]\n    return []",
    "label": true
  },
  {
    "code": "def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None, socket_options=None):\n    host, port = address\n    if host.startswith('['):\n        host = host.strip('[]')\n    err = None\n    family = allowed_gai_family()\n    try:\n        host.encode('idna')\n    except UnicodeError:\n        return six.raise_from(LocationParseError(u\"'%s', label empty or too long\" % host), None)\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket.socket(af, socktype, proto)\n            _set_socket_options(sock, socket_options)\n            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n        except socket.error as e:\n            err = e\n            if sock is not None:\n                sock.close()\n                sock = None\n    if err is not None:\n        raise err\n    raise socket.error('getaddrinfo returns an empty list')",
    "label": true
  },
  {
    "code": "def get_python_inc(plat_specific=0, prefix=None):\n    default_prefix = BASE_EXEC_PREFIX if plat_specific else BASE_PREFIX\n    resolved_prefix = prefix if prefix is not None else default_prefix\n    try:\n        getter = globals()[f'_get_python_inc_{os.name}']\n    except KeyError:\n        raise DistutilsPlatformError(\"I don't know where Python installs its C header files on platform '%s'\" % os.name)\n    return getter(resolved_prefix, prefix, plat_specific)",
    "label": true
  },
  {
    "code": "def get_import_name(importnode: ImportNode, modname: str | None) -> str | None:\n    if isinstance(importnode, nodes.ImportFrom) and importnode.level:\n        root = importnode.root()\n        if isinstance(root, nodes.Module):\n            try:\n                return root.relative_to_absolute_name(modname, level=importnode.level)\n            except TooManyLevelsError:\n                return modname\n    return modname",
    "label": true
  },
  {
    "code": "def _glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def show_compilers():\n    from ..ccompiler import show_compilers\n    show_compilers()",
    "label": true
  },
  {
    "code": "def suppress_type_checks(func: Callable[P, T] | None=None) -> Callable[P, T] | ContextManager[None]:\n\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n        global type_checks_suppressed\n        with type_checks_suppress_lock:\n            type_checks_suppressed += 1\n        assert func is not None\n        try:\n            return func(*args, **kwargs)\n        finally:\n            with type_checks_suppress_lock:\n                type_checks_suppressed -= 1\n\n    def cm() -> Generator[None, None, None]:\n        global type_checks_suppressed\n        with type_checks_suppress_lock:\n            type_checks_suppressed += 1\n        try:\n            yield\n        finally:\n            with type_checks_suppress_lock:\n                type_checks_suppressed -= 1\n    if func is None:\n        return contextmanager(cm)()\n    else:\n        update_wrapper(wrapper, func)\n        return wrapper",
    "label": true
  },
  {
    "code": "def _check_link_requires_python(link: Link, version_info: Tuple[int, int, int], ignore_requires_python: bool=False) -> bool:\n    try:\n        is_compatible = check_requires_python(link.requires_python, version_info=version_info)\n    except specifiers.InvalidSpecifier:\n        logger.debug('Ignoring invalid Requires-Python (%r) for link: %s', link.requires_python, link)\n    else:\n        if not is_compatible:\n            version = '.'.join(map(str, version_info))\n            if not ignore_requires_python:\n                logger.verbose('Link requires a different Python (%s not in: %r): %s', version, link.requires_python, link)\n                return False\n            logger.debug('Ignoring failed Requires-Python check (%s not in: %r) for link: %s', version, link.requires_python, link)\n    return True",
    "label": true
  },
  {
    "code": "def _set_socket_options(sock, options):\n    if options is None:\n        return\n    for opt in options:\n        sock.setsockopt(*opt)",
    "label": true
  },
  {
    "code": "def _set_start_from_first_decorator(node):\n    if getattr(node, 'decorators'):\n        first_child = node.decorators\n        node.fromlineno = first_child.fromlineno\n        node.col_offset = first_child.col_offset\n    return node",
    "label": true
  },
  {
    "code": "def iter_slices(string, slice_length):\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos:pos + slice_length]\n        pos += slice_length",
    "label": true
  },
  {
    "code": "def _verifyflags_enum() -> str:\n    enum = '\\n    class VerifyFlags(_IntFlag):\\n        VERIFY_DEFAULT = 0\\n        VERIFY_CRL_CHECK_LEAF = 1\\n        VERIFY_CRL_CHECK_CHAIN = 2\\n        VERIFY_X509_STRICT = 3\\n        VERIFY_X509_TRUSTED_FIRST = 4'\n    if PY310_PLUS:\n        enum += '\\n        VERIFY_ALLOW_PROXY_CERTS = 5\\n        VERIFY_X509_PARTIAL_CHAIN = 6\\n        '\n    return enum",
    "label": true
  },
  {
    "code": "def read_attr(attr_desc: str, package_dir: Optional[Mapping[str, str]]=None, root_dir: Optional[_Path]=None):\n    root_dir = root_dir or os.getcwd()\n    attrs_path = attr_desc.strip().split('.')\n    attr_name = attrs_path.pop()\n    module_name = '.'.join(attrs_path)\n    module_name = module_name or '__init__'\n    _parent_path, path, module_name = _find_module(module_name, package_dir, root_dir)\n    spec = _find_spec(module_name, path)\n    try:\n        return getattr(StaticModule(module_name, spec), attr_name)\n    except Exception:\n        module = _load_spec(spec, module_name)\n        return getattr(module, attr_name)",
    "label": true
  },
  {
    "code": "def railroad_to_html(diagrams: List[NamedDiagram], embed=False, **kwargs) -> str:\n    data = []\n    for diagram in diagrams:\n        if diagram.diagram is None:\n            continue\n        io = StringIO()\n        try:\n            css = kwargs.get('css')\n            diagram.diagram.writeStandalone(io.write, css=css)\n        except AttributeError:\n            diagram.diagram.writeSvg(io.write)\n        title = diagram.name\n        if diagram.index == 0:\n            title += ' (root)'\n        data.append({'title': title, 'text': '', 'svg': io.getvalue()})\n    return template.render(diagrams=data, embed=embed, **kwargs)",
    "label": true
  },
  {
    "code": "def create_list_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 2\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n    out.flags.unset_all(key)\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.append_nest_to_list(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']]', pos):\n        raise suffixed_err(src, pos, \"Expected ']]' at the end of an array declaration\")\n    return (pos + 2, key)",
    "label": true
  },
  {
    "code": "def get_unpacked_marks(obj: Union[object, type], *, consider_mro: bool=True) -> List[Mark]:\n    if isinstance(obj, type):\n        if not consider_mro:\n            mark_lists = [obj.__dict__.get('pytestmark', [])]\n        else:\n            mark_lists = [x.__dict__.get('pytestmark', []) for x in obj.__mro__]\n        mark_list = []\n        for item in mark_lists:\n            if isinstance(item, list):\n                mark_list.extend(item)\n            else:\n                mark_list.append(item)\n    else:\n        mark_attribute = getattr(obj, 'pytestmark', [])\n        if isinstance(mark_attribute, list):\n            mark_list = mark_attribute\n        else:\n            mark_list = [mark_attribute]\n    return list(normalize_mark_list(mark_list))",
    "label": true
  },
  {
    "code": "def check_typed_dict(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, dict):\n        raise TypeCheckError('is not a dict')\n    declared_keys = frozenset(origin_type.__annotations__)\n    if hasattr(origin_type, '__required_keys__'):\n        required_keys = origin_type.__required_keys__\n    else:\n        required_keys = declared_keys if origin_type.__total__ else frozenset()\n    existing_keys = frozenset(value)\n    extra_keys = existing_keys - declared_keys\n    if extra_keys:\n        keys_formatted = ', '.join((f'\"{key}\"' for key in sorted(extra_keys, key=repr)))\n        raise TypeCheckError(f'has unexpected extra key(s): {keys_formatted}')\n    missing_keys = required_keys - existing_keys\n    if missing_keys:\n        keys_formatted = ', '.join((f'\"{key}\"' for key in sorted(missing_keys, key=repr)))\n        raise TypeCheckError(f'is missing required key(s): {keys_formatted}')\n    for key, argtype in get_type_hints(origin_type).items():\n        argvalue = value.get(key, _missing)\n        if argvalue is not _missing:\n            try:\n                check_type_internal(argvalue, argtype, memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f'value of key {key!r}')\n                raise",
    "label": true
  },
  {
    "code": "def _rebuild_mod_path(orig_path, package_name, module):\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path",
    "label": true
  },
  {
    "code": "def unquote_header_value(value, is_filename=False):\n    if value and value[0] == value[-1] == '\"':\n        value = value[1:-1]\n        if not is_filename or value[:2] != '\\\\\\\\':\n            return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n    return value",
    "label": true
  },
  {
    "code": "def splitUp(pred):\n    res = re_splitComparison.match(pred)\n    if not res:\n        raise ValueError('bad package restriction syntax: %r' % pred)\n    comp, verStr = res.groups()\n    with version.suppress_known_deprecation():\n        other = version.StrictVersion(verStr)\n    return (comp, other)",
    "label": true
  },
  {
    "code": "def format_header_param_html5(name, value):\n    if isinstance(value, six.binary_type):\n        value = value.decode('utf-8')\n    value = _replace_multiple(value, _HTML5_REPLACEMENTS)\n    return u'%s=\"%s\"' % (name, value)",
    "label": true
  },
  {
    "code": "def report_by_type_stats(sect: reporter_nodes.Section, stats: LinterStats, old_stats: LinterStats | None) -> None:\n    nice_stats: dict[str, dict[str, str]] = {}\n    for node_type in ('module', 'class', 'method', 'function'):\n        node_type = cast(Literal['function', 'class', 'method', 'module'], node_type)\n        total = stats.get_node_count(node_type)\n        nice_stats[node_type] = {}\n        if total != 0:\n            undocumented_node = stats.get_undocumented(node_type)\n            documented = total - undocumented_node\n            percent = documented * 100.0 / total\n            nice_stats[node_type]['percent_documented'] = f'{percent:.2f}'\n            badname_node = stats.get_bad_names(node_type)\n            percent = badname_node * 100.0 / total\n            nice_stats[node_type]['percent_badname'] = f'{percent:.2f}'\n    lines = ['type', 'number', 'old number', 'difference', '%documented', '%badname']\n    for node_type in ('module', 'class', 'method', 'function'):\n        node_type = cast(Literal['function', 'class', 'method', 'module'], node_type)\n        new = stats.get_node_count(node_type)\n        old = old_stats.get_node_count(node_type) if old_stats else None\n        diff_str = lint_utils.diff_string(old, new) if old else None\n        lines += [node_type, str(new), str(old) if old else 'NC', diff_str if diff_str else 'NC', nice_stats[node_type].get('percent_documented', '0'), nice_stats[node_type].get('percent_badname', '0')]\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))",
    "label": true
  },
  {
    "code": "def _preconvert(item: Any) -> Union[str, List[Any]]:\n    if isinstance(item, (set, frozenset)):\n        return list(item)\n    if isinstance(item, WrapModes):\n        return str(item.name)\n    if isinstance(item, Path):\n        return str(item)\n    if callable(item) and hasattr(item, '__name__'):\n        return str(item.__name__)\n    raise TypeError(f'Unserializable object {item} of type {type(item)}')",
    "label": true
  },
  {
    "code": "def _is_invalid_metaclass(metaclass: nodes.ClassDef) -> bool:\n    try:\n        mro = metaclass.mro()\n    except (astroid.DuplicateBasesError, astroid.InconsistentMroError):\n        return True\n    return not any((is_builtin_object(cls) and cls.name == 'type' for cls in mro))",
    "label": true
  },
  {
    "code": "def _create_truststore_ssl_context() -> Optional['SSLContext']:\n    if sys.version_info < (3, 10):\n        raise CommandError('The truststore feature is only available for Python 3.10+')\n    try:\n        import ssl\n    except ImportError:\n        logger.warning('Disabling truststore since ssl support is missing')\n        return None\n    try:\n        import truststore\n    except ImportError:\n        raise CommandError(\"To use the truststore feature, 'truststore' must be installed into pip's current environment.\")\n    return truststore.SSLContext(ssl.PROTOCOL_TLS_CLIENT)",
    "label": true
  },
  {
    "code": "def code(func):\n    if ismethod(func):\n        func = func.__func__\n    if isfunction(func):\n        func = func.__code__\n    if istraceback(func):\n        func = func.tb_frame\n    if isframe(func):\n        func = func.f_code\n    if iscode(func):\n        return func\n    return",
    "label": true
  },
  {
    "code": "def _can_symlink_files(base_dir: Path) -> bool:\n    with TemporaryDirectory(dir=str(base_dir.resolve())) as tmp:\n        path1, path2 = (Path(tmp, 'file1.txt'), Path(tmp, 'file2.txt'))\n        path1.write_text('file1', encoding='utf-8')\n        with suppress(AttributeError, NotImplementedError, OSError):\n            os.symlink(path1, path2)\n            if path2.is_symlink() and path2.read_text(encoding='utf-8') == 'file1':\n                return True\n        try:\n            os.link(path1, path2)\n        except Exception as ex:\n            msg = 'File system does not seem to support either symlinks or hard links. Strict editable installs require one of them to be supported.'\n            raise LinksNotSupported(msg) from ex\n        return False",
    "label": true
  },
  {
    "code": "def _evaluate_markers(markers: List[Any], environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n            if isinstance(lhs, Variable):\n                lhs_value = _get_env(environment, lhs.value)\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                rhs_value = _get_env(environment, rhs.value)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in ['and', 'or']\n            if marker == 'or':\n                groups.append([])\n    return any((all(item) for item in groups))",
    "label": true
  },
  {
    "code": "def main() -> None:\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n    parser = argparse.ArgumentParser(description='OS distro info tool')\n    parser.add_argument('--json', '-j', help='Output in machine readable format', action='store_true')\n    parser.add_argument('--root-dir', '-r', type=str, dest='root_dir', help='Path to the root filesystem directory (defaults to /)')\n    args = parser.parse_args()\n    if args.root_dir:\n        dist = LinuxDistribution(include_lsb=False, include_uname=False, include_oslevel=False, root_dir=args.root_dir)\n    else:\n        dist = _distro\n    if args.json:\n        logger.info(json.dumps(dist.info(), indent=4, sort_keys=True))\n    else:\n        logger.info('Name: %s', dist.name(pretty=True))\n        distribution_version = dist.version(pretty=True)\n        logger.info('Version: %s', distribution_version)\n        distribution_codename = dist.codename()\n        logger.info('Codename: %s', distribution_codename)",
    "label": true
  },
  {
    "code": "def decode(s: Union[str, bytes, bytearray], strict: bool=False, uts46: bool=False, std3_rules: bool=False) -> str:\n    try:\n        if isinstance(s, (bytes, bytearray)):\n            s = s.decode('ascii')\n    except UnicodeDecodeError:\n        raise IDNAError('Invalid ASCII in A-label')\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split('.')\n    if not labels or labels == ['']:\n        raise IDNAError('Empty domain')\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError('Empty label')\n    if trailing_dot:\n        result.append('')\n    return '.'.join(result)",
    "label": true
  },
  {
    "code": "def inject_into_urllib3() -> None:\n    _validate_dependencies_met()\n    util.SSLContext = PyOpenSSLContext\n    util.ssl_.SSLContext = PyOpenSSLContext\n    util.IS_PYOPENSSL = True\n    util.ssl_.IS_PYOPENSSL = True",
    "label": true
  },
  {
    "code": "def configuration_to_dict(handlers: Tuple['ConfigHandler', ...]) -> dict:\n    config_dict: dict = defaultdict(dict)\n    for handler in handlers:\n        for option in handler.set_options:\n            value = _get_option(handler.target_obj, option)\n            config_dict[handler.section_prefix][option] = value\n    return config_dict",
    "label": true
  },
  {
    "code": "def get_best_invocation_for_this_python() -> str:\n    exe = sys.executable\n    exe_name = os.path.basename(exe)\n    found_executable = shutil.which(exe_name)\n    if found_executable and os.path.samefile(found_executable, exe):\n        return exe_name\n    return exe",
    "label": true
  },
  {
    "code": "def ensure_dir(path: str) -> None:\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST and e.errno != errno.ENOTEMPTY:\n            raise",
    "label": true
  },
  {
    "code": "def get_all_lexers(plugins=True):\n    for item in LEXERS.values():\n        yield item[1:]\n    if plugins:\n        for lexer in find_plugin_lexers():\n            yield (lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes)",
    "label": true
  },
  {
    "code": "def show_unified_diff(*, file_input: str, file_output: str, file_path: Optional[Path], output: Optional[TextIO]=None, color_output: bool=False) -> None:\n    printer = create_terminal_printer(color_output, output)\n    file_name = '' if file_path is None else str(file_path)\n    file_mtime = str(datetime.now() if file_path is None else datetime.fromtimestamp(file_path.stat().st_mtime))\n    unified_diff_lines = unified_diff(file_input.splitlines(keepends=True), file_output.splitlines(keepends=True), fromfile=file_name + ':before', tofile=file_name + ':after', fromfiledate=file_mtime, tofiledate=str(datetime.now()))\n    for line in unified_diff_lines:\n        printer.diff_line(line)",
    "label": true
  },
  {
    "code": "def _get_renamed_namedtuple_attributes(field_names):\n    names = list(field_names)\n    seen = set()\n    for i, name in enumerate(field_names):\n        if not all((c.isalnum() or c == '_' for c in name)) or keyword.iskeyword(name) or (not name) or name[0].isdigit() or name.startswith('_') or (name in seen):\n            names[i] = '_%d' % i\n        seen.add(name)\n    return tuple(names)",
    "label": true
  },
  {
    "code": "def object_type_repr(obj: t.Any) -> str:\n    if obj is None:\n        return 'None'\n    elif obj is Ellipsis:\n        return 'Ellipsis'\n    cls = type(obj)\n    if cls.__module__ == 'builtins':\n        return f'{cls.__name__} object'\n    return f'{cls.__module__}.{cls.__name__} object'",
    "label": true
  },
  {
    "code": "def _indent(text, prefix, predicate=default_predicate) -> str:\n\n    def prefixed_lines():\n        for line in text.splitlines(True):\n            yield (prefix + line if predicate(line) else line)\n    return ''.join(prefixed_lines())",
    "label": true
  },
  {
    "code": "def random_combination_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    indices = sorted((randrange(n) for i in range(r)))\n    return tuple((pool[i] for i in indices))",
    "label": true
  },
  {
    "code": "def makefile(self: socket_cls, mode: Literal['r'] | Literal['w'] | Literal['rw'] | Literal['wr'] | Literal['']='r', buffering: int | None=None, *args: typing.Any, **kwargs: typing.Any) -> typing.BinaryIO | typing.TextIO:\n    buffering = 0\n    return socket_cls.makefile(self, mode, buffering, *args, **kwargs)",
    "label": true
  },
  {
    "code": "def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n    if tokenizer.check('VARIABLE'):\n        return process_env_var(tokenizer.read().text.replace('.', '_'))\n    elif tokenizer.check('QUOTED_STRING'):\n        return process_python_str(tokenizer.read().text)\n    else:\n        tokenizer.raise_syntax_error(message='Expected a marker variable or quoted string')",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:\n    package_set, _ = create_package_set_from_installed()\n    would_be_installed = _simulate_installation_of(to_install, package_set)\n    whitelist = _create_whitelist(would_be_installed, package_set)\n    return (package_set, check_package_set(package_set, should_ignore=lambda name: name not in whitelist))",
    "label": true
  },
  {
    "code": "def _pipe_line_with_colons(colwidths, colaligns):\n    if not colaligns:\n        colaligns = [''] * len(colwidths)\n    segments = [_pipe_segment_with_colons(a, w) for a, w in zip(colaligns, colwidths)]\n    return '|' + '|'.join(segments) + '|'",
    "label": true
  },
  {
    "code": "def unpack_url(link: Link, location: str, download: Downloader, verbosity: int, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> Optional[File]:\n    if link.is_vcs:\n        unpack_vcs_link(link, location, verbosity=verbosity)\n        return None\n    assert not link.is_existing_dir()\n    if link.is_file:\n        file = get_file_url(link, download_dir, hashes=hashes)\n    else:\n        file = get_http_url(link, download, download_dir, hashes=hashes)\n    if not link.is_wheel:\n        unpack_file(file.path, location, file.content_type)\n    return file",
    "label": true
  },
  {
    "code": "def _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n    return decorate",
    "label": true
  },
  {
    "code": "def get_win_folder_via_ctypes(csidl_name: str) -> str:\n    csidl_const = {'CSIDL_APPDATA': 26, 'CSIDL_COMMON_APPDATA': 35, 'CSIDL_LOCAL_APPDATA': 28, 'CSIDL_PERSONAL': 5}.get(csidl_name)\n    if csidl_const is None:\n        raise ValueError(f'Unknown CSIDL name: {csidl_name}')\n    buf = ctypes.create_unicode_buffer(1024)\n    windll = getattr(ctypes, 'windll')\n    windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n    if any((ord(c) > 255 for c in buf)):\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n    return buf.value",
    "label": true
  },
  {
    "code": "def interpreter_version(*, warn: bool=False) -> str:\n    version = _get_config_var('py_version_nodot', warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
    "label": true
  },
  {
    "code": "def process_env_var(env_var: str) -> Variable:\n    if env_var == 'platform_python_implementation' or env_var == 'python_implementation':\n        return Variable('platform_python_implementation')\n    else:\n        return Variable(env_var)",
    "label": true
  },
  {
    "code": "def test_nostrictio_contentsfmode():\n    bench(False, dill.CONTENTS_FMODE, True)\n    teardown_module()",
    "label": true
  },
  {
    "code": "def _handle_no_cache_dir(option: Option, opt: str, value: str, parser: OptionParser) -> None:\n    if value is not None:\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n    parser.values.cache_dir = False",
    "label": true
  },
  {
    "code": "def _build_simple_row(padded_cells, rowfmt):\n    begin, sep, end = rowfmt\n    return (begin + sep.join(padded_cells) + end).rstrip()",
    "label": true
  },
  {
    "code": "def locate(iterable, pred=bool, window_size=None):\n    if window_size is None:\n        return compress(count(), map(pred, iterable))\n    if window_size < 1:\n        raise ValueError('window size must be at least 1')\n    it = windowed(iterable, window_size, fillvalue=_marker)\n    return compress(count(), starmap(pred, it))",
    "label": true
  },
  {
    "code": "def parse_format_method_string(format_string: str) -> tuple[list[tuple[str, list[tuple[bool, str]]]], int, int]:\n    keyword_arguments = []\n    implicit_pos_args_cnt = 0\n    explicit_pos_args = set()\n    for name in collect_string_fields(format_string):\n        if name and str(name).isdigit():\n            explicit_pos_args.add(str(name))\n        elif name:\n            keyname, fielditerator = split_format_field_names(name)\n            if isinstance(keyname, numbers.Number):\n                explicit_pos_args.add(str(keyname))\n            try:\n                keyword_arguments.append((keyname, list(fielditerator)))\n            except ValueError as e:\n                raise IncompleteFormatString() from e\n        else:\n            implicit_pos_args_cnt += 1\n    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))",
    "label": true
  },
  {
    "code": "def _sample_unweighted(iterable, k):\n    reservoir = take(k, iterable)\n    W = exp(log(random()) / k)\n    next_index = k + floor(log(random()) / log(1 - W))\n    for index, element in enumerate(iterable, k):\n        if index == next_index:\n            reservoir[randrange(k)] = element\n            W *= exp(log(random()) / k)\n            next_index += floor(log(random()) / log(1 - W)) + 1\n    return reservoir",
    "label": true
  },
  {
    "code": "def _get_encoding(encoding_or_label):\n    if hasattr(encoding_or_label, 'codec_info'):\n        return encoding_or_label\n    encoding = lookup(encoding_or_label)\n    if encoding is None:\n        raise LookupError('Unknown encoding label: %r' % encoding_or_label)\n    return encoding",
    "label": true
  },
  {
    "code": "def is_compatible(wheel, tags=None):\n    if not isinstance(wheel, Wheel):\n        wheel = Wheel(wheel)\n    result = False\n    if tags is None:\n        tags = COMPATIBLE_TAGS\n    for ver, abi, arch in tags:\n        if ver in wheel.pyver and abi in wheel.abi and (arch in wheel.arch):\n            result = True\n            break\n    return result",
    "label": true
  },
  {
    "code": "def parse_function_type_comment(type_comment: str) -> FunctionType | None:\n    if _ast_py3 is None:\n        return None\n    func_type = _ast_py3.parse(type_comment, '<type_comment>', 'func_type')\n    return FunctionType(argtypes=func_type.argtypes, returns=func_type.returns)",
    "label": true
  },
  {
    "code": "def _print_list_as_json(requested_items):\n    import json\n    result = {}\n    if 'lexer' in requested_items:\n        info = {}\n        for fullname, names, filenames, mimetypes in get_all_lexers():\n            info[fullname] = {'aliases': names, 'filenames': filenames, 'mimetypes': mimetypes}\n        result['lexers'] = info\n    if 'formatter' in requested_items:\n        info = {}\n        for cls in get_all_formatters():\n            doc = docstring_headline(cls)\n            info[cls.name] = {'aliases': cls.aliases, 'filenames': cls.filenames, 'doc': doc}\n        result['formatters'] = info\n    if 'filter' in requested_items:\n        info = {}\n        for name in get_all_filters():\n            cls = find_filter_class(name)\n            info[name] = {'doc': docstring_headline(cls)}\n        result['filters'] = info\n    if 'style' in requested_items:\n        info = {}\n        for name in get_all_styles():\n            cls = get_style_by_name(name)\n            info[name] = {'doc': docstring_headline(cls)}\n        result['styles'] = info\n    json.dump(result, sys.stdout)",
    "label": true
  },
  {
    "code": "def gen_lib_options(compiler, library_dirs, runtime_library_dirs, libraries):\n    lib_opts = []\n    for dir in library_dirs:\n        lib_opts.append(compiler.library_dir_option(dir))\n    for dir in runtime_library_dirs:\n        opt = compiler.runtime_library_dir_option(dir)\n        if isinstance(opt, list):\n            lib_opts = lib_opts + opt\n        else:\n            lib_opts.append(opt)\n    for lib in libraries:\n        lib_dir, lib_name = os.path.split(lib)\n        if lib_dir:\n            lib_file = compiler.find_library_file([lib_dir], lib_name)\n            if lib_file:\n                lib_opts.append(lib_file)\n            else:\n                compiler.warn(\"no library file corresponding to '%s' found (skipping)\" % lib)\n        else:\n            lib_opts.append(compiler.library_option(lib))\n    return lib_opts",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def platform_tags(linux: str, arch: str) -> Iterator[str]:\n    if not _have_compatible_abi(arch):\n        return\n    too_old_glibc2 = _GLibCVersion(2, 16)\n    if arch in {'x86_64', 'i686'}:\n        too_old_glibc2 = _GLibCVersion(2, 4)\n    current_glibc = _GLibCVersion(*_get_glibc_version())\n    glibc_max_list = [current_glibc]\n    for glibc_major in range(current_glibc.major - 1, 1, -1):\n        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n    for glibc_max in glibc_max_list:\n        if glibc_max.major == too_old_glibc2.major:\n            min_minor = too_old_glibc2.minor\n        else:\n            min_minor = -1\n        for glibc_minor in range(glibc_max.minor, min_minor, -1):\n            glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n            tag = 'manylinux_{}_{}'.format(*glibc_version)\n            if _is_compatible(tag, arch, glibc_version):\n                yield linux.replace('linux', tag)\n            if glibc_version in _LEGACY_MANYLINUX_MAP:\n                legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n                if _is_compatible(legacy_tag, arch, glibc_version):\n                    yield linux.replace('linux', legacy_tag)",
    "label": true
  },
  {
    "code": "def __getstate__():\n    state = {}\n    g = globals()\n    for k, v in _state_vars.items():\n        state[k] = g['_sget_' + v](g[k])\n    return state",
    "label": true
  },
  {
    "code": "def _check_no_input(message: str) -> None:\n    if os.environ.get('PIP_NO_INPUT'):\n        raise Exception(f'No input was expected ($PIP_NO_INPUT set); question: {message}')",
    "label": true
  },
  {
    "code": "def _is_owner_ignored(owner: SuccessfulInferenceResult, attrname: str | None, ignored_classes: Iterable[str], ignored_modules: Iterable[str]) -> bool:\n    if is_module_ignored(owner.root(), ignored_modules):\n        return True\n    ignored_classes = set(ignored_classes)\n    qname = owner.qname() if hasattr(owner, 'qname') else ''\n    return any((ignore in (attrname, qname) for ignore in ignored_classes))",
    "label": true
  },
  {
    "code": "def make_distribution_for_install_requirement(install_req: InstallRequirement) -> AbstractDistribution:\n    if install_req.editable:\n        return SourceDistribution(install_req)\n    if install_req.is_wheel:\n        return WheelDistribution(install_req)\n    return SourceDistribution(install_req)",
    "label": true
  },
  {
    "code": "def get_stderr_fileno() -> int:\n    try:\n        fileno = sys.stderr.fileno()\n        if fileno == -1:\n            raise AttributeError()\n        return fileno\n    except (AttributeError, io.UnsupportedOperation):\n        return sys.__stderr__.fileno()",
    "label": true
  },
  {
    "code": "def _get_previous_scripts(dist: 'Distribution') -> Optional[list]:\n    value = getattr(dist, 'entry_points', None) or {}\n    return value.get('console_scripts')",
    "label": true
  },
  {
    "code": "def parse_key(src: str, pos: Pos) -> tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: str | None = src[pos]\n        except IndexError:\n            char = None\n        if char != '.':\n            return (pos, key)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def _remove_and_clear_zip_directory_cache_data(normalized_path):\n\n    def clear_and_remove_cached_zip_archive_directory_data(path, old_entry):\n        old_entry.clear()\n    _update_zipimporter_cache(normalized_path, zipimport._zip_directory_cache, updater=clear_and_remove_cached_zip_archive_directory_data)",
    "label": true
  },
  {
    "code": "def _annotated_unpack_infer(stmt: nodes.NodeNG, context: InferenceContext | None=None) -> Generator[tuple[nodes.NodeNG, SuccessfulInferenceResult], None, None]:\n    if isinstance(stmt, (nodes.List, nodes.Tuple)):\n        for elt in stmt.elts:\n            inferred = utils.safe_infer(elt)\n            if inferred and (not isinstance(inferred, util.UninferableBase)):\n                yield (elt, inferred)\n        return\n    for inferred in stmt.infer(context):\n        if isinstance(inferred, util.UninferableBase):\n            continue\n        yield (stmt, inferred)",
    "label": true
  },
  {
    "code": "def _get_python_inc_posix(prefix, spec_prefix, plat_specific):\n    if IS_PYPY and sys.version_info < (3, 8):\n        return os.path.join(prefix, 'include')\n    return _get_python_inc_posix_python(plat_specific) or _extant(_get_python_inc_from_config(plat_specific, spec_prefix)) or _get_python_inc_posix_prefix(prefix)",
    "label": true
  },
  {
    "code": "def is_generator(func: object) -> bool:\n    genfunc = inspect.isgeneratorfunction(func)\n    return genfunc and (not iscoroutinefunction(func))",
    "label": true
  },
  {
    "code": "def write_docstring(tw: TerminalWriter, doc: str, indent: str='    ') -> None:\n    for line in doc.split('\\n'):\n        tw.line(indent + line)",
    "label": true
  },
  {
    "code": "def _is_binary_reader(stream: t.IO[t.Any], default: bool=False) -> bool:\n    try:\n        return isinstance(stream.read(0), bytes)\n    except Exception:\n        return default",
    "label": true
  },
  {
    "code": "def _get_url_from_path(path: str, name: str) -> Optional[str]:\n    if _looks_like_path(name) and os.path.isdir(path):\n        if is_installable_dir(path):\n            return path_to_url(path)\n        raise InstallationError(f\"Directory {name!r} is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\")\n    if not is_archive_file(path):\n        return None\n    if os.path.isfile(path):\n        return path_to_url(path)\n    urlreq_parts = name.split('@', 1)\n    if len(urlreq_parts) >= 2 and (not _looks_like_path(urlreq_parts[0])):\n        return None\n    logger.warning('Requirement %r looks like a filename, but the file does not exist', name)\n    return path_to_url(path)",
    "label": true
  },
  {
    "code": "def _looks_like_subscriptable(node: ClassDef) -> bool:\n    if node.qname().startswith('_collections') or node.qname().startswith('collections'):\n        try:\n            node.getattr('__class_getitem__')\n            return True\n        except AttributeInferenceError:\n            pass\n    return False",
    "label": true
  },
  {
    "code": "def _have_cython():\n    cython_impl = 'Cython.Distutils.build_ext'\n    try:\n        __import__(cython_impl, fromlist=['build_ext']).build_ext\n        return True\n    except Exception:\n        pass\n    return False",
    "label": true
  },
  {
    "code": "def parse(code: str, module_name: str='', path: str | None=None, apply_transforms: bool=True) -> nodes.Module:\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(manager=AstroidManager(), apply_transforms=apply_transforms)\n    return builder.string_build(code, modname=module_name, path=path)",
    "label": true
  },
  {
    "code": "def is_from_fallback_block(node: nodes.NodeNG) -> bool:\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n    if isinstance(context, nodes.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable((handler.body for handler in context.handlers))\n        handlers = context.handlers\n    has_fallback_imports = any((isinstance(import_node, (nodes.ImportFrom, nodes.Import)) for import_node in other_body))\n    ignores_import_error = _except_handlers_ignores_exceptions(handlers, (ImportError, ModuleNotFoundError))\n    return ignores_import_error or has_fallback_imports",
    "label": true
  },
  {
    "code": "def redact_netloc(netloc: str) -> str:\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    if password is None:\n        user = '****'\n        password = ''\n    else:\n        user = urllib.parse.quote(user)\n        password = ':****'\n    return '{user}{password}@{netloc}'.format(user=user, password=password, netloc=netloc)",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def insert_default_options() -> None:\n    options = get_default_options()\n    options.reverse()\n    for arg in options:\n        sys.argv.insert(1, arg)",
    "label": true
  },
  {
    "code": "def format_command_result(command_args: List[str], command_output: str) -> str:\n    command_desc = format_command_args(command_args)\n    text = f'Command arguments: {command_desc}\\n'\n    if not command_output:\n        text += 'Command output: None'\n    elif logger.getEffectiveLevel() > logging.DEBUG:\n        text += 'Command output: [use --verbose to show]'\n    else:\n        if not command_output.endswith('\\n'):\n            command_output += '\\n'\n        text += f'Command output:\\n{command_output}'\n    return text",
    "label": true
  },
  {
    "code": "def _get_leafs_and_levels(tree: HuffmanTree) -> tuple[list[HuffmanTree], list[int]]:\n    leafs = []\n    nodes = [tree]\n    levels = []\n    while nodes:\n        n = len(nodes)\n        leaves_counter = 0\n        for _ in range(0, n):\n            tree_curr = nodes.pop(0)\n            if tree_curr.left.is_leaf():\n                leafs.append(tree_curr.left)\n                leaves_counter += 1\n            else:\n                nodes.append(tree_curr.left)\n            if tree_curr.right.is_leaf():\n                leafs.append(tree_curr.right)\n                leaves_counter += 1\n            else:\n                nodes.append(tree_curr.right)\n        levels.append(leaves_counter)\n    return (leafs, levels)",
    "label": true
  },
  {
    "code": "def _print_help(what, name):\n    try:\n        if what == 'lexer':\n            cls = get_lexer_by_name(name)\n            print('Help on the %s lexer:' % cls.name)\n            print(dedent(cls.__doc__))\n        elif what == 'formatter':\n            cls = find_formatter_class(name)\n            print('Help on the %s formatter:' % cls.name)\n            print(dedent(cls.__doc__))\n        elif what == 'filter':\n            cls = find_filter_class(name)\n            print('Help on the %s filter:' % name)\n            print(dedent(cls.__doc__))\n        return 0\n    except (AttributeError, ValueError):\n        print('%s not found!' % what, file=sys.stderr)\n        return 1",
    "label": true
  },
  {
    "code": "def looks_like_numpy_member(member_name: str, node: NodeNG) -> bool:\n    if isinstance(node, Attribute) and node.attrname == member_name and isinstance(node.expr, Name) and _is_a_numpy_module(node.expr):\n        return True\n    if isinstance(node, Name) and node.name == member_name and node.root().name.startswith('numpy'):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def test_enummeta():\n    from http import HTTPStatus\n    import enum\n    assert dill.copy(HTTPStatus.OK) is HTTPStatus.OK\n    assert dill.copy(enum.EnumMeta) is enum.EnumMeta",
    "label": true
  },
  {
    "code": "def generic_tags(interpreter: Optional[str]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not interpreter:\n        interp_name = interpreter_name()\n        interp_version = interpreter_version(warn=warn)\n        interpreter = ''.join([interp_name, interp_version])\n    if abis is None:\n        abis = _generic_abi()\n    else:\n        abis = list(abis)\n    platforms = list(platforms or platform_tags())\n    if 'none' not in abis:\n        abis.append('none')\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)",
    "label": true
  },
  {
    "code": "def _has_ipv6(host: str) -> bool:\n    sock = None\n    has_ipv6 = False\n    if socket.has_ipv6:\n        try:\n            sock = socket.socket(socket.AF_INET6)\n            sock.bind((host, 0))\n            has_ipv6 = True\n        except Exception:\n            pass\n    if sock:\n        sock.close()\n    return has_ipv6",
    "label": true
  },
  {
    "code": "def _lookup_style(style):\n    if isinstance(style, str):\n        return get_style_by_name(style)\n    return style",
    "label": true
  },
  {
    "code": "def _get_allow_unicode_flag() -> int:\n    import doctest\n    return doctest.register_optionflag('ALLOW_UNICODE')",
    "label": true
  },
  {
    "code": "def _encode_invalid_chars(component: str | None, allowed_chars: typing.Container[str]) -> str | None:\n    if component is None:\n        return component\n    component = to_str(component)\n    component, percent_encodings = _PERCENT_RE.subn(lambda match: match.group(0).upper(), component)\n    uri_bytes = component.encode('utf-8', 'surrogatepass')\n    is_percent_encoded = percent_encodings == uri_bytes.count(b'%')\n    encoded_component = bytearray()\n    for i in range(0, len(uri_bytes)):\n        byte = uri_bytes[i:i + 1]\n        byte_ord = ord(byte)\n        if is_percent_encoded and byte == b'%' or (byte_ord < 128 and byte.decode() in allowed_chars):\n            encoded_component += byte\n            continue\n        encoded_component.extend(b'%' + hex(byte_ord)[2:].encode().zfill(2).upper())\n    return encoded_component.decode()",
    "label": true
  },
  {
    "code": "def _regexp_paths_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(re.compile(str(pathlib.PureWindowsPath(pattern)).replace('\\\\', '\\\\\\\\') + '|' + pathlib.PureWindowsPath(pattern).as_posix()))\n    return patterns",
    "label": true
  },
  {
    "code": "def _file_with_extension(directory, extension):\n    matching = (f for f in os.listdir(directory) if f.endswith(extension))\n    try:\n        file, = matching\n    except ValueError:\n        raise ValueError('No distribution was found. Ensure that `setup.py` is not empty and that it calls `setup()`.')\n    return file",
    "label": true
  },
  {
    "code": "def register_cleanup_lock_removal(lock_path: Path, register=atexit.register):\n    pid = os.getpid()\n\n    def cleanup_on_exit(lock_path: Path=lock_path, original_pid: int=pid) -> None:\n        current_pid = os.getpid()\n        if current_pid != original_pid:\n            return\n        try:\n            lock_path.unlink()\n        except OSError:\n            pass\n    return register(cleanup_on_exit)",
    "label": true
  },
  {
    "code": "def _check_typevar_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_pascal_case(name):\n        error_msgs.append(f'Type variable name \"{name}\" should be in PascalCase format. Type variable names should have the first letter of each word capitalized with no separation between each word.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def _verify_pre_check(filepath: AnyStr) -> bool:\n    try:\n        with tokenize.open(os.path.expanduser(filepath)) as f:\n            for tok_type, content, _, _, _ in tokenize.generate_tokens(f.readline):\n                if tok_type != tokenize.COMMENT:\n                    continue\n                match = OPTION_PO.search(content)\n                if match is not None:\n                    print('[ERROR] String \"pylint:\" found in comment. ' + 'No check run on file `{}.`\\n'.format(filepath))\n                    return False\n    except IndentationError as e:\n        print('[ERROR] python_ta could not check your code due to an ' + 'indentation error at line {}.'.format(e.lineno))\n        return False\n    except tokenize.TokenError as e:\n        print('[ERROR] python_ta could not check your code due to a ' + 'syntax error in your file.')\n        return False\n    except UnicodeDecodeError:\n        print('[ERROR] python_ta could not check your code due to an ' + 'invalid character. Please check the following lines in your file and all characters that are marked with a \ufffd.')\n        with open(os.path.expanduser(filepath), encoding='utf-8', errors='replace') as f:\n            for i, line in enumerate(f):\n                if '\ufffd' in line:\n                    print(f'  Line {i}: {line}', end='')\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _repr_tree_defs(data: _ImportTree, indent_str: str | None=None) -> str:\n    lines = []\n    nodes_items = data.items()\n    for i, (mod, (sub, files)) in enumerate(sorted(nodes_items, key=lambda x: x[0])):\n        files_list = '' if not files else f\"({','.join(sorted(files))})\"\n        if indent_str is None:\n            lines.append(f'{mod} {files_list}')\n            sub_indent_str = '  '\n        else:\n            lines.append(f'{indent_str}\\\\-{mod} {files_list}')\n            if i == len(nodes_items) - 1:\n                sub_indent_str = f'{indent_str}  '\n            else:\n                sub_indent_str = f'{indent_str}| '\n        if sub and isinstance(sub, dict):\n            lines.append(_repr_tree_defs(sub, sub_indent_str))\n    return '\\n'.join(lines)",
    "label": true
  },
  {
    "code": "def get_lexer_for_filename(_fn, code=None, **options):\n    res = find_lexer_class_for_filename(_fn, code)\n    if not res:\n        raise ClassNotFound('no lexer for filename %r found' % _fn)\n    return res(**options)",
    "label": true
  },
  {
    "code": "def _read_payload_from_msg(msg: Message) -> Optional[str]:\n    value = msg.get_payload().strip()\n    if value == 'UNKNOWN' or not value:\n        return None\n    return value",
    "label": true
  },
  {
    "code": "def _load_client_cert_chain(keychain, *paths):\n    certificates = []\n    identities = []\n    paths = (path for path in paths if path)\n    try:\n        for file_path in paths:\n            new_identities, new_certs = _load_items_from_file(keychain, file_path)\n            identities.extend(new_identities)\n            certificates.extend(new_certs)\n        if not identities:\n            new_identity = Security.SecIdentityRef()\n            status = Security.SecIdentityCreateWithCertificate(keychain, certificates[0], ctypes.byref(new_identity))\n            _assert_no_error(status)\n            identities.append(new_identity)\n            CoreFoundation.CFRelease(certificates.pop(0))\n        trust_chain = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        for item in itertools.chain(identities, certificates):\n            CoreFoundation.CFArrayAppendValue(trust_chain, item)\n        return trust_chain\n    finally:\n        for obj in itertools.chain(identities, certificates):\n            CoreFoundation.CFRelease(obj)",
    "label": true
  },
  {
    "code": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    tags = set()\n    interpreters, abis, platforms = tag.split('-')\n    for interpreter in interpreters.split('.'):\n        for abi in abis.split('.'):\n            for platform_ in platforms.split('.'):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)",
    "label": true
  },
  {
    "code": "def assignment(code: str, sort_type: str, extension: str, config: Config=DEFAULT_CONFIG) -> str:\n    if sort_type == 'assignments':\n        return assignments(code)\n    if sort_type not in type_mapping:\n        raise ValueError(f\"Trying to sort using an undefined sort_type. Defined sort types are {', '.join(type_mapping.keys())}.\")\n    variable_name, literal = code.split('=')\n    variable_name = variable_name.strip()\n    literal = literal.lstrip()\n    try:\n        value = ast.literal_eval(literal)\n    except Exception as error:\n        raise LiteralParsingFailure(code, error)\n    expected_type, sort_function = type_mapping[sort_type]\n    if type(value) != expected_type:\n        raise LiteralSortTypeMismatch(type(value), expected_type)\n    printer = ISortPrettyPrinter(config)\n    sorted_value_code = f'{variable_name} = {sort_function(value, printer)}'\n    if config.formatting_function:\n        sorted_value_code = config.formatting_function(sorted_value_code, extension, config).rstrip()\n    sorted_value_code += code[len(code.rstrip()):]\n    return sorted_value_code",
    "label": true
  },
  {
    "code": "def _error_line(error, obj, refimported):\n    import traceback\n    line = traceback.format_exc().splitlines()[-2].replace('[obj]', '[' + repr(obj) + ']')\n    return 'while testing (with refimported=%s):  %s' % (refimported, line.lstrip())",
    "label": true
  },
  {
    "code": "def find_path_to_project_root_from_repo_root(location: str, repo_root: str) -> Optional[str]:\n    orig_location = location\n    while not is_installable_dir(location):\n        last_location = location\n        location = os.path.dirname(location)\n        if location == last_location:\n            logger.warning('Could not find a Python project for directory %s (tried all parent directories)', orig_location)\n            return None\n    if os.path.samefile(repo_root, location):\n        return None\n    return os.path.relpath(location, repo_root)",
    "label": true
  },
  {
    "code": "def interpreter_version(*, warn: bool=False) -> str:\n    version = _get_config_var('py_version_nodot', warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
    "label": true
  },
  {
    "code": "def format_section(stream: TextIO, section: str, options: list[tuple[str, OptionDict, Any]], doc: str | None=None) -> None:\n    warnings.warn('format_section has been deprecated. It will be removed in pylint 3.0.', DeprecationWarning, stacklevel=2)\n    if doc:\n        print(_comment(doc), file=stream)\n    print(f'[{section}]', file=stream)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=DeprecationWarning)\n        _ini_format(stream, options)",
    "label": true
  },
  {
    "code": "def check_packages(dist, attr, value):\n    for pkgname in value:\n        if not re.match('\\\\w+(\\\\.\\\\w+)*', pkgname):\n            distutils.log.warn('WARNING: %r not a valid package name; please use only .-separated package names in setup.py', pkgname)",
    "label": true
  },
  {
    "code": "def _build_prompt(text: str, suffix: str, show_default: bool=False, default: t.Optional[t.Any]=None, show_choices: bool=True, type: t.Optional[ParamType]=None) -> str:\n    prompt = text\n    if type is not None and show_choices and isinstance(type, Choice):\n        prompt += f\" ({', '.join(map(str, type.choices))})\"\n    if default is not None and show_default:\n        prompt = f'{prompt} [{_format_default(default)}]'\n    return f'{prompt}{suffix}'",
    "label": true
  },
  {
    "code": "def _is_incomplete_option(ctx: Context, args: t.List[str], param: Parameter) -> bool:\n    if not isinstance(param, Option):\n        return False\n    if param.is_flag or param.count:\n        return False\n    last_option = None\n    for index, arg in enumerate(reversed(args)):\n        if index + 1 > param.nargs:\n            break\n        if _start_of_option(ctx, arg):\n            last_option = arg\n    return last_option is not None and last_option in param.opts",
    "label": true
  },
  {
    "code": "def show_vendor_versions() -> None:\n    logger.info('vendored library versions:')\n    vendor_txt_versions = create_vendor_txt_map()\n    with indent_log():\n        show_actual_vendor_versions(vendor_txt_versions)",
    "label": true
  },
  {
    "code": "def _get_pyvenv_cfg_lines() -> Optional[List[str]]:\n    pyvenv_cfg_file = os.path.join(sys.prefix, 'pyvenv.cfg')\n    try:\n        with open(pyvenv_cfg_file, encoding='utf-8') as f:\n            return f.read().splitlines()\n    except OSError:\n        return None",
    "label": true
  },
  {
    "code": "def _build_cmdtuple(path, cmdtuples):\n    for f in os.listdir(path):\n        real_f = os.path.join(path, f)\n        if os.path.isdir(real_f) and (not os.path.islink(real_f)):\n            _build_cmdtuple(real_f, cmdtuples)\n        else:\n            cmdtuples.append((os.remove, real_f))\n    cmdtuples.append((os.rmdir, path))",
    "label": true
  },
  {
    "code": "def _isrecursive(pattern):\n    if isinstance(pattern, bytes):\n        return pattern == b'**'\n    else:\n        return pattern == '**'",
    "label": true
  },
  {
    "code": "def quiet_subprocess_runner(cmd, cwd=None, extra_environ=None):\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    check_output(cmd, cwd=cwd, env=env, stderr=STDOUT)",
    "label": true
  },
  {
    "code": "def except_(*exceptions, replace=None, use=None):\n\n    def decorate(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except exceptions:\n                try:\n                    return eval(use)\n                except TypeError:\n                    return replace\n        return wrapper\n    return decorate",
    "label": true
  },
  {
    "code": "def get_module_constant(module, symbol, default=-1, paths=None):\n    try:\n        f, path, (suffix, mode, kind) = info = find_module(module, paths)\n    except ImportError:\n        return None\n    with maybe_close(f):\n        if kind == PY_COMPILED:\n            f.read(8)\n            code = marshal.load(f)\n        elif kind == PY_FROZEN:\n            code = _imp.get_frozen_object(module, paths)\n        elif kind == PY_SOURCE:\n            code = compile(f.read(), path, 'exec')\n        else:\n            imported = _imp.get_module(module, paths, info)\n            return getattr(imported, symbol, None)\n    return extract_constant(code, symbol, default)",
    "label": true
  },
  {
    "code": "def main() -> None:\n    app_name = 'MyApp'\n    app_author = 'MyCompany'\n    print(f'-- platformdirs {__version__} --')\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author, version='1.0')\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = PlatformDirs(app_name)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = PlatformDirs(app_name, appauthor=False)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')",
    "label": true
  },
  {
    "code": "def _make_text_stream(stream: t.BinaryIO, encoding: t.Optional[str], errors: t.Optional[str], force_readable: bool=False, force_writable: bool=False) -> t.TextIO:\n    if encoding is None:\n        encoding = get_best_encoding(stream)\n    if errors is None:\n        errors = 'replace'\n    return _NonClosingTextIOWrapper(stream, encoding, errors, line_buffering=True, force_readable=force_readable, force_writable=force_writable)",
    "label": true
  },
  {
    "code": "def get_package_data(name, version):\n    url = '%s/%s/package-%s.json' % (name[0].upper(), name, version)\n    url = urljoin(_external_data_base_url, url)\n    return _get_external_data(url)",
    "label": true
  },
  {
    "code": "def dump(object, **kwds):\n    import dill as pickle\n    import tempfile\n    kwds.setdefault('delete', True)\n    file = tempfile.NamedTemporaryFile(**kwds)\n    pickle.dump(object, file)\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def get_items_helper(self: BinarySearchTree) -> list[Any]:\n    if self.is_empty():\n        return []\n    else:\n        return get_items_helper(self._left) + [self._root] + get_items_helper(self._right)",
    "label": true
  },
  {
    "code": "def _get_versions(s):\n    result = []\n    for m in _VERSION_PATTERN.finditer(s):\n        result.append(NV(m.groups()[0]))\n    return set(result)",
    "label": true
  },
  {
    "code": "def _should_truncate_item(item: Item) -> bool:\n    verbose = item.config.option.verbose\n    return verbose < 2 and (not util.running_on_ci())",
    "label": true
  },
  {
    "code": "def transform_hits(hits: List[Dict[str, str]]) -> List['TransformedHit']:\n    packages: Dict[str, 'TransformedHit'] = OrderedDict()\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary']\n        version = hit['version']\n        if name not in packages.keys():\n            packages[name] = {'name': name, 'summary': summary, 'versions': [version]}\n        else:\n            packages[name]['versions'].append(version)\n            if version == highest_version(packages[name]['versions']):\n                packages[name]['summary'] = summary\n    return list(packages.values())",
    "label": true
  },
  {
    "code": "def powerset(iterable):\n    s = list(iterable)\n    return chain.from_iterable((combinations(s, r) for r in range(len(s) + 1)))",
    "label": true
  },
  {
    "code": "def join_continuation(lines):\n    lines = iter(lines)\n    for item in lines:\n        while item.endswith('\\\\'):\n            try:\n                item = item[:-2].strip() + next(lines)\n            except StopIteration:\n                return\n        yield item",
    "label": true
  },
  {
    "code": "def _load_schemes():\n    sysconfig_schemes = _load_sysconfig_schemes() or {}\n    return {scheme: {**INSTALL_SCHEMES.get(scheme, {}), **sysconfig_schemes.get(scheme, {})} for scheme in set(itertools.chain(INSTALL_SCHEMES, sysconfig_schemes))}",
    "label": true
  },
  {
    "code": "def validate_project_dynamic(pyproject: T) -> T:\n    project_table = pyproject.get('project', {})\n    dynamic = project_table.get('dynamic', [])\n    for field in dynamic:\n        if field in project_table:\n            msg = f'You cannot provide a value for `project.{field}` and '\n            msg += 'list it under `project.dynamic` at the same time'\n            name = f'data.project.{field}'\n            value = {field: project_table[field], '...': ' # ...', 'dynamic': dynamic}\n            raise RedefiningStaticFieldAsDynamic(msg, value, name, rule='PEP 621')\n    return pyproject",
    "label": true
  },
  {
    "code": "def validate(data, custom_formats={}, name_prefix=None):\n    validate_https___packaging_python_org_en_latest_specifications_declaring_build_dependencies(data, custom_formats, (name_prefix or 'data') + '')\n    return data",
    "label": true
  },
  {
    "code": "def run_pyreverse(argv: Sequence[str] | None=None) -> NoReturn:\n    from pylint.pyreverse.main import Run as PyreverseRun\n    PyreverseRun(argv or sys.argv[1:])",
    "label": true
  },
  {
    "code": "def infer_bool(node, context: InferenceContext | None=None):\n    if len(node.args) > 1:\n        raise UseInferenceDefault\n    if not node.args:\n        return nodes.Const(False)\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if isinstance(inferred, util.UninferableBase):\n        return util.Uninferable\n    bool_value = inferred.bool_value(context=context)\n    if isinstance(bool_value, util.UninferableBase):\n        return util.Uninferable\n    return nodes.Const(bool_value)",
    "label": true
  },
  {
    "code": "def function_name(func: Callable[..., Any]) -> str:\n    module = getattr(func, '__module__', '')\n    qualname = module + '.' if module not in ('builtins', '') else ''\n    return qualname + getattr(func, '__qualname__', repr(func))",
    "label": true
  },
  {
    "code": "def skip_comments_and_array_ws(src: str, pos: Pos) -> Pos:\n    while True:\n        pos_before_skip = pos\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        pos = skip_comment(src, pos)\n        if pos == pos_before_skip:\n            return pos",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def normalize_and_reduce_paths(paths):\n    reduced_paths = []\n    for p in paths:\n        np = os.path.normpath(p)\n        if np not in reduced_paths:\n            reduced_paths.append(np)\n    return reduced_paths",
    "label": true
  },
  {
    "code": "def _get_checker() -> 'doctest.OutputChecker':\n    global CHECKER_CLASS\n    if CHECKER_CLASS is None:\n        CHECKER_CLASS = _init_checker_class()\n    return CHECKER_CLASS()",
    "label": true
  },
  {
    "code": "def get_distribution(dist):\n    if isinstance(dist, str):\n        dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement):\n        dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError('Expected string, Requirement, or Distribution', dist)\n    return dist",
    "label": true
  },
  {
    "code": "def inspect(obj: Any, *, console: Optional['Console']=None, title: Optional[str]=None, help: bool=False, methods: bool=False, docs: bool=True, private: bool=False, dunder: bool=False, sort: bool=True, all: bool=False, value: bool=True) -> None:\n    _console = console or get_console()\n    from pip._vendor.rich._inspect import Inspect\n    is_inspect = obj is inspect\n    _inspect = Inspect(obj, title=title, help=is_inspect or help, methods=is_inspect or methods, docs=is_inspect or docs, private=private, dunder=dunder, sort=sort, all=all, value=value)\n    _console.print(_inspect)",
    "label": true
  },
  {
    "code": "def set_logging_handler(name: str='charset_normalizer', level: int=logging.INFO, format_string: str='%(asctime)s | %(levelname)s | %(message)s') -> None:\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(format_string))\n    logger.addHandler(handler)",
    "label": true
  },
  {
    "code": "def unpackb(packed, **kwargs):\n    unpacker = Unpacker(None, max_buffer_size=len(packed), **kwargs)\n    unpacker.feed(packed)\n    try:\n        ret = unpacker._unpack()\n    except OutOfData:\n        raise ValueError('Unpack failed: incomplete input')\n    except RecursionError as e:\n        if _is_recursionerror(e):\n            raise StackError\n        raise\n    if unpacker._got_extradata():\n        raise ExtraData(ret, unpacker._get_extradata())\n    return ret",
    "label": true
  },
  {
    "code": "def pep517_backend_reference(value: str) -> bool:\n    module, _, obj = value.partition(':')\n    identifiers = (i.strip() for i in _chain(module.split('.'), obj.split('.')))\n    return all((python_identifier(i) for i in identifiers if i))",
    "label": true
  },
  {
    "code": "def libc_ver() -> Tuple[str, str]:\n    glibc_version = glibc_version_string()\n    if glibc_version is None:\n        return ('', '')\n    else:\n        return ('glibc', glibc_version)",
    "label": true
  },
  {
    "code": "def binop_error_message(node: nodes.BinOp) -> str:\n    op_name = BINOP_TO_ENGLISH[node.op]\n    left_type = _get_name(node.left.inf_type.getValue())\n    right_type = _get_name(node.right.inf_type.getValue())\n    hint = binary_op_hints(node.op, [left_type, right_type]) or ''\n    return f'You cannot {op_name} {_correct_article(left_type)}, {node.left.as_string()}, and {_correct_article(right_type)}, {node.right.as_string()}. {hint}'",
    "label": true
  },
  {
    "code": "def isatty(stream: t.IO[t.Any]) -> bool:\n    try:\n        return stream.isatty()\n    except Exception:\n        return False",
    "label": true
  },
  {
    "code": "def _infer_old_style_string_formatting(instance: nodes.Const, other: nodes.NodeNG, context: InferenceContext) -> tuple[util.UninferableBase | nodes.Const]:\n    if isinstance(other, nodes.Tuple):\n        if util.Uninferable in other.elts:\n            return (util.Uninferable,)\n        inferred_positional = [helpers.safe_infer(i, context) for i in other.elts]\n        if all((isinstance(i, nodes.Const) for i in inferred_positional)):\n            values = tuple((i.value for i in inferred_positional))\n        else:\n            values = None\n    elif isinstance(other, nodes.Dict):\n        values: dict[Any, Any] = {}\n        for pair in other.items:\n            key = helpers.safe_infer(pair[0], context)\n            if not isinstance(key, nodes.Const):\n                return (util.Uninferable,)\n            value = helpers.safe_infer(pair[1], context)\n            if not isinstance(value, nodes.Const):\n                return (util.Uninferable,)\n            values[key.value] = value.value\n    elif isinstance(other, nodes.Const):\n        values = other.value\n    else:\n        return (util.Uninferable,)\n    try:\n        return (nodes.const_factory(instance.value % values),)\n    except (TypeError, KeyError, ValueError):\n        return (util.Uninferable,)",
    "label": true
  },
  {
    "code": "def _is_part_of_with_items(node: nodes.Call) -> bool:\n    frame = node.frame(future=True)\n    current = node\n    while current != frame:\n        if isinstance(current, nodes.With):\n            items_start = current.items[0][0].lineno\n            items_end = current.items[-1][0].tolineno\n            return items_start <= node.lineno <= items_end\n        current = current.parent\n    return False",
    "label": true
  },
  {
    "code": "def skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:\n    try:\n        while src[pos] in chars:\n            pos += 1\n    except IndexError:\n        pass\n    return pos",
    "label": true
  },
  {
    "code": "def distributions_from_metadata(path):\n    root = os.path.dirname(path)\n    if os.path.isdir(path):\n        if len(os.listdir(path)) == 0:\n            return\n        metadata = PathMetadata(root, path)\n    else:\n        metadata = FileMetadata(path)\n    entry = os.path.basename(path)\n    yield Distribution.from_location(root, entry, metadata, precedence=DEVELOP_DIST)",
    "label": true
  },
  {
    "code": "def is_line_commented(line: bytes) -> bool:\n    comment_idx = line.find(b'#')\n    if comment_idx == -1:\n        return False\n    if comment_part_of_string(line, comment_idx):\n        return is_line_commented(line[:comment_idx] + line[comment_idx + 1:])\n    return True",
    "label": true
  },
  {
    "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = ('', '')\n    return auth",
    "label": true
  },
  {
    "code": "def _compare_eq_iterable(left: Iterable[Any], right: Iterable[Any], verbose: int=0) -> List[str]:\n    if verbose <= 0 and (not running_on_ci()):\n        return ['Use -v to get more diff']\n    import difflib\n    left_formatting = pprint.pformat(left).splitlines()\n    right_formatting = pprint.pformat(right).splitlines()\n    lines_left = len(left_formatting)\n    lines_right = len(right_formatting)\n    if lines_left != lines_right:\n        left_formatting = _pformat_dispatch(left).splitlines()\n        right_formatting = _pformat_dispatch(right).splitlines()\n    if lines_left > 1 or lines_right > 1:\n        _surrounding_parens_on_own_lines(left_formatting)\n        _surrounding_parens_on_own_lines(right_formatting)\n    explanation = ['Full diff:']\n    explanation.extend((line.rstrip() for line in difflib.ndiff(right_formatting, left_formatting)))\n    return explanation",
    "label": true
  },
  {
    "code": "def register_encoder(encoder: E) -> E:\n    CUSTOM_ENCODERS.append(encoder)\n    return encoder",
    "label": true
  },
  {
    "code": "def parse_key_part(src: str, pos: Pos) -> tuple[Pos, str]:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char in BARE_KEY_CHARS:\n        start_pos = pos\n        pos = skip_chars(src, pos, BARE_KEY_CHARS)\n        return (pos, src[start_pos:pos])\n    if char == \"'\":\n        return parse_literal_str(src, pos)\n    if char == '\"':\n        return parse_one_line_basic_str(src, pos)\n    raise suffixed_err(src, pos, 'Invalid initial character for a key part')",
    "label": true
  },
  {
    "code": "def unwrap(s):\n    paragraphs = re.split('\\\\n\\\\n+', s)\n    cleaned = (para.replace('\\n', ' ') for para in paragraphs)\n    return '\\n'.join(cleaned)",
    "label": true
  },
  {
    "code": "def random_combination_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    indices = sorted((randrange(n) for i in range(r)))\n    return tuple((pool[i] for i in indices))",
    "label": true
  },
  {
    "code": "def remove_move(name):\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError('no such move, %r' % (name,))",
    "label": true
  },
  {
    "code": "def _check_multicommand(base_command: 'MultiCommand', cmd_name: str, cmd: 'Command', register: bool=False) -> None:\n    if not base_command.chain or not isinstance(cmd, MultiCommand):\n        return\n    if register:\n        hint = 'It is not possible to add multi commands as children to another multi command that is in chain mode.'\n    else:\n        hint = 'Found a multi command as subcommand to a multi command that is in chain mode. This is not supported.'\n    raise RuntimeError(f'{hint}. Command {base_command.name!r} is set to chain and {cmd_name!r} was added as a subcommand but it in itself is a multi command. ({cmd_name!r} is a {type(cmd).__name__} within a chained {type(base_command).__name__} named {base_command.name!r}).')",
    "label": true
  },
  {
    "code": "def confirmation_option(*param_decls: str, **kwargs: t.Any) -> t.Callable[[FC], FC]:\n\n    def callback(ctx: Context, param: Parameter, value: bool) -> None:\n        if not value:\n            ctx.abort()\n    if not param_decls:\n        param_decls = ('--yes',)\n    kwargs.setdefault('is_flag', True)\n    kwargs.setdefault('callback', callback)\n    kwargs.setdefault('expose_value', False)\n    kwargs.setdefault('prompt', 'Do you want to continue?')\n    kwargs.setdefault('help', 'Confirm the action without prompting.')\n    return option(*param_decls, **kwargs)",
    "label": true
  },
  {
    "code": "def progressbar(iterable: t.Optional[t.Iterable[V]]=None, length: t.Optional[int]=None, label: t.Optional[str]=None, show_eta: bool=True, show_percent: t.Optional[bool]=None, show_pos: bool=False, item_show_func: t.Optional[t.Callable[[t.Optional[V]], t.Optional[str]]]=None, fill_char: str='#', empty_char: str='-', bar_template: str='%(label)s  [%(bar)s]  %(info)s', info_sep: str='  ', width: int=36, file: t.Optional[t.TextIO]=None, color: t.Optional[bool]=None, update_min_steps: int=1) -> 'ProgressBar[V]':\n    from ._termui_impl import ProgressBar\n    color = resolve_color_default(color)\n    return ProgressBar(iterable=iterable, length=length, show_eta=show_eta, show_percent=show_percent, show_pos=show_pos, item_show_func=item_show_func, fill_char=fill_char, empty_char=empty_char, bar_template=bar_template, info_sep=info_sep, file=file, label=label, width=width, color=color, update_min_steps=update_min_steps)",
    "label": true
  },
  {
    "code": "def _http_get_download(session: PipSession, link: Link) -> Response:\n    target_url = link.url.split('#', 1)[0]\n    resp = session.get(target_url, headers=HEADERS, stream=True)\n    raise_for_status(resp)\n    return resp",
    "label": true
  },
  {
    "code": "def main() -> None:\n    app_name = 'MyApp'\n    app_author = 'MyCompany'\n    print(f'-- platformdirs {__version__} --')\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author, version='1.0')\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = PlatformDirs(app_name)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = PlatformDirs(app_name, appauthor=False)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')",
    "label": true
  },
  {
    "code": "def _cf_string_to_unicode(value: CFString) -> str | None:\n    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))\n    string = CoreFoundation.CFStringGetCStringPtr(value_as_void_p, CFConst.kCFStringEncodingUTF8)\n    if string is None:\n        buffer = ctypes.create_string_buffer(1024)\n        result = CoreFoundation.CFStringGetCString(value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8)\n        if not result:\n            raise OSError('Error copying C string from CFStringRef')\n        string = buffer.value\n    if string is not None:\n        string = string.decode('utf-8')\n    return string",
    "label": true
  },
  {
    "code": "def parse_wheel(wheel_zip: ZipFile, name: str) -> Tuple[str, Message]:\n    try:\n        info_dir = wheel_dist_info_dir(wheel_zip, name)\n        metadata = wheel_metadata(wheel_zip, info_dir)\n        version = wheel_version(metadata)\n    except UnsupportedWheel as e:\n        raise UnsupportedWheel('{} has an invalid wheel, {}'.format(name, str(e)))\n    check_compatibility(version, name)\n    return (info_dir, metadata)",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], Tuple[Node, ...], str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def mark_ends(iterable):\n    it = iter(iterable)\n    try:\n        b = next(it)\n    except StopIteration:\n        return\n    try:\n        for i in count():\n            a = b\n            b = next(it)\n            yield (i == 0, False, a)\n    except StopIteration:\n        yield (i == 0, True, a)",
    "label": true
  },
  {
    "code": "def _compare_eq_set(left: AbstractSet[Any], right: AbstractSet[Any], verbose: int=0) -> List[str]:\n    explanation = []\n    diff_left = left - right\n    diff_right = right - left\n    if diff_left:\n        explanation.append('Extra items in the left set:')\n        for item in diff_left:\n            explanation.append(saferepr(item))\n    if diff_right:\n        explanation.append('Extra items in the right set:')\n        for item in diff_right:\n            explanation.append(saferepr(item))\n    return explanation",
    "label": true
  },
  {
    "code": "def create_package_set_from_installed() -> Tuple[PackageSet, bool]:\n    package_set = {}\n    problems = False\n    env = get_default_environment()\n    for dist in env.iter_installed_distributions(local_only=False, skip=()):\n        name = dist.canonical_name\n        try:\n            dependencies = list(dist.iter_dependencies())\n            package_set[name] = PackageDetails(dist.version, dependencies)\n        except (OSError, ValueError) as e:\n            logger.warning('Error parsing requirements for %s: %s', name, e)\n            problems = True\n    return (package_set, problems)",
    "label": true
  },
  {
    "code": "def _infer_name_module(node: nodes.Import, name: str) -> Generator[InferenceResult, None, None]:\n    context = astroid.context.InferenceContext()\n    context.lookupname = name\n    return node.infer(context, asname=False)",
    "label": true
  },
  {
    "code": "def _is_a_numpy_module(node: Name) -> bool:\n    module_nickname = node.name\n    potential_import_target = [x for x in node.lookup(module_nickname)[1] if isinstance(x, Import)]\n    return any((('numpy', module_nickname) in target.names or ('numpy', None) in target.names for target in potential_import_target))",
    "label": true
  },
  {
    "code": "def _infer_prefix() -> str:\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API('prefix')\n    if _should_use_osx_framework_prefix():\n        return 'osx_framework_library'\n    implementation_suffixed = f'{sys.implementation.name}_{os.name}'\n    if implementation_suffixed in _AVAILABLE_SCHEMES:\n        return implementation_suffixed\n    if sys.implementation.name in _AVAILABLE_SCHEMES:\n        return sys.implementation.name\n    suffixed = f'{os.name}_prefix'\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if os.name in _AVAILABLE_SCHEMES:\n        return os.name\n    return 'posix_prefix'",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable: Iterable[_T], key: Optional[Callable[[_T], _U]]=None) -> Iterator[_T]:\n    seen: Set[Union[_T, _U]] = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def _cf_string_to_unicode(value):\n    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))\n    string = CoreFoundation.CFStringGetCStringPtr(value_as_void_p, CFConst.kCFStringEncodingUTF8)\n    if string is None:\n        buffer = ctypes.create_string_buffer(1024)\n        result = CoreFoundation.CFStringGetCString(value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8)\n        if not result:\n            raise OSError('Error copying C string from CFStringRef')\n        string = buffer.value\n    if string is not None:\n        string = string.decode('utf-8')\n    return string",
    "label": true
  },
  {
    "code": "def check_nsp(dist, attr, value):\n    ns_packages = value\n    assert_string_list(dist, attr, ns_packages)\n    for nsp in ns_packages:\n        if not dist.has_contents_for(nsp):\n            raise DistutilsSetupError('Distribution contains no modules or packages for ' + 'namespace package %r' % nsp)\n        parent, sep, child = nsp.rpartition('.')\n        if parent and parent not in ns_packages:\n            distutils.log.warn('WARNING: %r is declared as a package namespace, but %r is not: please correct this in setup.py', nsp, parent)\n        SetuptoolsDeprecationWarning.emit('The namespace_packages parameter is deprecated.', 'Please replace its usage with implicit namespaces (PEP 420).', see_docs='references/keywords.html#keyword-namespace-packages')",
    "label": true
  },
  {
    "code": "def scandir(path: Union[str, 'os.PathLike[str]']) -> List['os.DirEntry[str]']:\n    entries = []\n    with os.scandir(path) as s:\n        for entry in s:\n            try:\n                entry.is_file()\n            except OSError as err:\n                if _ignore_error(err):\n                    continue\n                raise\n            entries.append(entry)\n    entries.sort(key=lambda entry: entry.name)\n    return entries",
    "label": true
  },
  {
    "code": "def main(args: Optional[List[str]]=None) -> int:\n    if args is None:\n        args = sys.argv[1:]\n    warnings.filterwarnings(action='ignore', category=DeprecationWarning, module='.*pkg_resources')\n    deprecation.install_warning_logger()\n    autocomplete()\n    try:\n        cmd_name, cmd_args = parse_command(args)\n    except PipError as exc:\n        sys.stderr.write(f'ERROR: {exc}')\n        sys.stderr.write(os.linesep)\n        sys.exit(1)\n    try:\n        locale.setlocale(locale.LC_ALL, '')\n    except locale.Error as e:\n        logger.debug('Ignoring error %s when setting locale', e)\n    command = create_command(cmd_name, isolated='--isolated' in cmd_args)\n    return command.main(cmd_args)",
    "label": true
  },
  {
    "code": "def select_wait_for_socket(sock: socket.socket, read: bool=False, write: bool=False, timeout: float | None=None) -> bool:\n    if not read and (not write):\n        raise RuntimeError('must specify at least one of read=True, write=True')\n    rcheck = []\n    wcheck = []\n    if read:\n        rcheck.append(sock)\n    if write:\n        wcheck.append(sock)\n    fn = partial(select.select, rcheck, wcheck, wcheck)\n    rready, wready, xready = fn(timeout)\n    return bool(rready or wready or xready)",
    "label": true
  },
  {
    "code": "def product_star(integer_queue: Queue) -> int:\n    prod = 1\n    for i in range(len(integer_queue)):\n        item = integer_queue.dequeue()\n        prod *= item\n        integer_queue.enqueue(item)\n    return prod",
    "label": true
  },
  {
    "code": "def filter_except(validator, iterable, *exceptions):\n    for item in iterable:\n        try:\n            validator(item)\n        except exceptions:\n            pass\n        else:\n            yield item",
    "label": true
  },
  {
    "code": "def _infer_prefix() -> str:\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API('prefix')\n    if _should_use_osx_framework_prefix():\n        return 'osx_framework_library'\n    implementation_suffixed = f'{sys.implementation.name}_{os.name}'\n    if implementation_suffixed in _AVAILABLE_SCHEMES:\n        return implementation_suffixed\n    if sys.implementation.name in _AVAILABLE_SCHEMES:\n        return sys.implementation.name\n    suffixed = f'{os.name}_prefix'\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if os.name in _AVAILABLE_SCHEMES:\n        return os.name\n    return 'posix_prefix'",
    "label": true
  },
  {
    "code": "def dumpIO(object, **kwds):\n    import dill as pickle\n    from io import BytesIO as StringIO\n    file = StringIO()\n    pickle.dump(object, file)\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def extract_cookies_to_jar(jar, request, response):\n    if not (hasattr(response, '_original_response') and response._original_response):\n        return\n    req = MockRequest(request)\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)",
    "label": true
  },
  {
    "code": "def morsel_to_cookie(morsel):\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(time.strptime(morsel['expires'], time_template))\n    return create_cookie(comment=morsel['comment'], comment_url=bool(morsel['comment']), discard=False, domain=morsel['domain'], expires=expires, name=morsel.key, path=morsel['path'], port=None, rest={'HttpOnly': morsel['httponly']}, rfc2109=False, secure=bool(morsel['secure']), value=morsel.value, version=morsel['version'] or 0)",
    "label": true
  },
  {
    "code": "def get_requires_for_build_sdist(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_sdist\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)",
    "label": true
  },
  {
    "code": "def _parse(markup: str) -> Iterable[Tuple[int, Optional[str], Optional[Tag]]]:\n    position = 0\n    _divmod = divmod\n    _Tag = Tag\n    for match in RE_TAGS.finditer(markup):\n        full_text, escapes, tag_text = match.groups()\n        start, end = match.span()\n        if start > position:\n            yield (start, markup[position:start], None)\n        if escapes:\n            backslashes, escaped = _divmod(len(escapes), 2)\n            if backslashes:\n                yield (start, '\\\\' * backslashes, None)\n                start += backslashes * 2\n            if escaped:\n                yield (start, full_text[len(escapes):], None)\n                position = end\n                continue\n        text, equals, parameters = tag_text.partition('=')\n        yield (start, None, _Tag(text, parameters if equals else None))\n        position = end\n    if position < len(markup):\n        yield (position, markup[position:], None)",
    "label": true
  },
  {
    "code": "def test_dictproxy():\n    from dill._dill import DictProxyType\n    try:\n        m = DictProxyType({'foo': 'bar'})\n    except Exception:\n        m = type.__dict__\n    mp = dill.copy(m)\n    assert mp.items() == m.items()",
    "label": true
  },
  {
    "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def is_decorated_with_st_composite(node) -> bool:\n    if node.decorators and node.args.args and (node.args.args[0].name == 'draw'):\n        for decorator_attribute in node.decorators.nodes:\n            if decorator_attribute.as_string() in COMPOSITE_NAMES:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def inspect(obj: Any, *, console: Optional['Console']=None, title: Optional[str]=None, help: bool=False, methods: bool=False, docs: bool=True, private: bool=False, dunder: bool=False, sort: bool=True, all: bool=False, value: bool=True) -> None:\n    _console = console or get_console()\n    from pip._vendor.rich._inspect import Inspect\n    is_inspect = obj is inspect\n    _inspect = Inspect(obj, title=title, help=is_inspect or help, methods=is_inspect or methods, docs=is_inspect or docs, private=private, dunder=dunder, sort=sort, all=all, value=value)\n    _console.print(_inspect)",
    "label": true
  },
  {
    "code": "def compatible_platforms(provided, required):\n    if provided is None or required is None or provided == required:\n        return True\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n        if not provMac:\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = '%s.%s' % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= '10.3' or (dversion == 8 and macosversion >= '10.4'):\n                    return True\n            return False\n        if provMac.group(1) != reqMac.group(1) or provMac.group(3) != reqMac.group(3):\n            return False\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def get_cache_base(suffix=None):\n    if suffix is None:\n        suffix = '.distlib'\n    if os.name == 'nt' and 'LOCALAPPDATA' in os.environ:\n        result = os.path.expandvars('$localappdata')\n    else:\n        result = os.path.expanduser('~')\n    if os.path.isdir(result):\n        usable = os.access(result, os.W_OK)\n        if not usable:\n            logger.warning('Directory exists but is not writable: %s', result)\n    else:\n        try:\n            os.makedirs(result)\n            usable = True\n        except OSError:\n            logger.warning('Unable to create %s', result, exc_info=True)\n            usable = False\n    if not usable:\n        result = tempfile.mkdtemp()\n        logger.warning('Default location unusable, using %s', result)\n    return os.path.join(result, suffix)",
    "label": true
  },
  {
    "code": "def test_dict_contents():\n    c = type.__dict__\n    for i, j in c.items():\n        ok = dill.pickles(j)\n        if verbose:\n            print('%s: %s, %s' % (ok, type(j), j))\n        assert ok\n    if verbose:\n        print('')",
    "label": true
  },
  {
    "code": "def _ini_format(stream: TextIO, options: list[tuple[str, OptionDict, Any]]) -> None:\n    warnings.warn('_ini_format has been deprecated. It will be removed in pylint 3.0.', DeprecationWarning, stacklevel=2)\n    for optname, optdict, value in options:\n        if 'kwargs' in optdict:\n            assert isinstance(optdict['kwargs'], dict)\n            if 'new_names' in optdict['kwargs']:\n                continue\n        value = _format_option_value(optdict, value)\n        help_opt = optdict.get('help')\n        if help_opt:\n            assert isinstance(help_opt, str)\n            help_opt = normalize_text(help_opt, indent='# ')\n            print(file=stream)\n            print(help_opt, file=stream)\n        else:\n            print(file=stream)\n        if value in {'None', 'False'}:\n            print(f'#{optname}=', file=stream)\n        else:\n            value = str(value).strip()\n            if re.match('^([\\\\w-]+,)+[\\\\w-]+$', str(value)):\n                separator = '\\n ' + ' ' * len(optname)\n                value = separator.join((x + ',' for x in str(value).split(',')))\n                value = value[:-1]\n            print(f'{optname}={value}', file=stream)",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], MarkerAtom, str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def test_module_is_none():\n    assert obj.__module__ is None\n    assert dill.copy(obj)(3) == obj(3)",
    "label": true
  },
  {
    "code": "def _vertical_grid_common(need_trailing_char: bool, **interface: Any) -> str:\n    if not interface['imports']:\n        return ''\n    interface['statement'] += isort.comments.add_to_line(interface['comments'], '(', removed=interface['remove_comments'], comment_prefix=interface['comment_prefix']) + interface['line_separator'] + interface['indent'] + interface['imports'].pop(0)\n    while interface['imports']:\n        next_import = interface['imports'].pop(0)\n        next_statement = f\"{interface['statement']}, {next_import}\"\n        current_line_length = len(next_statement.split(interface['line_separator'])[-1])\n        if interface['imports'] or interface['include_trailing_comma']:\n            current_line_length += 1\n        if not interface['imports'] and need_trailing_char:\n            current_line_length += 1\n        if current_line_length > interface['line_length']:\n            next_statement = f\"{interface['statement']},{interface['line_separator']}{interface['indent']}{next_import}\"\n        interface['statement'] = next_statement\n    if interface['include_trailing_comma']:\n        interface['statement'] += ','\n    return str(interface['statement'])",
    "label": true
  },
  {
    "code": "def make_install_req_from_link(link: Link, template: InstallRequirement) -> InstallRequirement:\n    assert not template.editable, 'template is editable'\n    if template.req:\n        line = str(template.req)\n    else:\n        line = link.url\n    ireq = install_req_from_line(line, user_supplied=template.user_supplied, comes_from=template.comes_from, use_pep517=template.use_pep517, isolated=template.isolated, constraint=template.constraint, global_options=template.global_options, hash_options=template.hash_options, config_settings=template.config_settings)\n    ireq.original_link = template.original_link\n    ireq.link = link\n    ireq.extras = template.extras\n    return ireq",
    "label": true
  },
  {
    "code": "def nested_expr(opener: Union[str, ParserElement]='(', closer: Union[str, ParserElement]=')', content: typing.Optional[ParserElement]=None, ignore_expr: ParserElement=quoted_string(), *, ignoreExpr: ParserElement=quoted_string()) -> ParserElement:\n    if ignoreExpr != ignore_expr:\n        ignoreExpr = ignore_expr if ignoreExpr == quoted_string() else ignoreExpr\n    if opener == closer:\n        raise ValueError('opening and closing strings cannot be the same')\n    if content is None:\n        if isinstance(opener, str_type) and isinstance(closer, str_type):\n            opener = typing.cast(str, opener)\n            closer = typing.cast(str, closer)\n            if len(opener) == 1 and len(closer) == 1:\n                if ignoreExpr is not None:\n                    content = Combine(OneOrMore(~ignoreExpr + CharsNotIn(opener + closer + ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n                else:\n                    content = empty.copy() + CharsNotIn(opener + closer + ParserElement.DEFAULT_WHITE_CHARS).set_parse_action(lambda t: t[0].strip())\n            elif ignoreExpr is not None:\n                content = Combine(OneOrMore(~ignoreExpr + ~Literal(opener) + ~Literal(closer) + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n            else:\n                content = Combine(OneOrMore(~Literal(opener) + ~Literal(closer) + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n        else:\n            raise ValueError('opening and closing arguments must be strings if no content expression is given')\n    ret = Forward()\n    if ignoreExpr is not None:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer))\n    else:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ret | content) + Suppress(closer))\n    ret.set_name('nested %s%s expression' % (opener, closer))\n    return ret",
    "label": true
  },
  {
    "code": "def await_(obj):\n    obj_type = type(obj)\n    if obj_type is CoroutineType or (obj_type is GeneratorType and bool(obj.gi_code.co_flags & CO_ITERABLE_COROUTINE)) or isinstance(obj, Awaitable):\n        return do_await(obj).__await__()\n    else:\n        return do_yield_from(obj)",
    "label": true
  },
  {
    "code": "def to_railroad(element: pyparsing.ParserElement, diagram_kwargs: typing.Optional[dict]=None, vertical: int=3, show_results_names: bool=False, show_groups: bool=False) -> List[NamedDiagram]:\n    lookup = ConverterState(diagram_kwargs=diagram_kwargs or {})\n    _to_diagram_element(element, lookup=lookup, parent=None, vertical=vertical, show_results_names=show_results_names, show_groups=show_groups)\n    root_id = id(element)\n    if root_id in lookup:\n        if not element.customName:\n            lookup[root_id].name = ''\n        lookup[root_id].mark_for_extraction(root_id, lookup, force=True)\n    diags = list(lookup.diagrams.values())\n    if len(diags) > 1:\n        seen = set()\n        deduped_diags = []\n        for d in diags:\n            if d.name == '...':\n                continue\n            if d.name is not None and d.name not in seen:\n                seen.add(d.name)\n                deduped_diags.append(d)\n        resolved = [resolve_partial(partial) for partial in deduped_diags]\n    else:\n        resolved = [resolve_partial(partial) for partial in diags]\n    return sorted(resolved, key=lambda diag: diag.index)",
    "label": true
  },
  {
    "code": "def test_frame_related():\n    g = _g(1)\n    f = g.gi_frame\n    e, t = _f()\n    _is = lambda ok: ok\n    ok = dill.pickles(f)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(f), f))\n    assert not ok\n    ok = dill.pickles(g)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(g), g))\n    assert _is(not ok)\n    ok = dill.pickles(t)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(t), t))\n    assert not ok\n    ok = dill.pickles(e)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(e), e))\n    assert ok\n    if verbose:\n        print('')",
    "label": true
  },
  {
    "code": "def get_configuration_files() -> Dict[Kind, List[str]]:\n    global_config_files = [os.path.join(path, CONFIG_BASENAME) for path in appdirs.site_config_dirs('pip')]\n    site_config_file = os.path.join(sys.prefix, CONFIG_BASENAME)\n    legacy_config_file = os.path.join(os.path.expanduser('~'), 'pip' if WINDOWS else '.pip', CONFIG_BASENAME)\n    new_config_file = os.path.join(appdirs.user_config_dir('pip'), CONFIG_BASENAME)\n    return {kinds.GLOBAL: global_config_files, kinds.SITE: [site_config_file], kinds.USER: [legacy_config_file, new_config_file]}",
    "label": true
  },
  {
    "code": "def _version_from_file(lines):\n\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None",
    "label": true
  },
  {
    "code": "def is_comprehension(node: nodes.NodeNG) -> bool:\n    comprehensions = (nodes.ListComp, nodes.SetComp, nodes.DictComp, nodes.GeneratorExp)\n    return isinstance(node, comprehensions)",
    "label": true
  },
  {
    "code": "def ensure_relative(path):\n    drive, path = os.path.splitdrive(path)\n    if path[0:1] == os.sep:\n        path = drive + path[1:]\n    return path",
    "label": true
  },
  {
    "code": "def find_bridges_in_radius(bridges: list[list], lat: float, lon: float, radius: int, exclusions: list[int]) -> list[int]:\n    valid_bridges = []\n    for bridge in bridges:\n        if bridge[COLUMN_ID] not in exclusions:\n            bridge_lon = bridge[COLUMN_LON]\n            bridge_lat = bridge[COLUMN_LAT]\n            if calculate_distance(bridge_lat, bridge_lon, lat, lon) <= radius:\n                valid_bridges.append(bridge[COLUMN_ID])\n    return valid_bridges",
    "label": true
  },
  {
    "code": "def inherit_from_std_ex(node: nodes.NodeNG | astroid.Instance) -> bool:\n    ancestors = node.ancestors() if hasattr(node, 'ancestors') else []\n    return any((ancestor.name in {'Exception', 'BaseException'} and ancestor.root().name == EXCEPTIONS_MODULE for ancestor in itertools.chain([node], ancestors)))",
    "label": true
  },
  {
    "code": "def _apply_diagram_item_enhancements(fn):\n\n    def _inner(element: pyparsing.ParserElement, parent: typing.Optional[EditablePartial], lookup: ConverterState=None, vertical: int=None, index: int=0, name_hint: str=None, show_results_names: bool=False, show_groups: bool=False) -> typing.Optional[EditablePartial]:\n        ret = fn(element, parent, lookup, vertical, index, name_hint, show_results_names, show_groups)\n        if show_results_names and ret is not None:\n            element_results_name = element.resultsName\n            if element_results_name:\n                element_results_name += '' if element.modalResults else '*'\n                ret = EditablePartial.from_call(railroad.Group, item=ret, label=element_results_name)\n        return ret\n    return _inner",
    "label": true
  },
  {
    "code": "def _normalize_extra_values(results: Any) -> Any:\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == 'extra':\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == 'extra':\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = (lhs, op, rhs)\n    return results",
    "label": true
  },
  {
    "code": "def exists_case_sensitive(path: str) -> bool:\n    result = os.path.exists(path)\n    if (sys.platform.startswith('win') or sys.platform == 'darwin') and result:\n        directory, basename = os.path.split(path)\n        result = basename in os.listdir(directory)\n    return result",
    "label": true
  },
  {
    "code": "def get_resource_reader(package: types.ModuleType) -> Optional[ResourceReader]:\n    spec = package.__spec__\n    reader = getattr(spec.loader, 'get_resource_reader', None)\n    if reader is None:\n        return None\n    return reader(spec.name)",
    "label": true
  },
  {
    "code": "def pytest_collect_file(file_path: Path, parent: nodes.Collector) -> Optional['Module']:\n    if file_path.suffix == '.py':\n        if not parent.session.isinitpath(file_path):\n            if not path_matches_patterns(file_path, parent.config.getini('python_files') + ['__init__.py']):\n                return None\n        ihook = parent.session.gethookproxy(file_path)\n        module: Module = ihook.pytest_pycollect_makemodule(module_path=file_path, parent=parent)\n        return module\n    return None",
    "label": true
  },
  {
    "code": "def render_missing_return_type(_msg, node, source_lines=None):\n    start_line, start_col = (node.fromlineno, node.parent.col_offset)\n    end_line, end_col = (node.end_lineno, node.end_col_offset)\n    yield from render_context(start_line - 2, start_line, source_lines)\n    yield from ((line, slice(None, end_col + 1), LineType.ERROR, source_lines[line - 1]) for line in range(start_line, end_line + 1))\n    yield from render_context(end_line + 1, end_line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def add_n(obj: Union[int, list], n: int) -> Union[int, list]:\n    lst = []\n    if isinstance(obj, int):\n        return obj + n\n    else:\n        for i in range(len(obj)):\n            lst.append(add_n(obj[i], n))\n    return lst",
    "label": true
  },
  {
    "code": "def _get_encoding(encoding_or_label):\n    if hasattr(encoding_or_label, 'codec_info'):\n        return encoding_or_label\n    encoding = lookup(encoding_or_label)\n    if encoding is None:\n        raise LookupError('Unknown encoding label: %r' % encoding_or_label)\n    return encoding",
    "label": true
  },
  {
    "code": "def _module_map():\n    from collections import defaultdict\n    from types import SimpleNamespace\n    modmap = SimpleNamespace(by_name=defaultdict(list), by_id=defaultdict(list), top_level={})\n    for modname, module in sys.modules.items():\n        if modname in ('__main__', '__mp_main__') or not isinstance(module, ModuleType):\n            continue\n        if '.' not in modname:\n            modmap.top_level[id(module)] = modname\n        for objname, modobj in module.__dict__.items():\n            modmap.by_name[objname].append((modobj, modname))\n            modmap.by_id[id(modobj)].append((modobj, objname, modname))\n    return modmap",
    "label": true
  },
  {
    "code": "def _get_plugin_specs_as_list(specs: Union[None, types.ModuleType, str, Sequence[str]]) -> List[str]:\n    if specs is None:\n        return []\n    if isinstance(specs, types.ModuleType):\n        return []\n    if isinstance(specs, str):\n        return specs.split(',') if specs else []\n    if isinstance(specs, collections.abc.Sequence):\n        return list(specs)\n    raise UsageError(\"Plugins may be specified as a sequence or a ','-separated string of plugin names. Got: %r\" % specs)",
    "label": true
  },
  {
    "code": "def check_sequence(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, collections.abc.Sequence):\n        raise TypeCheckError('is not a sequence')\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for i, v in enumerate(samples):\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f'item {i}')\n                raise",
    "label": true
  },
  {
    "code": "def _linux_platforms(is_32bit: bool=_32_BIT_INTERPRETER) -> Iterator[str]:\n    linux = _normalize_string(sysconfig.get_platform())\n    if is_32bit:\n        if linux == 'linux_x86_64':\n            linux = 'linux_i686'\n        elif linux == 'linux_aarch64':\n            linux = 'linux_armv7l'\n    _, arch = linux.split('_', 1)\n    yield from _manylinux.platform_tags(linux, arch)\n    yield from _musllinux.platform_tags(arch)\n    yield linux",
    "label": true
  },
  {
    "code": "def is_node_in_type_annotation_context(node: nodes.NodeNG) -> bool:\n    current_node, parent_node = (node, node.parent)\n    while True:\n        if isinstance(parent_node, nodes.AnnAssign) and parent_node.annotation == current_node or (isinstance(parent_node, nodes.Arguments) and current_node in (*parent_node.annotations, *parent_node.posonlyargs_annotations, *parent_node.kwonlyargs_annotations, parent_node.varargannotation, parent_node.kwargannotation)) or (isinstance(parent_node, nodes.FunctionDef) and parent_node.returns == current_node):\n            return True\n        current_node, parent_node = (parent_node, parent_node.parent)\n        if isinstance(parent_node, nodes.Module):\n            return False",
    "label": true
  },
  {
    "code": "def register(linter: PyLinter) -> None:\n    linter.register_checker(TypeChecker(linter))\n    linter.register_checker(IterableChecker(linter))",
    "label": true
  },
  {
    "code": "def make_numbered_dir(root: Path, prefix: str, mode: int=448) -> Path:\n    for i in range(10):\n        max_existing = max(map(parse_num, find_suffixes(root, prefix)), default=-1)\n        new_number = max_existing + 1\n        new_path = root.joinpath(f'{prefix}{new_number}')\n        try:\n            new_path.mkdir(mode=mode)\n        except Exception:\n            pass\n        else:\n            _force_symlink(root, prefix + 'current', new_path)\n            return new_path\n    else:\n        raise OSError('could not create numbered dir with prefix {prefix} in {root} after 10 tries'.format(prefix=prefix, root=root))",
    "label": true
  },
  {
    "code": "def generate_metadata(build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str) -> str:\n    metadata_tmpdir = TempDirectory(kind='modern-metadata', globally_managed=True)\n    metadata_dir = metadata_tmpdir.path\n    with build_env:\n        runner = runner_with_spinner_message('Preparing metadata (pyproject.toml)')\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n    return os.path.join(metadata_dir, distinfo_dir)",
    "label": true
  },
  {
    "code": "def doctype_matches(text, regex):\n    m = doctype_lookup_re.search(text)\n    if m is None:\n        return False\n    doctype = m.group(1)\n    return re.compile(regex, re.I).match(doctype.strip()) is not None",
    "label": true
  },
  {
    "code": "def skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:\n    try:\n        while src[pos] in chars:\n            pos += 1\n    except IndexError:\n        pass\n    return pos",
    "label": true
  },
  {
    "code": "def builtin_words(names, backslash, suffix=NAME_END_RE):\n    prefix = '[\\\\-_^]?'\n    if backslash == 'mandatory':\n        prefix += '\\\\\\\\'\n    elif backslash == 'optional':\n        prefix += '\\\\\\\\?'\n    else:\n        assert backslash == 'disallowed'\n    return words(names, prefix, suffix)",
    "label": true
  },
  {
    "code": "def get_default_compiler(osname=None, platform=None):\n    if osname is None:\n        osname = os.name\n    if platform is None:\n        platform = sys.platform\n    for pattern, compiler in _default_compilers:\n        if re.match(pattern, platform) is not None or re.match(pattern, osname) is not None:\n            return compiler\n    return 'unix'",
    "label": true
  },
  {
    "code": "def print_yielded(func):\n    print_all = functools.partial(map, print)\n    print_results = compose(more_itertools.consume, print_all, func)\n    return functools.wraps(func)(print_results)",
    "label": true
  },
  {
    "code": "def find_on_path(importer, path_item, only=False):\n    path_item = _normalize_cached(path_item)\n    if _is_unpacked_egg(path_item):\n        yield Distribution.from_filename(path_item, metadata=PathMetadata(path_item, os.path.join(path_item, 'EGG-INFO')))\n        return\n    entries = (os.path.join(path_item, child) for child in safe_listdir(path_item))\n    for entry in sorted(entries):\n        fullpath = os.path.join(path_item, entry)\n        factory = dist_factory(path_item, entry, only)\n        for dist in factory(fullpath):\n            yield dist",
    "label": true
  },
  {
    "code": "def _looks_like_wheel(location: str) -> bool:\n    if not location.endswith(WHEEL_EXTENSION):\n        return False\n    if not os.path.isfile(location):\n        return False\n    if not Wheel.wheel_file_re.match(os.path.basename(location)):\n        return False\n    return zipfile.is_zipfile(location)",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str, *, validate: bool=False) -> NormalizedName:\n    if validate and (not _validate_regex.match(name)):\n        raise InvalidName(f'name is invalid: {name!r}')\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def _get_config_var(name: str, warn: bool=False) -> Union[int, str, None]:\n    value = sysconfig.get_config_var(name)\n    if value is None and warn:\n        logger.debug(\"Config variable '%s' is unset, Python ABI tag may be incorrect\", name)\n    return value",
    "label": true
  },
  {
    "code": "def raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:\n    msg = f'{option} error: {msg}'\n    msg = textwrap.fill(' '.join(msg.split()))\n    parser.error(msg)",
    "label": true
  },
  {
    "code": "def unpack_url(link: Link, location: str, download: Downloader, verbosity: int, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> Optional[File]:\n    if link.is_vcs:\n        unpack_vcs_link(link, location, verbosity=verbosity)\n        return None\n    assert not link.is_existing_dir()\n    if link.is_file:\n        file = get_file_url(link, download_dir, hashes=hashes)\n    else:\n        file = get_http_url(link, download, download_dir, hashes=hashes)\n    if not link.is_wheel:\n        unpack_file(file.path, location, file.content_type)\n    return file",
    "label": true
  },
  {
    "code": "def get_url_scheme(url: str) -> Optional[str]:\n    if ':' not in url:\n        return None\n    return url.split(':', 1)[0].lower()",
    "label": true
  },
  {
    "code": "def _set_module(typevarlike):\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod",
    "label": true
  },
  {
    "code": "def _legacy_key(s):\n\n    def get_parts(s):\n        result = []\n        for p in _VERSION_PART.split(s.lower()):\n            p = _VERSION_REPLACE.get(p, p)\n            if p:\n                if '0' <= p[:1] <= '9':\n                    p = p.zfill(8)\n                else:\n                    p = '*' + p\n                result.append(p)\n        result.append('*final')\n        return result\n    result = []\n    for p in get_parts(s):\n        if p.startswith('*'):\n            if p < '*final':\n                while result and result[-1] == '*final-':\n                    result.pop()\n            while result and result[-1] == '00000000':\n                result.pop()\n        result.append(p)\n    return tuple(result)",
    "label": true
  },
  {
    "code": "def get_console_script_specs(console: Dict[str, str]) -> List[str]:\n    console = console.copy()\n    scripts_to_generate = []\n    pip_script = console.pop('pip', None)\n    if pip_script:\n        if 'ENSUREPIP_OPTIONS' not in os.environ:\n            scripts_to_generate.append('pip = ' + pip_script)\n        if os.environ.get('ENSUREPIP_OPTIONS', '') != 'altinstall':\n            scripts_to_generate.append('pip{} = {}'.format(sys.version_info[0], pip_script))\n        scripts_to_generate.append(f'pip{get_major_minor_version()} = {pip_script}')\n        pip_ep = [k for k in console if re.match('pip(\\\\d+(\\\\.\\\\d+)?)?$', k)]\n        for k in pip_ep:\n            del console[k]\n    easy_install_script = console.pop('easy_install', None)\n    if easy_install_script:\n        if 'ENSUREPIP_OPTIONS' not in os.environ:\n            scripts_to_generate.append('easy_install = ' + easy_install_script)\n        scripts_to_generate.append('easy_install-{} = {}'.format(get_major_minor_version(), easy_install_script))\n        easy_install_ep = [k for k in console if re.match('easy_install(-\\\\d+\\\\.\\\\d+)?$', k)]\n        for k in easy_install_ep:\n            del console[k]\n    scripts_to_generate.extend(starmap('{} = {}'.format, console.items()))\n    return scripts_to_generate",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def __getattr__(name: str) -> Any:\n    if name == 'config':\n        from ._config import global_config\n        return global_config\n    raise AttributeError(f'module {__name__!r} has no attribute {name!r}')",
    "label": true
  },
  {
    "code": "def platform_tags(arch: str) -> Iterator[str]:\n    sys_musl = _get_musl_version(sys.executable)\n    if sys_musl is None:\n        return\n    for minor in range(sys_musl.minor, -1, -1):\n        yield f'musllinux_{sys_musl.major}_{minor}_{arch}'",
    "label": true
  },
  {
    "code": "def _create_dtypemeta(scalar_type):\n    if NumpyDType is True:\n        __hook__()\n    if scalar_type is None:\n        return NumpyDType\n    return type(NumpyDType(scalar_type))",
    "label": true
  },
  {
    "code": "def connection_requires_http_tunnel(proxy_url: Url | None=None, proxy_config: ProxyConfig | None=None, destination_scheme: str | None=None) -> bool:\n    if proxy_url is None:\n        return False\n    if destination_scheme == 'http':\n        return False\n    if proxy_url.scheme == 'https' and proxy_config and proxy_config.use_forwarding_for_https:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _get_system_sitepackages() -> Set[str]:\n    if hasattr(site, 'getsitepackages'):\n        system_sites = site.getsitepackages()\n    else:\n        system_sites = [get_purelib(), get_platlib()]\n    return {os.path.normcase(path) for path in system_sites}",
    "label": true
  },
  {
    "code": "def get_win_launcher(type):\n    launcher_fn = '%s.exe' % type\n    if is_64bit():\n        if get_platform() == 'win-arm64':\n            launcher_fn = launcher_fn.replace('.', '-arm64.')\n        else:\n            launcher_fn = launcher_fn.replace('.', '-64.')\n    else:\n        launcher_fn = launcher_fn.replace('.', '-32.')\n    return resource_string('setuptools', launcher_fn)",
    "label": true
  },
  {
    "code": "def _default_key_normalizer(key_class, request_context):\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n    return key_class(**context)",
    "label": true
  },
  {
    "code": "def load_source(file, **kwds):\n    alias = kwds.pop('alias', None)\n    mode = kwds.pop('mode', 'r')\n    fname = getattr(file, 'name', file)\n    source = open(fname, mode=mode, **kwds).read()\n    if not alias:\n        tag = source.strip().splitlines()[-1].split()\n        if tag[0] != '#NAME:':\n            stub = source.splitlines()[0]\n            raise IOError('unknown name for code: %s' % stub)\n        alias = tag[-1]\n    local = {}\n    exec(source, local)\n    _ = eval('%s' % alias, local)\n    return _",
    "label": true
  },
  {
    "code": "def assign_params(func, namespace):\n    sig = inspect.signature(func)\n    params = sig.parameters.keys()\n    call_ns = {k: namespace[k] for k in params if k in namespace}\n    return functools.partial(func, **call_ns)",
    "label": true
  },
  {
    "code": "def _complete_visible_commands(ctx: 'Context', incomplete: str) -> t.Iterator[t.Tuple[str, 'Command']]:\n    multi = t.cast(MultiCommand, ctx.command)\n    for name in multi.list_commands(ctx):\n        if name.startswith(incomplete):\n            command = multi.get_command(ctx, name)\n            if command is not None and (not command.hidden):\n                yield (name, command)",
    "label": true
  },
  {
    "code": "def create_proxy_ssl_context(ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None):\n    ssl_context = create_urllib3_context(ssl_version=resolve_ssl_version(ssl_version), cert_reqs=resolve_cert_reqs(cert_reqs))\n    if not ca_certs and (not ca_cert_dir) and (not ca_cert_data) and hasattr(ssl_context, 'load_default_certs'):\n        ssl_context.load_default_certs()\n    return ssl_context",
    "label": true
  },
  {
    "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if '=' not in item:\n            result[item] = None\n            continue\n        name, value = item.split('=', 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result",
    "label": true
  },
  {
    "code": "def first_invoke(func1, func2):\n\n    def wrapper(*args, **kwargs):\n        func1()\n        return func2(*args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def ulabel(label: Union[str, bytes, bytearray]) -> str:\n    if not isinstance(label, (bytes, bytearray)):\n        try:\n            label_bytes = label.encode('ascii')\n        except UnicodeEncodeError:\n            check_label(label)\n            return label\n    else:\n        label_bytes = label\n    label_bytes = label_bytes.lower()\n    if label_bytes.startswith(_alabel_prefix):\n        label_bytes = label_bytes[len(_alabel_prefix):]\n        if not label_bytes:\n            raise IDNAError('Malformed A-label, no Punycode eligible content found')\n        if label_bytes.decode('ascii')[-1] == '-':\n            raise IDNAError('A-label must not end with a hyphen')\n    else:\n        check_label(label_bytes)\n        return label_bytes.decode('ascii')\n    try:\n        label = label_bytes.decode('punycode')\n    except UnicodeError:\n        raise IDNAError('Invalid A-label')\n    check_label(label)\n    return label",
    "label": true
  },
  {
    "code": "def encoding_unicode_range(iana_name: str) -> List[str]:\n    if is_multi_byte_encoding(iana_name):\n        raise IOError('Function not supported on multi-byte code page')\n    decoder = importlib.import_module('encodings.{}'.format(iana_name)).IncrementalDecoder\n    p: IncrementalDecoder = decoder(errors='ignore')\n    seen_ranges: Dict[str, int] = {}\n    character_count: int = 0\n    for i in range(64, 255):\n        chunk: str = p.decode(bytes([i]))\n        if chunk:\n            character_range: Optional[str] = unicode_range(chunk)\n            if character_range is None:\n                continue\n            if is_unicode_range_secondary(character_range) is False:\n                if character_range not in seen_ranges:\n                    seen_ranges[character_range] = 0\n                seen_ranges[character_range] += 1\n                character_count += 1\n    return sorted([character_range for character_range in seen_ranges if seen_ranges[character_range] / character_count >= 0.15])",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], MarkerAtom, str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def assert_header_parsing(headers: httplib.HTTPMessage) -> None:\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError(f'expected httplib.Message, got {type(headers)}.')\n    unparsed_data = None\n    if not headers.is_multipart():\n        payload = headers.get_payload()\n        if isinstance(payload, (bytes, str)):\n            unparsed_data = payload\n    defects = [defect for defect in headers.defects if not isinstance(defect, (StartBoundaryNotFoundDefect, MultipartInvariantViolationDefect))]\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)",
    "label": true
  },
  {
    "code": "def _normalize_extra_values(results: Any) -> Any:\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == 'extra':\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == 'extra':\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = (lhs, op, rhs)\n    return results",
    "label": true
  },
  {
    "code": "def _validate_requirements(requirements: List[InstallRequirement]) -> Generator[Tuple[str, InstallRequirement], None, None]:\n    for req in requirements:\n        assert req.name, f'invalid to-be-installed requirement: {req}'\n        yield (req.name, req)",
    "label": true
  },
  {
    "code": "def get_help(parser: argparse.ArgumentParser) -> str:\n    formatter = parser._get_formatter()\n    formatter.add_usage(parser.usage, parser._actions, parser._mutually_exclusive_groups)\n    formatter.add_text(parser.description)\n    for action_group in parser._action_groups:\n        if action_group.title == 'Subcommands':\n            formatter.start_section(action_group.title)\n            formatter.add_text(action_group.description)\n            formatter.add_arguments(action_group._group_actions)\n            formatter.end_section()\n    formatter.add_text(parser.epilog)\n    return formatter.format_help()",
    "label": true
  },
  {
    "code": "def _dist_info_files(whl_zip):\n    res = []\n    for path in whl_zip.namelist():\n        m = re.match('[^/\\\\\\\\]+-[^/\\\\\\\\]+\\\\.dist-info/', path)\n        if m:\n            res.append(path)\n    if res:\n        return res\n    raise Exception('No .dist-info folder found in wheel')",
    "label": true
  },
  {
    "code": "def _search_zip(modpath: Sequence[str]) -> tuple[Literal[ModuleType.PY_ZIPMODULE], str, str]:\n    for filepath, importer in _get_zipimporters():\n        if PY310_PLUS:\n            found: Any = importer.find_spec(modpath[0])\n        else:\n            found = importer.find_module(modpath[0])\n        if found:\n            if PY310_PLUS:\n                if not importer.find_spec(os.path.sep.join(modpath)):\n                    raise ImportError('No module named %s in %s/%s' % ('.'.join(modpath[1:]), filepath, modpath))\n            elif not importer.find_module(os.path.sep.join(modpath)):\n                raise ImportError('No module named %s in %s/%s' % ('.'.join(modpath[1:]), filepath, modpath))\n            return (ModuleType.PY_ZIPMODULE, os.path.abspath(filepath) + os.path.sep + os.path.sep.join(modpath), filepath)\n    raise ImportError(f\"No module named {'.'.join(modpath)}\")",
    "label": true
  },
  {
    "code": "def infer_slice(node, context: InferenceContext | None=None):\n    args = node.args\n    if not 0 < len(args) <= 3:\n        raise UseInferenceDefault\n    infer_func = partial(helpers.safe_infer, context=context)\n    args = [infer_func(arg) for arg in args]\n    for arg in args:\n        if not arg or isinstance(arg, util.UninferableBase):\n            raise UseInferenceDefault\n        if not isinstance(arg, nodes.Const):\n            raise UseInferenceDefault\n        if not isinstance(arg.value, (type(None), int)):\n            raise UseInferenceDefault\n    if len(args) < 3:\n        args.extend([None] * (3 - len(args)))\n    slice_node = nodes.Slice(lineno=node.lineno, col_offset=node.col_offset, parent=node.parent)\n    slice_node.postinit(*args)\n    return slice_node",
    "label": true
  },
  {
    "code": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    for mark in item.iter_markers(name='skipif'):\n        if 'condition' not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs['condition'],)\n        if not conditions:\n            reason = mark.kwargs.get('reason', '')\n            return Skip(reason)\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n    for mark in item.iter_markers(name='skip'):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + ' - maybe you meant pytest.mark.skipif?') from None\n    return None",
    "label": true
  },
  {
    "code": "def in_type_checking_block(node: nodes.NodeNG) -> bool:\n    for ancestor in node.node_ancestors():\n        if not isinstance(ancestor, nodes.If):\n            continue\n        if isinstance(ancestor.test, nodes.Name):\n            if ancestor.test.name != 'TYPE_CHECKING':\n                continue\n            lookup_result = ancestor.test.lookup(ancestor.test.name)[1]\n            if not lookup_result:\n                return False\n            maybe_import_from = lookup_result[0]\n            if isinstance(maybe_import_from, nodes.ImportFrom) and maybe_import_from.modname == 'typing':\n                return True\n            inferred = safe_infer(ancestor.test)\n            if isinstance(inferred, nodes.Const) and inferred.value is False:\n                return True\n        elif isinstance(ancestor.test, nodes.Attribute):\n            if ancestor.test.attrname != 'TYPE_CHECKING':\n                continue\n            inferred_module = safe_infer(ancestor.test.expr)\n            if isinstance(inferred_module, nodes.Module) and inferred_module.name == 'typing':\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def install_wheel(name: str, wheel_path: str, scheme: Scheme, req_description: str, pycompile: bool=True, warn_script_location: bool=True, direct_url: Optional[DirectUrl]=None, requested: bool=False) -> None:\n    with ZipFile(wheel_path, allowZip64=True) as z:\n        with req_error_context(req_description):\n            _install_wheel(name=name, wheel_zip=z, wheel_path=wheel_path, scheme=scheme, pycompile=pycompile, warn_script_location=warn_script_location, direct_url=direct_url, requested=requested)",
    "label": true
  },
  {
    "code": "def cell_len(text: str, _cell_len: Callable[[str], int]=cached_cell_len) -> int:\n    if len(text) < 512:\n        return _cell_len(text)\n    _get_size = get_character_cell_size\n    total_size = sum((_get_size(character) for character in text))\n    return total_size",
    "label": true
  },
  {
    "code": "def get_related_files(tested_configuration_file: str | Path, suffix_filter: str) -> list[Path]:\n    conf_path = Path(tested_configuration_file)\n    return [p for p in conf_path.parent.iterdir() if str(p.stem).startswith(conf_path.stem) and str(p).endswith(suffix_filter)]",
    "label": true
  },
  {
    "code": "def unwrap(s):\n    paragraphs = re.split('\\\\n\\\\n+', s)\n    cleaned = (para.replace('\\n', ' ') for para in paragraphs)\n    return '\\n'.join(cleaned)",
    "label": true
  },
  {
    "code": "def show_compilers():\n    from ..ccompiler import show_compilers\n    show_compilers()",
    "label": true
  },
  {
    "code": "def _mediawiki_row_with_attrs(separator, cell_values, colwidths, colaligns):\n    alignment = {'left': '', 'right': 'align=\"right\"| ', 'center': 'align=\"center\"| ', 'decimal': 'align=\"right\"| '}\n    values_with_attrs = [' ' + alignment.get(a, '') + c + ' ' for c, a in zip(cell_values, colaligns)]\n    colsep = separator * 2\n    return (separator + colsep.join(values_with_attrs)).rstrip()",
    "label": true
  },
  {
    "code": "def _simulate_installation_of(to_install: List[InstallRequirement], package_set: PackageSet) -> Set[NormalizedName]:\n    installed = set()\n    for inst_req in to_install:\n        abstract_dist = make_distribution_for_install_requirement(inst_req)\n        dist = abstract_dist.get_metadata_distribution()\n        name = dist.canonical_name\n        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))\n        installed.add(name)\n    return installed",
    "label": true
  },
  {
    "code": "def do_int(value: t.Any, default: int=0, base: int=10) -> int:\n    try:\n        if isinstance(value, str):\n            return int(value, base)\n        return int(value)\n    except (TypeError, ValueError):\n        try:\n            return int(float(value))\n        except (TypeError, ValueError):\n            return default",
    "label": true
  },
  {
    "code": "def _cert_array_from_pem(pem_bundle):\n    pem_bundle = pem_bundle.replace(b'\\r\\n', b'\\n')\n    der_certs = [base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)]\n    if not der_certs:\n        raise ssl.SSLError('No root certificates specified')\n    cert_array = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n    if not cert_array:\n        raise ssl.SSLError('Unable to allocate memory!')\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError('Unable to allocate memory!')\n            cert = Security.SecCertificateCreateWithData(CoreFoundation.kCFAllocatorDefault, certdata)\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError('Unable to build cert object!')\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        CoreFoundation.CFRelease(cert_array)\n        raise\n    return cert_array",
    "label": true
  },
  {
    "code": "def parse_rgb_hex(hex_color: str) -> ColorTriplet:\n    assert len(hex_color) == 6, 'must be 6 characters'\n    color = ColorTriplet(int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16))\n    return color",
    "label": true
  },
  {
    "code": "def words(text: str) -> Iterable[Tuple[int, int, str]]:\n    position = 0\n    word_match = re_word.match(text, position)\n    while word_match is not None:\n        start, end = word_match.span()\n        word = word_match.group(0)\n        yield (start, end, word)\n        word_match = re_word.match(text, end)",
    "label": true
  },
  {
    "code": "def _encode_pth(content: str) -> bytes:\n    encoding = 'locale' if sys.version_info >= (3, 10) else None\n    with io.BytesIO() as buffer:\n        wrapper = io.TextIOWrapper(buffer, encoding)\n        wrapper.write(content)\n        wrapper.flush()\n        buffer.seek(0)\n        return buffer.read()",
    "label": true
  },
  {
    "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)",
    "label": true
  },
  {
    "code": "def make_proxy_method(code):\n\n    def proxy_wrapper(self, *args):\n        return code(self.__wrapped__, *args)\n    return proxy_wrapper",
    "label": true
  },
  {
    "code": "def check_none(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if value is not None:\n        raise TypeCheckError('is not None')",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e128(msg, _node, source_lines):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(0, col if col != 0 else None), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def ensure_binary(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def _add_plugins(run: Run, value: str | None) -> None:\n    assert value is not None\n    run._plugins.extend(utils._splitstrip(value))",
    "label": true
  },
  {
    "code": "def make_attrgetter(environment: 'Environment', attribute: t.Optional[t.Union[str, int]], postprocess: t.Optional[t.Callable[[t.Any], t.Any]]=None, default: t.Optional[t.Any]=None) -> t.Callable[[t.Any], t.Any]:\n    parts = _prepare_attribute_parts(attribute)\n\n    def attrgetter(item: t.Any) -> t.Any:\n        for part in parts:\n            item = environment.getitem(item, part)\n            if default is not None and isinstance(item, Undefined):\n                item = default\n        if postprocess is not None:\n            item = postprocess(item)\n        return item\n    return attrgetter",
    "label": true
  },
  {
    "code": "def modify_sys_path() -> None:\n    cwd = os.getcwd()\n    if sys.path[0] in ('', '.', cwd):\n        sys.path.pop(0)\n    env_pythonpath = os.environ.get('PYTHONPATH', '')\n    if env_pythonpath.startswith(':') and env_pythonpath not in (f':{cwd}', ':.'):\n        sys.path.pop(0)\n    elif env_pythonpath.endswith(':') and env_pythonpath not in (f'{cwd}:', '.:'):\n        sys.path.pop(1)",
    "label": true
  },
  {
    "code": "def import_type(line: str, config: Config=DEFAULT_CONFIG) -> Optional[str]:\n    if config.honor_noqa and line.lower().rstrip().endswith('noqa'):\n        return None\n    if 'isort:skip' in line or 'isort: skip' in line or 'isort: split' in line:\n        return None\n    if line.startswith(('import ', 'cimport ')):\n        return 'straight'\n    if line.startswith('from '):\n        return 'from'\n    return None",
    "label": true
  },
  {
    "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv",
    "label": true
  },
  {
    "code": "def _idna_encode(name: str) -> bytes:\n    if not name.isascii():\n        try:\n            import idna\n        except ImportError:\n            raise LocationParseError(\"Unable to parse URL without the 'idna' module\") from None\n        try:\n            return idna.encode(name.lower(), strict=True, std3_rules=True)\n        except idna.IDNAError:\n            raise LocationParseError(f\"Name '{name}' is not a valid IDNA label\") from None\n    return name.lower().encode('ascii')",
    "label": true
  },
  {
    "code": "def varnames(func):\n    func = code(func)\n    if not iscode(func):\n        return ()\n    return (func.co_varnames, func.co_cellvars)",
    "label": true
  },
  {
    "code": "def assert_header_parsing(headers):\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError('expected httplib.Message, got {0}.'.format(type(headers)))\n    defects = getattr(headers, 'defects', None)\n    get_payload = getattr(headers, 'get_payload', None)\n    unparsed_data = None\n    if get_payload:\n        if not headers.is_multipart():\n            payload = get_payload()\n            if isinstance(payload, (bytes, str)):\n                unparsed_data = payload\n    if defects:\n        defects = [defect for defect in defects if not isinstance(defect, (StartBoundaryNotFoundDefect, MultipartInvariantViolationDefect))]\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)",
    "label": true
  },
  {
    "code": "def get_filetype_from_line(l):\n    m = modeline_re.search(l)\n    if m:\n        return m.group(1)",
    "label": true
  },
  {
    "code": "def railroad_to_html(diagrams: List[NamedDiagram], embed=False, **kwargs) -> str:\n    data = []\n    for diagram in diagrams:\n        if diagram.diagram is None:\n            continue\n        io = StringIO()\n        try:\n            css = kwargs.get('css')\n            diagram.diagram.writeStandalone(io.write, css=css)\n        except AttributeError:\n            diagram.diagram.writeSvg(io.write)\n        title = diagram.name\n        if diagram.index == 0:\n            title += ' (root)'\n        data.append({'title': title, 'text': '', 'svg': io.getvalue()})\n    return template.render(diagrams=data, embed=embed, **kwargs)",
    "label": true
  },
  {
    "code": "def pick_bool(*values: Optional[bool]) -> bool:\n    assert values, '1 or more values required'\n    for value in values:\n        if value is not None:\n            return value\n    return bool(value)",
    "label": true
  },
  {
    "code": "def glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def token_map(func, *args) -> ParseAction:\n\n    def pa(s, l, t):\n        return [func(tokn, *args) for tokn in t]\n    func_name = getattr(func, '__name__', getattr(func, '__class__').__name__)\n    pa.__name__ = func_name\n    return pa",
    "label": true
  },
  {
    "code": "def norm_and_check(source_tree, requested):\n    if os.path.isabs(requested):\n        raise ValueError('paths must be relative')\n    abs_source = os.path.abspath(source_tree)\n    abs_requested = os.path.normpath(os.path.join(abs_source, requested))\n    norm_source = os.path.normcase(abs_source)\n    norm_requested = os.path.normcase(abs_requested)\n    if os.path.commonprefix([norm_source, norm_requested]) != norm_source:\n        raise ValueError('paths must be inside source tree')\n    return abs_requested",
    "label": true
  },
  {
    "code": "def valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:\n    if len(label) > (254 if trailing_dot else 253):\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def process_env_var(env_var: str) -> Variable:\n    if env_var == 'platform_python_implementation' or env_var == 'python_implementation':\n        return Variable('platform_python_implementation')\n    else:\n        return Variable(env_var)",
    "label": true
  },
  {
    "code": "def _prepare_download(resp: Response, link: Link, progress_bar: str) -> Iterable[bytes]:\n    total_length = _get_http_response_size(resp)\n    if link.netloc == PyPI.file_storage_domain:\n        url = link.show_url\n    else:\n        url = link.url_without_fragment\n    logged_url = redact_auth_from_url(url)\n    if total_length:\n        logged_url = '{} ({})'.format(logged_url, format_size(total_length))\n    if is_from_cache(resp):\n        logger.info('Using cached %s', logged_url)\n    else:\n        logger.info('Downloading %s', logged_url)\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif is_from_cache(resp):\n        show_progress = False\n    elif not total_length:\n        show_progress = True\n    elif total_length > 40 * 1000:\n        show_progress = True\n    else:\n        show_progress = False\n    chunks = response_chunks(resp, CONTENT_CHUNK_SIZE)\n    if not show_progress:\n        return chunks\n    renderer = get_download_progress_renderer(bar_type=progress_bar, size=total_length)\n    return renderer(chunks)",
    "label": true
  },
  {
    "code": "def check_package_data(dist, attr, value):\n    if not isinstance(value, dict):\n        raise DistutilsSetupError('{!r} must be a dictionary mapping package names to lists of string wildcard patterns'.format(attr))\n    for k, v in value.items():\n        if not isinstance(k, str):\n            raise DistutilsSetupError('keys of {!r} dict must be strings (got {!r})'.format(attr, k))\n        assert_string_list(dist, 'values of {!r} dict'.format(attr), v)",
    "label": true
  },
  {
    "code": "def get_list_opt(options, optname, default=None):\n    val = options.get(optname, default)\n    if isinstance(val, str):\n        return val.split()\n    elif isinstance(val, (list, tuple)):\n        return list(val)\n    else:\n        raise OptionError('Invalid type %r for option %s; you must give a list value' % (val, optname))",
    "label": true
  },
  {
    "code": "def install_req_from_line(name: str, comes_from: Optional[Union[str, InstallRequirement]]=None, *, use_pep517: Optional[bool]=None, isolated: bool=False, global_options: Optional[List[str]]=None, hash_options: Optional[Dict[str, List[str]]]=None, constraint: bool=False, line_source: Optional[str]=None, user_supplied: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    parts = parse_req_from_line(name, line_source)\n    return InstallRequirement(parts.requirement, comes_from, link=parts.link, markers=parts.markers, use_pep517=use_pep517, isolated=isolated, global_options=global_options, hash_options=hash_options, config_settings=config_settings, constraint=constraint, extras=parts.extras, user_supplied=user_supplied)",
    "label": true
  },
  {
    "code": "def triplewise(iterable):\n    for (a, _), (b, c) in pairwise(pairwise(iterable)):\n        yield (a, b, c)",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def intersperse(e, iterable, n=1):\n    if n == 0:\n        raise ValueError('n must be > 0')\n    elif n == 1:\n        return islice(interleave(repeat(e), iterable), 1, None)\n    else:\n        filler = repeat([e])\n        chunks = chunked(iterable, n)\n        return flatten(islice(interleave(filler, chunks), 1, None))",
    "label": true
  },
  {
    "code": "def getreportopt(config: Config) -> str:\n    reportchars: str = config.option.reportchars\n    old_aliases = {'F', 'S'}\n    reportopts = ''\n    for char in reportchars:\n        if char in old_aliases:\n            char = char.lower()\n        if char == 'a':\n            reportopts = 'sxXEf'\n        elif char == 'A':\n            reportopts = 'PpsxXEf'\n        elif char == 'N':\n            reportopts = ''\n        elif char not in reportopts:\n            reportopts += char\n    if not config.option.disable_warnings and 'w' not in reportopts:\n        reportopts = 'w' + reportopts\n    elif config.option.disable_warnings and 'w' in reportopts:\n        reportopts = reportopts.replace('w', '')\n    return reportopts",
    "label": true
  },
  {
    "code": "def parse_version(version):\n    x = (version & 4294901760) >> 16\n    y = (version & 65280) >> 8\n    z = version & 255\n    return (x, y, z)",
    "label": true
  },
  {
    "code": "def skip_line(line: str, in_quote: str, index: int, section_comments: Tuple[str, ...], needs_import: bool=True) -> Tuple[bool, str]:\n    should_skip = bool(in_quote)\n    if '\"' in line or \"'\" in line:\n        char_index = 0\n        while char_index < len(line):\n            if line[char_index] == '\\\\':\n                char_index += 1\n            elif in_quote:\n                if line[char_index:char_index + len(in_quote)] == in_quote:\n                    in_quote = ''\n            elif line[char_index] in (\"'\", '\"'):\n                long_quote = line[char_index:char_index + 3]\n                if long_quote in ('\"\"\"', \"'''\"):\n                    in_quote = long_quote\n                    char_index += 2\n                else:\n                    in_quote = line[char_index]\n            elif line[char_index] == '#':\n                break\n            char_index += 1\n    if ';' in line.split('#')[0] and needs_import:\n        for part in (part.strip() for part in line.split(';')):\n            if part and (not part.startswith('from ')) and (not part.startswith(('import ', 'cimport '))):\n                should_skip = True\n    return (bool(should_skip or in_quote), in_quote)",
    "label": true
  },
  {
    "code": "def _safe_segment(segment):\n    segment = re.sub('[^A-Za-z0-9.]+', '-', segment)\n    segment = re.sub('-[^A-Za-z0-9]+', '-', segment)\n    return re.sub('\\\\.[^A-Za-z0-9]+', '.', segment).strip('.-')",
    "label": true
  },
  {
    "code": "def _should_vertical(specification: int, exprs: Iterable[pyparsing.ParserElement]) -> bool:\n    if specification is None:\n        return False\n    else:\n        return len(_visible_exprs(exprs)) >= specification",
    "label": true
  },
  {
    "code": "def _find_vc2017():\n    root = os.environ.get('ProgramFiles(x86)') or os.environ.get('ProgramFiles')\n    if not root:\n        return (None, None)\n    try:\n        path = subprocess.check_output([os.path.join(root, 'Microsoft Visual Studio', 'Installer', 'vswhere.exe'), '-latest', '-prerelease', '-requires', 'Microsoft.VisualStudio.Component.VC.Tools.x86.x64', '-property', 'installationPath', '-products', '*'], encoding='mbcs', errors='strict').strip()\n    except (subprocess.CalledProcessError, OSError, UnicodeDecodeError):\n        return (None, None)\n    path = os.path.join(path, 'VC', 'Auxiliary', 'Build')\n    if os.path.isdir(path):\n        return (15, path)\n    return (None, None)",
    "label": true
  },
  {
    "code": "def _check_namedtuple_attributes(typename, attributes, rename=False):\n    attributes = tuple(attributes)\n    if rename:\n        attributes = _get_renamed_namedtuple_attributes(attributes)\n    for name in (typename,) + attributes:\n        if not isinstance(name, str):\n            raise AstroidTypeError('Type names and field names must be strings')\n        if not name.isidentifier():\n            raise AstroidValueError('Type names and field names must be valid' + f'identifiers: {name!r}')\n        if keyword.iskeyword(name):\n            raise AstroidValueError(f'Type names and field names cannot be a keyword: {name!r}')\n    seen = set()\n    for name in attributes:\n        if name.startswith('_') and (not rename):\n            raise AstroidValueError(f'Field names cannot start with an underscore: {name!r}')\n        if name in seen:\n            raise AstroidValueError(f'Encountered duplicate field name: {name!r}')\n        seen.add(name)\n    return attributes",
    "label": true
  },
  {
    "code": "def normalize_path(path: str, resolve_symlinks: bool=True) -> str:\n    path = os.path.expanduser(path)\n    if resolve_symlinks:\n        path = os.path.realpath(path)\n    else:\n        path = os.path.abspath(path)\n    return os.path.normcase(path)",
    "label": true
  },
  {
    "code": "def _validate_dependencies_met():\n    from cryptography.x509.extensions import Extensions\n    if getattr(Extensions, 'get_extension_for_class', None) is None:\n        raise ImportError(\"'cryptography' module missing required functionality.  Try upgrading to v1.3.4 or newer.\")\n    from OpenSSL.crypto import X509\n    x509 = X509()\n    if getattr(x509, '_x509', None) is None:\n        raise ImportError(\"'pyOpenSSL' module missing required functionality. Try upgrading to v0.14 or newer.\")",
    "label": true
  },
  {
    "code": "def get_windows_console_features() -> 'WindowsConsoleFeatures':\n    global _windows_console_features\n    if _windows_console_features is not None:\n        return _windows_console_features\n    from ._windows import get_windows_console_features\n    _windows_console_features = get_windows_console_features()\n    return _windows_console_features",
    "label": true
  },
  {
    "code": "def get_win_folder_if_csidl_name_not_env_var(csidl_name: str) -> str | None:\n    if csidl_name == 'CSIDL_PERSONAL':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Documents')\n    if csidl_name == 'CSIDL_DOWNLOADS':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Downloads')\n    if csidl_name == 'CSIDL_MYPICTURES':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Pictures')\n    if csidl_name == 'CSIDL_MYVIDEO':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Videos')\n    if csidl_name == 'CSIDL_MYMUSIC':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Music')\n    return None",
    "label": true
  },
  {
    "code": "def find_parent_package(packages: List[str], package_dir: Mapping[str, str], root_dir: _Path) -> Optional[str]:\n    packages = sorted(packages, key=len)\n    common_ancestors = []\n    for i, name in enumerate(packages):\n        if not all((n.startswith(f'{name}.') for n in packages[i + 1:])):\n            break\n        common_ancestors.append(name)\n    for name in common_ancestors:\n        pkg_path = find_package_path(name, package_dir, root_dir)\n        init = os.path.join(pkg_path, '__init__.py')\n        if os.path.isfile(init):\n            return name\n    return None",
    "label": true
  },
  {
    "code": "def _dump_str(v):\n    if sys.version_info < (3,) and hasattr(v, 'decode') and isinstance(v, str):\n        v = v.decode('utf-8')\n    v = '%r' % v\n    if v[0] == 'u':\n        v = v[1:]\n    singlequote = v.startswith(\"'\")\n    if singlequote or v.startswith('\"'):\n        v = v[1:-1]\n    if singlequote:\n        v = v.replace(\"\\\\'\", \"'\")\n        v = v.replace('\"', '\\\\\"')\n    v = v.split('\\\\x')\n    while len(v) > 1:\n        i = -1\n        if not v[0]:\n            v = v[1:]\n        v[0] = v[0].replace('\\\\\\\\', '\\\\')\n        joinx = v[0][i] != '\\\\'\n        while v[0][:i] and v[0][i] == '\\\\':\n            joinx = not joinx\n            i -= 1\n        if joinx:\n            joiner = 'x'\n        else:\n            joiner = 'u00'\n        v = [v[0] + joiner + v[1]] + v[2:]\n    return unicode('\"' + v[0] + '\"')",
    "label": true
  },
  {
    "code": "def is_installable_dir(path: str) -> bool:\n    if not os.path.isdir(path):\n        return False\n    if os.path.isfile(os.path.join(path, 'pyproject.toml')):\n        return True\n    if os.path.isfile(os.path.join(path, 'setup.py')):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def getsourcelines(object, lstrip=False, enclosing=False):\n    code, n = getblocks(object, lstrip=lstrip, enclosing=enclosing, locate=True)\n    return (code[-1], n[-1])",
    "label": true
  },
  {
    "code": "def infer_compression(url):\n    compression_indicator = url[-2:]\n    mapping = dict(gz='z', bz='j', xz='J')\n    return mapping.get(compression_indicator, 'z')",
    "label": true
  },
  {
    "code": "def get_iterating_dictionary_name(node: nodes.For | nodes.Comprehension) -> str | None:\n    if isinstance(node.iter, nodes.Call) and isinstance(node.iter.func, nodes.Attribute) and (node.iter.func.attrname == 'keys'):\n        inferred = safe_infer(node.iter.func)\n        if not isinstance(inferred, astroid.BoundMethod):\n            return None\n        return node.iter.as_string().rpartition('.keys')[0]\n    if isinstance(node.iter, (nodes.Name, nodes.Attribute)):\n        inferred = safe_infer(node.iter)\n        if not isinstance(inferred, nodes.Dict):\n            return None\n        return node.iter.as_string()\n    return None",
    "label": true
  },
  {
    "code": "def _mac_platforms(arch: str) -> List[str]:\n    match = _osx_arch_pat.match(arch)\n    if match:\n        name, major, minor, actual_arch = match.groups()\n        mac_version = (int(major), int(minor))\n        arches = ['{}_{}'.format(name, arch[len('macosx_'):]) for arch in mac_platforms(mac_version, actual_arch)]\n    else:\n        arches = [arch]\n    return arches",
    "label": true
  },
  {
    "code": "def load(__fp: BinaryIO, *, parse_float: ParseFloat=float) -> dict[str, Any]:\n    b = __fp.read()\n    try:\n        s = b.decode()\n    except AttributeError:\n        raise TypeError(\"File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`\") from None\n    return loads(s, parse_float=parse_float)",
    "label": true
  },
  {
    "code": "def ircformat(color, text):\n    if len(color) < 1:\n        return text\n    add = sub = ''\n    if '_' in color:\n        add += '\\x1d'\n        sub = '\\x1d' + sub\n        color = color.strip('_')\n    if '*' in color:\n        add += '\\x02'\n        sub = '\\x02' + sub\n        color = color.strip('*')\n    if len(color) > 0:\n        add += '\\x03' + str(IRC_COLOR_MAP[color]).zfill(2)\n        sub = '\\x03' + sub\n    return add + text + sub\n    return '<' + add + '>' + text + '</' + sub + '>'",
    "label": true
  },
  {
    "code": "def test_pickled_cadder():\n    pcadder = pickle.dumps(cadder)\n    pcadd5 = pickle.loads(pcadder)(x)\n    assert pcadd5(y) == x + y",
    "label": true
  },
  {
    "code": "def _is_bpo_43522_fixed(implementation_name: str, version_info: _TYPE_VERSION_INFO, pypy_version_info: _TYPE_VERSION_INFO | None) -> bool:\n    if implementation_name == 'pypy':\n        return pypy_version_info >= (7, 3, 8) and version_info >= (3, 8)\n    elif implementation_name == 'cpython':\n        major_minor = version_info[:2]\n        micro = version_info[2]\n        return major_minor == (3, 8) and micro >= 9 or (major_minor == (3, 9) and micro >= 3) or major_minor >= (3, 10)\n    else:\n        return False",
    "label": true
  },
  {
    "code": "def disable_importlib_metadata_finder(metadata):\n    try:\n        import importlib_metadata\n    except ImportError:\n        return\n    except AttributeError:\n        from .warnings import SetuptoolsWarning\n        SetuptoolsWarning.emit('Incompatibility problem.', '\\n            `importlib-metadata` version is incompatible with `setuptools`.\\n            This problem is likely to be solved by installing an updated version of\\n            `importlib-metadata`.\\n            ', see_url='https://github.com/python/importlib_metadata/issues/396')\n        raise\n    if importlib_metadata is metadata:\n        return\n    to_remove = [ob for ob in sys.meta_path if isinstance(ob, importlib_metadata.MetadataPathFinder)]\n    for item in to_remove:\n        sys.meta_path.remove(item)",
    "label": true
  },
  {
    "code": "def normalize_path(path: Any) -> str:\n    str_path = str(path)\n    parent, file_name = os.path.split(str_path)\n    if parent:\n        raise ValueError(f'{path!r} must be only a file name')\n    return file_name",
    "label": true
  },
  {
    "code": "def nth_permutation(iterable, r, index):\n    pool = list(iterable)\n    n = len(pool)\n    if r is None or r == n:\n        r, c = (n, factorial(n))\n    elif not 0 <= r < n:\n        raise ValueError\n    else:\n        c = factorial(n) // factorial(n - r)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    if c == 0:\n        return tuple()\n    result = [0] * r\n    q = index * factorial(n) // c if r < n else index\n    for d in range(1, n + 1):\n        q, i = divmod(q, d)\n        if 0 <= n - d < r:\n            result[n - d] = i\n        if q == 0:\n            break\n    return tuple(map(pool.pop, result))",
    "label": true
  },
  {
    "code": "def to_native_string(string, encoding='ascii'):\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n    return out",
    "label": true
  },
  {
    "code": "def file_ns_handler(importer, path_item, packageName, module):\n    subpath = os.path.join(path_item, packageName.split('.')[-1])\n    normalized = _normalize_cached(subpath)\n    for item in module.__path__:\n        if _normalize_cached(item) == normalized:\n            break\n    else:\n        return subpath",
    "label": true
  },
  {
    "code": "def _build_one(req: InstallRequirement, output_dir: str, verify: bool, build_options: List[str], global_options: List[str], editable: bool) -> Optional[str]:\n    artifact = 'editable' if editable else 'wheel'\n    try:\n        ensure_dir(output_dir)\n    except OSError as e:\n        logger.warning('Building %s for %s failed: %s', artifact, req.name, e)\n        return None\n    with req.build_env:\n        wheel_path = _build_one_inside_env(req, output_dir, build_options, global_options, editable)\n    if wheel_path and verify:\n        try:\n            _verify_one(req, wheel_path)\n        except (InvalidWheelFilename, UnsupportedWheel) as e:\n            logger.warning('Built %s for %s is invalid: %s', artifact, req.name, e)\n            return None\n    return wheel_path",
    "label": true
  },
  {
    "code": "def guess_decode(text):\n    try:\n        text = text.decode('utf-8')\n        return (text, 'utf-8')\n    except UnicodeDecodeError:\n        try:\n            import locale\n            prefencoding = locale.getpreferredencoding()\n            text = text.decode()\n            return (text, prefencoding)\n        except (UnicodeDecodeError, LookupError):\n            text = text.decode('latin1')\n            return (text, 'latin1')",
    "label": true
  },
  {
    "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)",
    "label": true
  },
  {
    "code": "def get_encodings_from_content(content):\n    warnings.warn('In requests 3.0, get_encodings_from_content will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)', DeprecationWarning)\n    charset_re = re.compile('<meta.*?charset=[\"\\\\\\']*(.+?)[\"\\\\\\'>]', flags=re.I)\n    pragma_re = re.compile('<meta.*?content=[\"\\\\\\']*;?charset=(.+?)[\"\\\\\\'>]', flags=re.I)\n    xml_re = re.compile('^<\\\\?xml.*?encoding=[\"\\\\\\']*(.+?)[\"\\\\\\'>]')\n    return charset_re.findall(content) + pragma_re.findall(content) + xml_re.findall(content)",
    "label": true
  },
  {
    "code": "def new_context(environment: 'Environment', template_name: t.Optional[str], blocks: t.Dict[str, t.Callable[['Context'], t.Iterator[str]]], vars: t.Optional[t.Dict[str, t.Any]]=None, shared: bool=False, globals: t.Optional[t.MutableMapping[str, t.Any]]=None, locals: t.Optional[t.Mapping[str, t.Any]]=None) -> 'Context':\n    if vars is None:\n        vars = {}\n    if shared:\n        parent = vars\n    else:\n        parent = dict(globals or (), **vars)\n    if locals:\n        if shared:\n            parent = dict(parent)\n        for key, value in locals.items():\n            if value is not missing:\n                parent[key] = value\n    return environment.context_class(environment, parent, template_name, blocks, globals=globals)",
    "label": true
  },
  {
    "code": "def _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n    try:\n        spec = Specifier(''.join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs)\n    oper: Optional[Operator] = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(f'Undefined {op!r} on {lhs!r} and {rhs!r}.')\n    return oper(lhs, rhs)",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def _needs_hiding(mod_name):\n    base_module = mod_name.split('.', 1)[0]\n    return base_module in _MODULES_TO_HIDE",
    "label": true
  },
  {
    "code": "def roundrobin(*iterables):\n    pending = len(iterables)\n    nexts = cycle((iter(it).__next__ for it in iterables))\n    while pending:\n        try:\n            for next in nexts:\n                yield next()\n        except StopIteration:\n            pending -= 1\n            nexts = cycle(islice(nexts, pending))",
    "label": true
  },
  {
    "code": "def find_plugin_styles():\n    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):\n        yield (entrypoint.name, entrypoint.load())",
    "label": true
  },
  {
    "code": "def check_modpath_has_init(path: str, mod_path: list[str]) -> bool:\n    modpath: list[str] = []\n    for part in mod_path:\n        modpath.append(part)\n        path = os.path.join(path, part)\n        if not _has_init(path):\n            old_namespace = util.is_namespace('.'.join(modpath))\n            if not old_namespace:\n                return False\n    return True",
    "label": true
  },
  {
    "code": "def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:\n    package_set, _ = create_package_set_from_installed()\n    would_be_installed = _simulate_installation_of(to_install, package_set)\n    whitelist = _create_whitelist(would_be_installed, package_set)\n    return (package_set, check_package_set(package_set, should_ignore=lambda name: name not in whitelist))",
    "label": true
  },
  {
    "code": "def patch_func(replacement, target_mod, func_name):\n    original = getattr(target_mod, func_name)\n    vars(replacement).setdefault('unpatched', original)\n    setattr(target_mod, func_name, replacement)",
    "label": true
  },
  {
    "code": "def collect_string_fields(format_string: str) -> Iterable[str | None]:\n    formatter = string.Formatter()\n    try:\n        parseiterator = formatter.parse(format_string)\n        for result in parseiterator:\n            if all((item is None for item in result[1:])):\n                continue\n            name = result[1]\n            nested = result[2]\n            yield name\n            if nested:\n                yield from collect_string_fields(nested)\n    except ValueError as exc:\n        if exc.args[0].startswith('cannot switch from manual'):\n            yield ''\n            yield '1'\n            return\n        raise IncompleteFormatString(format_string) from exc",
    "label": true
  },
  {
    "code": "def avg_length(tree: HuffmanTree, freq_dict: dict[int, int]) -> float:\n    accumulator = 0\n    total = 0\n    codes_f = get_codes(tree)\n    for symbol in freq_dict:\n        tmp = freq_dict[symbol]\n        accumulator += tmp * len(codes_f[symbol])\n        total += tmp\n    if total:\n        return accumulator / total\n    else:\n        return 0.0",
    "label": true
  },
  {
    "code": "def _cache_normalize_path(path: str) -> str:\n    if not path:\n        return _normalize_path(path)\n    return _cache_normalize_path_(path)",
    "label": true
  },
  {
    "code": "def get_annotation_label(ann: nodes.Name | nodes.NodeNG) -> str:\n    if isinstance(ann, nodes.Name) and ann.name is not None:\n        return ann.name\n    if isinstance(ann, nodes.NodeNG):\n        return ann.as_string()\n    return ''",
    "label": true
  },
  {
    "code": "def _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:\n    if not value:\n        return (None, None)\n    parts = value.split('.')\n    if len(parts) > 3:\n        return ((), 'at most three version parts are allowed')\n    if len(parts) == 1:\n        value = parts[0]\n        if len(value) > 1:\n            parts = [value[0], value[1:]]\n    try:\n        version_info = tuple((int(part) for part in parts))\n    except ValueError:\n        return ((), 'each version part must be an integer')\n    return (version_info, None)",
    "label": true
  },
  {
    "code": "def map_except(function, iterable, *exceptions):\n    for item in iterable:\n        try:\n            yield function(item)\n        except exceptions:\n            pass",
    "label": true
  },
  {
    "code": "def morsel_to_cookie(morsel):\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(time.strptime(morsel['expires'], time_template))\n    return create_cookie(comment=morsel['comment'], comment_url=bool(morsel['comment']), discard=False, domain=morsel['domain'], expires=expires, name=morsel.key, path=morsel['path'], port=None, rest={'HttpOnly': morsel['httponly']}, rfc2109=False, secure=bool(morsel['secure']), value=morsel.value, version=morsel['version'] or 0)",
    "label": true
  },
  {
    "code": "def key_value_rule(src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat) -> Pos:\n    pos, key, value = parse_key_value_pair(src, pos, parse_float)\n    key_parent, key_stem = (key[:-1], key[-1])\n    abs_key_parent = header + key_parent\n    if out.flags.is_(abs_key_parent, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Can not mutate immutable namespace {abs_key_parent}')\n    out.flags.set_for_relative_key(header, key, Flags.EXPLICIT_NEST)\n    try:\n        nest = out.data.get_or_create_nest(abs_key_parent)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Can not overwrite a value')\n    if key_stem in nest:\n        raise suffixed_err(src, pos, 'Can not overwrite a value')\n    if isinstance(value, (dict, list)):\n        out.flags.set(header + key, Flags.FROZEN, recursive=True)\n    nest[key_stem] = value\n    return pos",
    "label": true
  },
  {
    "code": "def _forgiving_version(version):\n    version = version.replace(' ', '.')\n    match = _PEP440_FALLBACK.search(version)\n    if match:\n        safe = match['safe']\n        rest = version[len(safe):]\n    else:\n        safe = '0'\n        rest = version\n    local = f'sanitized.{_safe_segment(rest)}'.strip('.')\n    return f'{safe}.dev0+{local}'",
    "label": true
  },
  {
    "code": "def _import_module(name):\n    __import__(name)\n    return sys.modules[name]",
    "label": true
  },
  {
    "code": "def _non_empty_string_transformer(value: str) -> str:\n    if not value:\n        raise argparse.ArgumentTypeError('Option cannot be an empty string.')\n    return pylint_utils._unquote(value)",
    "label": true
  },
  {
    "code": "def remove_whitespace(content: str, line_separator: str='\\n') -> str:\n    content = content.replace(line_separator, '').replace(' ', '').replace('\\x0c', '')\n    return content",
    "label": true
  },
  {
    "code": "def _dnsname_match(dn, hostname, max_wildcards=1):\n    pats = []\n    if not dn:\n        return False\n    parts = dn.split('.')\n    leftmost = parts[0]\n    remainder = parts[1:]\n    wildcards = leftmost.count('*')\n    if wildcards > max_wildcards:\n        raise CertificateError('too many wildcards in certificate DNS name: ' + repr(dn))\n    if not wildcards:\n        return dn.lower() == hostname.lower()\n    if leftmost == '*':\n        pats.append('[^.]+')\n    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n        pats.append(re.escape(leftmost))\n    else:\n        pats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n    for frag in remainder:\n        pats.append(re.escape(frag))\n    pat = re.compile('\\\\A' + '\\\\.'.join(pats) + '\\\\Z', re.IGNORECASE)\n    return pat.match(hostname)",
    "label": true
  },
  {
    "code": "def _prepend_row_index(rows, index):\n    if index is None or index is False:\n        return rows\n    if isinstance(index, Sized) and len(index) != len(rows):\n        raise ValueError('index must be as long as the number of data rows: ' + 'len(index)={} len(rows)={}'.format(len(index), len(rows)))\n    sans_rows, separating_lines = _remove_separating_lines(rows)\n    new_rows = []\n    index_iter = iter(index)\n    for row in sans_rows:\n        index_v = next(index_iter)\n        new_rows.append([index_v] + list(row))\n    rows = new_rows\n    _reinsert_separating_lines(rows, separating_lines)\n    return rows",
    "label": true
  },
  {
    "code": "def get_app_dir(app_name: str, roaming: bool=True, force_posix: bool=False) -> str:\n    if WIN:\n        key = 'APPDATA' if roaming else 'LOCALAPPDATA'\n        folder = os.environ.get(key)\n        if folder is None:\n            folder = os.path.expanduser('~')\n        return os.path.join(folder, app_name)\n    if force_posix:\n        return os.path.join(os.path.expanduser(f'~/.{_posixify(app_name)}'))\n    if sys.platform == 'darwin':\n        return os.path.join(os.path.expanduser('~/Library/Application Support'), app_name)\n    return os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')), _posixify(app_name))",
    "label": true
  },
  {
    "code": "def hash_file(path: str, blocksize: int=1 << 20) -> Tuple[Any, int]:\n    h = hashlib.sha256()\n    length = 0\n    with open(path, 'rb') as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    return (h, length)",
    "label": true
  },
  {
    "code": "def _ensure_immutable_ids(ids: Optional[Union[Sequence[Optional[object]], Callable[[Any], Optional[object]]]]) -> Optional[Union[Tuple[Optional[object], ...], Callable[[Any], Optional[object]]]]:\n    if ids is None:\n        return None\n    if callable(ids):\n        return ids\n    return tuple(ids)",
    "label": true
  },
  {
    "code": "def test_sequence(value: t.Any) -> bool:\n    try:\n        len(value)\n        value.__getitem__\n    except Exception:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def get_unicode_from_response(r):\n    warnings.warn('In requests 3.0, get_unicode_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)', DeprecationWarning)\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n    try:\n        return str(r.content, encoding, errors='replace')\n    except TypeError:\n        return r.content",
    "label": true
  },
  {
    "code": "def _collect_zipimporter_cache_entries(normalized_path, cache):\n    result = []\n    prefix_len = len(normalized_path)\n    for p in cache:\n        np = normalize_path(p)\n        if np.startswith(normalized_path) and np[prefix_len:prefix_len + 1] in (os.sep, ''):\n            result.append(p)\n    return result",
    "label": true
  },
  {
    "code": "def path_to_url(path: str) -> str:\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib.parse.urljoin('file:', urllib.request.pathname2url(path))\n    return url",
    "label": true
  },
  {
    "code": "def try_encode(string, enc):\n    try:\n        return string.encode(enc)\n    except UnicodeEncodeError:\n        return None",
    "label": true
  },
  {
    "code": "def check_invalid_constraint_type(req: InstallRequirement) -> str:\n    problem = ''\n    if not req.name:\n        problem = 'Unnamed requirements are not allowed as constraints'\n    elif req.editable:\n        problem = 'Editable requirements are not allowed as constraints'\n    elif req.extras:\n        problem = 'Constraints cannot have extras'\n    if problem:\n        deprecated(reason='Constraints are only allowed to take the form of a package name and a version specifier. Other forms were originally permitted as an accident of the implementation, but were undocumented. The new implementation of the resolver no longer supports these forms.', replacement='replacing the constraint with a requirement', gone_in=None, issue=8210)\n    return problem",
    "label": true
  },
  {
    "code": "def get_project_data(name):\n    url = '%s/%s/project.json' % (name[0].upper(), name)\n    url = urljoin(_external_data_base_url, url)\n    result = _get_external_data(url)\n    return result",
    "label": true
  },
  {
    "code": "def literal_substitute(t: type, type_map: Dict[str, type]) -> type:\n    if isinstance(t, TypeVar) and t.__name__ in type_map:\n        return type_map[t.__name__]\n    elif isinstance(t, TypeVar):\n        return TypeVar(t.__name__)\n    elif isinstance(t, ForwardRef):\n        return ForwardRef(literal_substitute(t.__forward_arg__, type_map))\n    elif isinstance(t, TuplePlus):\n        subbed_args = [literal_substitute(t1, type_map) for t1 in t.__constraints__]\n        return TuplePlus('tup+', *subbed_args)\n    elif is_callable(t):\n        args = list((literal_substitute(t1, type_map) for t1 in t.__args__[:-1]))\n        res = literal_substitute(t.__args__[-1], type_map)\n        new_t = Callable[args, res]\n        if hasattr(t, '__polymorphic_tvars__'):\n            new_t.__polymorphic_tvars__ = t.__polymorphic_tvars__.copy()\n        return new_t\n    elif isinstance(t, _GenericAlias) and t.__args__ is not None:\n        return t.copy_with(tuple((literal_substitute(t1, type_map) for t1 in t.__args__)))\n    else:\n        return t",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def dump(o, f, encoder=None):\n    if not f.write:\n        raise TypeError('You can only dump an object to a file descriptor')\n    d = dumps(o, encoder=encoder)\n    f.write(d)\n    return d",
    "label": true
  },
  {
    "code": "def format(tokens, formatter, outfile=None):\n    try:\n        if not outfile:\n            realoutfile = getattr(formatter, 'encoding', None) and BytesIO() or StringIO()\n            formatter.format(tokens, realoutfile)\n            return realoutfile.getvalue()\n        else:\n            formatter.format(tokens, outfile)\n    except TypeError:\n        from pygments.formatter import Formatter\n        if isinstance(formatter, type) and issubclass(formatter, Formatter):\n            raise TypeError('format() argument must be a formatter instance, not a class')\n        raise",
    "label": true
  },
  {
    "code": "def remove_big(s: Stack) -> None:\n    sigma_stack = Stack()\n    while not s.is_empty():\n        sigma_stack.push(s.pop())\n    while not sigma_stack.is_empty():\n        skibidi = sigma_stack.pop()\n        if skibidi > 5:\n            s.push(skibidi)",
    "label": true
  },
  {
    "code": "def cleanup_numbered_dir(root: Path, prefix: str, keep: int, consider_lock_dead_if_created_before: float) -> None:\n    if not root.exists():\n        return\n    for path in cleanup_candidates(root, prefix, keep):\n        try_cleanup(path, consider_lock_dead_if_created_before)\n    for path in root.glob('garbage-*'):\n        try_cleanup(path, consider_lock_dead_if_created_before)\n    cleanup_dead_symlinks(root)",
    "label": true
  },
  {
    "code": "def current_umask() -> int:\n    mask = os.umask(0)\n    os.umask(mask)\n    return mask",
    "label": true
  },
  {
    "code": "def create_cache(size: int) -> t.Optional[t.MutableMapping[t.Tuple[weakref.ref, str], 'Template']]:\n    if size == 0:\n        return None\n    if size < 0:\n        return {}\n    return LRUCache(size)",
    "label": true
  },
  {
    "code": "def _pythonlib_compat():\n    from distutils import sysconfig\n    if not sysconfig.get_config_var('Py_ENABLED_SHARED'):\n        return\n    yield 'python{}.{}{}'.format(sys.hexversion >> 24, sys.hexversion >> 16 & 255, sysconfig.get_config_var('ABIFLAGS'))",
    "label": true
  },
  {
    "code": "def _match_hostname(cert, asserted_hostname):\n    stripped_hostname = asserted_hostname.strip('u[]')\n    if is_ipaddress(stripped_hostname):\n        asserted_hostname = stripped_hostname\n    try:\n        match_hostname(cert, asserted_hostname)\n    except CertificateError as e:\n        log.warning('Certificate did not match expected hostname: %s. Certificate: %s', asserted_hostname, cert)\n        e._peer_cert = cert\n        raise",
    "label": true
  },
  {
    "code": "def get_similar_commands(name: str) -> Optional[str]:\n    from difflib import get_close_matches\n    name = name.lower()\n    close_commands = get_close_matches(name, commands_dict.keys())\n    if close_commands:\n        return close_commands[0]\n    else:\n        return None",
    "label": true
  },
  {
    "code": "def get_dist_name(dist: importlib.metadata.Distribution) -> str:\n    name = cast(Any, dist).name\n    if not isinstance(name, str):\n        raise BadMetadata(dist, reason=\"invalid metadata entry 'name'\")\n    return name",
    "label": true
  },
  {
    "code": "def nth_combination(iterable, r, index):\n    pool = tuple(iterable)\n    n = len(pool)\n    if r < 0 or r > n:\n        raise ValueError\n    c = 1\n    k = min(r, n - r)\n    for i in range(1, k + 1):\n        c = c * (n - k + i) // i\n    if index < 0:\n        index += c\n    if index < 0 or index >= c:\n        raise IndexError\n    result = []\n    while r:\n        c, n, r = (c * r // n, n - 1, r - 1)\n        while index >= c:\n            index -= c\n            c, n = (c * (n - r) // n, n - 1)\n        result.append(pool[-1 - n])\n    return tuple(result)",
    "label": true
  },
  {
    "code": "def dump_file(filename, head=None):\n    if head is None:\n        log.info('%s', filename)\n    else:\n        log.info(head)\n    file = open(filename)\n    try:\n        log.info(file.read())\n    finally:\n        file.close()",
    "label": true
  },
  {
    "code": "def always_reversible(iterable):\n    try:\n        return reversed(iterable)\n    except TypeError:\n        return reversed(list(iterable))",
    "label": true
  },
  {
    "code": "def run_script(dist_spec, script_name):\n    ns = sys._getframe(1).f_globals\n    name = ns['__name__']\n    ns.clear()\n    ns['__name__'] = name\n    require(dist_spec)[0].run_script(script_name, ns)",
    "label": true
  },
  {
    "code": "def stagger(iterable, offsets=(-1, 0, 1), longest=False, fillvalue=None):\n    children = tee(iterable, len(offsets))\n    return zip_offset(*children, offsets=offsets, longest=longest, fillvalue=fillvalue)",
    "label": true
  },
  {
    "code": "def make_command(*args: Union[str, HiddenText, CommandArgs]) -> CommandArgs:\n    command_args: CommandArgs = []\n    for arg in args:\n        if isinstance(arg, list):\n            command_args.extend(arg)\n        else:\n            command_args.append(arg)\n    return command_args",
    "label": true
  },
  {
    "code": "def scan_module(egg_dir, base, name, stubs):\n    filename = os.path.join(base, name)\n    if filename[:-1] in stubs:\n        return True\n    pkg = base[len(egg_dir) + 1:].replace(os.sep, '.')\n    module = pkg + (pkg and '.' or '') + os.path.splitext(name)[0]\n    if sys.version_info < (3, 7):\n        skip = 12\n    else:\n        skip = 16\n    f = open(filename, 'rb')\n    f.read(skip)\n    code = marshal.load(f)\n    f.close()\n    safe = True\n    symbols = dict.fromkeys(iter_symbols(code))\n    for bad in ['__file__', '__path__']:\n        if bad in symbols:\n            log.warn('%s: module references %s', module, bad)\n            safe = False\n    if 'inspect' in symbols:\n        for bad in ['getsource', 'getabsfile', 'getsourcefile', 'getfilegetsourcelines', 'findsource', 'getcomments', 'getframeinfo', 'getinnerframes', 'getouterframes', 'stack', 'trace']:\n            if bad in symbols:\n                log.warn('%s: module MAY be using inspect.%s', module, bad)\n                safe = False\n    return safe",
    "label": true
  },
  {
    "code": "def ratio_reduce(total: int, ratios: List[int], maximums: List[int], values: List[int]) -> List[int]:\n    ratios = [ratio if _max else 0 for ratio, _max in zip(ratios, maximums)]\n    total_ratio = sum(ratios)\n    if not total_ratio:\n        return values[:]\n    total_remaining = total\n    result: List[int] = []\n    append = result.append\n    for ratio, maximum, value in zip(ratios, maximums, values):\n        if ratio and total_ratio > 0:\n            distributed = min(maximum, round(ratio * total_remaining / total_ratio))\n            append(value - distributed)\n            total_remaining -= distributed\n            total_ratio -= ratio\n        else:\n            append(value)\n    return result",
    "label": true
  },
  {
    "code": "def validate(data: Any) -> bool:\n    with detailed_errors():\n        _validate(data, custom_formats=FORMAT_FUNCTIONS)\n    reduce(lambda acc, fn: fn(acc), EXTRA_VALIDATIONS, data)\n    return True",
    "label": true
  },
  {
    "code": "def find_eggs_in_zip(importer, path_item, only=False):\n    if importer.archive.endswith('.whl'):\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        return\n    for subitem in metadata.resource_listdir(''):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith(('.dist-info', '.egg-info')):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('terminal reporting', 'Reporting', after='general')\n    group.addoption('--durations', action='store', type=int, default=None, metavar='N', help='Show N slowest setup/test durations (N=0 for all)')\n    group.addoption('--durations-min', action='store', type=float, default=0.005, metavar='N', help='Minimal duration in seconds for inclusion in slowest list. Default: 0.005.')",
    "label": true
  },
  {
    "code": "def _encode_invalid_chars(component, allowed_chars, encoding='utf-8'):\n    if component is None:\n        return component\n    component = six.ensure_text(component)\n    component, percent_encodings = PERCENT_RE.subn(lambda match: match.group(0).upper(), component)\n    uri_bytes = component.encode('utf-8', 'surrogatepass')\n    is_percent_encoded = percent_encodings == uri_bytes.count(b'%')\n    encoded_component = bytearray()\n    for i in range(0, len(uri_bytes)):\n        byte = uri_bytes[i:i + 1]\n        byte_ord = ord(byte)\n        if is_percent_encoded and byte == b'%' or (byte_ord < 128 and byte.decode() in allowed_chars):\n            encoded_component += byte\n            continue\n        encoded_component.extend(b'%' + hex(byte_ord)[2:].encode().zfill(2).upper())\n    return encoded_component.decode(encoding)",
    "label": true
  },
  {
    "code": "def _get_option(target_obj: Target, key: str):\n    getter_name = f'get_{key}'\n    by_attribute = functools.partial(getattr, target_obj, key)\n    getter = getattr(target_obj, getter_name, by_attribute)\n    return getter()",
    "label": true
  },
  {
    "code": "def _cmpkey(epoch: int, release: Tuple[int, ...], pre: Optional[Tuple[str, int]], post: Optional[Tuple[str, int]], dev: Optional[Tuple[str, int]], local: Optional[Tuple[SubLocalType]]) -> CmpKey:\n    _release = tuple(reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release)))))\n    if pre is None and post is None and (dev is not None):\n        _pre: PrePostDevType = NegativeInfinity\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n    if post is None:\n        _post: PrePostDevType = NegativeInfinity\n    else:\n        _post = post\n    if dev is None:\n        _dev: PrePostDevType = Infinity\n    else:\n        _dev = dev\n    if local is None:\n        _local: LocalType = NegativeInfinity\n    else:\n        _local = tuple(((i, '') if isinstance(i, int) else (NegativeInfinity, i) for i in local))\n    return (epoch, _release, _pre, _post, _dev, _local)",
    "label": true
  },
  {
    "code": "def clear_caches() -> None:\n    from .environment import get_spontaneous_environment\n    from .lexer import _lexer_cache\n    get_spontaneous_environment.cache_clear()\n    _lexer_cache.clear()",
    "label": true
  },
  {
    "code": "def _check_generic(cls, parameters, elen=_marker):\n    if not elen:\n        raise TypeError(f'{cls} is not a generic class')\n    if elen is _marker:\n        if not hasattr(cls, '__parameters__') or not cls.__parameters__:\n            raise TypeError(f'{cls} is not a generic class')\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, '__parameters__'):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum((isinstance(p, TypeVarTuple) for p in parameters))\n            if num_tv_tuples > 0 and alen >= elen - num_tv_tuples:\n                return\n        raise TypeError(f\"Too {('many' if alen > elen else 'few')} parameters for {cls}; actual {alen}, expected {elen}\")",
    "label": true
  },
  {
    "code": "def rehash(path: str, blocksize: int=1 << 20) -> Tuple[str, str]:\n    h, length = hash_file(path, blocksize)\n    digest = 'sha256=' + urlsafe_b64encode(h.digest()).decode('latin1').rstrip('=')\n    return (digest, str(length))",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    xmlpath = config.option.xmlpath\n    if xmlpath and (not hasattr(config, 'workerinput')):\n        junit_family = config.getini('junit_family')\n        config.stash[xml_key] = LogXML(xmlpath, config.option.junitprefix, config.getini('junit_suite_name'), config.getini('junit_logging'), config.getini('junit_duration_report'), junit_family, config.getini('junit_log_passing_tests'))\n        config.pluginmanager.register(config.stash[xml_key])",
    "label": true
  },
  {
    "code": "def _handle_no_use_pep517(option: Option, opt: str, value: str, parser: OptionParser) -> None:\n    if value is not None:\n        msg = 'A value was passed for --no-use-pep517,\\n        probably using either the PIP_NO_USE_PEP517 environment variable\\n        or the \"no-use-pep517\" config file option. Use an appropriate value\\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\\n        config file option instead.\\n        '\n        raise_option_error(parser, option=option, msg=msg)\n    packages = ('setuptools', 'wheel')\n    if not all((importlib.util.find_spec(package) for package in packages)):\n        msg = f\"It is not possible to use --no-use-pep517 without {' and '.join(packages)} installed.\"\n        raise_option_error(parser, option=option, msg=msg)\n    parser.values.use_pep517 = False",
    "label": true
  },
  {
    "code": "def _make_index_content(response: Response, cache_link_parsing: bool=True) -> IndexContent:\n    encoding = _get_encoding_from_headers(response.headers)\n    return IndexContent(response.content, response.headers['Content-Type'], encoding=encoding, url=response.url, cache_link_parsing=cache_link_parsing)",
    "label": true
  },
  {
    "code": "def _definition_equivalent_to_call(definition: _ParameterSignature, call: _CallSignature) -> bool:\n    if definition.kwargs:\n        if definition.kwargs not in call.starred_kws:\n            return False\n    elif call.starred_kws:\n        return False\n    if definition.varargs:\n        if definition.varargs not in call.starred_args:\n            return False\n    elif call.starred_args:\n        return False\n    if any((kw not in call.kws for kw in definition.kwonlyargs)):\n        return False\n    if definition.args != call.args:\n        return False\n    return all((kw in call.args or kw in definition.kwonlyargs for kw in call.kws))",
    "label": true
  },
  {
    "code": "def add_metaclass(metaclass):\n\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper",
    "label": true
  },
  {
    "code": "def repeatfunc(func, times=None, *args):\n    if times is None:\n        return starmap(func, repeat(args))\n    return starmap(func, repeat(args, times))",
    "label": true
  },
  {
    "code": "def morsel_to_cookie(morsel):\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(time.strptime(morsel['expires'], time_template))\n    return create_cookie(comment=morsel['comment'], comment_url=bool(morsel['comment']), discard=False, domain=morsel['domain'], expires=expires, name=morsel.key, path=morsel['path'], port=None, rest={'HttpOnly': morsel['httponly']}, rfc2109=False, secure=bool(morsel['secure']), value=morsel.value, version=morsel['version'] or 0)",
    "label": true
  },
  {
    "code": "def check_callable(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not callable(value):\n        raise TypeCheckError('is not callable')\n    if args:\n        try:\n            signature = inspect.signature(value)\n        except (TypeError, ValueError):\n            return\n        argument_types = args[0]\n        if isinstance(argument_types, list) and (not any((type(item) is ParamSpec for item in argument_types))):\n            unfulfilled_kwonlyargs = [param.name for param in signature.parameters.values() if param.kind == Parameter.KEYWORD_ONLY and param.default == Parameter.empty]\n            if unfulfilled_kwonlyargs:\n                raise TypeCheckError(f\"has mandatory keyword-only arguments in its declaration: {', '.join(unfulfilled_kwonlyargs)}\")\n            num_positional_args = num_mandatory_pos_args = 0\n            has_varargs = False\n            for param in signature.parameters.values():\n                if param.kind in (Parameter.POSITIONAL_ONLY, Parameter.POSITIONAL_OR_KEYWORD):\n                    num_positional_args += 1\n                    if param.default is Parameter.empty:\n                        num_mandatory_pos_args += 1\n                elif param.kind == Parameter.VAR_POSITIONAL:\n                    has_varargs = True\n            if num_mandatory_pos_args > len(argument_types):\n                raise TypeCheckError(f'has too many mandatory positional arguments in its declaration; expected {len(argument_types)} but {num_mandatory_pos_args} mandatory positional argument(s) declared')\n            elif not has_varargs and num_positional_args < len(argument_types):\n                raise TypeCheckError(f'has too few arguments in its declaration; expected {len(argument_types)} but {num_positional_args} argument(s) declared')",
    "label": true
  },
  {
    "code": "def _make_new_npgettext(func: t.Callable[[str, str, str, int], str]) -> t.Callable[..., str]:\n\n    @pass_context\n    def npgettext(__context: Context, __string_ctx: str, __singular: str, __plural: str, __num: int, **variables: t.Any) -> str:\n        variables.setdefault('context', __string_ctx)\n        variables.setdefault('num', __num)\n        rv = __context.call(func, __string_ctx, __singular, __plural, __num)\n        if __context.eval_ctx.autoescape:\n            rv = Markup(rv)\n        return rv % variables\n    return npgettext",
    "label": true
  },
  {
    "code": "def run():\n    __builtins__\n    script_name = sys.argv[1]\n    namespace = dict(__file__=script_name, __name__='__main__', __doc__=None)\n    sys.argv[:] = sys.argv[1:]\n    open_ = getattr(tokenize, 'open', open)\n    with open_(script_name) as fid:\n        script = fid.read()\n    norm_script = script.replace('\\\\r\\\\n', '\\\\n')\n    code = compile(norm_script, script_name, 'exec')\n    exec(code, namespace)",
    "label": true
  },
  {
    "code": "def _dnsname_match(dn: typing.Any, hostname: str, max_wildcards: int=1) -> typing.Match[str] | None | bool:\n    pats = []\n    if not dn:\n        return False\n    parts = dn.split('.')\n    leftmost = parts[0]\n    remainder = parts[1:]\n    wildcards = leftmost.count('*')\n    if wildcards > max_wildcards:\n        raise CertificateError('too many wildcards in certificate DNS name: ' + repr(dn))\n    if not wildcards:\n        return bool(dn.lower() == hostname.lower())\n    if leftmost == '*':\n        pats.append('[^.]+')\n    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n        pats.append(re.escape(leftmost))\n    else:\n        pats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n    for frag in remainder:\n        pats.append(re.escape(frag))\n    pat = re.compile('\\\\A' + '\\\\.'.join(pats) + '\\\\Z', re.IGNORECASE)\n    return pat.match(hostname)",
    "label": true
  },
  {
    "code": "def build_wheel_legacy(name: str, setup_py_path: str, source_dir: str, global_options: List[str], build_options: List[str], tempd: str) -> Optional[str]:\n    wheel_args = make_setuptools_bdist_wheel_args(setup_py_path, global_options=global_options, build_options=build_options, destination_dir=tempd)\n    spin_message = f'Building wheel for {name} (setup.py)'\n    with open_spinner(spin_message) as spinner:\n        logger.debug('Destination directory: %s', tempd)\n        try:\n            output = call_subprocess(wheel_args, command_desc='python setup.py bdist_wheel', cwd=source_dir, spinner=spinner)\n        except Exception:\n            spinner.finish('error')\n            logger.error('Failed building wheel for %s', name)\n            return None\n        names = os.listdir(tempd)\n        wheel_path = get_legacy_build_wheel_path(names=names, temp_dir=tempd, name=name, command_args=wheel_args, command_output=output)\n        return wheel_path",
    "label": true
  },
  {
    "code": "def patch_error_messages(patch_data: dict):\n    for file_name, file_data in patch_data.items():\n        for checker_name, checker_data in file_data.items():\n            checker = getattr(import_module(file_name), checker_name)\n            if hasattr(checker, 'msgs'):\n                for error_id, new_msg in checker_data.items():\n                    lst_msg = list(checker.msgs[error_id])\n                    lst_msg[0] = new_msg\n                    checker.msgs[error_id] = tuple(lst_msg)\n            else:\n                print('no msgs attribute!')",
    "label": true
  },
  {
    "code": "def _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version",
    "label": true
  },
  {
    "code": "def _infer_str_format_call(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[nodes.Const | util.UninferableBase]:\n    call = arguments.CallSite.from_call(node, context=context)\n    if isinstance(node.func.expr, nodes.Name):\n        value: nodes.Const | None = helpers.safe_infer(node.func.expr)\n        if value is None:\n            return iter([util.Uninferable])\n    else:\n        value = node.func.expr\n    format_template = value.value\n    inferred_positional = [helpers.safe_infer(i, context) for i in call.positional_arguments]\n    if not all((isinstance(i, nodes.Const) for i in inferred_positional)):\n        return iter([util.Uninferable])\n    pos_values: list[str] = [i.value for i in inferred_positional]\n    inferred_keyword = {k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()}\n    if not all((isinstance(i, nodes.Const) for i in inferred_keyword.values())):\n        return iter([util.Uninferable])\n    keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}\n    try:\n        formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n        return iter([util.Uninferable])\n    return iter([nodes.const_factory(formatted_string)])",
    "label": true
  },
  {
    "code": "def canonicalize_version(version: Union[Version, str], *, strip_trailing_zero: bool=True) -> str:\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            return version\n    else:\n        parsed = version\n    parts = []\n    if parsed.epoch != 0:\n        parts.append(f'{parsed.epoch}!')\n    release_segment = '.'.join((str(x) for x in parsed.release))\n    if strip_trailing_zero:\n        release_segment = re.sub('(\\\\.0)+$', '', release_segment)\n    parts.append(release_segment)\n    if parsed.pre is not None:\n        parts.append(''.join((str(x) for x in parsed.pre)))\n    if parsed.post is not None:\n        parts.append(f'.post{parsed.post}')\n    if parsed.dev is not None:\n        parts.append(f'.dev{parsed.dev}')\n    if parsed.local is not None:\n        parts.append(f'+{parsed.local}')\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def pretty_repr(_object: Any, *, max_width: int=80, indent_size: int=4, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> str:\n    if _safe_isinstance(_object, Node):\n        node = _object\n    else:\n        node = traverse(_object, max_length=max_length, max_string=max_string, max_depth=max_depth)\n    repr_str: str = node.render(max_width=max_width, indent_size=indent_size, expand_all=expand_all)\n    return repr_str",
    "label": true
  },
  {
    "code": "def _check_method_and_attr_name(node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not (_is_in_snake_case(name) or (name.startswith('__') and _is_in_snake_case(name[2:]))):\n        error_msgs.append(f'''{node_type.capitalize()} name \"{name}\" should be in snake_case format. {node_type.capitalize()} names should be lowercase, with words separated by underscores. A single leading underscore can be used to denote a private {node_type} while a double leading underscore invokes Python's name-mangling rules.''')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def _clear_cached_macosx_ver():\n    global _syscfg_macosx_ver\n    _syscfg_macosx_ver = None",
    "label": true
  },
  {
    "code": "def _is_linux_armhf(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.Arm) and (f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5) and (f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD)",
    "label": true
  },
  {
    "code": "def process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))",
    "label": true
  },
  {
    "code": "def _override_config(linter: PyLinter, config_location: AnyStr) -> None:\n    linter.set_current_module(config_location)\n    config_file_parser = _ConfigurationFileParser(verbose=True, linter=linter)\n    try:\n        _, config_args = config_file_parser.parse_config_file(file_path=config_location)\n    except OSError as ex:\n        print(ex, file=sys.stderr)\n        sys.exit(32)\n    try:\n        linter._parse_configuration_file(config_args)\n    except _UnrecognizedOptionError as exc:\n        unrecognized_options_message = ', '.join(exc.options)\n        linter.add_message('unrecognized-option', args=unrecognized_options_message, line=0)\n    linter._emit_stashed_messages()\n    linter.config_file = config_location",
    "label": true
  },
  {
    "code": "def extract_constant(code, symbol, default=-1):\n    if symbol not in code.co_names:\n        return None\n    name_idx = list(code.co_names).index(symbol)\n    STORE_NAME = 90\n    STORE_GLOBAL = 97\n    LOAD_CONST = 100\n    const = default\n    for byte_code in dis.Bytecode(code):\n        op = byte_code.opcode\n        arg = byte_code.arg\n        if op == LOAD_CONST:\n            const = code.co_consts[arg]\n        elif arg == name_idx and (op == STORE_NAME or op == STORE_GLOBAL):\n            return const\n        else:\n            const = default",
    "label": true
  },
  {
    "code": "def get_scope_node(node: nodes.Node, scope: Scope) -> Optional[Union[nodes.Item, nodes.Collector]]:\n    import _pytest.python\n    if scope is Scope.Function:\n        return node.getparent(nodes.Item)\n    elif scope is Scope.Class:\n        return node.getparent(_pytest.python.Class)\n    elif scope is Scope.Module:\n        return node.getparent(_pytest.python.Module)\n    elif scope is Scope.Package:\n        return node.getparent(_pytest.python.Package)\n    elif scope is Scope.Session:\n        return node.getparent(_pytest.main.Session)\n    else:\n        assert_never(scope)",
    "label": true
  },
  {
    "code": "def non_empty_lines(path):\n    with open(path) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line",
    "label": true
  },
  {
    "code": "def _python_build():\n    if _sys_home:\n        return _is_python_source_dir(_sys_home)\n    return _is_python_source_dir(project_base)",
    "label": true
  },
  {
    "code": "def _debug(msg: str) -> None:\n    if not DEBUG_CONTRACTS:\n        return\n    print('[PyTA]', msg, file=sys.stderr)",
    "label": true
  },
  {
    "code": "def _get_pdata_path(base_name: Path, recurs: int, pylint_home: Path=PYLINT_HOME_AS_PATH) -> Path:\n    underscored_name = '_'.join((str(p.replace(':', '_').replace('/', '_').replace('\\\\', '_')) for p in base_name.parts))\n    return pylint_home / f'{underscored_name}_{recurs}.stats'",
    "label": true
  },
  {
    "code": "def split_after(iterable, pred, maxsplit=-1):\n    if maxsplit == 0:\n        yield list(iterable)\n        return\n    buf = []\n    it = iter(iterable)\n    for item in it:\n        buf.append(item)\n        if pred(item) and buf:\n            yield buf\n            if maxsplit == 1:\n                buf = list(it)\n                if buf:\n                    yield buf\n                return\n            buf = []\n            maxsplit -= 1\n    if buf:\n        yield buf",
    "label": true
  },
  {
    "code": "def _py_version_validator(_: Any, name: str, value: Any) -> tuple[int, int, int]:\n    if not isinstance(value, tuple):\n        try:\n            value = tuple((int(val) for val in value.split('.')))\n        except (ValueError, AttributeError):\n            raise optparse.OptionValueError(f\"Invalid format for {name}, should be version string. E.g., '3.8'\") from None\n    return value",
    "label": true
  },
  {
    "code": "def _iter_lexerclasses(plugins=True):\n    for key in sorted(LEXERS):\n        module_name, name = LEXERS[key][:2]\n        if name not in _lexer_cache:\n            _load_lexers(module_name)\n        yield _lexer_cache[name]\n    if plugins:\n        yield from find_plugin_lexers()",
    "label": true
  },
  {
    "code": "def _base_class_object_build(node: nodes.Module | nodes.ClassDef, member: type, basenames: list[str], name: str | None=None, localname: str | None=None) -> nodes.ClassDef:\n    class_name = name or getattr(member, '__name__', None) or localname\n    assert isinstance(class_name, str)\n    klass = build_class(class_name, basenames, member.__doc__)\n    klass._newstyle = isinstance(member, type)\n    node.add_local_node(klass, localname)\n    try:\n        if issubclass(member, Exception):\n            instdict = member().__dict__\n        else:\n            raise TypeError\n    except TypeError:\n        pass\n    else:\n        for item_name, obj in instdict.items():\n            valnode = nodes.EmptyNode()\n            valnode.object = obj\n            valnode.parent = klass\n            valnode.lineno = 1\n            klass.instance_attrs[item_name] = [valnode]\n    return klass",
    "label": true
  },
  {
    "code": "def as_base_candidate(candidate: Candidate) -> Optional[BaseCandidate]:\n    base_candidate_classes = (AlreadyInstalledCandidate, EditableCandidate, LinkCandidate)\n    if isinstance(candidate, base_candidate_classes):\n        return candidate\n    return None",
    "label": true
  },
  {
    "code": "def wrap_file(file: BinaryIO, total: int, *, description: str='Reading...', auto_refresh: bool=True, console: Optional[Console]=None, transient: bool=False, get_time: Optional[Callable[[], float]]=None, refresh_per_second: float=10, style: StyleType='bar.back', complete_style: StyleType='bar.complete', finished_style: StyleType='bar.finished', pulse_style: StyleType='bar.pulse', disable: bool=False) -> ContextManager[BinaryIO]:\n    columns: List['ProgressColumn'] = [TextColumn('[progress.description]{task.description}')] if description else []\n    columns.extend((BarColumn(style=style, complete_style=complete_style, finished_style=finished_style, pulse_style=pulse_style), DownloadColumn(), TimeRemainingColumn()))\n    progress = Progress(*columns, auto_refresh=auto_refresh, console=console, transient=transient, get_time=get_time, refresh_per_second=refresh_per_second or 10, disable=disable)\n    reader = progress.wrap_file(file, total=total, description=description)\n    return _ReadContext(progress, reader)",
    "label": true
  },
  {
    "code": "def maybe_delete_a_numbered_dir(path: Path) -> None:\n    path = ensure_extended_length_path(path)\n    lock_path = None\n    try:\n        lock_path = create_cleanup_lock(path)\n        parent = path.parent\n        garbage = parent.joinpath(f'garbage-{uuid.uuid4()}')\n        path.rename(garbage)\n        rm_rf(garbage)\n    except OSError:\n        return\n    finally:\n        if lock_path is not None:\n            try:\n                lock_path.unlink()\n            except OSError:\n                pass",
    "label": true
  },
  {
    "code": "def _selection_sort_by_len(lst: list[str]) -> None:\n    for i in range(len(lst)):\n        mini = i\n        for j in range(i, len(lst)):\n            if len(lst[j]) < len(lst[mini]):\n                mini = j\n        lst[i], lst[mini] = (lst[mini], lst[i])",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def const_factory(value: Any) -> ConstFactoryResult:\n    assert not isinstance(value, NodeNG)\n    if value.__class__ not in CONST_CLS:\n        node = EmptyNode()\n        node.object = value\n        return node\n    instance: List | Set | Tuple | Dict\n    initializer_cls = CONST_CLS[value.__class__]\n    if issubclass(initializer_cls, (List, Set, Tuple)):\n        instance = initializer_cls()\n        instance.postinit(_create_basic_elements(value, instance))\n        return instance\n    if issubclass(initializer_cls, Dict):\n        instance = initializer_cls()\n        instance.postinit(_create_dict_items(value, instance))\n        return instance\n    return Const(value)",
    "label": true
  },
  {
    "code": "def suffixed_err(src: str, pos: Pos, msg: str) -> TOMLDecodeError:\n\n    def coord_repr(src: str, pos: Pos) -> str:\n        if pos >= len(src):\n            return 'end of document'\n        line = src.count('\\n', 0, pos) + 1\n        if line == 1:\n            column = pos + 1\n        else:\n            column = pos - src.rindex('\\n', 0, pos)\n        return f'line {line}, column {column}'\n    return TOMLDecodeError(f'{msg} (at {coord_repr(src, pos)})')",
    "label": true
  },
  {
    "code": "def _resolve_context(cli: BaseCommand, ctx_args: t.MutableMapping[str, t.Any], prog_name: str, args: t.List[str]) -> Context:\n    ctx_args['resilient_parsing'] = True\n    ctx = cli.make_context(prog_name, args.copy(), **ctx_args)\n    args = ctx.protected_args + ctx.args\n    while args:\n        command = ctx.command\n        if isinstance(command, MultiCommand):\n            if not command.chain:\n                name, cmd, args = command.resolve_command(ctx, args)\n                if cmd is None:\n                    return ctx\n                ctx = cmd.make_context(name, args, parent=ctx, resilient_parsing=True)\n                args = ctx.protected_args + ctx.args\n            else:\n                sub_ctx = ctx\n                while args:\n                    name, cmd, args = command.resolve_command(ctx, args)\n                    if cmd is None:\n                        return ctx\n                    sub_ctx = cmd.make_context(name, args, parent=ctx, allow_extra_args=True, allow_interspersed_args=False, resilient_parsing=True)\n                    args = sub_ctx.args\n                ctx = sub_ctx\n                args = [*sub_ctx.protected_args, *sub_ctx.args]\n        else:\n            break\n    return ctx",
    "label": true
  },
  {
    "code": "def remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)",
    "label": true
  },
  {
    "code": "def _expand_numparse(disable_numparse, column_count):\n    if isinstance(disable_numparse, Iterable):\n        numparses = [True] * column_count\n        for index in disable_numparse:\n            numparses[index] = False\n        return numparses\n    else:\n        return [not disable_numparse] * column_count",
    "label": true
  },
  {
    "code": "def _infer_map(node: nodes.Dict, context: InferenceContext | None) -> dict[SuccessfulInferenceResult, SuccessfulInferenceResult]:\n    values: dict[SuccessfulInferenceResult, SuccessfulInferenceResult] = {}\n    for name, value in node.items:\n        if isinstance(name, nodes.DictUnpack):\n            double_starred = helpers.safe_infer(value, context)\n            if not double_starred:\n                raise InferenceError\n            if not isinstance(double_starred, nodes.Dict):\n                raise InferenceError(node=node, context=context)\n            unpack_items = _infer_map(double_starred, context)\n            values = _update_with_replacement(values, unpack_items)\n        else:\n            key = helpers.safe_infer(name, context=context)\n            safe_value = helpers.safe_infer(value, context=context)\n            if any((not elem for elem in (key, safe_value))):\n                raise InferenceError(node=node, context=context)\n            values = _update_with_replacement(values, {key: safe_value})\n    return values",
    "label": true
  },
  {
    "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = ('', '')\n    return auth",
    "label": true
  },
  {
    "code": "def is_response_to_head(response):\n    method = response._method\n    if isinstance(method, int):\n        return method == 3\n    return method.upper() == 'HEAD'",
    "label": true
  },
  {
    "code": "def check_dist_restriction(options: Values, check_target: bool=False) -> None:\n    dist_restriction_set = any([options.python_version, options.platforms, options.abis, options.implementation])\n    binary_only = FormatControl(set(), {':all:'})\n    sdist_dependencies_allowed = options.format_control != binary_only and (not options.ignore_dependencies)\n    if dist_restriction_set and sdist_dependencies_allowed:\n        raise CommandError('When restricting platform and interpreter constraints using --python-version, --platform, --abi, or --implementation, either --no-deps must be set, or --only-binary=:all: must be set and --no-binary must not be set (or must be set to :none:).')\n    if check_target:\n        if dist_restriction_set and (not options.target_dir):\n            raise CommandError(\"Can not use any platform or abi specific options unless installing via '--target'\")",
    "label": true
  },
  {
    "code": "def build_sdist(sdist_directory, config_settings):\n    backend = _build_backend()\n    try:\n        return backend.build_sdist(sdist_directory, config_settings)\n    except getattr(backend, 'UnsupportedOperation', _DummyException):\n        raise GotUnsupportedOperation(traceback.format_exc())",
    "label": true
  },
  {
    "code": "def _get_env(environment: Dict[str, str], name: str) -> str:\n    value: Union[str, Undefined] = environment.get(name, _undefined)\n    if isinstance(value, Undefined):\n        raise UndefinedEnvironmentName(f'{name!r} does not exist in evaluation environment.')\n    return value",
    "label": true
  },
  {
    "code": "def skip_until(src: str, pos: Pos, expect: str, *, error_on: frozenset[str], error_on_eof: bool) -> Pos:\n    try:\n        new_pos = src.index(expect, pos)\n    except ValueError:\n        new_pos = len(src)\n        if error_on_eof:\n            raise suffixed_err(src, new_pos, f'Expected {expect!r}') from None\n    if not error_on.isdisjoint(src[pos:new_pos]):\n        while src[pos] not in error_on:\n            pos += 1\n        raise suffixed_err(src, pos, f'Found invalid character {src[pos]!r}')\n    return new_pos",
    "label": true
  },
  {
    "code": "def get_filter(unicode: str) -> Optional[Filter]:\n    unicode = unicode.lower()\n    if unicode == 'd':\n        return DurationFilter()\n    elif unicode == 'l':\n        return LocationFilter()\n    elif unicode == 'c':\n        return CustomerFilter()\n    elif unicode == 'r':\n        return ResetFilter()\n    return None",
    "label": true
  },
  {
    "code": "def main(argv: Optional[List[str]]=None) -> None:\n    parser = argparse.ArgumentParser(description='Takes one or more file paths and reports their detected encodings')\n    parser.add_argument('input', help='File whose encoding we would like to determine. (default: stdin)', type=argparse.FileType('rb'), nargs='*', default=[sys.stdin.buffer])\n    parser.add_argument('--minimal', help='Print only the encoding to standard output', action='store_true')\n    parser.add_argument('-l', '--legacy', help='Rename legacy encodings to more modern ones.', action='store_true')\n    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')\n    args = parser.parse_args(argv)\n    for f in args.input:\n        if f.isatty():\n            print('You are running chardetect interactively. Press CTRL-D twice at the start of a blank line to signal the end of your input. If you want help, run chardetect --help\\n', file=sys.stderr)\n        print(description_of(f, f.name, minimal=args.minimal, should_rename_legacy=args.legacy))",
    "label": true
  },
  {
    "code": "def validate_invariants(obj: object) -> None:\n    klass = obj.__class__\n    klass_mod = _get_module(klass)\n    try:\n        _check_invariants(obj, klass, klass_mod.__dict__)\n    except PyTAContractError as e:\n        raise AssertionError(str(e)) from None",
    "label": true
  },
  {
    "code": "def pytest_generate_tests(metafunc: 'Metafunc') -> None:\n    for marker in metafunc.definition.iter_markers(name='parametrize'):\n        metafunc.parametrize(*marker.args, **marker.kwargs, _param_mark=marker)",
    "label": true
  },
  {
    "code": "def _req_set_item_sorter(item: Tuple[str, InstallRequirement], weights: Dict[Optional[str], int]) -> Tuple[int, str]:\n    name = canonicalize_name(item[0])\n    return (weights[name], name)",
    "label": true
  },
  {
    "code": "def _get_python_version(version: str) -> PythonVersion:\n    if len(version) > 1:\n        return (int(version[0]), int(version[1:]))\n    else:\n        return (int(version[0]),)",
    "label": true
  },
  {
    "code": "def make_setuptools_egg_info_args(setup_py_path: str, egg_info_dir: Optional[str], no_user_config: bool) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, no_user_config=no_user_config)\n    args += ['egg_info']\n    if egg_info_dir:\n        args += ['--egg-base', egg_info_dir]\n    return args",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e266(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip('#'))\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1] + \"  # THERE SHOULD ONLY BE ONE '#'\")\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def parse_header_links(value):\n    links = []\n    replace_chars = ' \\'\"'\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(', *<', value):\n        try:\n            url, params = val.split(';', 1)\n        except ValueError:\n            url, params = (val, '')\n        link = {'url': url.strip('<> \\'\"')}\n        for param in params.split(';'):\n            try:\n                key, value = param.split('=')\n            except ValueError:\n                break\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n        links.append(link)\n    return links",
    "label": true
  },
  {
    "code": "def remove_successive(all_couples: CplIndexToCplLines_T) -> None:\n    couple: LineSetStartCouple\n    for couple in tuple(all_couples.keys()):\n        to_remove = []\n        test = couple.increment(Index(1))\n        while test in all_couples:\n            all_couples[couple].first_file.end = all_couples[test].first_file.end\n            all_couples[couple].second_file.end = all_couples[test].second_file.end\n            all_couples[couple].effective_cmn_lines_nb += 1\n            to_remove.append(test)\n            test = test.increment(Index(1))\n        for target in to_remove:\n            try:\n                all_couples.pop(target)\n            except KeyError:\n                pass",
    "label": true
  },
  {
    "code": "def _const_compare_digest_backport(a, b):\n    result = abs(len(a) - len(b))\n    for left, right in zip(bytearray(a), bytearray(b)):\n        result |= left ^ right\n    return result == 0",
    "label": true
  },
  {
    "code": "def _simple_escaping_wrapper(func: 't.Callable[_P, str]') -> 't.Callable[_P, Markup]':\n\n    @functools.wraps(func)\n    def wrapped(self: 'Markup', *args: '_P.args', **kwargs: '_P.kwargs') -> 'Markup':\n        arg_list = _escape_argspec(list(args), enumerate(args), self.escape)\n        _escape_argspec(kwargs, kwargs.items(), self.escape)\n        return self.__class__(func(self, *arg_list, **kwargs))\n    return wrapped",
    "label": true
  },
  {
    "code": "def _getoption():\n    var = (('acd', 'acd'), ('ai', 'ai'), ('akm', 'akm'), ('al', 'al'), ('aleph', 'aleph'), ('allowrevins', 'allowrevins'), ('altkeymap', 'altkeymap'), ('ambiwidth', 'ambiwidth'), ('ambw', 'ambw'), ('anti', 'anti'), ('antialias', 'antialias'), ('ar', 'ar'), ('arab', 'arab'), ('arabic', 'arabic'), ('arabicshape', 'arabicshape'), ('ari', 'ari'), ('arshape', 'arshape'), ('autochdir', 'autochdir'), ('autoindent', 'autoindent'), ('autoread', 'autoread'), ('autowrite', 'autowrite'), ('autowriteall', 'autowriteall'), ('aw', 'aw'), ('awa', 'awa'), ('background', 'background'), ('backspace', 'backspace'), ('backup', 'backup'), ('backupcopy', 'backupcopy'), ('backupdir', 'backupdir'), ('backupext', 'backupext'), ('backupskip', 'backupskip'), ('balloondelay', 'balloondelay'), ('ballooneval', 'ballooneval'), ('balloonexpr', 'balloonexpr'), ('bdir', 'bdir'), ('bdlay', 'bdlay'), ('beval', 'beval'), ('bex', 'bex'), ('bexpr', 'bexpr'), ('bg', 'bg'), ('bh', 'bh'), ('bin', 'bin'), ('binary', 'binary'), ('biosk', 'biosk'), ('bioskey', 'bioskey'), ('bk', 'bk'), ('bkc', 'bkc'), ('bl', 'bl'), ('bomb', 'bomb'), ('breakat', 'breakat'), ('brk', 'brk'), ('browsedir', 'browsedir'), ('bs', 'bs'), ('bsdir', 'bsdir'), ('bsk', 'bsk'), ('bt', 'bt'), ('bufhidden', 'bufhidden'), ('buflisted', 'buflisted'), ('buftype', 'buftype'), ('casemap', 'casemap'), ('cb', 'cb'), ('cc', 'cc'), ('ccv', 'ccv'), ('cd', 'cd'), ('cdpath', 'cdpath'), ('cedit', 'cedit'), ('cf', 'cf'), ('cfu', 'cfu'), ('ch', 'ch'), ('charconvert', 'charconvert'), ('ci', 'ci'), ('cin', 'cin'), ('cindent', 'cindent'), ('cink', 'cink'), ('cinkeys', 'cinkeys'), ('cino', 'cino'), ('cinoptions', 'cinoptions'), ('cinw', 'cinw'), ('cinwords', 'cinwords'), ('clipboard', 'clipboard'), ('cmdheight', 'cmdheight'), ('cmdwinheight', 'cmdwinheight'), ('cmp', 'cmp'), ('cms', 'cms'), ('co', 'co'), ('cocu', 'cocu'), ('cole', 'cole'), ('colorcolumn', 'colorcolumn'), ('columns', 'columns'), ('com', 'com'), ('comments', 'comments'), ('commentstring', 'commentstring'), ('compatible', 'compatible'), ('complete', 'complete'), ('completefunc', 'completefunc'), ('completeopt', 'completeopt'), ('concealcursor', 'concealcursor'), ('conceallevel', 'conceallevel'), ('confirm', 'confirm'), ('consk', 'consk'), ('conskey', 'conskey'), ('copyindent', 'copyindent'), ('cot', 'cot'), ('cp', 'cp'), ('cpo', 'cpo'), ('cpoptions', 'cpoptions'), ('cpt', 'cpt'), ('crb', 'crb'), ('cryptmethod', 'cryptmethod'), ('cscopepathcomp', 'cscopepathcomp'), ('cscopeprg', 'cscopeprg'), ('cscopequickfix', 'cscopequickfix'), ('cscoperelative', 'cscoperelative'), ('cscopetag', 'cscopetag'), ('cscopetagorder', 'cscopetagorder'), ('cscopeverbose', 'cscopeverbose'), ('cspc', 'cspc'), ('csprg', 'csprg'), ('csqf', 'csqf'), ('csre', 'csre'), ('cst', 'cst'), ('csto', 'csto'), ('csverb', 'csverb'), ('cuc', 'cuc'), ('cul', 'cul'), ('cursorbind', 'cursorbind'), ('cursorcolumn', 'cursorcolumn'), ('cursorline', 'cursorline'), ('cwh', 'cwh'), ('debug', 'debug'), ('deco', 'deco'), ('def', 'def'), ('define', 'define'), ('delcombine', 'delcombine'), ('dex', 'dex'), ('dg', 'dg'), ('dict', 'dict'), ('dictionary', 'dictionary'), ('diff', 'diff'), ('diffexpr', 'diffexpr'), ('diffopt', 'diffopt'), ('digraph', 'digraph'), ('dip', 'dip'), ('dir', 'dir'), ('directory', 'directory'), ('display', 'display'), ('dy', 'dy'), ('ea', 'ea'), ('ead', 'ead'), ('eadirection', 'eadirection'), ('eb', 'eb'), ('ed', 'ed'), ('edcompatible', 'edcompatible'), ('ef', 'ef'), ('efm', 'efm'), ('ei', 'ei'), ('ek', 'ek'), ('enc', 'enc'), ('encoding', 'encoding'), ('endofline', 'endofline'), ('eol', 'eol'), ('ep', 'ep'), ('equalalways', 'equalalways'), ('equalprg', 'equalprg'), ('errorbells', 'errorbells'), ('errorfile', 'errorfile'), ('errorformat', 'errorformat'), ('esckeys', 'esckeys'), ('et', 'et'), ('eventignore', 'eventignore'), ('ex', 'ex'), ('expandtab', 'expandtab'), ('exrc', 'exrc'), ('fcl', 'fcl'), ('fcs', 'fcs'), ('fdc', 'fdc'), ('fde', 'fde'), ('fdi', 'fdi'), ('fdl', 'fdl'), ('fdls', 'fdls'), ('fdm', 'fdm'), ('fdn', 'fdn'), ('fdo', 'fdo'), ('fdt', 'fdt'), ('fen', 'fen'), ('fenc', 'fenc'), ('fencs', 'fencs'), ('fex', 'fex'), ('ff', 'ff'), ('ffs', 'ffs'), ('fic', 'fic'), ('fileencoding', 'fileencoding'), ('fileencodings', 'fileencodings'), ('fileformat', 'fileformat'), ('fileformats', 'fileformats'), ('fileignorecase', 'fileignorecase'), ('filetype', 'filetype'), ('fillchars', 'fillchars'), ('fk', 'fk'), ('fkmap', 'fkmap'), ('flp', 'flp'), ('fml', 'fml'), ('fmr', 'fmr'), ('fo', 'fo'), ('foldclose', 'foldclose'), ('foldcolumn', 'foldcolumn'), ('foldenable', 'foldenable'), ('foldexpr', 'foldexpr'), ('foldignore', 'foldignore'), ('foldlevel', 'foldlevel'), ('foldlevelstart', 'foldlevelstart'), ('foldmarker', 'foldmarker'), ('foldmethod', 'foldmethod'), ('foldminlines', 'foldminlines'), ('foldnestmax', 'foldnestmax'), ('foldopen', 'foldopen'), ('foldtext', 'foldtext'), ('formatexpr', 'formatexpr'), ('formatlistpat', 'formatlistpat'), ('formatoptions', 'formatoptions'), ('formatprg', 'formatprg'), ('fp', 'fp'), ('fs', 'fs'), ('fsync', 'fsync'), ('ft', 'ft'), ('gcr', 'gcr'), ('gd', 'gd'), ('gdefault', 'gdefault'), ('gfm', 'gfm'), ('gfn', 'gfn'), ('gfs', 'gfs'), ('gfw', 'gfw'), ('ghr', 'ghr'), ('go', 'go'), ('gp', 'gp'), ('grepformat', 'grepformat'), ('grepprg', 'grepprg'), ('gtl', 'gtl'), ('gtt', 'gtt'), ('guicursor', 'guicursor'), ('guifont', 'guifont'), ('guifontset', 'guifontset'), ('guifontwide', 'guifontwide'), ('guiheadroom', 'guiheadroom'), ('guioptions', 'guioptions'), ('guipty', 'guipty'), ('guitablabel', 'guitablabel'), ('guitabtooltip', 'guitabtooltip'), ('helpfile', 'helpfile'), ('helpheight', 'helpheight'), ('helplang', 'helplang'), ('hf', 'hf'), ('hh', 'hh'), ('hi', 'hi'), ('hid', 'hid'), ('hidden', 'hidden'), ('highlight', 'highlight'), ('history', 'history'), ('hk', 'hk'), ('hkmap', 'hkmap'), ('hkmapp', 'hkmapp'), ('hkp', 'hkp'), ('hl', 'hl'), ('hlg', 'hlg'), ('hls', 'hls'), ('hlsearch', 'hlsearch'), ('ic', 'ic'), ('icon', 'icon'), ('iconstring', 'iconstring'), ('ignorecase', 'ignorecase'), ('im', 'im'), ('imactivatefunc', 'imactivatefunc'), ('imactivatekey', 'imactivatekey'), ('imaf', 'imaf'), ('imak', 'imak'), ('imc', 'imc'), ('imcmdline', 'imcmdline'), ('imd', 'imd'), ('imdisable', 'imdisable'), ('imi', 'imi'), ('iminsert', 'iminsert'), ('ims', 'ims'), ('imsearch', 'imsearch'), ('imsf', 'imsf'), ('imstatusfunc', 'imstatusfunc'), ('inc', 'inc'), ('include', 'include'), ('includeexpr', 'includeexpr'), ('incsearch', 'incsearch'), ('inde', 'inde'), ('indentexpr', 'indentexpr'), ('indentkeys', 'indentkeys'), ('indk', 'indk'), ('inex', 'inex'), ('inf', 'inf'), ('infercase', 'infercase'), ('inoremap', 'inoremap'), ('insertmode', 'insertmode'), ('invacd', 'invacd'), ('invai', 'invai'), ('invakm', 'invakm'), ('invallowrevins', 'invallowrevins'), ('invaltkeymap', 'invaltkeymap'), ('invanti', 'invanti'), ('invantialias', 'invantialias'), ('invar', 'invar'), ('invarab', 'invarab'), ('invarabic', 'invarabic'), ('invarabicshape', 'invarabicshape'), ('invari', 'invari'), ('invarshape', 'invarshape'), ('invautochdir', 'invautochdir'), ('invautoindent', 'invautoindent'), ('invautoread', 'invautoread'), ('invautowrite', 'invautowrite'), ('invautowriteall', 'invautowriteall'), ('invaw', 'invaw'), ('invawa', 'invawa'), ('invbackup', 'invbackup'), ('invballooneval', 'invballooneval'), ('invbeval', 'invbeval'), ('invbin', 'invbin'), ('invbinary', 'invbinary'), ('invbiosk', 'invbiosk'), ('invbioskey', 'invbioskey'), ('invbk', 'invbk'), ('invbl', 'invbl'), ('invbomb', 'invbomb'), ('invbuflisted', 'invbuflisted'), ('invcf', 'invcf'), ('invci', 'invci'), ('invcin', 'invcin'), ('invcindent', 'invcindent'), ('invcompatible', 'invcompatible'), ('invconfirm', 'invconfirm'), ('invconsk', 'invconsk'), ('invconskey', 'invconskey'), ('invcopyindent', 'invcopyindent'), ('invcp', 'invcp'), ('invcrb', 'invcrb'), ('invcscoperelative', 'invcscoperelative'), ('invcscopetag', 'invcscopetag'), ('invcscopeverbose', 'invcscopeverbose'), ('invcsre', 'invcsre'), ('invcst', 'invcst'), ('invcsverb', 'invcsverb'), ('invcuc', 'invcuc'), ('invcul', 'invcul'), ('invcursorbind', 'invcursorbind'), ('invcursorcolumn', 'invcursorcolumn'), ('invcursorline', 'invcursorline'), ('invdeco', 'invdeco'), ('invdelcombine', 'invdelcombine'), ('invdg', 'invdg'), ('invdiff', 'invdiff'), ('invdigraph', 'invdigraph'), ('invea', 'invea'), ('inveb', 'inveb'), ('inved', 'inved'), ('invedcompatible', 'invedcompatible'), ('invek', 'invek'), ('invendofline', 'invendofline'), ('inveol', 'inveol'), ('invequalalways', 'invequalalways'), ('inverrorbells', 'inverrorbells'), ('invesckeys', 'invesckeys'), ('invet', 'invet'), ('invex', 'invex'), ('invexpandtab', 'invexpandtab'), ('invexrc', 'invexrc'), ('invfen', 'invfen'), ('invfic', 'invfic'), ('invfileignorecase', 'invfileignorecase'), ('invfk', 'invfk'), ('invfkmap', 'invfkmap'), ('invfoldenable', 'invfoldenable'), ('invgd', 'invgd'), ('invgdefault', 'invgdefault'), ('invguipty', 'invguipty'), ('invhid', 'invhid'), ('invhidden', 'invhidden'), ('invhk', 'invhk'), ('invhkmap', 'invhkmap'), ('invhkmapp', 'invhkmapp'), ('invhkp', 'invhkp'), ('invhls', 'invhls'), ('invhlsearch', 'invhlsearch'), ('invic', 'invic'), ('invicon', 'invicon'), ('invignorecase', 'invignorecase'), ('invim', 'invim'), ('invimc', 'invimc'), ('invimcmdline', 'invimcmdline'), ('invimd', 'invimd'), ('invimdisable', 'invimdisable'), ('invincsearch', 'invincsearch'), ('invinf', 'invinf'), ('invinfercase', 'invinfercase'), ('invinsertmode', 'invinsertmode'), ('invis', 'invis'), ('invjoinspaces', 'invjoinspaces'), ('invjs', 'invjs'), ('invlazyredraw', 'invlazyredraw'), ('invlbr', 'invlbr'), ('invlinebreak', 'invlinebreak'), ('invlisp', 'invlisp'), ('invlist', 'invlist'), ('invloadplugins', 'invloadplugins'), ('invlpl', 'invlpl'), ('invlz', 'invlz'), ('invma', 'invma'), ('invmacatsui', 'invmacatsui'), ('invmagic', 'invmagic'), ('invmh', 'invmh'), ('invml', 'invml'), ('invmod', 'invmod'), ('invmodeline', 'invmodeline'), ('invmodifiable', 'invmodifiable'), ('invmodified', 'invmodified'), ('invmore', 'invmore'), ('invmousef', 'invmousef'), ('invmousefocus', 'invmousefocus'), ('invmousehide', 'invmousehide'), ('invnu', 'invnu'), ('invnumber', 'invnumber'), ('invodev', 'invodev'), ('invopendevice', 'invopendevice'), ('invpaste', 'invpaste'), ('invpi', 'invpi'), ('invpreserveindent', 'invpreserveindent'), ('invpreviewwindow', 'invpreviewwindow'), ('invprompt', 'invprompt'), ('invpvw', 'invpvw'), ('invreadonly', 'invreadonly'), ('invrelativenumber', 'invrelativenumber'), ('invremap', 'invremap'), ('invrestorescreen', 'invrestorescreen'), ('invrevins', 'invrevins'), ('invri', 'invri'), ('invrightleft', 'invrightleft'), ('invrl', 'invrl'), ('invrnu', 'invrnu'), ('invro', 'invro'), ('invrs', 'invrs'), ('invru', 'invru'), ('invruler', 'invruler'), ('invsb', 'invsb'), ('invsc', 'invsc'), ('invscb', 'invscb'), ('invscrollbind', 'invscrollbind'), ('invscs', 'invscs'), ('invsecure', 'invsecure'), ('invsft', 'invsft'), ('invshellslash', 'invshellslash'), ('invshelltemp', 'invshelltemp'), ('invshiftround', 'invshiftround'), ('invshortname', 'invshortname'), ('invshowcmd', 'invshowcmd'), ('invshowfulltag', 'invshowfulltag'), ('invshowmatch', 'invshowmatch'), ('invshowmode', 'invshowmode'), ('invsi', 'invsi'), ('invsm', 'invsm'), ('invsmartcase', 'invsmartcase'), ('invsmartindent', 'invsmartindent'), ('invsmarttab', 'invsmarttab'), ('invsmd', 'invsmd'), ('invsn', 'invsn'), ('invsol', 'invsol'), ('invspell', 'invspell'), ('invsplitbelow', 'invsplitbelow'), ('invsplitright', 'invsplitright'), ('invspr', 'invspr'), ('invsr', 'invsr'), ('invssl', 'invssl'), ('invsta', 'invsta'), ('invstartofline', 'invstartofline'), ('invstmp', 'invstmp'), ('invswapfile', 'invswapfile'), ('invswf', 'invswf'), ('invta', 'invta'), ('invtagbsearch', 'invtagbsearch'), ('invtagrelative', 'invtagrelative'), ('invtagstack', 'invtagstack'), ('invtbi', 'invtbi'), ('invtbidi', 'invtbidi'), ('invtbs', 'invtbs'), ('invtermbidi', 'invtermbidi'), ('invterse', 'invterse'), ('invtextauto', 'invtextauto'), ('invtextmode', 'invtextmode'), ('invtf', 'invtf'), ('invtgst', 'invtgst'), ('invtildeop', 'invtildeop'), ('invtimeout', 'invtimeout'), ('invtitle', 'invtitle'), ('invto', 'invto'), ('invtop', 'invtop'), ('invtr', 'invtr'), ('invttimeout', 'invttimeout'), ('invttybuiltin', 'invttybuiltin'), ('invttyfast', 'invttyfast'), ('invtx', 'invtx'), ('invudf', 'invudf'), ('invundofile', 'invundofile'), ('invvb', 'invvb'), ('invvisualbell', 'invvisualbell'), ('invwa', 'invwa'), ('invwarn', 'invwarn'), ('invwb', 'invwb'), ('invweirdinvert', 'invweirdinvert'), ('invwfh', 'invwfh'), ('invwfw', 'invwfw'), ('invwic', 'invwic'), ('invwildignorecase', 'invwildignorecase'), ('invwildmenu', 'invwildmenu'), ('invwinfixheight', 'invwinfixheight'), ('invwinfixwidth', 'invwinfixwidth'), ('invwiv', 'invwiv'), ('invwmnu', 'invwmnu'), ('invwrap', 'invwrap'), ('invwrapscan', 'invwrapscan'), ('invwrite', 'invwrite'), ('invwriteany', 'invwriteany'), ('invwritebackup', 'invwritebackup'), ('invws', 'invws'), ('is', 'is'), ('isf', 'isf'), ('isfname', 'isfname'), ('isi', 'isi'), ('isident', 'isident'), ('isk', 'isk'), ('iskeyword', 'iskeyword'), ('isp', 'isp'), ('isprint', 'isprint'), ('joinspaces', 'joinspaces'), ('js', 'js'), ('key', 'key'), ('keymap', 'keymap'), ('keymodel', 'keymodel'), ('keywordprg', 'keywordprg'), ('km', 'km'), ('kmp', 'kmp'), ('kp', 'kp'), ('langmap', 'langmap'), ('langmenu', 'langmenu'), ('laststatus', 'laststatus'), ('lazyredraw', 'lazyredraw'), ('lbr', 'lbr'), ('lcs', 'lcs'), ('linebreak', 'linebreak'), ('lines', 'lines'), ('linespace', 'linespace'), ('lisp', 'lisp'), ('lispwords', 'lispwords'), ('list', 'list'), ('listchars', 'listchars'), ('lm', 'lm'), ('lmap', 'lmap'), ('loadplugins', 'loadplugins'), ('lpl', 'lpl'), ('ls', 'ls'), ('lsp', 'lsp'), ('lw', 'lw'), ('lz', 'lz'), ('ma', 'ma'), ('macatsui', 'macatsui'), ('magic', 'magic'), ('makeef', 'makeef'), ('makeprg', 'makeprg'), ('mat', 'mat'), ('matchpairs', 'matchpairs'), ('matchtime', 'matchtime'), ('maxcombine', 'maxcombine'), ('maxfuncdepth', 'maxfuncdepth'), ('maxmapdepth', 'maxmapdepth'), ('maxmem', 'maxmem'), ('maxmempattern', 'maxmempattern'), ('maxmemtot', 'maxmemtot'), ('mco', 'mco'), ('mef', 'mef'), ('menuitems', 'menuitems'), ('mfd', 'mfd'), ('mh', 'mh'), ('mis', 'mis'), ('mkspellmem', 'mkspellmem'), ('ml', 'ml'), ('mls', 'mls'), ('mm', 'mm'), ('mmd', 'mmd'), ('mmp', 'mmp'), ('mmt', 'mmt'), ('mod', 'mod'), ('modeline', 'modeline'), ('modelines', 'modelines'), ('modifiable', 'modifiable'), ('modified', 'modified'), ('more', 'more'), ('mouse', 'mouse'), ('mousef', 'mousef'), ('mousefocus', 'mousefocus'), ('mousehide', 'mousehide'), ('mousem', 'mousem'), ('mousemodel', 'mousemodel'), ('mouses', 'mouses'), ('mouseshape', 'mouseshape'), ('mouset', 'mouset'), ('mousetime', 'mousetime'), ('mp', 'mp'), ('mps', 'mps'), ('msm', 'msm'), ('mzq', 'mzq'), ('mzquantum', 'mzquantum'), ('nf', 'nf'), ('nnoremap', 'nnoremap'), ('noacd', 'noacd'), ('noai', 'noai'), ('noakm', 'noakm'), ('noallowrevins', 'noallowrevins'), ('noaltkeymap', 'noaltkeymap'), ('noanti', 'noanti'), ('noantialias', 'noantialias'), ('noar', 'noar'), ('noarab', 'noarab'), ('noarabic', 'noarabic'), ('noarabicshape', 'noarabicshape'), ('noari', 'noari'), ('noarshape', 'noarshape'), ('noautochdir', 'noautochdir'), ('noautoindent', 'noautoindent'), ('noautoread', 'noautoread'), ('noautowrite', 'noautowrite'), ('noautowriteall', 'noautowriteall'), ('noaw', 'noaw'), ('noawa', 'noawa'), ('nobackup', 'nobackup'), ('noballooneval', 'noballooneval'), ('nobeval', 'nobeval'), ('nobin', 'nobin'), ('nobinary', 'nobinary'), ('nobiosk', 'nobiosk'), ('nobioskey', 'nobioskey'), ('nobk', 'nobk'), ('nobl', 'nobl'), ('nobomb', 'nobomb'), ('nobuflisted', 'nobuflisted'), ('nocf', 'nocf'), ('noci', 'noci'), ('nocin', 'nocin'), ('nocindent', 'nocindent'), ('nocompatible', 'nocompatible'), ('noconfirm', 'noconfirm'), ('noconsk', 'noconsk'), ('noconskey', 'noconskey'), ('nocopyindent', 'nocopyindent'), ('nocp', 'nocp'), ('nocrb', 'nocrb'), ('nocscoperelative', 'nocscoperelative'), ('nocscopetag', 'nocscopetag'), ('nocscopeverbose', 'nocscopeverbose'), ('nocsre', 'nocsre'), ('nocst', 'nocst'), ('nocsverb', 'nocsverb'), ('nocuc', 'nocuc'), ('nocul', 'nocul'), ('nocursorbind', 'nocursorbind'), ('nocursorcolumn', 'nocursorcolumn'), ('nocursorline', 'nocursorline'), ('nodeco', 'nodeco'), ('nodelcombine', 'nodelcombine'), ('nodg', 'nodg'), ('nodiff', 'nodiff'), ('nodigraph', 'nodigraph'), ('noea', 'noea'), ('noeb', 'noeb'), ('noed', 'noed'), ('noedcompatible', 'noedcompatible'), ('noek', 'noek'), ('noendofline', 'noendofline'), ('noeol', 'noeol'), ('noequalalways', 'noequalalways'), ('noerrorbells', 'noerrorbells'), ('noesckeys', 'noesckeys'), ('noet', 'noet'), ('noex', 'noex'), ('noexpandtab', 'noexpandtab'), ('noexrc', 'noexrc'), ('nofen', 'nofen'), ('nofic', 'nofic'), ('nofileignorecase', 'nofileignorecase'), ('nofk', 'nofk'), ('nofkmap', 'nofkmap'), ('nofoldenable', 'nofoldenable'), ('nogd', 'nogd'), ('nogdefault', 'nogdefault'), ('noguipty', 'noguipty'), ('nohid', 'nohid'), ('nohidden', 'nohidden'), ('nohk', 'nohk'), ('nohkmap', 'nohkmap'), ('nohkmapp', 'nohkmapp'), ('nohkp', 'nohkp'), ('nohls', 'nohls'), ('nohlsearch', 'nohlsearch'), ('noic', 'noic'), ('noicon', 'noicon'), ('noignorecase', 'noignorecase'), ('noim', 'noim'), ('noimc', 'noimc'), ('noimcmdline', 'noimcmdline'), ('noimd', 'noimd'), ('noimdisable', 'noimdisable'), ('noincsearch', 'noincsearch'), ('noinf', 'noinf'), ('noinfercase', 'noinfercase'), ('noinsertmode', 'noinsertmode'), ('nois', 'nois'), ('nojoinspaces', 'nojoinspaces'), ('nojs', 'nojs'), ('nolazyredraw', 'nolazyredraw'), ('nolbr', 'nolbr'), ('nolinebreak', 'nolinebreak'), ('nolisp', 'nolisp'), ('nolist', 'nolist'), ('noloadplugins', 'noloadplugins'), ('nolpl', 'nolpl'), ('nolz', 'nolz'), ('noma', 'noma'), ('nomacatsui', 'nomacatsui'), ('nomagic', 'nomagic'), ('nomh', 'nomh'), ('noml', 'noml'), ('nomod', 'nomod'), ('nomodeline', 'nomodeline'), ('nomodifiable', 'nomodifiable'), ('nomodified', 'nomodified'), ('nomore', 'nomore'), ('nomousef', 'nomousef'), ('nomousefocus', 'nomousefocus'), ('nomousehide', 'nomousehide'), ('nonu', 'nonu'), ('nonumber', 'nonumber'), ('noodev', 'noodev'), ('noopendevice', 'noopendevice'), ('nopaste', 'nopaste'), ('nopi', 'nopi'), ('nopreserveindent', 'nopreserveindent'), ('nopreviewwindow', 'nopreviewwindow'), ('noprompt', 'noprompt'), ('nopvw', 'nopvw'), ('noreadonly', 'noreadonly'), ('norelativenumber', 'norelativenumber'), ('noremap', 'noremap'), ('norestorescreen', 'norestorescreen'), ('norevins', 'norevins'), ('nori', 'nori'), ('norightleft', 'norightleft'), ('norl', 'norl'), ('nornu', 'nornu'), ('noro', 'noro'), ('nors', 'nors'), ('noru', 'noru'), ('noruler', 'noruler'), ('nosb', 'nosb'), ('nosc', 'nosc'), ('noscb', 'noscb'), ('noscrollbind', 'noscrollbind'), ('noscs', 'noscs'), ('nosecure', 'nosecure'), ('nosft', 'nosft'), ('noshellslash', 'noshellslash'), ('noshelltemp', 'noshelltemp'), ('noshiftround', 'noshiftround'), ('noshortname', 'noshortname'), ('noshowcmd', 'noshowcmd'), ('noshowfulltag', 'noshowfulltag'), ('noshowmatch', 'noshowmatch'), ('noshowmode', 'noshowmode'), ('nosi', 'nosi'), ('nosm', 'nosm'), ('nosmartcase', 'nosmartcase'), ('nosmartindent', 'nosmartindent'), ('nosmarttab', 'nosmarttab'), ('nosmd', 'nosmd'), ('nosn', 'nosn'), ('nosol', 'nosol'), ('nospell', 'nospell'), ('nosplitbelow', 'nosplitbelow'), ('nosplitright', 'nosplitright'), ('nospr', 'nospr'), ('nosr', 'nosr'), ('nossl', 'nossl'), ('nosta', 'nosta'), ('nostartofline', 'nostartofline'), ('nostmp', 'nostmp'), ('noswapfile', 'noswapfile'), ('noswf', 'noswf'), ('nota', 'nota'), ('notagbsearch', 'notagbsearch'), ('notagrelative', 'notagrelative'), ('notagstack', 'notagstack'), ('notbi', 'notbi'), ('notbidi', 'notbidi'), ('notbs', 'notbs'), ('notermbidi', 'notermbidi'), ('noterse', 'noterse'), ('notextauto', 'notextauto'), ('notextmode', 'notextmode'), ('notf', 'notf'), ('notgst', 'notgst'), ('notildeop', 'notildeop'), ('notimeout', 'notimeout'), ('notitle', 'notitle'), ('noto', 'noto'), ('notop', 'notop'), ('notr', 'notr'), ('nottimeout', 'nottimeout'), ('nottybuiltin', 'nottybuiltin'), ('nottyfast', 'nottyfast'), ('notx', 'notx'), ('noudf', 'noudf'), ('noundofile', 'noundofile'), ('novb', 'novb'), ('novisualbell', 'novisualbell'), ('nowa', 'nowa'), ('nowarn', 'nowarn'), ('nowb', 'nowb'), ('noweirdinvert', 'noweirdinvert'), ('nowfh', 'nowfh'), ('nowfw', 'nowfw'), ('nowic', 'nowic'), ('nowildignorecase', 'nowildignorecase'), ('nowildmenu', 'nowildmenu'), ('nowinfixheight', 'nowinfixheight'), ('nowinfixwidth', 'nowinfixwidth'), ('nowiv', 'nowiv'), ('nowmnu', 'nowmnu'), ('nowrap', 'nowrap'), ('nowrapscan', 'nowrapscan'), ('nowrite', 'nowrite'), ('nowriteany', 'nowriteany'), ('nowritebackup', 'nowritebackup'), ('nows', 'nows'), ('nrformats', 'nrformats'), ('nu', 'nu'), ('number', 'number'), ('numberwidth', 'numberwidth'), ('nuw', 'nuw'), ('odev', 'odev'), ('oft', 'oft'), ('ofu', 'ofu'), ('omnifunc', 'omnifunc'), ('opendevice', 'opendevice'), ('operatorfunc', 'operatorfunc'), ('opfunc', 'opfunc'), ('osfiletype', 'osfiletype'), ('pa', 'pa'), ('para', 'para'), ('paragraphs', 'paragraphs'), ('paste', 'paste'), ('pastetoggle', 'pastetoggle'), ('patchexpr', 'patchexpr'), ('patchmode', 'patchmode'), ('path', 'path'), ('pdev', 'pdev'), ('penc', 'penc'), ('pex', 'pex'), ('pexpr', 'pexpr'), ('pfn', 'pfn'), ('ph', 'ph'), ('pheader', 'pheader'), ('pi', 'pi'), ('pm', 'pm'), ('pmbcs', 'pmbcs'), ('pmbfn', 'pmbfn'), ('popt', 'popt'), ('preserveindent', 'preserveindent'), ('previewheight', 'previewheight'), ('previewwindow', 'previewwindow'), ('printdevice', 'printdevice'), ('printencoding', 'printencoding'), ('printexpr', 'printexpr'), ('printfont', 'printfont'), ('printheader', 'printheader'), ('printmbcharset', 'printmbcharset'), ('printmbfont', 'printmbfont'), ('printoptions', 'printoptions'), ('prompt', 'prompt'), ('pt', 'pt'), ('pumheight', 'pumheight'), ('pvh', 'pvh'), ('pvw', 'pvw'), ('qe', 'qe'), ('quoteescape', 'quoteescape'), ('rdt', 'rdt'), ('re', 're'), ('readonly', 'readonly'), ('redrawtime', 'redrawtime'), ('regexpengine', 'regexpengine'), ('relativenumber', 'relativenumber'), ('remap', 'remap'), ('report', 'report'), ('restorescreen', 'restorescreen'), ('revins', 'revins'), ('ri', 'ri'), ('rightleft', 'rightleft'), ('rightleftcmd', 'rightleftcmd'), ('rl', 'rl'), ('rlc', 'rlc'), ('rnu', 'rnu'), ('ro', 'ro'), ('rs', 'rs'), ('rtp', 'rtp'), ('ru', 'ru'), ('ruf', 'ruf'), ('ruler', 'ruler'), ('rulerformat', 'rulerformat'), ('runtimepath', 'runtimepath'), ('sb', 'sb'), ('sbo', 'sbo'), ('sbr', 'sbr'), ('sc', 'sc'), ('scb', 'scb'), ('scr', 'scr'), ('scroll', 'scroll'), ('scrollbind', 'scrollbind'), ('scrolljump', 'scrolljump'), ('scrolloff', 'scrolloff'), ('scrollopt', 'scrollopt'), ('scs', 'scs'), ('sect', 'sect'), ('sections', 'sections'), ('secure', 'secure'), ('sel', 'sel'), ('selection', 'selection'), ('selectmode', 'selectmode'), ('sessionoptions', 'sessionoptions'), ('sft', 'sft'), ('sh', 'sh'), ('shcf', 'shcf'), ('shell', 'shell'), ('shellcmdflag', 'shellcmdflag'), ('shellpipe', 'shellpipe'), ('shellquote', 'shellquote'), ('shellredir', 'shellredir'), ('shellslash', 'shellslash'), ('shelltemp', 'shelltemp'), ('shelltype', 'shelltype'), ('shellxescape', 'shellxescape'), ('shellxquote', 'shellxquote'), ('shiftround', 'shiftround'), ('shiftwidth', 'shiftwidth'), ('shm', 'shm'), ('shortmess', 'shortmess'), ('shortname', 'shortname'), ('showbreak', 'showbreak'), ('showcmd', 'showcmd'), ('showfulltag', 'showfulltag'), ('showmatch', 'showmatch'), ('showmode', 'showmode'), ('showtabline', 'showtabline'), ('shq', 'shq'), ('si', 'si'), ('sidescroll', 'sidescroll'), ('sidescrolloff', 'sidescrolloff'), ('siso', 'siso'), ('sj', 'sj'), ('slm', 'slm'), ('sm', 'sm'), ('smartcase', 'smartcase'), ('smartindent', 'smartindent'), ('smarttab', 'smarttab'), ('smc', 'smc'), ('smd', 'smd'), ('sn', 'sn'), ('so', 'so'), ('softtabstop', 'softtabstop'), ('sol', 'sol'), ('sp', 'sp'), ('spc', 'spc'), ('spell', 'spell'), ('spellcapcheck', 'spellcapcheck'), ('spellfile', 'spellfile'), ('spelllang', 'spelllang'), ('spellsuggest', 'spellsuggest'), ('spf', 'spf'), ('spl', 'spl'), ('splitbelow', 'splitbelow'), ('splitright', 'splitright'), ('spr', 'spr'), ('sps', 'sps'), ('sr', 'sr'), ('srr', 'srr'), ('ss', 'ss'), ('ssl', 'ssl'), ('ssop', 'ssop'), ('st', 'st'), ('sta', 'sta'), ('stal', 'stal'), ('startofline', 'startofline'), ('statusline', 'statusline'), ('stl', 'stl'), ('stmp', 'stmp'), ('sts', 'sts'), ('su', 'su'), ('sua', 'sua'), ('suffixes', 'suffixes'), ('suffixesadd', 'suffixesadd'), ('sw', 'sw'), ('swapfile', 'swapfile'), ('swapsync', 'swapsync'), ('swb', 'swb'), ('swf', 'swf'), ('switchbuf', 'switchbuf'), ('sws', 'sws'), ('sxe', 'sxe'), ('sxq', 'sxq'), ('syn', 'syn'), ('synmaxcol', 'synmaxcol'), ('syntax', 'syntax'), ('t_AB', 't_AB'), ('t_AF', 't_AF'), ('t_AL', 't_AL'), ('t_CS', 't_CS'), ('t_CV', 't_CV'), ('t_Ce', 't_Ce'), ('t_Co', 't_Co'), ('t_Cs', 't_Cs'), ('t_DL', 't_DL'), ('t_EI', 't_EI'), ('t_F1', 't_F1'), ('t_F2', 't_F2'), ('t_F3', 't_F3'), ('t_F4', 't_F4'), ('t_F5', 't_F5'), ('t_F6', 't_F6'), ('t_F7', 't_F7'), ('t_F8', 't_F8'), ('t_F9', 't_F9'), ('t_IE', 't_IE'), ('t_IS', 't_IS'), ('t_K1', 't_K1'), ('t_K3', 't_K3'), ('t_K4', 't_K4'), ('t_K5', 't_K5'), ('t_K6', 't_K6'), ('t_K7', 't_K7'), ('t_K8', 't_K8'), ('t_K9', 't_K9'), ('t_KA', 't_KA'), ('t_KB', 't_KB'), ('t_KC', 't_KC'), ('t_KD', 't_KD'), ('t_KE', 't_KE'), ('t_KF', 't_KF'), ('t_KG', 't_KG'), ('t_KH', 't_KH'), ('t_KI', 't_KI'), ('t_KJ', 't_KJ'), ('t_KK', 't_KK'), ('t_KL', 't_KL'), ('t_RI', 't_RI'), ('t_RV', 't_RV'), ('t_SI', 't_SI'), ('t_Sb', 't_Sb'), ('t_Sf', 't_Sf'), ('t_WP', 't_WP'), ('t_WS', 't_WS'), ('t_ZH', 't_ZH'), ('t_ZR', 't_ZR'), ('t_al', 't_al'), ('t_bc', 't_bc'), ('t_cd', 't_cd'), ('t_ce', 't_ce'), ('t_cl', 't_cl'), ('t_cm', 't_cm'), ('t_cs', 't_cs'), ('t_da', 't_da'), ('t_db', 't_db'), ('t_dl', 't_dl'), ('t_fs', 't_fs'), ('t_k1', 't_k1'), ('t_k2', 't_k2'), ('t_k3', 't_k3'), ('t_k4', 't_k4'), ('t_k5', 't_k5'), ('t_k6', 't_k6'), ('t_k7', 't_k7'), ('t_k8', 't_k8'), ('t_k9', 't_k9'), ('t_kB', 't_kB'), ('t_kD', 't_kD'), ('t_kI', 't_kI'), ('t_kN', 't_kN'), ('t_kP', 't_kP'), ('t_kb', 't_kb'), ('t_kd', 't_kd'), ('t_ke', 't_ke'), ('t_kh', 't_kh'), ('t_kl', 't_kl'), ('t_kr', 't_kr'), ('t_ks', 't_ks'), ('t_ku', 't_ku'), ('t_le', 't_le'), ('t_mb', 't_mb'), ('t_md', 't_md'), ('t_me', 't_me'), ('t_mr', 't_mr'), ('t_ms', 't_ms'), ('t_nd', 't_nd'), ('t_op', 't_op'), ('t_se', 't_se'), ('t_so', 't_so'), ('t_sr', 't_sr'), ('t_te', 't_te'), ('t_ti', 't_ti'), ('t_ts', 't_ts'), ('t_u7', 't_u7'), ('t_ue', 't_ue'), ('t_us', 't_us'), ('t_ut', 't_ut'), ('t_vb', 't_vb'), ('t_ve', 't_ve'), ('t_vi', 't_vi'), ('t_vs', 't_vs'), ('t_xs', 't_xs'), ('ta', 'ta'), ('tabline', 'tabline'), ('tabpagemax', 'tabpagemax'), ('tabstop', 'tabstop'), ('tag', 'tag'), ('tagbsearch', 'tagbsearch'), ('taglength', 'taglength'), ('tagrelative', 'tagrelative'), ('tags', 'tags'), ('tagstack', 'tagstack'), ('tal', 'tal'), ('tb', 'tb'), ('tbi', 'tbi'), ('tbidi', 'tbidi'), ('tbis', 'tbis'), ('tbs', 'tbs'), ('tenc', 'tenc'), ('term', 'term'), ('termbidi', 'termbidi'), ('termencoding', 'termencoding'), ('terse', 'terse'), ('textauto', 'textauto'), ('textmode', 'textmode'), ('textwidth', 'textwidth'), ('tf', 'tf'), ('tgst', 'tgst'), ('thesaurus', 'thesaurus'), ('tildeop', 'tildeop'), ('timeout', 'timeout'), ('timeoutlen', 'timeoutlen'), ('title', 'title'), ('titlelen', 'titlelen'), ('titleold', 'titleold'), ('titlestring', 'titlestring'), ('tl', 'tl'), ('tm', 'tm'), ('to', 'to'), ('toolbar', 'toolbar'), ('toolbariconsize', 'toolbariconsize'), ('top', 'top'), ('tpm', 'tpm'), ('tr', 'tr'), ('ts', 'ts'), ('tsl', 'tsl'), ('tsr', 'tsr'), ('ttimeout', 'ttimeout'), ('ttimeoutlen', 'ttimeoutlen'), ('ttm', 'ttm'), ('tty', 'tty'), ('ttybuiltin', 'ttybuiltin'), ('ttyfast', 'ttyfast'), ('ttym', 'ttym'), ('ttymouse', 'ttymouse'), ('ttyscroll', 'ttyscroll'), ('ttytype', 'ttytype'), ('tw', 'tw'), ('tx', 'tx'), ('uc', 'uc'), ('udf', 'udf'), ('udir', 'udir'), ('ul', 'ul'), ('undodir', 'undodir'), ('undofile', 'undofile'), ('undolevels', 'undolevels'), ('undoreload', 'undoreload'), ('updatecount', 'updatecount'), ('updatetime', 'updatetime'), ('ur', 'ur'), ('ut', 'ut'), ('vb', 'vb'), ('vbs', 'vbs'), ('vdir', 'vdir'), ('ve', 've'), ('verbose', 'verbose'), ('verbosefile', 'verbosefile'), ('vfile', 'vfile'), ('vi', 'vi'), ('viewdir', 'viewdir'), ('viewoptions', 'viewoptions'), ('viminfo', 'viminfo'), ('virtualedit', 'virtualedit'), ('visualbell', 'visualbell'), ('vnoremap', 'vnoremap'), ('vop', 'vop'), ('wa', 'wa'), ('wak', 'wak'), ('warn', 'warn'), ('wb', 'wb'), ('wc', 'wc'), ('wcm', 'wcm'), ('wd', 'wd'), ('weirdinvert', 'weirdinvert'), ('wfh', 'wfh'), ('wfw', 'wfw'), ('wh', 'wh'), ('whichwrap', 'whichwrap'), ('wi', 'wi'), ('wic', 'wic'), ('wig', 'wig'), ('wildchar', 'wildchar'), ('wildcharm', 'wildcharm'), ('wildignore', 'wildignore'), ('wildignorecase', 'wildignorecase'), ('wildmenu', 'wildmenu'), ('wildmode', 'wildmode'), ('wildoptions', 'wildoptions'), ('wim', 'wim'), ('winaltkeys', 'winaltkeys'), ('window', 'window'), ('winfixheight', 'winfixheight'), ('winfixwidth', 'winfixwidth'), ('winheight', 'winheight'), ('winminheight', 'winminheight'), ('winminwidth', 'winminwidth'), ('winwidth', 'winwidth'), ('wiv', 'wiv'), ('wiw', 'wiw'), ('wm', 'wm'), ('wmh', 'wmh'), ('wmnu', 'wmnu'), ('wmw', 'wmw'), ('wop', 'wop'), ('wrap', 'wrap'), ('wrapmargin', 'wrapmargin'), ('wrapscan', 'wrapscan'), ('write', 'write'), ('writeany', 'writeany'), ('writebackup', 'writebackup'), ('writedelay', 'writedelay'), ('ws', 'ws'), ('ww', 'ww'))\n    return var",
    "label": true
  },
  {
    "code": "def _make_cached_stream_func(src_func: t.Callable[[], t.Optional[t.TextIO]], wrapper_func: t.Callable[[], t.TextIO]) -> t.Callable[[], t.Optional[t.TextIO]]:\n    cache: t.MutableMapping[t.TextIO, t.TextIO] = WeakKeyDictionary()\n\n    def func() -> t.Optional[t.TextIO]:\n        stream = src_func()\n        if stream is None:\n            return None\n        try:\n            rv = cache.get(stream)\n        except Exception:\n            rv = None\n        if rv is not None:\n            return rv\n        rv = wrapper_func()\n        try:\n            cache[stream] = rv\n        except Exception:\n            pass\n        return rv\n    return func",
    "label": true
  },
  {
    "code": "def clean_duplicates_mro(sequences, cls, context):\n    for sequence in sequences:\n        names = [(node.lineno, node.qname()) if node.name else None for node in sequence]\n        last_index = dict(map(reversed, enumerate(names)))\n        if names and names[0] is not None and (last_index[names[0]] != 0):\n            raise DuplicateBasesError(message='Duplicates found in MROs {mros} for {cls!r}.', mros=sequences, cls=cls, context=context)\n        yield [node for i, (node, name) in enumerate(zip(sequence, names)) if name is None or last_index[name] == i]",
    "label": true
  },
  {
    "code": "def get_subscript_const_value(node: nodes.Subscript) -> nodes.Const:\n    inferred = safe_infer(node.slice)\n    if not isinstance(inferred, nodes.Const):\n        raise InferredTypeError('Subscript.slice cannot be inferred as a nodes.Const')\n    return inferred",
    "label": true
  },
  {
    "code": "def sieve(n):\n    isqrt = getattr(math, 'isqrt', lambda x: int(math.sqrt(x)))\n    data = bytearray((0, 1)) * (n // 2)\n    data[:3] = (0, 0, 0)\n    limit = isqrt(n) + 1\n    for p in compress(range(limit), data):\n        data[p * p:n:p + p] = bytes(len(range(p * p, n, p + p)))\n    data[2] = 1\n    return iter_index(data, 1) if n > 2 else iter([])",
    "label": true
  },
  {
    "code": "def _cert_array_from_pem(pem_bundle: bytes) -> CFArray:\n    pem_bundle = pem_bundle.replace(b'\\r\\n', b'\\n')\n    der_certs = [base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)]\n    if not der_certs:\n        raise ssl.SSLError('No root certificates specified')\n    cert_array = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n    if not cert_array:\n        raise ssl.SSLError('Unable to allocate memory!')\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError('Unable to allocate memory!')\n            cert = Security.SecCertificateCreateWithData(CoreFoundation.kCFAllocatorDefault, certdata)\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError('Unable to build cert object!')\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        CoreFoundation.CFRelease(cert_array)\n        raise\n    return cert_array",
    "label": true
  },
  {
    "code": "def random_product(*args, repeat=1):\n    pools = [tuple(pool) for pool in args] * repeat\n    return tuple((choice(pool) for pool in pools))",
    "label": true
  },
  {
    "code": "def check_config_h():\n    from distutils import sysconfig\n    if 'GCC' in sys.version:\n        return (CONFIG_H_OK, \"sys.version mentions 'GCC'\")\n    if 'Clang' in sys.version:\n        return (CONFIG_H_OK, \"sys.version mentions 'Clang'\")\n    fn = sysconfig.get_config_h_filename()\n    try:\n        config_h = open(fn)\n        try:\n            if '__GNUC__' in config_h.read():\n                return (CONFIG_H_OK, \"'%s' mentions '__GNUC__'\" % fn)\n            else:\n                return (CONFIG_H_NOTOK, \"'%s' does not mention '__GNUC__'\" % fn)\n        finally:\n            config_h.close()\n    except OSError as exc:\n        return (CONFIG_H_UNCERTAIN, \"couldn't read '{}': {}\".format(fn, exc.strerror))",
    "label": true
  },
  {
    "code": "def _get_python_inc_posix_python(plat_specific):\n    if not python_build:\n        return\n    if plat_specific:\n        return _sys_home or project_base\n    incdir = os.path.join(get_config_var('srcdir'), 'Include')\n    return os.path.normpath(incdir)",
    "label": true
  },
  {
    "code": "def _append_line(lines, colwidths, colaligns, linefmt):\n    lines.append(_build_line(colwidths, colaligns, linefmt))\n    return lines",
    "label": true
  },
  {
    "code": "def enable_vt_processing(fd):\n    if win32.windll is None or not win32.winapi_test():\n        return False\n    try:\n        handle = get_osfhandle(fd)\n        mode = win32.GetConsoleMode(handle)\n        win32.SetConsoleMode(handle, mode | win32.ENABLE_VIRTUAL_TERMINAL_PROCESSING)\n        mode = win32.GetConsoleMode(handle)\n        if mode & win32.ENABLE_VIRTUAL_TERMINAL_PROCESSING:\n            return True\n    except (OSError, TypeError):\n        return False",
    "label": true
  },
  {
    "code": "def infer_name(self: nodes.Name | nodes.AssignName, context: InferenceContext | None=None, **kwargs: Any) -> Generator[InferenceResult, None, None]:\n    frame, stmts = self.lookup(self.name)\n    if not stmts:\n        parent_function = _higher_function_scope(self.scope())\n        if parent_function:\n            _, stmts = parent_function.lookup(self.name)\n        if not stmts:\n            raise NameInferenceError(name=self.name, scope=self.scope(), context=context)\n    context = copy_context(context)\n    context.lookupname = self.name\n    context.constraints[self.name] = constraint.get_constraints(self, frame)\n    return bases._infer_stmts(stmts, context, frame)",
    "label": true
  },
  {
    "code": "def _handle_config_settings(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    key, sep, val = value.partition('=')\n    if sep != '=':\n        parser.error(f'Arguments to {opt_str} must be of the form KEY=VAL')\n    dest = getattr(parser.values, option.dest)\n    if dest is None:\n        dest = {}\n        setattr(parser.values, option.dest, dest)\n    if key in dest:\n        if isinstance(dest[key], list):\n            dest[key].append(val)\n        else:\n            dest[key] = [dest[key], val]\n    else:\n        dest[key] = val",
    "label": true
  },
  {
    "code": "def infer_property(node: nodes.Call, context: InferenceContext | None=None) -> objects.Property:\n    if len(node.args) < 1:\n        raise UseInferenceDefault\n    getter = node.args[0]\n    try:\n        inferred = next(getter.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n        raise UseInferenceDefault\n    prop_func = objects.Property(function=inferred, name=inferred.name, lineno=node.lineno, parent=node, col_offset=node.col_offset)\n    prop_func.postinit(body=[], args=inferred.args, doc_node=getattr(inferred, 'doc_node', None))\n    return prop_func",
    "label": true
  },
  {
    "code": "def ratio_distribute(total: int, ratios: List[int], minimums: Optional[List[int]]=None) -> List[int]:\n    if minimums:\n        ratios = [ratio if _min else 0 for ratio, _min in zip(ratios, minimums)]\n    total_ratio = sum(ratios)\n    assert total_ratio > 0, 'Sum of ratios must be > 0'\n    total_remaining = total\n    distributed_total: List[int] = []\n    append = distributed_total.append\n    if minimums is None:\n        _minimums = [0] * len(ratios)\n    else:\n        _minimums = minimums\n    for ratio, minimum in zip(ratios, _minimums):\n        if total_ratio > 0:\n            distributed = max(minimum, ceil(ratio * total_remaining / total_ratio))\n        else:\n            distributed = total_remaining\n        append(distributed)\n        total_ratio -= ratio\n        total_remaining -= distributed\n    return distributed_total",
    "label": true
  },
  {
    "code": "def adjacent(predicate, iterable, distance=1):\n    if distance < 0:\n        raise ValueError('distance must be at least 0')\n    i1, i2 = tee(iterable)\n    padding = [False] * distance\n    selected = chain(padding, map(predicate, i1), padding)\n    adjacent_to_selected = map(any, windowed(selected, 2 * distance + 1))\n    return zip(adjacent_to_selected, i2)",
    "label": true
  },
  {
    "code": "def test_code_to_tempfile():\n    if not WINDOWS:\n        pyfile = dump_source(f, alias='_f')\n        _f = load_source(pyfile)\n        assert _f(4) == f(4)",
    "label": true
  },
  {
    "code": "def _custom_manylinux_platforms(arch: str) -> List[str]:\n    arches = [arch]\n    arch_prefix, arch_sep, arch_suffix = arch.partition('_')\n    if arch_prefix == 'manylinux2014':\n        if arch_suffix in {'i686', 'x86_64'}:\n            arches.append('manylinux2010' + arch_sep + arch_suffix)\n            arches.append('manylinux1' + arch_sep + arch_suffix)\n    elif arch_prefix == 'manylinux2010':\n        arches.append('manylinux1' + arch_sep + arch_suffix)\n    return arches",
    "label": true
  },
  {
    "code": "def get_all_distribution_names(url=None):\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()",
    "label": true
  },
  {
    "code": "def set_partitions(iterable, k=None):\n    L = list(iterable)\n    n = len(L)\n    if k is not None:\n        if k < 1:\n            raise ValueError(\"Can't partition in a negative or zero number of groups\")\n        elif k > n:\n            return\n\n    def set_partitions_helper(L, k):\n        n = len(L)\n        if k == 1:\n            yield [L]\n        elif n == k:\n            yield [[s] for s in L]\n        else:\n            e, *M = L\n            for p in set_partitions_helper(M, k - 1):\n                yield [[e], *p]\n            for p in set_partitions_helper(M, k):\n                for i in range(len(p)):\n                    yield (p[:i] + [[e] + p[i]] + p[i + 1:])\n    if k is None:\n        for k in range(1, n + 1):\n            yield from set_partitions_helper(L, k)\n    else:\n        yield from set_partitions_helper(L, k)",
    "label": true
  },
  {
    "code": "def _print_hard_fail(config: Config, offending_file: Optional[str]=None, message: Optional[str]=None) -> None:\n    message = message or f\"Unrecoverable exception thrown when parsing {offending_file or ''}! This should NEVER happen.\\nIf encountered, please open an issue: https://github.com/PyCQA/isort/issues/new\"\n    printer = create_terminal_printer(color=config.color_output, error=config.format_error, success=config.format_success)\n    printer.error(message)",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], Tuple[Node, ...], str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def get_win_folder_from_registry(csidl_name: str) -> str:\n    shell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': 'Common AppData', 'CSIDL_LOCAL_APPDATA': 'Local AppData', 'CSIDL_PERSONAL': 'Personal'}.get(csidl_name)\n    if shell_folder_name is None:\n        raise ValueError(f'Unknown CSIDL name: {csidl_name}')\n    if sys.platform != 'win32':\n        raise NotImplementedError\n    import winreg\n    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, 'Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Explorer\\\\Shell Folders')\n    directory, _ = winreg.QueryValueEx(key, shell_folder_name)\n    return str(directory)",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        interp = 'pp3'\n    elif interp_name == 'cp':\n        interp = 'cp' + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)",
    "label": true
  },
  {
    "code": "def loadIO_source(buffer, **kwds):\n    alias = kwds.pop('alias', None)\n    source = getattr(buffer, 'getvalue', buffer)\n    if source != buffer:\n        source = source()\n    source = source.decode()\n    if not alias:\n        tag = source.strip().splitlines()[-1].split()\n        if tag[0] != '#NAME:':\n            stub = source.splitlines()[0]\n            raise IOError('unknown name for code: %s' % stub)\n        alias = tag[-1]\n    local = {}\n    exec(source, local)\n    _ = eval('%s' % alias, local)\n    return _",
    "label": true
  },
  {
    "code": "def key_value_rule(src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat) -> Pos:\n    pos, key, value = parse_key_value_pair(src, pos, parse_float)\n    key_parent, key_stem = (key[:-1], key[-1])\n    abs_key_parent = header + key_parent\n    relative_path_cont_keys = (header + key[:i] for i in range(1, len(key)))\n    for cont_key in relative_path_cont_keys:\n        if out.flags.is_(cont_key, Flags.EXPLICIT_NEST):\n            raise suffixed_err(src, pos, f'Cannot redefine namespace {cont_key}')\n        out.flags.add_pending(cont_key, Flags.EXPLICIT_NEST)\n    if out.flags.is_(abs_key_parent, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {abs_key_parent}')\n    try:\n        nest = out.data.get_or_create_nest(abs_key_parent)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if key_stem in nest:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value')\n    if isinstance(value, (dict, list)):\n        out.flags.set(header + key, Flags.FROZEN, recursive=True)\n    nest[key_stem] = value\n    return pos",
    "label": true
  },
  {
    "code": "def _sample_unweighted(iterable, k):\n    reservoir = take(k, iterable)\n    W = exp(log(random()) / k)\n    next_index = k + floor(log(random()) / log(1 - W))\n    for index, element in enumerate(iterable, k):\n        if index == next_index:\n            reservoir[randrange(k)] = element\n            W *= exp(log(random()) / k)\n            next_index += floor(log(random()) / log(1 - W)) + 1\n    return reservoir",
    "label": true
  },
  {
    "code": "def check_graphviz_availability() -> None:\n    if shutil.which('dot') is None:\n        print(\"'Graphviz' needs to be installed for your chosen output format.\")\n        sys.exit(32)",
    "label": true
  },
  {
    "code": "def safeformat(obj: object) -> str:\n    try:\n        return pprint.pformat(obj)\n    except Exception as exc:\n        return _format_repr_exception(exc, obj)",
    "label": true
  },
  {
    "code": "def type_check_simple(func: callable, args: list, expected: Union[type, tuple]) -> tuple[bool, object]:\n    try:\n        args_copy = deepcopy(args)\n        returned = func(*args_copy)\n    except Exception as exn:\n        return (False, error_message(func, args, exn))\n    if isinstance(returned, expected):\n        return (True, returned)\n    return (False, type_error_message(func.__name__, expected.__name__, returned))",
    "label": true
  },
  {
    "code": "def default_subprocess_runner(cmd, cwd=None, extra_environ=None):\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    check_call(cmd, cwd=cwd, env=env)",
    "label": true
  },
  {
    "code": "def _infer_dunder_doc_attribute(node: nodes.Module | nodes.ClassDef | nodes.FunctionDef) -> str | None:\n    try:\n        docstring = node['__doc__']\n    except KeyError:\n        return None\n    docstring = utils.safe_infer(docstring)\n    if not docstring:\n        return None\n    if not isinstance(docstring, nodes.Const):\n        return None\n    return str(docstring.value)",
    "label": true
  },
  {
    "code": "def filter_except(validator, iterable, *exceptions):\n    for item in iterable:\n        try:\n            validator(item)\n        except exceptions:\n            pass\n        else:\n            yield item",
    "label": true
  },
  {
    "code": "def write_toplevel_names(cmd, basename, filename):\n    pkgs = dict.fromkeys([k.split('.', 1)[0] for k in cmd.distribution.iter_distribution_names()])\n    cmd.write_file('top-level names', filename, '\\n'.join(sorted(pkgs)) + '\\n')",
    "label": true
  },
  {
    "code": "def build_source(location: str, *, candidates_from_page: CandidatesFromPage, page_validator: PageValidator, expand_dir: bool, cache_link_parsing: bool) -> Tuple[Optional[str], Optional[LinkSource]]:\n    path: Optional[str] = None\n    url: Optional[str] = None\n    if os.path.exists(location):\n        url = path_to_url(location)\n        path = location\n    elif location.startswith('file:'):\n        url = location\n        path = url_to_path(location)\n    elif is_url(location):\n        url = location\n    if url is None:\n        msg = \"Location '%s' is ignored: it is either a non-existing path or lacks a specific scheme.\"\n        logger.warning(msg, location)\n        return (None, None)\n    if path is None:\n        source: LinkSource = _RemoteFileSource(candidates_from_page=candidates_from_page, page_validator=page_validator, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    if os.path.isdir(path):\n        if expand_dir:\n            source = _FlatDirectorySource(candidates_from_page=candidates_from_page, path=path)\n        else:\n            source = _IndexDirectorySource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    elif os.path.isfile(path):\n        source = _LocalFileSource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    logger.warning(\"Location '%s' is ignored: it is neither a file nor a directory.\", location)\n    return (url, None)",
    "label": true
  },
  {
    "code": "def load_formatter_from_file(filename, formattername='CustomFormatter', **options):\n    try:\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        if formattername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' % (formattername, filename))\n        formatter_class = custom_namespace[formattername]\n        return formatter_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom formatter: %s' % err)",
    "label": true
  },
  {
    "code": "def extract_cookies_to_jar(jar, request, response):\n    if not (hasattr(response, '_original_response') and response._original_response):\n        return\n    req = MockRequest(request)\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)",
    "label": true
  },
  {
    "code": "def _version_from_file(lines):\n\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None",
    "label": true
  },
  {
    "code": "def _compare_eq_dict(left: Mapping[Any, Any], right: Mapping[Any, Any], verbose: int=0) -> List[str]:\n    explanation: List[str] = []\n    set_left = set(left)\n    set_right = set(right)\n    common = set_left.intersection(set_right)\n    same = {k: left[k] for k in common if left[k] == right[k]}\n    if same and verbose < 2:\n        explanation += ['Omitting %s identical items, use -vv to show' % len(same)]\n    elif same:\n        explanation += ['Common items:']\n        explanation += pprint.pformat(same).splitlines()\n    diff = {k for k in common if left[k] != right[k]}\n    if diff:\n        explanation += ['Differing items:']\n        for k in diff:\n            explanation += [saferepr({k: left[k]}) + ' != ' + saferepr({k: right[k]})]\n    extra_left = set_left - set_right\n    len_extra_left = len(extra_left)\n    if len_extra_left:\n        explanation.append('Left contains %d more item%s:' % (len_extra_left, '' if len_extra_left == 1 else 's'))\n        explanation.extend(pprint.pformat({k: left[k] for k in extra_left}).splitlines())\n    extra_right = set_right - set_left\n    len_extra_right = len(extra_right)\n    if len_extra_right:\n        explanation.append('Right contains %d more item%s:' % (len_extra_right, '' if len_extra_right == 1 else 's'))\n        explanation.extend(pprint.pformat({k: right[k] for k in extra_right}).splitlines())\n    return explanation",
    "label": true
  },
  {
    "code": "def _check_no_input(message: str) -> None:\n    if os.environ.get('PIP_NO_INPUT'):\n        raise Exception(f'No input was expected ($PIP_NO_INPUT set); question: {message}')",
    "label": true
  },
  {
    "code": "def get_entrypoints(dist: BaseDistribution) -> Tuple[Dict[str, str], Dict[str, str]]:\n    console_scripts = {}\n    gui_scripts = {}\n    for entry_point in dist.iter_entry_points():\n        if entry_point.group == 'console_scripts':\n            console_scripts[entry_point.name] = entry_point.value\n        elif entry_point.group == 'gui_scripts':\n            gui_scripts[entry_point.name] = entry_point.value\n    return (console_scripts, gui_scripts)",
    "label": true
  },
  {
    "code": "def build_function(name: str, args: list[str] | None=None, posonlyargs: list[str] | None=None, defaults: list[Any] | None=None, doc: str | None=None, kwonlyargs: list[str] | None=None, kwonlydefaults: list[Any] | None=None) -> nodes.FunctionDef:\n    func = nodes.FunctionDef(name)\n    argsnode = nodes.Arguments(parent=func)\n    if args is not None:\n        arguments = [nodes.AssignName(name=arg, parent=argsnode) for arg in args]\n    else:\n        arguments = None\n    default_nodes: list[nodes.NodeNG] | None = []\n    if defaults is not None:\n        for default in defaults:\n            default_node = nodes.const_factory(default)\n            default_node.parent = argsnode\n            default_nodes.append(default_node)\n    else:\n        default_nodes = None\n    kwonlydefault_nodes: list[nodes.NodeNG | None] | None = []\n    if kwonlydefaults is not None:\n        for kwonlydefault in kwonlydefaults:\n            kwonlydefault_node = nodes.const_factory(kwonlydefault)\n            kwonlydefault_node.parent = argsnode\n            kwonlydefault_nodes.append(kwonlydefault_node)\n    else:\n        kwonlydefault_nodes = None\n    argsnode.postinit(args=arguments, defaults=default_nodes, kwonlyargs=[nodes.AssignName(name=arg, parent=argsnode) for arg in kwonlyargs or ()], kw_defaults=kwonlydefault_nodes, annotations=[], posonlyargs=[nodes.AssignName(name=arg, parent=argsnode) for arg in posonlyargs or ()])\n    func.postinit(args=argsnode, body=[], doc_node=nodes.Const(value=doc) if doc else None)\n    if args:\n        register_arguments(func)\n    return func",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    reporter = TerminalReporter(config, sys.stdout)\n    config.pluginmanager.register(reporter, 'terminalreporter')\n    if config.option.debug or config.option.traceconfig:\n\n        def mywriter(tags, args):\n            msg = ' '.join(map(str, args))\n            reporter.write_line('[traceconfig] ' + msg)\n        config.trace.root.setprocessor('pytest:config', mywriter)",
    "label": true
  },
  {
    "code": "def get_required_dists(dists, dist):\n    if dist not in dists:\n        raise DistlibException('given distribution %r is not a member of the list' % dist.name)\n    graph = make_graph(dists)\n    req = set()\n    todo = graph.adjacency_list[dist]\n    seen = set((t[0] for t in todo))\n    while todo:\n        d = todo.pop()[0]\n        req.add(d)\n        pred_list = graph.adjacency_list[d]\n        for pred in pred_list:\n            d = pred[0]\n            if d not in req and d not in seen:\n                seen.add(d)\n                todo.append(pred)\n    return req",
    "label": true
  },
  {
    "code": "def _default_success_debug_action(instring: str, startloc: int, endloc: int, expr: 'ParserElement', toks: ParseResults, cache_hit: bool=False):\n    cache_hit_str = '*' if cache_hit else ''\n    print(f'{cache_hit_str}Matched {expr} -> {toks.as_list()}')",
    "label": true
  },
  {
    "code": "def create_Callable(args: Iterable[type], rtype: type, class_poly_vars: Set[type]=None) -> Callable:\n    poly_vars = set(class_poly_vars) if class_poly_vars else set()\n    c = Callable.copy_with(tuple([*args, rtype]))\n    poly_vars.update(_get_poly_vars(c))\n    c.__polymorphic_tvars__ = frozenset(poly_vars)\n    return c",
    "label": true
  },
  {
    "code": "def safe_range(*args: int) -> range:\n    rng = range(*args)\n    if len(rng) > MAX_RANGE:\n        raise OverflowError(f'Range too big. The sandbox blocks ranges larger than MAX_RANGE ({MAX_RANGE}).')\n    return rng",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable, key=None):\n    seen = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def _make_new_gettext(func: t.Callable[[str], str]) -> t.Callable[..., str]:\n\n    @pass_context\n    def gettext(__context: Context, __string: str, **variables: t.Any) -> str:\n        rv = __context.call(func, __string)\n        if __context.eval_ctx.autoescape:\n            rv = Markup(rv)\n        return rv % variables\n    return gettext",
    "label": true
  },
  {
    "code": "def create_tree(base_dir, files, mode=511, verbose=1, dry_run=0):\n    need_dir = set()\n    for file in files:\n        need_dir.add(os.path.join(base_dir, os.path.dirname(file)))\n    for dir in sorted(need_dir):\n        mkpath(dir, mode, verbose=verbose, dry_run=dry_run)",
    "label": true
  },
  {
    "code": "def get_build_platform():\n    from sysconfig import get_platform\n    plat = get_platform()\n    if sys.platform == 'darwin' and (not plat.startswith('macosx-')):\n        try:\n            version = _macos_vers()\n            machine = os.uname()[4].replace(' ', '_')\n            return 'macosx-%d.%d-%s' % (int(version[0]), int(version[1]), _macos_arch(machine))\n        except ValueError:\n            pass\n    return plat",
    "label": true
  },
  {
    "code": "def get_requires_for_build_sdist(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_sdist\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)",
    "label": true
  },
  {
    "code": "def generate_metadata(build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str) -> str:\n    metadata_tmpdir = TempDirectory(kind='modern-metadata', globally_managed=True)\n    metadata_dir = metadata_tmpdir.path\n    with build_env:\n        runner = runner_with_spinner_message('Preparing metadata (pyproject.toml)')\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n    return os.path.join(metadata_dir, distinfo_dir)",
    "label": true
  },
  {
    "code": "def is_postponed_evaluation_enabled(node: nodes.NodeNG) -> bool:\n    module = node.root()\n    return 'annotations' in module.future_imports",
    "label": true
  },
  {
    "code": "def _ancestors_to_call(klass_node: nodes.ClassDef, method_name: str='__init__') -> dict[nodes.ClassDef, bases.UnboundMethod]:\n    to_call: dict[nodes.ClassDef, bases.UnboundMethod] = {}\n    for base_node in klass_node.ancestors(recurs=False):\n        try:\n            init_node = next(base_node.igetattr(method_name))\n            if not isinstance(init_node, astroid.UnboundMethod):\n                continue\n            if init_node.is_abstract():\n                continue\n            to_call[base_node] = init_node\n        except astroid.InferenceError:\n            continue\n    return to_call",
    "label": true
  },
  {
    "code": "def _create_weakref(obj, *args):\n    from weakref import ref\n    if obj is None:\n        from collections import UserDict\n        return ref(UserDict(), *args)\n    return ref(obj, *args)",
    "label": true
  },
  {
    "code": "def _set_platform_dir_class() -> type[PlatformDirsABC]:\n    if sys.platform == 'win32':\n        from .windows import Windows as Result\n    elif sys.platform == 'darwin':\n        from .macos import MacOS as Result\n    else:\n        from .unix import Unix as Result\n    if os.getenv('ANDROID_DATA') == '/data' and os.getenv('ANDROID_ROOT') == '/system':\n        if os.getenv('SHELL') or os.getenv('PREFIX'):\n            return Result\n        from .android import _android_folder\n        if _android_folder() is not None:\n            from .android import Android\n            return Android\n    return Result",
    "label": true
  },
  {
    "code": "def with_class(classname, namespace=''):\n    classattr = f'{namespace}:class' if namespace else 'class'\n    return with_attribute(**{classattr: classname})",
    "label": true
  },
  {
    "code": "def _assert_no_error(error, exception_class=None):\n    if error == 0:\n        return\n    cf_error_string = Security.SecCopyErrorMessageString(error, None)\n    output = _cf_string_to_unicode(cf_error_string)\n    CoreFoundation.CFRelease(cf_error_string)\n    if output is None or output == u'':\n        output = u'OSStatus %s' % error\n    if exception_class is None:\n        exception_class = ssl.SSLError\n    raise exception_class(output)",
    "label": true
  },
  {
    "code": "def _require_version_compare(fn: Callable[['Specifier', ParsedVersion, str], bool]) -> Callable[['Specifier', ParsedVersion, str], bool]:\n\n    @functools.wraps(fn)\n    def wrapped(self: 'Specifier', prospective: ParsedVersion, spec: str) -> bool:\n        if not isinstance(prospective, Version):\n            return False\n        return fn(self, prospective, spec)\n    return wrapped",
    "label": true
  },
  {
    "code": "def platform_tags(linux: str, arch: str) -> Iterator[str]:\n    if not _have_compatible_abi(arch):\n        return\n    too_old_glibc2 = _GLibCVersion(2, 16)\n    if arch in {'x86_64', 'i686'}:\n        too_old_glibc2 = _GLibCVersion(2, 4)\n    current_glibc = _GLibCVersion(*_get_glibc_version())\n    glibc_max_list = [current_glibc]\n    for glibc_major in range(current_glibc.major - 1, 1, -1):\n        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n    for glibc_max in glibc_max_list:\n        if glibc_max.major == too_old_glibc2.major:\n            min_minor = too_old_glibc2.minor\n        else:\n            min_minor = -1\n        for glibc_minor in range(glibc_max.minor, min_minor, -1):\n            glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n            tag = 'manylinux_{}_{}'.format(*glibc_version)\n            if _is_compatible(tag, arch, glibc_version):\n                yield linux.replace('linux', tag)\n            if glibc_version in _LEGACY_MANYLINUX_MAP:\n                legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n                if _is_compatible(legacy_tag, arch, glibc_version):\n                    yield linux.replace('linux', legacy_tag)",
    "label": true
  },
  {
    "code": "def _function_type(function: nodes.Lambda | bases.UnboundMethod, builtins: nodes.Module) -> nodes.ClassDef:\n    if isinstance(function, scoped_nodes.Lambda):\n        if function.root().name == 'builtins':\n            cls_name = 'builtin_function_or_method'\n        else:\n            cls_name = 'function'\n    elif isinstance(function, bases.BoundMethod):\n        cls_name = 'method'\n    else:\n        cls_name = 'function'\n    return _build_proxy_class(cls_name, builtins)",
    "label": true
  },
  {
    "code": "def nth_product(index, *args):\n    pools = list(map(tuple, reversed(args)))\n    ns = list(map(len, pools))\n    c = reduce(mul, ns)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    result = []\n    for pool, n in zip(pools, ns):\n        result.append(pool[index % n])\n        index //= n\n    return tuple(reversed(result))",
    "label": true
  },
  {
    "code": "def process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))",
    "label": true
  },
  {
    "code": "def get_python_path(filepath: str) -> str:\n    warnings.warn(\"get_python_path has been deprecated because assumption that there's always an __init__.py is not true since python 3.3 and is causing problems, particularly with PEP 420.Use discover_package_path and pass source root(s).\", DeprecationWarning, stacklevel=2)\n    return discover_package_path(filepath, [])",
    "label": true
  },
  {
    "code": "def _looks_like_dataclass_decorator(node: nodes.NodeNG, decorator_names: frozenset[str]=DATACLASSES_DECORATORS) -> bool:\n    if isinstance(node, nodes.Call):\n        node = node.func\n    try:\n        inferred = next(node.infer())\n    except (InferenceError, StopIteration):\n        inferred = Uninferable\n    if isinstance(inferred, UninferableBase):\n        if isinstance(node, nodes.Name):\n            return node.name in decorator_names\n        if isinstance(node, nodes.Attribute):\n            return node.attrname in decorator_names\n        return False\n    return isinstance(inferred, nodes.FunctionDef) and inferred.name in decorator_names and (inferred.root().name in DATACLASS_MODULES)",
    "label": true
  },
  {
    "code": "def _collect_type_vars(types, typevar_types=None):\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if isinstance(t, typevar_types) and t not in tvars and (not _is_unpack(t)):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)",
    "label": true
  },
  {
    "code": "def _check_argument_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_snake_case(name):\n        error_msgs.append(f'Argument name \"{name}\" should be in snake_case format. Argument names should be lowercase, with words separated by underscores. A single leading underscore can be used to indicate that the argument is not being used but is still needed somehow.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def unzip_file(filename: str, location: str, flatten: bool=True) -> None:\n    ensure_dir(location)\n    zipfp = open(filename, 'rb')\n    try:\n        zip = zipfile.ZipFile(zipfp, allowZip64=True)\n        leading = has_leading_dir(zip.namelist()) and flatten\n        for info in zip.infolist():\n            name = info.filename\n            fn = name\n            if leading:\n                fn = split_leading_dir(name)[1]\n            fn = os.path.join(location, fn)\n            dir = os.path.dirname(fn)\n            if not is_within_directory(location, fn):\n                message = 'The zip file ({}) has a file ({}) trying to install outside target directory ({})'\n                raise InstallationError(message.format(filename, fn, location))\n            if fn.endswith('/') or fn.endswith('\\\\'):\n                ensure_dir(fn)\n            else:\n                ensure_dir(dir)\n                fp = zip.open(name)\n                try:\n                    with open(fn, 'wb') as destfp:\n                        shutil.copyfileobj(fp, destfp)\n                finally:\n                    fp.close()\n                    if zip_item_is_executable(info):\n                        set_extracted_file_to_default_mode_plus_executable(fn)\n    finally:\n        zipfp.close()",
    "label": true
  },
  {
    "code": "def node_frame_class(node: nodes.NodeNG) -> nodes.ClassDef | None:\n    klass = node.frame(future=True)\n    nodes_to_check = (nodes.NodeNG, astroid.UnboundMethod, astroid.BaseInstance)\n    while klass and isinstance(klass, nodes_to_check) and (not isinstance(klass, nodes.ClassDef)):\n        if klass.parent is None:\n            return None\n        klass = klass.parent.frame(future=True)\n    return klass",
    "label": true
  },
  {
    "code": "def _augment_exception(exc, version, arch=''):\n    message = exc.args[0]\n    if 'vcvarsall' in message.lower() or 'visual c' in message.lower():\n        tmpl = 'Microsoft Visual C++ {version:0.1f} or greater is required.'\n        message = tmpl.format(**locals())\n        msdownload = 'www.microsoft.com/download/details.aspx?id=%d'\n        if version == 9.0:\n            if arch.lower().find('ia64') > -1:\n                message += ' Get it with \"Microsoft Windows SDK 7.0\"'\n            else:\n                message += ' Get it from http://aka.ms/vcpython27'\n        elif version == 10.0:\n            message += ' Get it with \"Microsoft Windows SDK 7.1\": '\n            message += msdownload % 8279\n        elif version >= 14.0:\n            message += ' Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/'\n    exc.args = (message,)",
    "label": true
  },
  {
    "code": "def merge_dicts(d1: dict, d2: dict) -> dict:\n    for k, v in d2.items():\n        if k in d1 and isinstance(d1[k], dict) and isinstance(v, Mapping):\n            merge_dicts(d1[k], v)\n        else:\n            d1[k] = d2[k]",
    "label": true
  },
  {
    "code": "def build_sdist(sdist_directory, config_settings):\n    backend = _build_backend()\n    try:\n        return backend.build_sdist(sdist_directory, config_settings)\n    except getattr(backend, 'UnsupportedOperation', _DummyException):\n        raise GotUnsupportedOperation(traceback.format_exc())",
    "label": true
  },
  {
    "code": "def implements(obj: BaseChecker, interface: type[Interface] | tuple[type[Interface], ...]) -> bool:\n    warnings.warn('implements has been deprecated in favour of using basic inheritance patterns without using __implements__.', DeprecationWarning, stacklevel=2)\n    implements_ = getattr(obj, '__implements__', ())\n    if not isinstance(implements_, (list, tuple)):\n        implements_ = (implements_,)\n    return any((issubclass(i, interface) for i in implements_))",
    "label": true
  },
  {
    "code": "def docstringify(docstring: nodes.Const | None, default_type: str='default') -> Docstring:\n    best_match = (0, DOCSTRING_TYPES.get(default_type, Docstring)(docstring))\n    for docstring_type in (SphinxDocstring, EpytextDocstring, GoogleDocstring, NumpyDocstring):\n        instance = docstring_type(docstring)\n        matching_sections = instance.matching_sections()\n        if matching_sections > best_match[0]:\n            best_match = (matching_sections, instance)\n    return best_match[1]",
    "label": true
  },
  {
    "code": "def _write_contents(target, source):\n    child = target.joinpath(source.name)\n    if source.is_dir():\n        child.mkdir()\n        for item in source.iterdir():\n            _write_contents(child, item)\n    else:\n        child.write_bytes(source.read_bytes())\n    return child",
    "label": true
  },
  {
    "code": "def _looks_like_dataclass_field_call(node: nodes.Call, check_scope: bool=True) -> bool:\n    if check_scope:\n        stmt = node.statement(future=True)\n        scope = stmt.scope()\n        if not (isinstance(stmt, nodes.AnnAssign) and stmt.value is not None and isinstance(scope, nodes.ClassDef) and is_decorated_with_dataclass(scope)):\n            return False\n    try:\n        inferred = next(node.func.infer())\n    except (InferenceError, StopIteration):\n        return False\n    if not isinstance(inferred, nodes.FunctionDef):\n        return False\n    return inferred.name == FIELD_NAME and inferred.root().name in DATACLASS_MODULES",
    "label": true
  },
  {
    "code": "def _parse_version_parts(s: str) -> Iterator[str]:\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n        if not part or part == '.':\n            continue\n        if part[:1] in '0123456789':\n            yield part.zfill(8)\n        else:\n            yield ('*' + part)\n    yield '*final'",
    "label": true
  },
  {
    "code": "def _ssl_wrap_socket_impl(sock, ssl_context, tls_in_tls, server_hostname=None):\n    if tls_in_tls:\n        if not SSLTransport:\n            raise ProxySchemeUnsupported(\"TLS in TLS requires support for the 'ssl' module\")\n        SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n        return SSLTransport(sock, ssl_context, server_hostname)\n    if server_hostname:\n        return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n    else:\n        return ssl_context.wrap_socket(sock)",
    "label": true
  },
  {
    "code": "def rewind_body(body: typing.IO[typing.AnyStr], body_pos: _TYPE_BODY_POSITION) -> None:\n    body_seek = getattr(body, 'seek', None)\n    if body_seek is not None and isinstance(body_pos, int):\n        try:\n            body_seek(body_pos)\n        except OSError as e:\n            raise UnrewindableBodyError('An error occurred when rewinding request body for redirect/retry.') from e\n    elif body_pos is _FAILEDTELL:\n        raise UnrewindableBodyError('Unable to record file position for rewinding request body during a redirect/retry.')\n    else:\n        raise ValueError(f'body_pos must be of type integer, instead it was {type(body_pos)}.')",
    "label": true
  },
  {
    "code": "def match_only_at_col(n):\n\n    def verify_col(strg, locn, toks):\n        if col(locn, strg) != n:\n            raise ParseException(strg, locn, f'matched token not at column {n}')\n    return verify_col",
    "label": true
  },
  {
    "code": "def parse_inline_table(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, dict]:\n    pos += 1\n    nested_dict = NestedDict()\n    flags = Flags()\n    pos = skip_chars(src, pos, TOML_WS)\n    if src.startswith('}', pos):\n        return (pos + 1, nested_dict.dict)\n    while True:\n        pos, key, value = parse_key_value_pair(src, pos, parse_float)\n        key_parent, key_stem = (key[:-1], key[-1])\n        if flags.is_(key, Flags.FROZEN):\n            raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n        try:\n            nest = nested_dict.get_or_create_nest(key_parent, access_lists=False)\n        except KeyError:\n            raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n        if key_stem in nest:\n            raise suffixed_err(src, pos, f'Duplicate inline table key {key_stem!r}')\n        nest[key_stem] = value\n        pos = skip_chars(src, pos, TOML_WS)\n        c = src[pos:pos + 1]\n        if c == '}':\n            return (pos + 1, nested_dict.dict)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed inline table')\n        if isinstance(value, (dict, list)):\n            flags.set(key, Flags.FROZEN, recursive=True)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def check_legacy_setup_py_options(options: Values, reqs: List[InstallRequirement]) -> None:\n    has_build_options = _has_option(options, reqs, 'build_options')\n    has_global_options = _has_option(options, reqs, 'global_options')\n    if has_build_options or has_global_options:\n        deprecated(reason='--build-option and --global-option are deprecated.', issue=11859, replacement='to use --config-settings', gone_in='23.3')\n        logger.warning('Implying --no-binary=:all: due to the presence of --build-option / --global-option. ')\n        options.format_control.disallow_binaries()",
    "label": true
  },
  {
    "code": "def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    if session_hooks is None or session_hooks.get('response') == []:\n        return request_hooks\n    if request_hooks is None or request_hooks.get('response') == []:\n        return session_hooks\n    return merge_setting(request_hooks, session_hooks, dict_class)",
    "label": true
  },
  {
    "code": "def unpack(path: str, dest: str='.') -> None:\n    with WheelFile(path) as wf:\n        namever = wf.parsed_filename.group('namever')\n        destination = Path(dest) / namever\n        print(f'Unpacking to: {destination}...', end='', flush=True)\n        for zinfo in wf.filelist:\n            wf.extract(zinfo, destination)\n            permissions = zinfo.external_attr >> 16 & 511\n            destination.joinpath(zinfo.filename).chmod(permissions)\n    print('OK')",
    "label": true
  },
  {
    "code": "def doubler(f):\n\n    def inner(*args, **kwds):\n        fx = f(*args, **kwds)\n        return 2 * fx\n    return inner",
    "label": true
  },
  {
    "code": "def fancy_getopt(options, negative_opt, object, args):\n    parser = FancyGetopt(options)\n    parser.set_negative_aliases(negative_opt)\n    return parser.getopt(args, object)",
    "label": true
  },
  {
    "code": "def parse_key_value_pair(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, Key, Any]:\n    pos, key = parse_key(src, pos)\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char != '=':\n        raise suffixed_err(src, pos, \"Expected '=' after a key in a key/value pair\")\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, value = parse_value(src, pos, parse_float)\n    return (pos, key, value)",
    "label": true
  },
  {
    "code": "def setup_logging(verbosity: int, no_color: bool, user_log_file: Optional[str]) -> int:\n    if verbosity >= 2:\n        level_number = logging.DEBUG\n    elif verbosity == 1:\n        level_number = VERBOSE\n    elif verbosity == -1:\n        level_number = logging.WARNING\n    elif verbosity == -2:\n        level_number = logging.ERROR\n    elif verbosity <= -3:\n        level_number = logging.CRITICAL\n    else:\n        level_number = logging.INFO\n    level = logging.getLevelName(level_number)\n    include_user_log = user_log_file is not None\n    if include_user_log:\n        additional_log_file = user_log_file\n        root_level = 'DEBUG'\n    else:\n        additional_log_file = '/dev/null'\n        root_level = level\n    vendored_log_level = 'WARNING' if level in ['INFO', 'ERROR'] else 'DEBUG'\n    log_streams = {'stdout': 'ext://sys.stdout', 'stderr': 'ext://sys.stderr'}\n    handler_classes = {'stream': 'pip._internal.utils.logging.RichPipStreamHandler', 'file': 'pip._internal.utils.logging.BetterRotatingFileHandler'}\n    handlers = ['console', 'console_errors', 'console_subprocess'] + (['user_log'] if include_user_log else [])\n    logging.config.dictConfig({'version': 1, 'disable_existing_loggers': False, 'filters': {'exclude_warnings': {'()': 'pip._internal.utils.logging.MaxLevelFilter', 'level': logging.WARNING}, 'restrict_to_subprocess': {'()': 'logging.Filter', 'name': subprocess_logger.name}, 'exclude_subprocess': {'()': 'pip._internal.utils.logging.ExcludeLoggerFilter', 'name': subprocess_logger.name}}, 'formatters': {'indent': {'()': IndentingFormatter, 'format': '%(message)s'}, 'indent_with_timestamp': {'()': IndentingFormatter, 'format': '%(message)s', 'add_timestamp': True}}, 'handlers': {'console': {'level': level, 'class': handler_classes['stream'], 'no_color': no_color, 'stream': log_streams['stdout'], 'filters': ['exclude_subprocess', 'exclude_warnings'], 'formatter': 'indent'}, 'console_errors': {'level': 'WARNING', 'class': handler_classes['stream'], 'no_color': no_color, 'stream': log_streams['stderr'], 'filters': ['exclude_subprocess'], 'formatter': 'indent'}, 'console_subprocess': {'level': level, 'class': handler_classes['stream'], 'stream': log_streams['stderr'], 'no_color': no_color, 'filters': ['restrict_to_subprocess'], 'formatter': 'indent'}, 'user_log': {'level': 'DEBUG', 'class': handler_classes['file'], 'filename': additional_log_file, 'encoding': 'utf-8', 'delay': True, 'formatter': 'indent_with_timestamp'}}, 'root': {'level': root_level, 'handlers': handlers}, 'loggers': {'pip._vendor': {'level': vendored_log_level}}})\n    return level_number",
    "label": true
  },
  {
    "code": "def unpack(src_dir, dst_dir):\n    for dirpath, dirnames, filenames in os.walk(src_dir):\n        subdir = os.path.relpath(dirpath, src_dir)\n        for f in filenames:\n            src = os.path.join(dirpath, f)\n            dst = os.path.join(dst_dir, subdir, f)\n            os.renames(src, dst)\n        for n, d in reversed(list(enumerate(dirnames))):\n            src = os.path.join(dirpath, d)\n            dst = os.path.join(dst_dir, subdir, d)\n            if not os.path.exists(dst):\n                os.renames(src, dst)\n                del dirnames[n]\n    for dirpath, dirnames, filenames in os.walk(src_dir, topdown=True):\n        assert not filenames\n        os.rmdir(dirpath)",
    "label": true
  },
  {
    "code": "def _have_compatible_abi(executable: str, archs: Sequence[str]) -> bool:\n    if 'armv7l' in archs:\n        return _is_linux_armhf(executable)\n    if 'i686' in archs:\n        return _is_linux_i686(executable)\n    allowed_archs = {'x86_64', 'aarch64', 'ppc64', 'ppc64le', 's390x', 'loongarch64'}\n    return any((arch in allowed_archs for arch in archs))",
    "label": true
  },
  {
    "code": "def load(fp: IO, *, parse_float: ParseFloat=float) -> Dict[str, Any]:\n    s = fp.read()\n    if isinstance(s, bytes):\n        s = s.decode()\n    else:\n        warnings.warn('Text file object support is deprecated in favor of binary file objects. Use `open(\"foo.toml\", \"rb\")` to open the file in binary mode.', DeprecationWarning)\n    return loads(s, parse_float=parse_float)",
    "label": true
  },
  {
    "code": "def get_optionflags(parent):\n    optionflags_str = parent.config.getini('doctest_optionflags')\n    flag_lookup_table = _get_flag_lookup()\n    flag_acc = 0\n    for flag in optionflags_str:\n        flag_acc |= flag_lookup_table[flag]\n    return flag_acc",
    "label": true
  },
  {
    "code": "def is_decorated_with_attrs(node, decorator_names=ATTRS_NAMES) -> bool:\n    if not node.decorators:\n        return False\n    for decorator_attribute in node.decorators.nodes:\n        if isinstance(decorator_attribute, Call):\n            decorator_attribute = decorator_attribute.func\n        if decorator_attribute.as_string() in decorator_names:\n            return True\n        inferred = safe_infer(decorator_attribute)\n        if inferred and inferred.root().name == 'attr._next_gen':\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _get_codes_helper(tree: HuffmanTree, path: str) -> dict[int, str]:\n    if tree.is_leaf():\n        return {tree.symbol: path}\n    else:\n        left = _get_codes_helper(tree.left, path + '0')\n        right = _get_codes_helper(tree.right, path + '1')\n        left.update(right)\n        return left",
    "label": true
  },
  {
    "code": "def _infer_user() -> str:\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API('user')\n    if is_osx_framework() and (not running_under_virtualenv()):\n        suffixed = 'osx_framework_user'\n    else:\n        suffixed = f'{os.name}_user'\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if 'posix_user' not in _AVAILABLE_SCHEMES:\n        raise UserInstallationInvalid()\n    return 'posix_user'",
    "label": true
  },
  {
    "code": "def get_bin_prefix() -> str:\n    prefix = os.path.normpath(sys.prefix)\n    if WINDOWS:\n        bin_py = os.path.join(prefix, 'Scripts')\n        if not os.path.exists(bin_py):\n            bin_py = os.path.join(prefix, 'bin')\n        return bin_py\n    if sys.platform[:6] == 'darwin' and prefix[:16] == '/System/Library/':\n        return '/usr/local/bin'\n    return os.path.join(prefix, 'bin')",
    "label": true
  },
  {
    "code": "def escape(pathname):\n    drive, pathname = os.path.splitdrive(pathname)\n    if isinstance(pathname, bytes):\n        pathname = magic_check_bytes.sub(b'[\\\\1]', pathname)\n    else:\n        pathname = magic_check.sub('[\\\\1]', pathname)\n    return drive + pathname",
    "label": true
  },
  {
    "code": "def constrained_batches(iterable, max_size, max_count=None, get_len=len, strict=True):\n    if max_size <= 0:\n        raise ValueError('maximum size must be greater than zero')\n    batch = []\n    batch_size = 0\n    batch_count = 0\n    for item in iterable:\n        item_len = get_len(item)\n        if strict and item_len > max_size:\n            raise ValueError('item size exceeds maximum size')\n        reached_count = batch_count == max_count\n        reached_size = item_len + batch_size > max_size\n        if batch_count and (reached_size or reached_count):\n            yield tuple(batch)\n            batch.clear()\n            batch_size = 0\n            batch_count = 0\n        batch.append(item)\n        batch_size += item_len\n        batch_count += 1\n    if batch:\n        yield tuple(batch)",
    "label": true
  },
  {
    "code": "def _hash_dict(d: Dict[str, str]) -> str:\n    s = json.dumps(d, sort_keys=True, separators=(',', ':'), ensure_ascii=True)\n    return hashlib.sha224(s.encode('ascii')).hexdigest()",
    "label": true
  },
  {
    "code": "def optimize(node: nodes.Node, environment: 'Environment') -> nodes.Node:\n    optimizer = Optimizer(environment)\n    return t.cast(nodes.Node, optimizer.visit(node))",
    "label": true
  },
  {
    "code": "def filesys_decode(path):\n    if isinstance(path, str):\n        return path\n    fs_enc = sys.getfilesystemencoding() or 'utf-8'\n    candidates = (fs_enc, 'utf-8')\n    for enc in candidates:\n        try:\n            return path.decode(enc)\n        except UnicodeDecodeError:\n            continue",
    "label": true
  },
  {
    "code": "def condition_as_parse_action(fn: ParseCondition, message: typing.Optional[str]=None, fatal: bool=False) -> ParseAction:\n    msg = message if message is not None else 'failed user-defined condition'\n    exc_type = ParseFatalException if fatal else ParseException\n    fn = _trim_arity(fn)\n\n    @wraps(fn)\n    def pa(s, l, t):\n        if not bool(fn(s, l, t)):\n            raise exc_type(s, l, msg)\n    return pa",
    "label": true
  },
  {
    "code": "def _is_in_upper_case_with_underscores(name: str) -> bool:\n    pattern = '(_?[A-Z][A-Z0-9_]*)$'\n    return re.match(pattern, name) is not None",
    "label": true
  },
  {
    "code": "def _get_external_data(url):\n    result = {}\n    try:\n        resp = urlopen(url)\n        headers = resp.info()\n        ct = headers.get('Content-Type')\n        if not ct.startswith('application/json'):\n            logger.debug('Unexpected response for JSON request: %s', ct)\n        else:\n            reader = codecs.getreader('utf-8')(resp)\n            result = json.load(reader)\n    except Exception as e:\n        logger.exception('Failed to get external data for %s: %s', url, e)\n    return result",
    "label": true
  },
  {
    "code": "def unsafe(f: F) -> F:\n    f.unsafe_callable = True\n    return f",
    "label": true
  },
  {
    "code": "def urlsafe_b64decode(data: bytes) -> bytes:\n    pad = b'=' * (4 - (len(data) & 3))\n    return base64.urlsafe_b64decode(data + pad)",
    "label": true
  },
  {
    "code": "def _build_result(state):\n    mapping = state.mapping\n    all_keys = {id(v): k for k, v in mapping.items()}\n    all_keys[id(None)] = None\n    graph = DirectedGraph()\n    graph.add(None)\n    connected = {None}\n    for key, criterion in state.criteria.items():\n        if not _has_route_to_root(state.criteria, key, all_keys, connected):\n            continue\n        if key not in graph:\n            graph.add(key)\n        for p in criterion.iter_parent():\n            try:\n                pkey = all_keys[id(p)]\n            except KeyError:\n                continue\n            if pkey not in graph:\n                graph.add(pkey)\n            graph.connect(pkey, key)\n    return Result(mapping={k: v for k, v in mapping.items() if k in connected}, graph=graph, criteria=state.criteria)",
    "label": true
  },
  {
    "code": "def direct_url_as_pep440_direct_reference(direct_url: DirectUrl, name: str) -> str:\n    direct_url.validate()\n    requirement = name + ' @ '\n    fragments = []\n    if isinstance(direct_url.info, VcsInfo):\n        requirement += '{}+{}@{}'.format(direct_url.info.vcs, direct_url.url, direct_url.info.commit_id)\n    elif isinstance(direct_url.info, ArchiveInfo):\n        requirement += direct_url.url\n        if direct_url.info.hash:\n            fragments.append(direct_url.info.hash)\n    else:\n        assert isinstance(direct_url.info, DirInfo)\n        requirement += direct_url.url\n    if direct_url.subdirectory:\n        fragments.append('subdirectory=' + direct_url.subdirectory)\n    if fragments:\n        requirement += '#' + '&'.join(fragments)\n    return requirement",
    "label": true
  },
  {
    "code": "def connection_requires_http_tunnel(proxy_url=None, proxy_config=None, destination_scheme=None):\n    if proxy_url is None:\n        return False\n    if destination_scheme == 'http':\n        return False\n    if proxy_url.scheme == 'https' and proxy_config and proxy_config.use_forwarding_for_https:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _set_config(dist: 'Distribution', field: str, value: Any):\n    setter = getattr(dist.metadata, f'set_{field}', None)\n    if setter:\n        setter(value)\n    elif hasattr(dist.metadata, field) or field in SETUPTOOLS_PATCHES:\n        setattr(dist.metadata, field, value)\n    else:\n        setattr(dist, field, value)",
    "label": true
  },
  {
    "code": "def _looks_like_wheel(location: str) -> bool:\n    if not location.endswith(WHEEL_EXTENSION):\n        return False\n    if not os.path.isfile(location):\n        return False\n    if not Wheel.wheel_file_re.match(os.path.basename(location)):\n        return False\n    return zipfile.is_zipfile(location)",
    "label": true
  },
  {
    "code": "def _expand_default(self: optparse.HelpFormatter, option: Option) -> str:\n    if self.parser is None or not self.default_tag:\n        return str(option.help)\n    optname = option._long_opts[0][2:]\n    try:\n        provider = self.parser.options_manager._all_options[optname]\n    except KeyError:\n        value = None\n    else:\n        optdict = provider.get_option_def(optname)\n        optname = provider.option_attrname(optname, optdict)\n        value = getattr(provider.config, optname, optdict)\n        value = utils._format_option_value(optdict, value)\n    if value is optparse.NO_DEFAULT or not value:\n        value = self.NO_DEFAULT_VALUE\n    return option.help.replace(self.default_tag, str(value))",
    "label": true
  },
  {
    "code": "def is_class_attr(name: str, klass: nodes.ClassDef) -> bool:\n    try:\n        klass.getattr(name)\n        return True\n    except astroid.NotFoundError:\n        return False",
    "label": true
  },
  {
    "code": "def _remove_path_dot_segments(path: str) -> str:\n    segments = path.split('/')\n    output = []\n    for segment in segments:\n        if segment == '.':\n            continue\n        if segment != '..':\n            output.append(segment)\n        elif output:\n            output.pop()\n    if path.startswith('/') and (not output or output[0]):\n        output.insert(0, '')\n    if path.endswith(('/.', '/..')):\n        output.append('')\n    return '/'.join(output)",
    "label": true
  },
  {
    "code": "def _replace_return_val_assertion(assertion: str, return_val_var_name: Optional[str]) -> str:\n    if FUNCTION_RETURN_VALUE in assertion:\n        return assertion.replace(FUNCTION_RETURN_VALUE, return_val_var_name)\n    return assertion",
    "label": true
  },
  {
    "code": "def report_total_messages_stats(sect: Section, stats: LinterStats, previous_stats: LinterStats | None) -> None:\n    lines = ['type', 'number', 'previous', 'difference']\n    lines += checkers.table_lines_from_stats(stats, previous_stats, 'message_types')\n    sect.append(Table(children=lines, cols=4, rheaders=1))",
    "label": true
  },
  {
    "code": "def emit_pragma_representer(action: str, messages: list[str]) -> PragmaRepresenter:\n    if not messages and action in MESSAGE_KEYWORDS:\n        raise InvalidPragmaError('The keyword is not followed by message identifier', action)\n    return PragmaRepresenter(action, messages)",
    "label": true
  },
  {
    "code": "def _detect_program_name(path: t.Optional[str]=None, _main: t.Optional[ModuleType]=None) -> str:\n    if _main is None:\n        _main = sys.modules['__main__']\n    if not path:\n        path = sys.argv[0]\n    if getattr(_main, '__package__', None) in {None, ''} or (os.name == 'nt' and _main.__package__ == '' and (not os.path.exists(path)) and os.path.exists(f'{path}.exe')):\n        return os.path.basename(path)\n    py_module = t.cast(str, _main.__package__)\n    name = os.path.splitext(os.path.basename(path))[0]\n    if name != '__main__':\n        py_module = f'{py_module}.{name}'\n    return f\"python -m {py_module.lstrip('.')}\"",
    "label": true
  },
  {
    "code": "def _qualified_names(modname: str | None) -> list[str]:\n    names = modname.split('.') if modname is not None else ''\n    return ['.'.join(names[0:i + 1]) for i in range(len(names))]",
    "label": true
  },
  {
    "code": "def original_text_for(expr: ParserElement, as_string: bool=True, *, asString: bool=True) -> ParserElement:\n    asString = asString and as_string\n    locMarker = Empty().set_parse_action(lambda s, loc, t: loc)\n    endlocMarker = locMarker.copy()\n    endlocMarker.callPreparse = False\n    matchExpr = locMarker('_original_start') + expr + endlocMarker('_original_end')\n    if asString:\n        extractText = lambda s, l, t: s[t._original_start:t._original_end]\n    else:\n\n        def extractText(s, l, t):\n            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]\n    matchExpr.set_parse_action(extractText)\n    matchExpr.ignoreExprs = expr.ignoreExprs\n    matchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)\n    return matchExpr",
    "label": true
  },
  {
    "code": "def _nose_tools_trivial_transform():\n    stub = _BUILDER.string_build('__all__ = []')\n    all_entries = ['ok_', 'eq_']\n    for pep8_name, method in _nose_tools_functions():\n        all_entries.append(pep8_name)\n        stub[pep8_name] = method\n    all_assign = stub['__all__'].parent\n    all_object = astroid.List(all_entries)\n    all_object.parent = all_assign\n    all_assign.value = all_object\n    return stub",
    "label": true
  },
  {
    "code": "def _index_name_nodes(index: str, loop_node: Union[nodes.For, nodes.Comprehension]) -> List[Union[nodes.AssignName, nodes.Name]]:\n    scope = loop_node.scope()\n    if isinstance(loop_node, nodes.For):\n        body = loop_node\n    else:\n        body = loop_node.parent\n    return [name_node for name_node in body.nodes_of_class((nodes.AssignName, nodes.Name)) if name_node.name == index and name_node != loop_node.target and (name_node.lookup(name_node.name)[0] == scope)]",
    "label": true
  },
  {
    "code": "def _get_user_dirs_folder(key: str) -> str | None:\n    user_dirs_config_path = Path(Unix().user_config_dir) / 'user-dirs.dirs'\n    if user_dirs_config_path.exists():\n        parser = ConfigParser()\n        with user_dirs_config_path.open() as stream:\n            parser.read_string(f'[top]\\n{stream.read()}')\n        if key not in parser['top']:\n            return None\n        path = parser['top'][key].strip('\"')\n        return path.replace('$HOME', os.path.expanduser('~'))\n    return None",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def do_override():\n    if enabled():\n        warn_distutils_present()\n        ensure_local_distutils()",
    "label": true
  },
  {
    "code": "def _csv_open(fn, mode, **kwargs):\n    if sys.version_info[0] < 3:\n        mode += 'b'\n    else:\n        kwargs['newline'] = ''\n        kwargs['encoding'] = 'utf-8'\n    return open(fn, mode, **kwargs)",
    "label": true
  },
  {
    "code": "def unpack_directory(filename, extract_dir, progress_filter=default_filter):\n    if not os.path.isdir(filename):\n        raise UnrecognizedFormat('%s is not a directory' % filename)\n    paths = {filename: ('', extract_dir)}\n    for base, dirs, files in os.walk(filename):\n        src, dst = paths[base]\n        for d in dirs:\n            paths[os.path.join(base, d)] = (src + d + '/', os.path.join(dst, d))\n        for f in files:\n            target = os.path.join(dst, f)\n            target = progress_filter(src + f, target)\n            if not target:\n                continue\n            ensure_directory(target)\n            f = os.path.join(base, f)\n            shutil.copyfile(f, target)\n            shutil.copystat(f, target)",
    "label": true
  },
  {
    "code": "def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n    formats = [cpu_arch]\n    if cpu_arch == 'x86_64':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat64', 'fat32'])\n    elif cpu_arch == 'i386':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat32', 'fat'])\n    elif cpu_arch == 'ppc64':\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append('fat64')\n    elif cpu_arch == 'ppc':\n        if version > (10, 6):\n            return []\n        formats.extend(['fat32', 'fat'])\n    if cpu_arch in {'arm64', 'x86_64'}:\n        formats.append('universal2')\n    if cpu_arch in {'x86_64', 'i386', 'ppc64', 'ppc', 'intel'}:\n        formats.append('universal')\n    return formats",
    "label": true
  },
  {
    "code": "def read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:\n    try:\n        return source.read(path)\n    except (BadZipFile, KeyError, RuntimeError) as e:\n        raise UnsupportedWheel(f'could not read {path!r} file: {e!r}')",
    "label": true
  },
  {
    "code": "def get_supported_platform():\n    plat = get_build_platform()\n    m = macosVersionString.match(plat)\n    if m is not None and sys.platform == 'darwin':\n        try:\n            plat = 'macosx-%s-%s' % ('.'.join(_macos_vers()[:2]), m.group(3))\n        except ValueError:\n            pass\n    return plat",
    "label": true
  },
  {
    "code": "def from_package(package: types.ModuleType):\n    spec = wrap_spec(package)\n    reader = spec.loader.get_resource_reader(spec.name)\n    return reader.files()",
    "label": true
  },
  {
    "code": "def change_root(new_root, pathname):\n    if os.name == 'posix':\n        if not os.path.isabs(pathname):\n            return os.path.join(new_root, pathname)\n        else:\n            return os.path.join(new_root, pathname[1:])\n    elif os.name == 'nt':\n        drive, path = os.path.splitdrive(pathname)\n        if path[0] == '\\\\':\n            path = path[1:]\n        return os.path.join(new_root, path)\n    raise DistutilsPlatformError(f\"nothing known about platform '{os.name}'\")",
    "label": true
  },
  {
    "code": "def format_full_version(info: 'sys._version_info') -> str:\n    version = '{0.major}.{0.minor}.{0.micro}'.format(info)\n    kind = info.releaselevel\n    if kind != 'final':\n        version += kind[0] + str(info.serial)\n    return version",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        interp = 'pp3'\n    elif interp_name == 'cp':\n        interp = 'cp' + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)",
    "label": true
  },
  {
    "code": "def check_list_path_option(options: Values) -> None:\n    if options.path and (options.user or options.local):\n        raise CommandError(\"Cannot combine '--path' with '--user' or '--local'\")",
    "label": true
  },
  {
    "code": "def update_dist_caches(dist_path, fix_zipimporter_caches):\n    normalized_path = normalize_path(dist_path)\n    _uncache(normalized_path, sys.path_importer_cache)\n    if fix_zipimporter_caches:\n        _replace_zip_directory_cache_data(normalized_path)\n    else:\n        _remove_and_clear_zip_directory_cache_data(normalized_path)",
    "label": true
  },
  {
    "code": "def before_log(logger: 'logging.Logger', log_level: int) -> typing.Callable[['RetryCallState'], None]:\n\n    def log_it(retry_state: 'RetryCallState') -> None:\n        if retry_state.fn is None:\n            fn_name = '<unknown>'\n        else:\n            fn_name = _utils.get_callback_name(retry_state.fn)\n        logger.log(log_level, f\"Starting call to '{fn_name}', this is the {_utils.to_ordinal(retry_state.attempt_number)} time calling it.\")\n    return log_it",
    "label": true
  },
  {
    "code": "def pkginfo_to_metadata(egg_info_path: str, pkginfo_path: str) -> Message:\n    with open(pkginfo_path, encoding='utf-8') as headers:\n        pkg_info = Parser().parse(headers)\n    pkg_info.replace_header('Metadata-Version', '2.1')\n    del pkg_info['Provides-Extra']\n    del pkg_info['Requires-Dist']\n    requires_path = os.path.join(egg_info_path, 'requires.txt')\n    if os.path.exists(requires_path):\n        with open(requires_path, encoding='utf-8') as requires_file:\n            requires = requires_file.read()\n        parsed_requirements = sorted(split_sections(requires), key=lambda x: x[0] or '')\n        for extra, reqs in parsed_requirements:\n            for key, value in generate_requirements({extra: reqs}):\n                if (key, value) not in pkg_info.items():\n                    pkg_info[key] = value\n    description = pkg_info['Description']\n    if description:\n        description_lines = pkg_info['Description'].splitlines()\n        dedented_description = '\\n'.join((description_lines[0].lstrip(), textwrap.dedent('\\n'.join(description_lines[1:])), '\\n'))\n        pkg_info.set_payload(dedented_description)\n        del pkg_info['Description']\n    return pkg_info",
    "label": true
  },
  {
    "code": "def _imp(*args, **kwds):\n    before = set(sys.modules.keys())\n    mod = __import__(*args, **kwds)\n    after = set(sys.modules.keys()).difference(before)\n    for m in after:\n        memorise(sys.modules[m])\n    return mod",
    "label": true
  },
  {
    "code": "def get_unpatched(item):\n    lookup = get_unpatched_class if isinstance(item, type) else get_unpatched_function if isinstance(item, types.FunctionType) else lambda item: None\n    return lookup(item)",
    "label": true
  },
  {
    "code": "def make_setuptools_bdist_wheel_args(setup_py_path: str, global_options: Sequence[str], build_options: Sequence[str], destination_dir: str) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, global_options=global_options, unbuffered_output=True)\n    args += ['bdist_wheel', '-d', destination_dir]\n    args += build_options\n    return args",
    "label": true
  },
  {
    "code": "def load_module_from_name(dotted_name: str) -> types.ModuleType:\n    try:\n        return sys.modules[dotted_name]\n    except KeyError:\n        pass\n    with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(io.StringIO()) as stdout:\n        module = importlib.import_module(dotted_name)\n    stderr_value = stderr.getvalue()\n    if stderr_value:\n        logger.error('Captured stderr while importing %s:\\n%s', dotted_name, stderr_value)\n    stdout_value = stdout.getvalue()\n    if stdout_value:\n        logger.info('Captured stdout while importing %s:\\n%s', dotted_name, stdout_value)\n    return module",
    "label": true
  },
  {
    "code": "def install(console: Optional['Console']=None, overflow: 'OverflowMethod'='ignore', crop: bool=False, indent_guides: bool=False, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> None:\n    from pip._vendor.rich import get_console\n    console = console or get_console()\n    assert console is not None\n\n    def display_hook(value: Any) -> None:\n        \"\"\"Replacement sys.displayhook which prettifies objects with Rich.\"\"\"\n        if value is not None:\n            assert console is not None\n            builtins._ = None\n            console.print(value if _safe_isinstance(value, RichRenderable) else Pretty(value, overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all), crop=crop)\n            builtins._ = value\n    if 'get_ipython' in globals():\n        ip = get_ipython()\n        from IPython.core.formatters import BaseFormatter\n\n        class RichFormatter(BaseFormatter):\n            pprint: bool = True\n\n            def __call__(self, value: Any) -> Any:\n                if self.pprint:\n                    return _ipy_display_hook(value, console=get_console(), overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all)\n                else:\n                    return repr(value)\n        rich_formatter = RichFormatter()\n        ip.display_formatter.formatters['text/plain'] = rich_formatter\n    else:\n        sys.displayhook = display_hook",
    "label": true
  },
  {
    "code": "def split_provision(value):\n    global _provision_rx\n    if _provision_rx is None:\n        _provision_rx = re.compile('([a-zA-Z_]\\\\w*(?:\\\\.[a-zA-Z_]\\\\w*)*)(?:\\\\s*\\\\(\\\\s*([^)\\\\s]+)\\\\s*\\\\))?$', re.ASCII)\n    value = value.strip()\n    m = _provision_rx.match(value)\n    if not m:\n        raise ValueError('illegal provides specification: %r' % value)\n    ver = m.group(2) or None\n    if ver:\n        with version.suppress_known_deprecation():\n            ver = version.StrictVersion(ver)\n    return (m.group(1), ver)",
    "label": true
  },
  {
    "code": "def test_lambdify():\n    try:\n        from sympy import symbols, lambdify\n    except ImportError:\n        return\n    settings['recurse'] = True\n    x = symbols('x')\n    y = x ** 2\n    f = lambdify([x], y)\n    z = min\n    d = globals()\n    globalvars(f, recurse=True, builtin=True)\n    assert z is min\n    assert d is globals()",
    "label": true
  },
  {
    "code": "def _select_scheme(ob, name):\n    scheme = _inject_headers(name, _load_scheme(_resolve_scheme(name)))\n    vars(ob).update(_remove_set(ob, _scheme_attrs(scheme)))",
    "label": true
  },
  {
    "code": "def store_mark(obj, mark: Mark) -> None:\n    assert isinstance(mark, Mark), mark\n    obj.pytestmark = [*get_unpacked_marks(obj, consider_mro=False), mark]",
    "label": true
  },
  {
    "code": "def pytest_collection_modifyitems(items: List[nodes.Item], config: Config) -> None:\n    deselect_prefixes = tuple(config.getoption('deselect') or [])\n    if not deselect_prefixes:\n        return\n    remaining = []\n    deselected = []\n    for colitem in items:\n        if colitem.nodeid.startswith(deselect_prefixes):\n            deselected.append(colitem)\n        else:\n            remaining.append(colitem)\n    if deselected:\n        config.hook.pytest_deselected(items=deselected)\n        items[:] = remaining",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def request(method, url, **kwargs):\n    with sessions.Session() as session:\n        return session.request(method=method, url=url, **kwargs)",
    "label": true
  },
  {
    "code": "def make_analysator(f):\n\n    def text_analyse(text):\n        try:\n            rv = f(text)\n        except Exception:\n            return 0.0\n        if not rv:\n            return 0.0\n        try:\n            return min(1.0, max(0.0, float(rv)))\n        except (ValueError, TypeError):\n            return 0.0\n    text_analyse.__doc__ = f.__doc__\n    return staticmethod(text_analyse)",
    "label": true
  },
  {
    "code": "def new_month(customer_list: list[Customer], month: int, year: int) -> None:\n    for cust in customer_list:\n        cust.new_month(month, year)",
    "label": true
  },
  {
    "code": "def wrap_object_attribute(module, name, factory, args=(), kwargs={}):\n    path, attribute = name.rsplit('.', 1)\n    parent = resolve_path(module, path)[2]\n    wrapper = AttributeWrapper(attribute, factory, args, kwargs)\n    apply_patch(parent, attribute, wrapper)\n    return wrapper",
    "label": true
  },
  {
    "code": "def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:\n    if not tokenizer.check('SEMICOLON'):\n        tokenizer.raise_syntax_error(f'Expected end or semicolon (after {after})', span_start=span_start)\n    tokenizer.read()\n    marker = _parse_marker(tokenizer)\n    tokenizer.consume('WS')\n    return marker",
    "label": true
  },
  {
    "code": "def invalid_marker(text):\n    try:\n        evaluate_marker(text)\n    except SyntaxError as e:\n        e.filename = None\n        e.lineno = None\n        return e\n    return False",
    "label": true
  },
  {
    "code": "def _as_bool(value: str) -> bool:\n    try:\n        return _STR_BOOLEAN_MAPPING[value.lower()]\n    except KeyError:\n        raise ValueError(f'invalid truth value {value}')",
    "label": true
  },
  {
    "code": "def _pairwise(iterable):\n    a, b = tee(iterable)\n    next(b, None)\n    yield from zip(a, b)",
    "label": true
  },
  {
    "code": "def test_dtype():\n    try:\n        import numpy as np\n        dti = np.dtype('int')\n        assert np.dtype == dill.copy(np.dtype)\n        assert dti == dill.copy(dti)\n    except ImportError:\n        pass",
    "label": true
  },
  {
    "code": "def _column_type(strings, has_invisible=True, numparse=True):\n    types = [_type(s, has_invisible, numparse) for s in strings]\n    return reduce(_more_generic, types, bool)",
    "label": true
  },
  {
    "code": "def _get_editable_info(dist: BaseDistribution) -> _EditableInfo:\n    editable_project_location = dist.editable_project_location\n    assert editable_project_location\n    location = os.path.normcase(os.path.abspath(editable_project_location))\n    from pip._internal.vcs import RemoteNotFoundError, RemoteNotValidError, vcs\n    vcs_backend = vcs.get_backend_for_dir(location)\n    if vcs_backend is None:\n        display = _format_as_name_version(dist)\n        logger.debug('No VCS found for editable requirement \"%s\" in: %r', display, location)\n        return _EditableInfo(requirement=location, comments=[f'# Editable install with no version control ({display})'])\n    vcs_name = type(vcs_backend).__name__\n    try:\n        req = vcs_backend.get_src_requirement(location, dist.raw_name)\n    except RemoteNotFoundError:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(requirement=location, comments=[f'# Editable {vcs_name} install with no remote ({display})'])\n    except RemoteNotValidError as ex:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(requirement=location, comments=[f'# Editable {vcs_name} install ({display}) with either a deleted local remote or invalid URI:', f\"# '{ex.url}'\"])\n    except BadCommand:\n        logger.warning('cannot determine version of editable source in %s (%s command not found in path)', location, vcs_backend.name)\n        return _EditableInfo(requirement=location, comments=[])\n    except InstallationError as exc:\n        logger.warning('Error when trying to get requirement for VCS system %s', exc)\n    else:\n        return _EditableInfo(requirement=req, comments=[])\n    logger.warning('Could not determine repository location of %s', location)\n    return _EditableInfo(requirement=location, comments=['## !! Could not determine repository location'])",
    "label": true
  },
  {
    "code": "def cpython_tags(python_version: Optional[PythonVersion]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    interpreter = f'cp{_version_nodot(python_version[:2])}'\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    for explicit_abi in ('abi3', 'none'):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    if _abi3_applies(python_version):\n        yield from (Tag(interpreter, 'abi3', platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, 'none', platform_) for platform_ in platforms)\n    if _abi3_applies(python_version):\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = 'cp{version}'.format(version=_version_nodot((python_version[0], minor_version)))\n                yield Tag(interpreter, 'abi3', platform_)",
    "label": true
  },
  {
    "code": "def description_of(lines: Iterable[bytes], name: str='stdin', minimal: bool=False, should_rename_legacy: bool=False) -> Optional[str]:\n    u = UniversalDetector(should_rename_legacy=should_rename_legacy)\n    for line in lines:\n        line = bytearray(line)\n        u.feed(line)\n        if u.done:\n            break\n    u.close()\n    result = u.result\n    if minimal:\n        return result['encoding']\n    if result['encoding']:\n        return f\"{name}: {result['encoding']} with confidence {result['confidence']}\"\n    return f'{name}: no result'",
    "label": true
  },
  {
    "code": "def _check_class_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_pascal_case(name):\n        error_msgs.append(f'Class name \"{name}\" should be in PascalCase format. Class names should have the first letter of each word capitalized with no separation between each word. A single leading underscore can be used to denote a private class.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def get_module(module, paths, info):\n    spec = find_spec(module, paths)\n    if not spec:\n        raise ImportError(\"Can't find %s\" % module)\n    return module_from_spec(spec)",
    "label": true
  },
  {
    "code": "def _deque_mock():\n    base_deque_class = '\\n    class deque(object):\\n        maxlen = 0\\n        def __init__(self, iterable=None, maxlen=None):\\n            self.iterable = iterable or []\\n        def append(self, x): pass\\n        def appendleft(self, x): pass\\n        def clear(self): pass\\n        def count(self, x): return 0\\n        def extend(self, iterable): pass\\n        def extendleft(self, iterable): pass\\n        def pop(self): return self.iterable[0]\\n        def popleft(self): return self.iterable[0]\\n        def remove(self, value): pass\\n        def reverse(self): return reversed(self.iterable)\\n        def rotate(self, n=1): return self\\n        def __iter__(self): return self\\n        def __reversed__(self): return self.iterable[::-1]\\n        def __getitem__(self, index): return self.iterable[index]\\n        def __setitem__(self, index, value): pass\\n        def __delitem__(self, index): pass\\n        def __bool__(self): return bool(self.iterable)\\n        def __nonzero__(self): return bool(self.iterable)\\n        def __contains__(self, o): return o in self.iterable\\n        def __len__(self): return len(self.iterable)\\n        def __copy__(self): return deque(self.iterable)\\n        def copy(self): return deque(self.iterable)\\n        def index(self, x, start=0, end=0): return 0\\n        def insert(self, i, x): pass\\n        def __add__(self, other): pass\\n        def __iadd__(self, other): pass\\n        def __mul__(self, other): pass\\n        def __imul__(self, other): pass\\n        def __rmul__(self, other): pass'\n    if PY39_PLUS:\n        base_deque_class += '\\n        @classmethod\\n        def __class_getitem__(self, item): return cls'\n    return base_deque_class",
    "label": true
  },
  {
    "code": "def _patch_distribution_metadata():\n    from . import _core_metadata\n    'Patch write_pkg_file and read_pkg_file for higher metadata standards'\n    for attr in ('write_pkg_info', 'write_pkg_file', 'read_pkg_file', 'get_metadata_version'):\n        new_val = getattr(_core_metadata, attr)\n        setattr(distutils.dist.DistributionMetadata, attr, new_val)",
    "label": true
  },
  {
    "code": "def build_frequency_dict(text: bytes) -> dict[int, int]:\n    out = {}\n    for byte in text:\n        if byte not in out:\n            out[byte] = 0\n        out[byte] += 1\n    return out",
    "label": true
  },
  {
    "code": "def _parse_multi_options(options, split_token=','):\n    if options:\n        return [o.strip() for o in options.split(split_token) if o.strip()]\n    else:\n        return options",
    "label": true
  },
  {
    "code": "def escape(s: t.Any) -> Markup:\n    if hasattr(s, '__html__'):\n        return Markup(s.__html__())\n    return Markup(str(s).replace('&', '&amp;').replace('>', '&gt;').replace('<', '&lt;').replace(\"'\", '&#39;').replace('\"', '&#34;'))",
    "label": true
  },
  {
    "code": "def escape(markup: str, _escape: _EscapeSubMethod=re.compile('(\\\\\\\\*)(\\\\[[a-z#/@][^[]*?])').sub) -> str:\n\n    def escape_backslashes(match: Match[str]) -> str:\n        \"\"\"Called by re.sub replace matches.\"\"\"\n        backslashes, text = match.groups()\n        return f'{backslashes}{backslashes}\\\\{text}'\n    markup = _escape(escape_backslashes, markup)\n    return markup",
    "label": true
  },
  {
    "code": "def _get_first_non_fixture_func(obj: object, names: Iterable[str]) -> Optional[object]:\n    for name in names:\n        meth: Optional[object] = getattr(obj, name, None)\n        if meth is not None and fixtures.getfixturemarker(meth) is None:\n            return meth\n    return None",
    "label": true
  },
  {
    "code": "def _parse_content_type_header(header):\n    tokens = header.split(';')\n    content_type, params = (tokens[0].strip(), tokens[1:])\n    params_dict = {}\n    items_to_strip = '\"\\' '\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = (param, True)\n            index_of_equals = param.find('=')\n            if index_of_equals != -1:\n                key = param[:index_of_equals].strip(items_to_strip)\n                value = param[index_of_equals + 1:].strip(items_to_strip)\n            params_dict[key.lower()] = value\n    return (content_type, params_dict)",
    "label": true
  },
  {
    "code": "def valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('debugconfig')\n    group.addoption('--setuponly', '--setup-only', action='store_true', help='Only setup fixtures, do not execute tests')\n    group.addoption('--setupshow', '--setup-show', action='store_true', help='Show setup of fixtures while executing tests')",
    "label": true
  },
  {
    "code": "def make_dist(name, version, **kwargs):\n    summary = kwargs.pop('summary', 'Placeholder for summary')\n    md = Metadata(**kwargs)\n    md.name = name\n    md.version = version\n    md.summary = summary or 'Placeholder for summary'\n    return Distribution(md)",
    "label": true
  },
  {
    "code": "def show_actual_vendor_versions(vendor_txt_versions: Dict[str, str]) -> None:\n    for module_name, expected_version in vendor_txt_versions.items():\n        extra_message = ''\n        actual_version = get_vendor_version_from_module(module_name)\n        if not actual_version:\n            extra_message = ' (Unable to locate actual module version, using vendor.txt specified version)'\n            actual_version = expected_version\n        elif parse_version(actual_version) != parse_version(expected_version):\n            extra_message = ' (CONFLICT: vendor.txt suggests version should be {})'.format(expected_version)\n        logger.info('%s==%s%s', module_name, actual_version, extra_message)",
    "label": true
  },
  {
    "code": "def set_config(**kwargs: Any) -> Callable[[Callable[..., None]], Callable[..., None]]:\n\n    def _wrapper(fun: Callable[..., None]) -> Callable[..., None]:\n\n        @functools.wraps(fun)\n        def _forward(self: CheckerTestCase, *args: Any, **test_function_kwargs: Any) -> None:\n            \"\"\"Set option via argparse.\"\"\"\n            for key, value in kwargs.items():\n                self.linter.set_option(key, value)\n            self.checker.open()\n            fun(self, *args, **test_function_kwargs)\n        return _forward\n    return _wrapper",
    "label": true
  },
  {
    "code": "def _coerce_parse_result(results: Union[ParseResults, List[Any]]) -> List[Any]:\n    if isinstance(results, ParseResults):\n        return [_coerce_parse_result(i) for i in results]\n    else:\n        return results",
    "label": true
  },
  {
    "code": "def parse_hex_char(src: str, pos: Pos, hex_len: int) -> tuple[Pos, str]:\n    hex_str = src[pos:pos + hex_len]\n    if len(hex_str) != hex_len or not HEXDIGIT_CHARS.issuperset(hex_str):\n        raise suffixed_err(src, pos, 'Invalid hex value')\n    pos += hex_len\n    hex_int = int(hex_str, 16)\n    if not is_unicode_scalar_value(hex_int):\n        raise suffixed_err(src, pos, 'Escaped character is not a Unicode scalar value')\n    return (pos, chr(hex_int))",
    "label": true
  },
  {
    "code": "def set_cell_size(text: str, total: int) -> str:\n    if _is_single_cell_widths(text):\n        size = len(text)\n        if size < total:\n            return text + ' ' * (total - size)\n        return text[:total]\n    if total <= 0:\n        return ''\n    cell_size = cell_len(text)\n    if cell_size == total:\n        return text\n    if cell_size < total:\n        return text + ' ' * (total - cell_size)\n    start = 0\n    end = len(text)\n    while True:\n        pos = (start + end) // 2\n        before = text[:pos + 1]\n        before_len = cell_len(before)\n        if before_len == total + 1 and cell_len(before[-1]) == 2:\n            return before[:-1] + ' '\n        if before_len == total:\n            return before\n        if before_len > total:\n            end = pos\n        else:\n            start = pos",
    "label": true
  },
  {
    "code": "def wrap_pytest_function_for_tracing(pyfuncitem):\n    _pdb = pytestPDB._init_pdb('runcall')\n    testfunction = pyfuncitem.obj\n\n    @functools.wraps(testfunction)\n    def wrapper(*args, **kwargs):\n        func = functools.partial(testfunction, *args, **kwargs)\n        _pdb.runcall(func)\n    pyfuncitem.obj = wrapper",
    "label": true
  },
  {
    "code": "def size(s: Stack) -> int:\n    side_stack = Stack()\n    count = 0\n    while not s.is_empty():\n        side_stack.push(s.pop())\n        count += 1\n    while not side_stack.is_empty():\n        s.push(side_stack.pop())\n    return count",
    "label": true
  },
  {
    "code": "def run_pyta(filename: str, config_file: str) -> None:\n    import json\n    error_message = '\\nCould not run PythonTA correctly.\\nPlease make sure you have run the setup.py provided on Quercus: that should install PythonTA for you.\\nPlease attend office hours if you require assistance in running PythonTA.'\n    try:\n        import python_ta\n        with open(config_file) as cf:\n            config_dict = json.loads(cf.read())\n            config_dict['output-format'] = 'python_ta.reporters.PlainReporter'\n        python_ta.check_all(filename, config=config_dict)\n    except:\n        pass\n    else:\n        return\n    print(error_message)",
    "label": true
  },
  {
    "code": "def get_best_invocation_for_this_python() -> str:\n    exe = sys.executable\n    exe_name = os.path.basename(exe)\n    found_executable = shutil.which(exe_name)\n    if found_executable and os.path.samefile(found_executable, exe):\n        return exe_name\n    return exe",
    "label": true
  },
  {
    "code": "def pytest_runtest_teardown(item: Item, nextitem: Optional[Item]) -> None:\n    _update_current_test_var(item, 'teardown')\n    item.session._setupstate.teardown_exact(nextitem)\n    _update_current_test_var(item, None)",
    "label": true
  },
  {
    "code": "def is_classdef_type(node: nodes.ClassDef) -> bool:\n    if node.name == 'type':\n        return True\n    return any((isinstance(b, nodes.Name) and b.name == 'type' for b in node.bases))",
    "label": true
  },
  {
    "code": "def yield_fixture(fixture_function=None, *args, scope='function', params=None, autouse=False, ids=None, name=None):\n    warnings.warn(YIELD_FIXTURE, stacklevel=2)\n    return fixture(fixture_function, *args, scope=scope, params=params, autouse=autouse, ids=ids, name=name)",
    "label": true
  },
  {
    "code": "def infer_type_sub(node, context: InferenceContext | None=None):\n    node_scope, _ = node.scope().lookup('type')\n    if not isinstance(node_scope, nodes.Module) or node_scope.qname() != 'builtins':\n        raise UseInferenceDefault()\n    class_src = '\\n    class type:\\n        def __class_getitem__(cls, key):\\n            return cls\\n     '\n    node = extract_node(class_src)\n    return node.infer(context=context)",
    "label": true
  },
  {
    "code": "def dict_to_sequence(d):\n    if hasattr(d, 'items'):\n        d = d.items()\n    return d",
    "label": true
  },
  {
    "code": "def _looks_like_path(name: str) -> bool:\n    if os.path.sep in name:\n        return True\n    if os.path.altsep is not None and os.path.altsep in name:\n        return True\n    if name.startswith('.'):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def cell_len(text: str, _cell_len: Callable[[str], int]=cached_cell_len) -> int:\n    if len(text) < 512:\n        return _cell_len(text)\n    _get_size = get_character_cell_size\n    total_size = sum((_get_size(character) for character in text))\n    return total_size",
    "label": true
  },
  {
    "code": "def parse_multiline_str(src: str, pos: Pos, *, literal: bool) -> tuple[Pos, str]:\n    pos += 3\n    if src.startswith('\\n', pos):\n        pos += 1\n    if literal:\n        delim = \"'\"\n        end_pos = skip_until(src, pos, \"'''\", error_on=ILLEGAL_MULTILINE_LITERAL_STR_CHARS, error_on_eof=True)\n        result = src[pos:end_pos]\n        pos = end_pos + 3\n    else:\n        delim = '\"'\n        pos, result = parse_basic_str(src, pos, multiline=True)\n    if not src.startswith(delim, pos):\n        return (pos, result)\n    pos += 1\n    if not src.startswith(delim, pos):\n        return (pos, result + delim)\n    pos += 1\n    return (pos, result + delim * 2)",
    "label": true
  },
  {
    "code": "def _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n    try:\n        spec = Specifier(''.join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs)\n    oper: Optional[Operator] = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(f'Undefined {op!r} on {lhs!r} and {rhs!r}.')\n    return oper(lhs, rhs)",
    "label": true
  },
  {
    "code": "def _check_regexp_csv(value: list[str] | tuple[str] | str) -> Iterable[str]:\n    if isinstance(value, (list, tuple)):\n        yield from value\n    else:\n        regexps: deque[deque[str] | None] = deque([None])\n        open_braces = False\n        for char in value:\n            if char == '{':\n                open_braces = True\n            elif char == '}' and open_braces:\n                open_braces = False\n            if char == ',' and (not open_braces):\n                regexps.append(None)\n            elif regexps[-1] is None:\n                regexps.pop()\n                regexps.append(deque([char]))\n            else:\n                regexps[-1].append(char)\n        yield from (''.join(regexp).strip() for regexp in regexps if regexp is not None)",
    "label": true
  },
  {
    "code": "def class_instance_as_index(node: SuccessfulInferenceResult) -> nodes.Const | None:\n    context = InferenceContext()\n    try:\n        for inferred in node.igetattr('__index__', context=context):\n            if not isinstance(inferred, bases.BoundMethod):\n                continue\n            context.boundnode = node\n            context.callcontext = CallContext(args=[], callee=inferred)\n            for result in inferred.infer_call_result(node, context=context):\n                if isinstance(result, nodes.Const) and isinstance(result.value, int):\n                    return result\n    except InferenceError:\n        pass\n    return None",
    "label": true
  },
  {
    "code": "def get_module_part(dotted_name: str, context_file: str | None=None) -> str:\n    if dotted_name.startswith('os.path'):\n        return 'os.path'\n    parts = dotted_name.split('.')\n    if context_file is not None:\n        if parts[0] in BUILTIN_MODULES:\n            if len(parts) > 2:\n                raise ImportError(dotted_name)\n            return parts[0]\n    path: list[str] | None = None\n    starti = 0\n    if parts[0] == '':\n        assert context_file is not None, 'explicit relative import, but no context_file?'\n        path = []\n        starti = 1\n    while starti < len(parts) and parts[starti] == '':\n        starti += 1\n        assert context_file is not None, 'explicit relative import, but no context_file?'\n        context_file = os.path.dirname(context_file)\n    for i in range(starti, len(parts)):\n        try:\n            file_from_modpath(parts[starti:i + 1], path=path, context_file=context_file)\n        except ImportError:\n            if i < max(1, len(parts) - 2):\n                raise\n            return '.'.join(parts[:i])\n    return dotted_name",
    "label": true
  },
  {
    "code": "def retry(*r_args, **r_kwargs):\n\n    def decorate(func):\n\n        @functools.wraps(func)\n        def wrapper(*f_args, **f_kwargs):\n            bound = functools.partial(func, *f_args, **f_kwargs)\n            return retry_call(bound, *r_args, **r_kwargs)\n        return wrapper\n    return decorate",
    "label": true
  },
  {
    "code": "def _is_trailing_comma(tokens: list[tokenize.TokenInfo], index: int) -> bool:\n    token = tokens[index]\n    if token.exact_type != tokenize.COMMA:\n        return False\n    left_tokens = list(itertools.islice(tokens, index + 1, None))\n    more_tokens_on_line = False\n    for remaining_token in left_tokens:\n        if remaining_token.start[0] == token.start[0]:\n            more_tokens_on_line = True\n            if remaining_token.type not in (tokenize.NEWLINE, tokenize.COMMENT):\n                return False\n    if not more_tokens_on_line:\n        return False\n\n    def get_curline_index_start() -> int:\n        \"\"\"Get the index denoting the start of the current line.\"\"\"\n        for subindex, token in enumerate(reversed(tokens[:index])):\n            if token.type == tokenize.NEWLINE:\n                return index - subindex\n        return 0\n    curline_start = get_curline_index_start()\n    expected_tokens = {'return', 'yield'}\n    return any(('=' in prevtoken.string or prevtoken.string in expected_tokens for prevtoken in tokens[curline_start:index]))",
    "label": true
  },
  {
    "code": "def _binary_operators_from_module(module: types.ModuleType) -> dict[type[ast.operator], str]:\n    binary_operators = {module.Add: '+', module.BitAnd: '&', module.BitOr: '|', module.BitXor: '^', module.Div: '/', module.FloorDiv: '//', module.MatMult: '@', module.Mod: '%', module.Mult: '*', module.Pow: '**', module.Sub: '-', module.LShift: '<<', module.RShift: '>>'}\n    return binary_operators",
    "label": true
  },
  {
    "code": "def install_req_from_editable(editable_req: str, comes_from: Optional[Union[InstallRequirement, str]]=None, *, use_pep517: Optional[bool]=None, isolated: bool=False, global_options: Optional[List[str]]=None, hash_options: Optional[Dict[str, List[str]]]=None, constraint: bool=False, user_supplied: bool=False, permit_editable_wheels: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    parts = parse_req_from_editable(editable_req)\n    return InstallRequirement(parts.requirement, comes_from=comes_from, user_supplied=user_supplied, editable=True, permit_editable_wheels=permit_editable_wheels, link=parts.link, constraint=constraint, use_pep517=use_pep517, isolated=isolated, global_options=global_options, hash_options=hash_options, config_settings=config_settings, extras=parts.extras)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e272(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def deinit():\n    if orig_stdout is not None:\n        sys.stdout = orig_stdout\n    if orig_stderr is not None:\n        sys.stderr = orig_stderr",
    "label": true
  },
  {
    "code": "def find_bridge_by_id(bridges: list[list], bridge_id: int) -> list:\n    for bridge in bridges:\n        if bridge[COLUMN_ID] == bridge_id:\n            return bridge\n    return []",
    "label": true
  },
  {
    "code": "def get_all_styles():\n    yield from STYLE_MAP\n    for name, _ in find_plugin_styles():\n        yield name",
    "label": true
  },
  {
    "code": "def _combining_class(cp: int) -> int:\n    v = unicodedata.combining(chr(cp))\n    if v == 0:\n        if not unicodedata.name(chr(cp)):\n            raise ValueError('Unknown character in unicodedata')\n    return v",
    "label": true
  },
  {
    "code": "def _infer_binary_operation(left: InferenceResult, right: InferenceResult, binary_opnode: nodes.AugAssign | nodes.BinOp, context: InferenceContext, flow_factory: GetFlowFactory) -> Generator[InferenceResult | util.BadBinaryOperationMessage, None, None]:\n    context, reverse_context = _get_binop_contexts(context, left, right)\n    left_type = helpers.object_type(left)\n    right_type = helpers.object_type(right)\n    methods = flow_factory(left, left_type, binary_opnode, right, right_type, context, reverse_context)\n    for method in methods:\n        try:\n            results = list(method())\n        except AttributeError:\n            continue\n        except AttributeInferenceError:\n            continue\n        except InferenceError:\n            yield util.Uninferable\n            return\n        else:\n            if any((isinstance(result, util.UninferableBase) for result in results)):\n                yield util.Uninferable\n                return\n            if all(map(_is_not_implemented, results)):\n                continue\n            not_implemented = sum((1 for result in results if _is_not_implemented(result)))\n            if not_implemented and not_implemented != len(results):\n                yield util.Uninferable\n                return\n            yield from results\n            return\n    yield util.BadBinaryOperationMessage(left_type, binary_opnode.op, right_type)",
    "label": true
  },
  {
    "code": "def method_caller(method_name, *args, **kwargs):\n\n    def call_method(target):\n        func = getattr(target, method_name)\n        return func(*args, **kwargs)\n    return call_method",
    "label": true
  },
  {
    "code": "def make_str(value: t.Any) -> str:\n    if isinstance(value, bytes):\n        try:\n            return value.decode(sys.getfilesystemencoding())\n        except UnicodeError:\n            return value.decode('utf-8', 'replace')\n    return str(value)",
    "label": true
  },
  {
    "code": "def _should_use_importlib_metadata() -> bool:\n    with contextlib.suppress(KeyError, ValueError):\n        return bool(strtobool(os.environ['_PIP_USE_IMPORTLIB_METADATA']))\n    if sys.version_info < (3, 11):\n        return False\n    import importlib.metadata\n    return bool(getattr(importlib.metadata, '_PIP_USE_IMPORTLIB_METADATA', True))",
    "label": true
  },
  {
    "code": "def error_of_type(handler: nodes.ExceptHandler, error_type: str | type[Exception] | tuple[str | type[Exception], ...]) -> bool:\n\n    def stringify_error(error: str | type[Exception]) -> str:\n        if not isinstance(error, str):\n            return error.__name__\n        return error\n    if not isinstance(error_type, tuple):\n        error_type = (error_type,)\n    expected_errors = {stringify_error(error) for error in error_type}\n    if not handler.type:\n        return False\n    return handler.catch(expected_errors)",
    "label": true
  },
  {
    "code": "def sort_code_string(code: str, extension: Optional[str]=None, config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, disregard_skip: bool=False, show_diff: Union[bool, TextIO]=False, **config_kwargs: Any) -> str:\n    input_stream = StringIO(code)\n    output_stream = StringIO()\n    config = _config(path=file_path, config=config, **config_kwargs)\n    sort_stream(input_stream, output_stream, extension=extension, config=config, file_path=file_path, disregard_skip=disregard_skip, show_diff=show_diff)\n    output_stream.seek(0)\n    return output_stream.read()",
    "label": true
  },
  {
    "code": "def compute_tagline(tags: list[str]) -> str:\n    impls = sorted({tag.split('-')[0] for tag in tags})\n    abivers = sorted({tag.split('-')[1] for tag in tags})\n    platforms = sorted({tag.split('-')[2] for tag in tags})\n    return '-'.join(['.'.join(impls), '.'.join(abivers), '.'.join(platforms)])",
    "label": true
  },
  {
    "code": "def parse_rgb_hex(hex_color: str) -> ColorTriplet:\n    assert len(hex_color) == 6, 'must be 6 characters'\n    color = ColorTriplet(int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16))\n    return color",
    "label": true
  },
  {
    "code": "def create_urllib3_context(ssl_version=None, cert_reqs=None, options=None, ciphers=None):\n    if not ssl_version or ssl_version == PROTOCOL_TLS:\n        ssl_version = PROTOCOL_TLS_CLIENT\n    context = SSLContext(ssl_version)\n    context.set_ciphers(ciphers or DEFAULT_CIPHERS)\n    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs\n    if options is None:\n        options = 0\n        options |= OP_NO_SSLv2\n        options |= OP_NO_SSLv3\n        options |= OP_NO_COMPRESSION\n        options |= OP_NO_TICKET\n    context.options |= options\n    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(context, 'post_handshake_auth', None) is not None:\n        context.post_handshake_auth = True\n\n    def disable_check_hostname():\n        if getattr(context, 'check_hostname', None) is not None:\n            context.check_hostname = False\n    if cert_reqs == ssl.CERT_REQUIRED:\n        context.verify_mode = cert_reqs\n        disable_check_hostname()\n    else:\n        disable_check_hostname()\n        context.verify_mode = cert_reqs\n    if hasattr(context, 'keylog_filename'):\n        sslkeylogfile = os.environ.get('SSLKEYLOGFILE')\n        if sslkeylogfile:\n            context.keylog_filename = sslkeylogfile\n    return context",
    "label": true
  },
  {
    "code": "def _version_from_file(lines):\n\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None",
    "label": true
  },
  {
    "code": "def _override_check_invalid_name_in_main():\n    old_visit_assignname = NameChecker.visit_assignname\n\n    def patched_visit_assignname(self, node):\n        if _is_in_main(node):\n            self._check_name('variable', node.name, node)\n        else:\n            old_visit_assignname(self, node)\n    NameChecker.visit_assignname = patched_visit_assignname",
    "label": true
  },
  {
    "code": "def console_main() -> int:\n    try:\n        code = main()\n        sys.stdout.flush()\n        return code\n    except BrokenPipeError:\n        devnull = os.open(os.devnull, os.O_WRONLY)\n        os.dup2(devnull, sys.stdout.fileno())\n        return 1",
    "label": true
  },
  {
    "code": "def iter_decode(input, fallback_encoding, errors='replace'):\n    decoder = IncrementalDecoder(fallback_encoding, errors)\n    generator = _iter_decode_generator(input, decoder)\n    encoding = next(generator)\n    return (generator, encoding)",
    "label": true
  },
  {
    "code": "def join_lines(lines_enum: ReqFileLines) -> ReqFileLines:\n    primary_line_number = None\n    new_line: List[str] = []\n    for line_number, line in lines_enum:\n        if not line.endswith('\\\\') or COMMENT_RE.match(line):\n            if COMMENT_RE.match(line):\n                line = ' ' + line\n            if new_line:\n                new_line.append(line)\n                assert primary_line_number is not None\n                yield (primary_line_number, ''.join(new_line))\n                new_line = []\n            else:\n                yield (line_number, line)\n        else:\n            if not new_line:\n                primary_line_number = line_number\n            new_line.append(line.strip('\\\\'))\n    if new_line:\n        assert primary_line_number is not None\n        yield (primary_line_number, ''.join(new_line))",
    "label": true
  },
  {
    "code": "def truncate_if_required(explanation: List[str], item: Item, max_length: Optional[int]=None) -> List[str]:\n    if _should_truncate_item(item):\n        return _truncate_explanation(explanation)\n    return explanation",
    "label": true
  },
  {
    "code": "def _subprocess_transform():\n    communicate = (bytes('string', 'ascii'), bytes('string', 'ascii'))\n    communicate_signature = 'def communicate(self, input=None, timeout=None)'\n    args = '        self, args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None,\\n        preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None,\\n        universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True,\\n        start_new_session=False, pass_fds=(), *, encoding=None, errors=None, text=None'\n    if PY39_PLUS:\n        args += ', user=None, group=None, extra_groups=None, umask=-1'\n    if PY310_PLUS:\n        args += ', pipesize=-1'\n    if PY311_PLUS:\n        args += ', process_group=None'\n    init = f'\\n        def __init__({args}):\\n            pass'\n    wait_signature = 'def wait(self, timeout=None)'\n    ctx_manager = '\\n        def __enter__(self): return self\\n        def __exit__(self, *args): pass\\n    '\n    py3_args = 'args = []'\n    check_output_signature = '\\n    check_output(\\n        args, *,\\n        stdin=None,\\n        stderr=None,\\n        shell=False,\\n        cwd=None,\\n        encoding=None,\\n        errors=None,\\n        universal_newlines=False,\\n        timeout=None,\\n        env=None,\\n        text=None,\\n        restore_signals=True,\\n        preexec_fn=None,\\n        pass_fds=(),\\n        input=None,\\n        bufsize=0,\\n        executable=None,\\n        close_fds=False,\\n        startupinfo=None,\\n        creationflags=0,\\n        start_new_session=False\\n    ):\\n    '.strip()\n    code = textwrap.dedent(f'\\n    def {check_output_signature}\\n        if universal_newlines:\\n            return \"\"\\n        return b\"\"\\n\\n    class Popen(object):\\n        returncode = pid = 0\\n        stdin = stdout = stderr = file()\\n        {py3_args}\\n\\n        {communicate_signature}:\\n            return {communicate!r}\\n        {wait_signature}:\\n            return self.returncode\\n        def poll(self):\\n            return self.returncode\\n        def send_signal(self, signal):\\n            pass\\n        def terminate(self):\\n            pass\\n        def kill(self):\\n            pass\\n        {ctx_manager}\\n       ')\n    if PY39_PLUS:\n        code += '\\n    @classmethod\\n    def __class_getitem__(cls, item):\\n        pass\\n        '\n    init_lines = textwrap.dedent(init).splitlines()\n    indented_init = '\\n'.join((' ' * 4 + line for line in init_lines))\n    code += indented_init\n    return parse(code)",
    "label": true
  },
  {
    "code": "def infer_typing_attr(node: Subscript, ctx: context.InferenceContext | None=None) -> Iterator[ClassDef]:\n    try:\n        value = next(node.value.infer())\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not value.qname().startswith('typing.') or value.qname() in TYPING_ALIAS:\n        raise UseInferenceDefault\n    if isinstance(value, ClassDef) and value.qname() in {'typing.Generic', 'typing.Annotated', 'typing_extensions.Annotated'}:\n        func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n        value.locals['__class_getitem__'] = [func_to_add]\n        if isinstance(node.parent, ClassDef) and node in node.parent.bases and getattr(node.parent, '__cache', None):\n            cache = node.parent.__cache\n            if cache.get(node.parent.slots) is not None:\n                del cache[node.parent.slots]\n        return iter([value])\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(value.qname().split('.')[-1]))\n    return node.infer(context=ctx)",
    "label": true
  },
  {
    "code": "def any_specified_encoding(sequence: bytes, search_zone: int=8192) -> Optional[str]:\n    if not isinstance(sequence, bytes):\n        raise TypeError\n    seq_len: int = len(sequence)\n    results: List[str] = findall(RE_POSSIBLE_ENCODING_INDICATION, sequence[:min(seq_len, search_zone)].decode('ascii', errors='ignore'))\n    if len(results) == 0:\n        return None\n    for specified_encoding in results:\n        specified_encoding = specified_encoding.lower().replace('-', '_')\n        encoding_alias: str\n        encoding_iana: str\n        for encoding_alias, encoding_iana in aliases.items():\n            if encoding_alias == specified_encoding:\n                return encoding_iana\n            if encoding_iana == specified_encoding:\n                return encoding_iana\n    return None",
    "label": true
  },
  {
    "code": "def _unpack_zipfile_obj(zipfile_obj, extract_dir, progress_filter=default_filter):\n    for info in zipfile_obj.infolist():\n        name = info.filename\n        if name.startswith('/') or '..' in name.split('/'):\n            continue\n        target = os.path.join(extract_dir, *name.split('/'))\n        target = progress_filter(name, target)\n        if not target:\n            continue\n        if name.endswith('/'):\n            ensure_directory(target)\n        else:\n            ensure_directory(target)\n            data = zipfile_obj.read(info.filename)\n            with open(target, 'wb') as f:\n                f.write(data)\n        unix_attributes = info.external_attr >> 16\n        if unix_attributes:\n            os.chmod(target, unix_attributes)",
    "label": true
  },
  {
    "code": "def _set_default(type_param, default):\n    if isinstance(default, (tuple, list)):\n        type_param.__default__ = tuple((typing._type_check(d, 'Default must be a type') for d in default))\n    elif default != _marker:\n        if isinstance(type_param, ParamSpec) and default is ...:\n            type_param.__default__ = default\n        else:\n            type_param.__default__ = typing._type_check(default, 'Default must be a type')\n    else:\n        type_param.__default__ = None",
    "label": true
  },
  {
    "code": "def parse_key(src: str, pos: Pos) -> Tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: Optional[str] = src[pos]\n        except IndexError:\n            char = None\n        if char != '.':\n            return (pos, key)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def sample(iterable, k, weights=None):\n    if k == 0:\n        return []\n    iterable = iter(iterable)\n    if weights is None:\n        return _sample_unweighted(iterable, k)\n    else:\n        weights = iter(weights)\n        return _sample_weighted(iterable, k, weights)",
    "label": true
  },
  {
    "code": "def get_node_last_lineno(node: nodes.NodeNG) -> int:\n    if getattr(node, 'finalbody', False):\n        return get_node_last_lineno(node.finalbody[-1])\n    if getattr(node, 'orelse', False):\n        return get_node_last_lineno(node.orelse[-1])\n    if getattr(node, 'handlers', False):\n        return get_node_last_lineno(node.handlers[-1])\n    if getattr(node, 'body', False):\n        return get_node_last_lineno(node.body[-1])\n    return node.lineno",
    "label": true
  },
  {
    "code": "def get_bridge_condition(bridges: list[list], bridge_id: int) -> float:\n    bridge = find_bridge_by_id(bridges, bridge_id)\n    if bridge == []:\n        return MISSING_BCI\n    for i in range(len(bridge[COLUMN_BCI][INDEX_BCI_YEARS])):\n        if bridge[COLUMN_BCI][INDEX_BCI_SCORES][i] != MISSING_BCI:\n            return bridge[COLUMN_BCI][INDEX_BCI_SCORES][i]\n    return MISSING_BCI",
    "label": true
  },
  {
    "code": "def _parse_extras(tokenizer: Tokenizer) -> List[str]:\n    if not tokenizer.check('LEFT_BRACKET', peek=True):\n        return []\n    with tokenizer.enclosing_tokens('LEFT_BRACKET', 'RIGHT_BRACKET', around='extras'):\n        tokenizer.consume('WS')\n        extras = _parse_extras_list(tokenizer)\n        tokenizer.consume('WS')\n    return extras",
    "label": true
  },
  {
    "code": "def time(raw: str) -> Time:\n    value = parse_rfc3339(raw)\n    if not isinstance(value, _datetime.time):\n        raise ValueError('time() only accepts time strings.')\n    return item(value)",
    "label": true
  },
  {
    "code": "def get_exception_handlers(node: nodes.NodeNG, exception: type[Exception] | str=Exception) -> list[nodes.ExceptHandler] | None:\n    context = find_try_except_wrapper_node(node)\n    if isinstance(context, nodes.TryExcept):\n        return [handler for handler in context.handlers if error_of_type(handler, exception)]\n    return []",
    "label": true
  },
  {
    "code": "def parse_annotations(node: nodes.NodeNG, class_tvars: Optional[List[type]]=None) -> List[Tuple[type, str]]:\n    if isinstance(node, nodes.FunctionDef):\n        arg_types = []\n        no_class_tvars = class_tvars is None\n        is_methodcall = isinstance(node.parent, nodes.ClassDef)\n        if no_class_tvars and is_methodcall:\n            self_type = _node_to_type(node.parent.name)\n        elif no_class_tvars or not is_methodcall:\n            self_type = None\n        elif node.parent.name in _BUILTIN_TO_TYPING:\n            self_type = eval(_BUILTIN_TO_TYPING[node.parent.name])[tuple((_node_to_type(tv) for tv in class_tvars))]\n        else:\n            self_type = _node_to_type(node.parent.name)\n        for arg, annotation in zip(node.args.args, node.args.annotations):\n            if getattr(arg, 'name', None) == 'self' and annotation is None:\n                arg_types.append(self_type)\n            else:\n                arg_types.append(_ann_node_to_type(annotation).getValue())\n        alternatives = []\n        for num_optional in range(len(node.args.defaults) + 1):\n            alternatives.append(arg_types[:len(arg_types) - num_optional])\n        rtype = _ann_node_to_type(node.returns).getValue()\n        callables = [(create_Callable(arg_types, rtype, class_tvars), node.type) for arg_types in alternatives]\n        return callables\n    elif isinstance(node, nodes.AssignName) and isinstance(node.parent, nodes.AnnAssign):\n        return [_ann_node_to_type(node.parent.annotation).getValue(), 'attribute']",
    "label": true
  },
  {
    "code": "def _has_default_namedtuple_repr(obj: object) -> bool:\n    obj_file = None\n    try:\n        obj_file = inspect.getfile(obj.__repr__)\n    except (OSError, TypeError):\n        pass\n    default_repr_file = inspect.getfile(_dummy_namedtuple.__repr__)\n    return obj_file == default_repr_file",
    "label": true
  },
  {
    "code": "def _maybe_adjust_parameters(cls):\n    tvars = []\n    if '__orig_bases__' in cls.__dict__:\n        tvars = _collect_type_vars(cls.__orig_bases__)\n        gvars = None\n        for base in cls.__orig_bases__:\n            if isinstance(base, typing._GenericAlias) and base.__origin__ in (typing.Generic, Protocol):\n                the_base = base.__origin__.__name__\n                if gvars is not None:\n                    raise TypeError('Cannot inherit from Generic[...] and/or Protocol[...] multiple types.')\n                gvars = base.__parameters__\n        if gvars is None:\n            gvars = tvars\n        else:\n            tvarset = set(tvars)\n            gvarset = set(gvars)\n            if not tvarset <= gvarset:\n                s_vars = ', '.join((str(t) for t in tvars if t not in gvarset))\n                s_args = ', '.join((str(g) for g in gvars))\n                raise TypeError(f'Some type variables ({s_vars}) are not listed in {the_base}[{s_args}]')\n            tvars = gvars\n    cls.__parameters__ = tuple(tvars)",
    "label": true
  },
  {
    "code": "def is_python(text, filename='<string>'):\n    try:\n        compile(text, filename, 'exec')\n    except (SyntaxError, TypeError):\n        return False\n    else:\n        return True",
    "label": true
  },
  {
    "code": "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    tokenizer.consume('WS')\n    name_token = tokenizer.expect('IDENTIFIER', expected='package name at the start of dependency specifier')\n    name = name_token.text\n    tokenizer.consume('WS')\n    extras = _parse_extras(tokenizer)\n    tokenizer.consume('WS')\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n    tokenizer.expect('END', expected='end of dependency specifier')\n    return ParsedRequirement(name, url, extras, specifier, marker)",
    "label": true
  },
  {
    "code": "def ssl_wrap_socket(sock: socket.socket, keyfile: str | None=None, certfile: str | None=None, cert_reqs: int | None=None, ca_certs: str | None=None, server_hostname: str | None=None, ssl_version: int | None=None, ciphers: str | None=None, ssl_context: ssl.SSLContext | None=None, ca_cert_dir: str | None=None, key_password: str | None=None, ca_cert_data: None | str | bytes=None, tls_in_tls: bool=False) -> ssl.SSLSocket | SSLTransportType:\n    context = ssl_context\n    if context is None:\n        context = create_urllib3_context(ssl_version, cert_reqs, ciphers=ciphers)\n    if ca_certs or ca_cert_dir or ca_cert_data:\n        try:\n            context.load_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)\n        except OSError as e:\n            raise SSLError(e) from e\n    elif ssl_context is None and hasattr(context, 'load_default_certs'):\n        context.load_default_certs()\n    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):\n        raise SSLError('Client private key is encrypted, password is required')\n    if certfile:\n        if key_password is None:\n            context.load_cert_chain(certfile, keyfile)\n        else:\n            context.load_cert_chain(certfile, keyfile, key_password)\n    try:\n        context.set_alpn_protocols(ALPN_PROTOCOLS)\n    except NotImplementedError:\n        pass\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n    return ssl_sock",
    "label": true
  },
  {
    "code": "def get_config_var(name):\n    if name == 'SO':\n        import warnings\n        warnings.warn('SO is deprecated, use EXT_SUFFIX', DeprecationWarning, 2)\n    return get_config_vars().get(name)",
    "label": true
  },
  {
    "code": "def apply_filters(stream, filters, lexer=None):\n\n    def _apply(filter_, stream):\n        yield from filter_.filter(lexer, stream)\n    for filter_ in filters:\n        stream = _apply(filter_, stream)\n    return stream",
    "label": true
  },
  {
    "code": "def register_check(check, codes=None):\n\n    def _add_check(check, kind, codes, args):\n        if check in _checks[kind]:\n            _checks[kind][check][0].extend(codes or [])\n        else:\n            _checks[kind][check] = (codes or [''], args)\n    if inspect.isfunction(check):\n        args = _get_parameters(check)\n        if args and args[0] in ('physical_line', 'logical_line'):\n            if codes is None:\n                codes = ERRORCODE_REGEX.findall(check.__doc__ or '')\n            _add_check(check, args[0], codes, args)\n    elif inspect.isclass(check):\n        if _get_parameters(check.__init__)[:2] == ['self', 'tree']:\n            _add_check(check, 'tree', codes, None)\n    return check",
    "label": true
  },
  {
    "code": "def directory_size(path: str) -> Union[int, float]:\n    size = 0.0\n    for root, _dirs, files in os.walk(path):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            size += file_size(file_path)\n    return size",
    "label": true
  },
  {
    "code": "def _get_binop_flow(left: InferenceResult, left_type: InferenceResult | None, binary_opnode: nodes.AugAssign | nodes.BinOp, right: InferenceResult, right_type: InferenceResult | None, context: InferenceContext, reverse_context: InferenceContext) -> list[functools.partial[Generator[InferenceResult, None, None]]]:\n    op = binary_opnode.op\n    if _same_type(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_subtype(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_supertype(left_type, right_type):\n        methods = [_bin_op(right, binary_opnode, op, left, reverse_context, reverse=True), _bin_op(left, binary_opnode, op, right, context)]\n    else:\n        methods = [_bin_op(left, binary_opnode, op, right, context), _bin_op(right, binary_opnode, op, left, reverse_context, reverse=True)]\n    if PY310_PLUS and op == '|' and (isinstance(left, (bases.UnionType, nodes.ClassDef)) or (isinstance(left, nodes.Const) and left.value is None)) and (isinstance(right, (bases.UnionType, nodes.ClassDef)) or (isinstance(right, nodes.Const) and right.value is None)):\n        methods.extend([functools.partial(_bin_op_or_union_type, left, right)])\n    return methods",
    "label": true
  },
  {
    "code": "def open_source_file(filename: str) -> tuple[TextIOWrapper, str, str]:\n    with open(filename, 'rb') as byte_stream:\n        encoding = detect_encoding(byte_stream.readline)[0]\n    stream = open(filename, newline=None, encoding=encoding)\n    data = stream.read()\n    return (stream, encoding, data)",
    "label": true
  },
  {
    "code": "def show_formats():\n    from ..fancy_getopt import FancyGetopt\n    formats = []\n    for format in bdist.format_commands:\n        formats.append(('formats=' + format, None, bdist.format_commands[format][1]))\n    pretty_printer = FancyGetopt(formats)\n    pretty_printer.print_help('List of available distribution formats:')",
    "label": true
  },
  {
    "code": "def create_command(name: str, **kwargs: Any) -> Command:\n    module_path, class_name, summary = commands_dict[name]\n    module = importlib.import_module(module_path)\n    command_class = getattr(module, class_name)\n    command = command_class(name=name, summary=summary, **kwargs)\n    return command",
    "label": true
  },
  {
    "code": "def _visible_exprs(exprs: Iterable[pyparsing.ParserElement]):\n    non_diagramming_exprs = (pyparsing.ParseElementEnhance, pyparsing.PositionToken, pyparsing.And._ErrorStop)\n    return [e for e in exprs if not (e.customName or e.resultsName or isinstance(e, non_diagramming_exprs))]",
    "label": true
  },
  {
    "code": "def check_nfc(label: str) -> None:\n    if unicodedata.normalize('NFC', label) != label:\n        raise IDNAError('Label must be in Normalization Form C')",
    "label": true
  },
  {
    "code": "def single_line(val):\n    if '\\n' in val:\n        msg = 'newlines are not allowed in `summary` and will break in the future'\n        SetuptoolsDeprecationWarning.emit('Invalid config.', msg)\n        val = val.strip().split('\\n')[0]\n    return val",
    "label": true
  },
  {
    "code": "def get_supported(version: Optional[str]=None, platforms: Optional[List[str]]=None, impl: Optional[str]=None, abis: Optional[List[str]]=None) -> List[Tag]:\n    supported: List[Tag] = []\n    python_version: Optional[PythonVersion] = None\n    if version is not None:\n        python_version = _get_python_version(version)\n    interpreter = _get_custom_interpreter(impl, version)\n    platforms = _expand_allowed_platforms(platforms)\n    is_cpython = (impl or interpreter_name()) == 'cp'\n    if is_cpython:\n        supported.extend(cpython_tags(python_version=python_version, abis=abis, platforms=platforms))\n    else:\n        supported.extend(generic_tags(interpreter=interpreter, abis=abis, platforms=platforms))\n    supported.extend(compatible_tags(python_version=python_version, interpreter=interpreter, platforms=platforms))\n    return supported",
    "label": true
  },
  {
    "code": "def pager(generator: t.Iterable[str], color: t.Optional[bool]=None) -> None:\n    stdout = _default_text_stdout()\n    if stdout is None:\n        stdout = StringIO()\n    if not isatty(sys.stdin) or not isatty(stdout):\n        return _nullpager(stdout, generator, color)\n    pager_cmd = (os.environ.get('PAGER', None) or '').strip()\n    if pager_cmd:\n        if WIN:\n            return _tempfilepager(generator, pager_cmd, color)\n        return _pipepager(generator, pager_cmd, color)\n    if os.environ.get('TERM') in ('dumb', 'emacs'):\n        return _nullpager(stdout, generator, color)\n    if WIN or sys.platform.startswith('os2'):\n        return _tempfilepager(generator, 'more <', color)\n    if hasattr(os, 'system') and os.system('(less) 2>/dev/null') == 0:\n        return _pipepager(generator, 'less', color)\n    import tempfile\n    fd, filename = tempfile.mkstemp()\n    os.close(fd)\n    try:\n        if hasattr(os, 'system') and os.system(f'more \"{filename}\"') == 0:\n            return _pipepager(generator, 'more', color)\n        return _nullpager(stdout, generator, color)\n    finally:\n        os.unlink(filename)",
    "label": true
  },
  {
    "code": "def write_exports(exports, stream):\n    if sys.version_info[0] >= 3:\n        stream = codecs.getwriter('utf-8')(stream)\n    cp = configparser.ConfigParser()\n    for k, v in exports.items():\n        cp.add_section(k)\n        for entry in v.values():\n            if entry.suffix is None:\n                s = entry.prefix\n            else:\n                s = '%s:%s' % (entry.prefix, entry.suffix)\n            if entry.flags:\n                s = '%s [%s]' % (s, ', '.join(entry.flags))\n            cp.set(k, entry.name, s)\n    cp.write(stream)",
    "label": true
  },
  {
    "code": "def _bypass_ensure_directory(path):\n    if not WRITE_SUPPORT:\n        raise IOError('\"os.mkdir\" not supported on this platform.')\n    dirname, filename = split(path)\n    if dirname and filename and (not isdir(dirname)):\n        _bypass_ensure_directory(dirname)\n        try:\n            mkdir(dirname, 493)\n        except FileExistsError:\n            pass",
    "label": true
  },
  {
    "code": "def do_striptags(value: 't.Union[str, HasHTML]') -> str:\n    if hasattr(value, '__html__'):\n        value = t.cast('HasHTML', value).__html__()\n    return Markup(str(value)).striptags()",
    "label": true
  },
  {
    "code": "def is_connection_dropped(conn):\n    sock = getattr(conn, 'sock', False)\n    if sock is False:\n        return False\n    if sock is None:\n        return True\n    try:\n        return wait_for_read(sock, timeout=0.0)\n    except NoWayToWaitForSocketError:\n        return False",
    "label": true
  },
  {
    "code": "def render_trailing_whitespace(msg, _node, source_lines=None):\n    line = msg.line\n    start_index, end_index = (len(source_lines[line - 1].rstrip()), len(source_lines[line - 1]))\n    yield from render_context(line - 1, line, source_lines)\n    yield (line, slice(start_index, end_index), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 2, source_lines)",
    "label": true
  },
  {
    "code": "def preprocess(content: str) -> ReqFileLines:\n    lines_enum: ReqFileLines = enumerate(content.splitlines(), start=1)\n    lines_enum = join_lines(lines_enum)\n    lines_enum = ignore_comments(lines_enum)\n    lines_enum = expand_env_variables(lines_enum)\n    return lines_enum",
    "label": true
  },
  {
    "code": "def _check_download_dir(link: Link, download_dir: str, hashes: Optional[Hashes], warn_on_hash_mismatch: bool=True) -> Optional[str]:\n    download_path = os.path.join(download_dir, link.filename)\n    if not os.path.exists(download_path):\n        return None\n    logger.info('File was already downloaded %s', download_path)\n    if hashes:\n        try:\n            hashes.check_against_path(download_path)\n        except HashMismatch:\n            if warn_on_hash_mismatch:\n                logger.warning('Previously-downloaded file %s has bad hash. Re-downloading.', download_path)\n            os.unlink(download_path)\n            return None\n    return download_path",
    "label": true
  },
  {
    "code": "def find_package_path(name: str, package_dir: Mapping[str, str], root_dir: _Path) -> str:\n    parts = name.split('.')\n    for i in range(len(parts), 0, -1):\n        partial_name = '.'.join(parts[:i])\n        if partial_name in package_dir:\n            parent = package_dir[partial_name]\n            return os.path.join(root_dir, parent, *parts[i:])\n    parent = package_dir.get('') or ''\n    return os.path.join(root_dir, *parent.split('/'), *parts)",
    "label": true
  },
  {
    "code": "def _coerce_parse_result(results: Union[ParseResults, List[Any]]) -> List[Any]:\n    if isinstance(results, ParseResults):\n        return [_coerce_parse_result(i) for i in results]\n    else:\n        return results",
    "label": true
  },
  {
    "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = (path, netloc)\n    if auth:\n        netloc = '@'.join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:\n        path = ''\n    return urlunparse((scheme, netloc, path, '', query, fragment))",
    "label": true
  },
  {
    "code": "def handle_requirement_line(line: ParsedLine, options: Optional[optparse.Values]=None) -> ParsedRequirement:\n    line_comes_from = '{} {} (line {})'.format('-c' if line.constraint else '-r', line.filename, line.lineno)\n    assert line.is_requirement\n    if line.is_editable:\n        return ParsedRequirement(requirement=line.requirement, is_editable=line.is_editable, comes_from=line_comes_from, constraint=line.constraint)\n    else:\n        req_options = {}\n        for dest in SUPPORTED_OPTIONS_REQ_DEST:\n            if dest in line.opts.__dict__ and line.opts.__dict__[dest]:\n                req_options[dest] = line.opts.__dict__[dest]\n        line_source = f'line {line.lineno} of {line.filename}'\n        return ParsedRequirement(requirement=line.requirement, is_editable=line.is_editable, comes_from=line_comes_from, constraint=line.constraint, options=req_options, line_source=line_source)",
    "label": true
  },
  {
    "code": "def _lookup_in_mro(node, name) -> list:\n    attrs = node.locals.get(name, [])\n    nodes = itertools.chain.from_iterable((ancestor.locals.get(name, []) for ancestor in node.ancestors(recurs=True)))\n    values = list(itertools.chain(attrs, nodes))\n    if not values:\n        raise AttributeInferenceError(attribute=name, target=node)\n    return values",
    "label": true
  },
  {
    "code": "def random_combination(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    indices = sorted(sample(range(n), r))\n    return tuple((pool[i] for i in indices))",
    "label": true
  },
  {
    "code": "def fake_traceback(exc_value: BaseException, tb: t.Optional[TracebackType], filename: str, lineno: int) -> TracebackType:\n    if tb is not None:\n        locals = get_template_locals(tb.tb_frame.f_locals)\n        locals.pop('__jinja_exception__', None)\n    else:\n        locals = {}\n    globals = {'__name__': filename, '__file__': filename, '__jinja_exception__': exc_value}\n    code: CodeType = compile('\\n' * (lineno - 1) + 'raise __jinja_exception__', filename, 'exec')\n    location = 'template'\n    if tb is not None:\n        function = tb.tb_frame.f_code.co_name\n        if function == 'root':\n            location = 'top-level template code'\n        elif function.startswith('block_'):\n            location = f'block {function[6:]!r}'\n    if sys.version_info >= (3, 8):\n        code = code.replace(co_name=location)\n    else:\n        code = CodeType(code.co_argcount, code.co_kwonlyargcount, code.co_nlocals, code.co_stacksize, code.co_flags, code.co_code, code.co_consts, code.co_names, code.co_varnames, code.co_filename, location, code.co_firstlineno, code.co_lnotab, code.co_freevars, code.co_cellvars)\n    try:\n        exec(code, globals, locals)\n    except BaseException:\n        return sys.exc_info()[2].tb_next",
    "label": true
  },
  {
    "code": "def render_scope(scope: 'Mapping[str, Any]', *, title: Optional[TextType]=None, sort_keys: bool=True, indent_guides: bool=False, max_length: Optional[int]=None, max_string: Optional[int]=None) -> 'ConsoleRenderable':\n    highlighter = ReprHighlighter()\n    items_table = Table.grid(padding=(0, 1), expand=False)\n    items_table.add_column(justify='right')\n\n    def sort_items(item: Tuple[str, Any]) -> Tuple[bool, str]:\n        \"\"\"Sort special variables first, then alphabetically.\"\"\"\n        key, _ = item\n        return (not key.startswith('__'), key.lower())\n    items = sorted(scope.items(), key=sort_items) if sort_keys else scope.items()\n    for key, value in items:\n        key_text = Text.assemble((key, 'scope.key.special' if key.startswith('__') else 'scope.key'), (' =', 'scope.equals'))\n        items_table.add_row(key_text, Pretty(value, highlighter=highlighter, indent_guides=indent_guides, max_length=max_length, max_string=max_string))\n    return Panel.fit(items_table, title=title, border_style='scope.border', padding=(0, 1))",
    "label": true
  },
  {
    "code": "def get_lexer(environment: 'Environment') -> 'Lexer':\n    key = (environment.block_start_string, environment.block_end_string, environment.variable_start_string, environment.variable_end_string, environment.comment_start_string, environment.comment_end_string, environment.line_statement_prefix, environment.line_comment_prefix, environment.trim_blocks, environment.lstrip_blocks, environment.newline_sequence, environment.keep_trailing_newline)\n    lexer = _lexer_cache.get(key)\n    if lexer is None:\n        _lexer_cache[key] = lexer = Lexer(environment)\n    return lexer",
    "label": true
  },
  {
    "code": "def pytest_fixture_post_finalizer(fixturedef: FixtureDef[object]) -> None:\n    if fixturedef.cached_result is not None:\n        config = fixturedef._fixturemanager.config\n        if config.option.setupshow:\n            _show_fixture_action(fixturedef, 'TEARDOWN')\n            if hasattr(fixturedef, 'cached_param'):\n                del fixturedef.cached_param",
    "label": true
  },
  {
    "code": "def fib(n):\n    assert n >= 0\n    if n <= 1:\n        return n\n    else:\n        return fib(n - 1) + fib(n - 2)",
    "label": true
  },
  {
    "code": "def _idna_encode(name):\n    if name and any((ord(x) >= 128 for x in name)):\n        try:\n            from pip._vendor import idna\n        except ImportError:\n            six.raise_from(LocationParseError(\"Unable to parse URL without the 'idna' module\"), None)\n        try:\n            return idna.encode(name.lower(), strict=True, std3_rules=True)\n        except idna.IDNAError:\n            six.raise_from(LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name), None)\n    return name.lower().encode('ascii')",
    "label": true
  },
  {
    "code": "def parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> Tuple[Pos, list]:\n    pos += 1\n    array: list = []\n    pos = skip_comments_and_array_ws(src, pos)\n    if src.startswith(']', pos):\n        return (pos + 1, array)\n    while True:\n        pos, val = parse_value(src, pos, parse_float)\n        array.append(val)\n        pos = skip_comments_and_array_ws(src, pos)\n        c = src[pos:pos + 1]\n        if c == ']':\n            return (pos + 1, array)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed array')\n        pos += 1\n        pos = skip_comments_and_array_ws(src, pos)\n        if src.startswith(']', pos):\n            return (pos + 1, array)",
    "label": true
  },
  {
    "code": "def test_iter_encode():\n    assert b''.join(iter_encode([], 'latin1')) == b''\n    assert b''.join(iter_encode([''], 'latin1')) == b''\n    assert b''.join(iter_encode(['\u00e9'], 'latin1')) == b'\\xe9'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'latin1')) == b'\\xe9'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16')) == b'\\xe9\\x00'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16le')) == b'\\xe9\\x00'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16be')) == b'\\x00\\xe9'\n    assert b''.join(iter_encode(['', 'h\\uf7e9', '', 'llo'], 'x-user-defined')) == b'h\\xe9llo'",
    "label": true
  },
  {
    "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def normalize_path(path: Any) -> str:\n    str_path = str(path)\n    parent, file_name = os.path.split(str_path)\n    if parent:\n        raise ValueError(f'{path!r} must be only a file name')\n    return file_name",
    "label": true
  },
  {
    "code": "def _handle_ns(packageName, path_item):\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n    try:\n        spec = importer.find_spec(packageName)\n    except AttributeError:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            loader = importer.find_module(packageName)\n    else:\n        loader = spec.loader if spec else None\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = types.ModuleType(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module, '__path__'):\n        raise TypeError('Not a package:', packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        importlib.import_module(packageName)\n        _rebuild_mod_path(path, packageName, module)\n    return subpath",
    "label": true
  },
  {
    "code": "def _version2fieldlist(version):\n    if version == '1.0':\n        return _241_FIELDS\n    elif version == '1.1':\n        return _314_FIELDS\n    elif version == '1.2':\n        return _345_FIELDS\n    elif version in ('1.3', '2.1'):\n        return _345_FIELDS + tuple((f for f in _566_FIELDS if f not in _345_FIELDS))\n    elif version == '2.0':\n        raise ValueError('Metadata 2.0 is withdrawn and not supported')\n    elif version == '2.2':\n        return _643_FIELDS\n    raise MetadataUnrecognizedVersionError(version)",
    "label": true
  },
  {
    "code": "def build_class(name: str, basenames: Iterable[str]=(), doc: str | None=None) -> nodes.ClassDef:\n    node = nodes.ClassDef(name)\n    node.postinit(bases=[nodes.Name(name=base, parent=node) for base in basenames], body=[], decorators=None, doc_node=nodes.Const(value=doc) if doc else None)\n    return node",
    "label": true
  },
  {
    "code": "def _pathlib_compat(path):\n    try:\n        return path.__fspath__()\n    except AttributeError:\n        return str(path)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e211(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def get_session():\n    adapter = CacheControlAdapter(DictCache(), cache_etags=True, serializer=None, heuristic=None)\n    sess = requests.Session()\n    sess.mount('http://', adapter)\n    sess.mount('https://', adapter)\n    sess.cache_controller = adapter.controller\n    return sess",
    "label": true
  },
  {
    "code": "def split_when(iterable, pred, maxsplit=-1):\n    if maxsplit == 0:\n        yield list(iterable)\n        return\n    it = iter(iterable)\n    try:\n        cur_item = next(it)\n    except StopIteration:\n        return\n    buf = [cur_item]\n    for next_item in it:\n        if pred(cur_item, next_item):\n            yield buf\n            if maxsplit == 1:\n                yield ([next_item] + list(it))\n                return\n            buf = []\n            maxsplit -= 1\n        buf.append(next_item)\n        cur_item = next_item\n    yield buf",
    "label": true
  },
  {
    "code": "def print_results(hits: List['TransformedHit'], name_column_width: Optional[int]=None, terminal_width: Optional[int]=None) -> None:\n    if not hits:\n        return\n    if name_column_width is None:\n        name_column_width = max([len(hit['name']) + len(highest_version(hit.get('versions', ['-']))) for hit in hits]) + 4\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary'] or ''\n        latest = highest_version(hit.get('versions', ['-']))\n        if terminal_width is not None:\n            target_width = terminal_width - name_column_width - 5\n            if target_width > 10:\n                summary_lines = textwrap.wrap(summary, target_width)\n                summary = ('\\n' + ' ' * (name_column_width + 3)).join(summary_lines)\n        name_latest = f'{name} ({latest})'\n        line = f'{name_latest:{name_column_width}} - {summary}'\n        try:\n            write_output(line)\n            print_dist_installation_info(name, latest)\n        except UnicodeEncodeError:\n            pass",
    "label": true
  },
  {
    "code": "def url_to_path(url: str) -> str:\n    assert url.startswith('file:'), f'You can only turn file: urls into filenames (not {url!r})'\n    _, netloc, path, _, _ = urllib.parse.urlsplit(url)\n    if not netloc or netloc == 'localhost':\n        netloc = ''\n    elif WINDOWS:\n        netloc = '\\\\\\\\' + netloc\n    else:\n        raise ValueError(f'non-local file URIs are not supported on this platform: {url!r}')\n    path = urllib.request.url2pathname(netloc + path)\n    if WINDOWS and (not netloc) and (len(path) >= 3) and (path[0] == '/') and (path[1] in string.ascii_letters) and (path[2:4] in (':', ':/')):\n        path = path[1:]\n    return path",
    "label": true
  },
  {
    "code": "def calculate_average_condition(bridge: list, start: int, stop: int) -> float:\n    if bridge == []:\n        return MISSING_BCI\n    years = bridge[COLUMN_BCI][INDEX_BCI_YEARS]\n    scores = bridge[COLUMN_BCI][INDEX_BCI_SCORES]\n    index_of_years_in_range = []\n    for i in range(len(years)):\n        if start <= int(years[i]) < stop:\n            index_of_years_in_range.append(i)\n    bci_scores = []\n    for i in index_of_years_in_range:\n        if years[i] != MISSING_BCI:\n            bci_scores.append(scores[i])\n    if index_of_years_in_range == [] or bci_scores == []:\n        return 0.0\n    return sum(bci_scores) / len(bci_scores)",
    "label": true
  },
  {
    "code": "def calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n    lon1, lat1, lon2, lat2 = (math.radians(lon1), math.radians(lat1), math.radians(lon2), math.radians(lat2))\n    lon_diff = lon2 - lon1\n    lat_diff = lat2 - lat1\n    a = math.sin(lat_diff / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(lon_diff / 2) ** 2\n    c = 2 * math.asin(math.sqrt(a))\n    return round(c * EARTH_RADIUS, 3)",
    "label": true
  },
  {
    "code": "def ignore_comments(lines_enum: ReqFileLines) -> ReqFileLines:\n    for line_number, line in lines_enum:\n        line = COMMENT_RE.sub('', line)\n        line = line.strip()\n        if line:\n            yield (line_number, line)",
    "label": true
  },
  {
    "code": "def insert_missing_modules(modules: Dict[str, ModuleType], module_name: str) -> None:\n    module_parts = module_name.split('.')\n    child_module: Union[ModuleType, None] = None\n    module: Union[ModuleType, None] = None\n    child_name: str = ''\n    while module_name:\n        if module_name not in modules:\n            try:\n                if not sys.meta_path:\n                    raise ModuleNotFoundError\n                module = importlib.import_module(module_name)\n            except ModuleNotFoundError:\n                module = ModuleType(module_name, doc=\"Empty module created by pytest's importmode=importlib.\")\n        else:\n            module = modules[module_name]\n        if child_module:\n            if not hasattr(module, child_name):\n                setattr(module, child_name, child_module)\n                modules[module_name] = module\n        child_module, child_name = (module, module_name.rpartition('.')[-1])\n        module_parts.pop(-1)\n        module_name = '.'.join(module_parts)",
    "label": true
  },
  {
    "code": "def trace_parse_action(f: ParseAction) -> ParseAction:\n    f = _trim_arity(f)\n\n    def z(*paArgs):\n        thisFunc = f.__name__\n        s, l, t = paArgs[-3:]\n        if len(paArgs) > 3:\n            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc\n        sys.stderr.write(f'>>entering {thisFunc}(line: {line(l, s)!r}, {l}, {t!r})\\n')\n        try:\n            ret = f(*paArgs)\n        except Exception as exc:\n            sys.stderr.write(f'<<leaving {thisFunc} (exception: {exc})\\n')\n            raise\n        sys.stderr.write(f'<<leaving {thisFunc} (ret: {ret!r})\\n')\n        return ret\n    z.__name__ = f.__name__\n    return z",
    "label": true
  },
  {
    "code": "def load_plugins() -> None:\n    for ep in entry_points(group='typeguard.checker_lookup'):\n        try:\n            plugin = ep.load()\n        except Exception as exc:\n            warnings.warn(f'Failed to load plugin {ep.name!r}: {qualified_name(exc)}: {exc}', stacklevel=2)\n            continue\n        if not callable(plugin):\n            warnings.warn(f'Plugin {ep} returned a non-callable object: {plugin!r}', stacklevel=2)\n            continue\n        checker_lookup_functions.insert(0, plugin)",
    "label": true
  },
  {
    "code": "def get_lexer_for_mimetype(_mime, **options):\n    for modname, name, _, _, mimetypes in LEXERS.values():\n        if _mime in mimetypes:\n            if name not in _lexer_cache:\n                _load_lexers(modname)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _mime in cls.mimetypes:\n            return cls(**options)\n    raise ClassNotFound('no lexer for mimetype %r found' % _mime)",
    "label": true
  },
  {
    "code": "def test_method_with_internal_import_should_work():\n    import re\n    back_fn = dill.loads(dill.dumps(get_fun_with_internal_import()))\n    import inspect\n    if hasattr(inspect, 'getclosurevars'):\n        vars = inspect.getclosurevars(back_fn)\n        assert vars.globals == {}\n        assert vars.nonlocals == {}\n    assert back_fn() == re.compile('$')\n    assert '__builtins__' in back_fn.__globals__",
    "label": true
  },
  {
    "code": "def parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:\n    if multiline:\n        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape_multiline\n    else:\n        error_on = ILLEGAL_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape\n    result = ''\n    start_pos = pos\n    while True:\n        try:\n            char = src[pos]\n        except IndexError:\n            raise suffixed_err(src, pos, 'Unterminated string') from None\n        if char == '\"':\n            if not multiline:\n                return (pos + 1, result + src[start_pos:pos])\n            if src.startswith('\"\"\"', pos):\n                return (pos + 3, result + src[start_pos:pos])\n            pos += 1\n            continue\n        if char == '\\\\':\n            result += src[start_pos:pos]\n            pos, parsed_escape = parse_escapes(src, pos)\n            result += parsed_escape\n            start_pos = pos\n            continue\n        if char in error_on:\n            raise suffixed_err(src, pos, f'Illegal character {char!r}')\n        pos += 1",
    "label": true
  },
  {
    "code": "def _replace_multiple(value, needles_and_replacements):\n\n    def replacer(match):\n        return needles_and_replacements[match.group(0)]\n    pattern = re.compile('|'.join([re.escape(needle) for needle in needles_and_replacements.keys()]))\n    result = pattern.sub(replacer, value)\n    return result",
    "label": true
  },
  {
    "code": "def _handle_python_version(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    version_info, error_msg = _convert_python_version(value)\n    if error_msg is not None:\n        msg = 'invalid --python-version value: {!r}: {}'.format(value, error_msg)\n        raise_option_error(parser, option=option, msg=msg)\n    parser.values.python_version = version_info",
    "label": true
  },
  {
    "code": "def get_lexer_for_mimetype(_mime, **options):\n    for modname, name, _, _, mimetypes in LEXERS.values():\n        if _mime in mimetypes:\n            if name not in _lexer_cache:\n                _load_lexers(modname)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _mime in cls.mimetypes:\n            return cls(**options)\n    raise ClassNotFound('no lexer for mimetype %r found' % _mime)",
    "label": true
  },
  {
    "code": "def _display_value(value: Any, max_length: int=_DEFAULT_MAX_VALUE_LENGTH) -> str:\n    s = repr(value)\n    if not DEBUG_CONTRACTS and len(s) > max_length:\n        i = (max_length - 3) // 2\n        return s[:i] + '...' + s[-i:]\n    else:\n        return s",
    "label": true
  },
  {
    "code": "def build_source(location: str, *, candidates_from_page: CandidatesFromPage, page_validator: PageValidator, expand_dir: bool, cache_link_parsing: bool) -> Tuple[Optional[str], Optional[LinkSource]]:\n    path: Optional[str] = None\n    url: Optional[str] = None\n    if os.path.exists(location):\n        url = path_to_url(location)\n        path = location\n    elif location.startswith('file:'):\n        url = location\n        path = url_to_path(location)\n    elif is_url(location):\n        url = location\n    if url is None:\n        msg = \"Location '%s' is ignored: it is either a non-existing path or lacks a specific scheme.\"\n        logger.warning(msg, location)\n        return (None, None)\n    if path is None:\n        source: LinkSource = _RemoteFileSource(candidates_from_page=candidates_from_page, page_validator=page_validator, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    if os.path.isdir(path):\n        if expand_dir:\n            source = _FlatDirectorySource(candidates_from_page=candidates_from_page, path=path)\n        else:\n            source = _IndexDirectorySource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    elif os.path.isfile(path):\n        source = _LocalFileSource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    logger.warning(\"Location '%s' is ignored: it is neither a file nor a directory.\", location)\n    return (url, None)",
    "label": true
  },
  {
    "code": "def h(x):\n\n    def g(x):\n        return x\n    return g(x) - x",
    "label": true
  },
  {
    "code": "def _match_vcs_scheme(url: str) -> Optional[str]:\n    for scheme in vcs.schemes:\n        if url.lower().startswith(scheme) and url[len(scheme)] in '+:':\n            return scheme\n    return None",
    "label": true
  },
  {
    "code": "def service_request_transform(node):\n    code = '\\n    def __getattr__(self, attr):\\n        return 0\\n    '\n    func_getattr = extract_node(code)\n    node.locals['__getattr__'] = [func_getattr]\n    return node",
    "label": true
  },
  {
    "code": "def _normalize_extra_values(results: Any) -> Any:\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == 'extra':\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == 'extra':\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = (lhs, op, rhs)\n    return results",
    "label": true
  },
  {
    "code": "def _proxy_helper(obj):\n    _repr = repr(obj)\n    try:\n        _str = str(obj)\n    except ReferenceError:\n        return id(None)\n    if _str == _repr:\n        return id(obj)\n    try:\n        address = int(_str.rstrip('>').split(' at ')[-1], base=16)\n    except ValueError:\n        if not IS_PYPY:\n            address = int(_repr.rstrip('>').split(' at ')[-1], base=16)\n        else:\n            objects = iter(gc.get_objects())\n            for _obj in objects:\n                if repr(_obj) == _str:\n                    return id(_obj)\n            msg = \"Cannot reference object for proxy at '%s'\" % id(obj)\n            raise ReferenceError(msg)\n    return address",
    "label": true
  },
  {
    "code": "def _enable_all_extensions(run: Run, value: str | None) -> None:\n    assert value is None\n    for filename in Path(extensions.__file__).parent.iterdir():\n        if filename.suffix == '.py' and (not filename.stem.startswith('_')):\n            extension_name = f'pylint.extensions.{filename.stem}'\n            if extension_name not in run._plugins:\n                run._plugins.append(extension_name)",
    "label": true
  },
  {
    "code": "def check_typevar(value: Any, origin_type: TypeVar, args: tuple[Any, ...], memo: TypeCheckMemo, *, subclass_check: bool=False) -> None:\n    if origin_type.__bound__ is not None:\n        annotation = Type[origin_type.__bound__] if subclass_check else origin_type.__bound__\n        check_type_internal(value, annotation, memo)\n    elif origin_type.__constraints__:\n        for constraint in origin_type.__constraints__:\n            annotation = Type[constraint] if subclass_check else constraint\n            try:\n                check_type_internal(value, annotation, memo)\n            except TypeCheckError:\n                pass\n            else:\n                break\n        else:\n            formatted_constraints = ', '.join((get_type_name(constraint) for constraint in origin_type.__constraints__))\n            raise TypeCheckError(f'does not match any of the constraints ({formatted_constraints})')",
    "label": true
  },
  {
    "code": "def dumpIO_source(object, **kwds):\n    from .source import importable, getname\n    from io import BytesIO as StringIO\n    alias = kwds.pop('alias', '')\n    name = str(alias) or getname(object)\n    name = '\\n#NAME: %s\\n' % name\n    file = StringIO()\n    file.write(b(''.join([importable(object, alias=alias), name])))\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def make_player(generic_name: str) -> Player:\n    name = input(f'Enter a name for {generic_name}: ')\n    type = input('What type would you like this player to be?\\n(1 - Random Player, 2 - User Player, 3 - Strategic Player)\\n')\n    while not type.isnumeric() or not 1 <= int(type) <= 3:\n        print('Oops, looks like you entered something invalid.')\n        type = input('What type would you like this player to be?\\n(1 - Random Player, 2 - User Player, 3 - Strategic Player)')\n    type = int(type)\n    if type == 1:\n        return RandomPlayer(name)\n    elif type == 2:\n        return UserPlayer(name)\n    elif type == 3:\n        return StrategicPlayer(name)",
    "label": true
  },
  {
    "code": "def wrap_stream(stream, convert, strip, autoreset, wrap):\n    if wrap:\n        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)\n        if wrapper.should_wrap():\n            stream = wrapper.stream\n    return stream",
    "label": true
  },
  {
    "code": "def _http_transform():\n    code = textwrap.dedent('\\n    from enum import IntEnum\\n    from collections import namedtuple\\n    _HTTPStatus = namedtuple(\\'_HTTPStatus\\', \\'value phrase description\\')\\n\\n    class HTTPStatus(IntEnum):\\n\\n        @property\\n        def phrase(self):\\n            return \"\"\\n        @property\\n        def value(self):\\n            return 0\\n        @property\\n        def description(self):\\n            return \"\"\\n\\n        # informational\\n        CONTINUE = _HTTPStatus(100, \\'Continue\\', \\'Request received, please continue\\')\\n        SWITCHING_PROTOCOLS = _HTTPStatus(101, \\'Switching Protocols\\',\\n                \\'Switching to new protocol; obey Upgrade header\\')\\n        PROCESSING = _HTTPStatus(102, \\'Processing\\', \\'\\')\\n        OK = _HTTPStatus(200, \\'OK\\', \\'Request fulfilled, document follows\\')\\n        CREATED = _HTTPStatus(201, \\'Created\\', \\'Document created, URL follows\\')\\n        ACCEPTED = _HTTPStatus(202, \\'Accepted\\',\\n            \\'Request accepted, processing continues off-line\\')\\n        NON_AUTHORITATIVE_INFORMATION = _HTTPStatus(203,\\n            \\'Non-Authoritative Information\\', \\'Request fulfilled from cache\\')\\n        NO_CONTENT = _HTTPStatus(204, \\'No Content\\', \\'Request fulfilled, nothing follows\\')\\n        RESET_CONTENT =_HTTPStatus(205, \\'Reset Content\\', \\'Clear input form for further input\\')\\n        PARTIAL_CONTENT = _HTTPStatus(206, \\'Partial Content\\', \\'Partial content follows\\')\\n        MULTI_STATUS = _HTTPStatus(207, \\'Multi-Status\\', \\'\\')\\n        ALREADY_REPORTED = _HTTPStatus(208, \\'Already Reported\\', \\'\\')\\n        IM_USED = _HTTPStatus(226, \\'IM Used\\', \\'\\')\\n        MULTIPLE_CHOICES = _HTTPStatus(300, \\'Multiple Choices\\',\\n            \\'Object has several resources -- see URI list\\')\\n        MOVED_PERMANENTLY = _HTTPStatus(301, \\'Moved Permanently\\',\\n            \\'Object moved permanently -- see URI list\\')\\n        FOUND = _HTTPStatus(302, \\'Found\\', \\'Object moved temporarily -- see URI list\\')\\n        SEE_OTHER = _HTTPStatus(303, \\'See Other\\', \\'Object moved -- see Method and URL list\\')\\n        NOT_MODIFIED = _HTTPStatus(304, \\'Not Modified\\',\\n            \\'Document has not changed since given time\\')\\n        USE_PROXY = _HTTPStatus(305, \\'Use Proxy\\',\\n            \\'You must use proxy specified in Location to access this resource\\')\\n        TEMPORARY_REDIRECT = _HTTPStatus(307, \\'Temporary Redirect\\',\\n            \\'Object moved temporarily -- see URI list\\')\\n        PERMANENT_REDIRECT = _HTTPStatus(308, \\'Permanent Redirect\\',\\n            \\'Object moved permanently -- see URI list\\')\\n        BAD_REQUEST = _HTTPStatus(400, \\'Bad Request\\',\\n            \\'Bad request syntax or unsupported method\\')\\n        UNAUTHORIZED = _HTTPStatus(401, \\'Unauthorized\\',\\n            \\'No permission -- see authorization schemes\\')\\n        PAYMENT_REQUIRED = _HTTPStatus(402, \\'Payment Required\\',\\n            \\'No payment -- see charging schemes\\')\\n        FORBIDDEN = _HTTPStatus(403, \\'Forbidden\\',\\n            \\'Request forbidden -- authorization will not help\\')\\n        NOT_FOUND = _HTTPStatus(404, \\'Not Found\\',\\n            \\'Nothing matches the given URI\\')\\n        METHOD_NOT_ALLOWED = _HTTPStatus(405, \\'Method Not Allowed\\',\\n            \\'Specified method is invalid for this resource\\')\\n        NOT_ACCEPTABLE = _HTTPStatus(406, \\'Not Acceptable\\',\\n            \\'URI not available in preferred format\\')\\n        PROXY_AUTHENTICATION_REQUIRED = _HTTPStatus(407,\\n            \\'Proxy Authentication Required\\',\\n            \\'You must authenticate with this proxy before proceeding\\')\\n        REQUEST_TIMEOUT = _HTTPStatus(408, \\'Request Timeout\\',\\n            \\'Request timed out; try again later\\')\\n        CONFLICT = _HTTPStatus(409, \\'Conflict\\', \\'Request conflict\\')\\n        GONE = _HTTPStatus(410, \\'Gone\\',\\n            \\'URI no longer exists and has been permanently removed\\')\\n        LENGTH_REQUIRED = _HTTPStatus(411, \\'Length Required\\',\\n            \\'Client must specify Content-Length\\')\\n        PRECONDITION_FAILED = _HTTPStatus(412, \\'Precondition Failed\\',\\n            \\'Precondition in headers is false\\')\\n        REQUEST_ENTITY_TOO_LARGE = _HTTPStatus(413, \\'Request Entity Too Large\\',\\n            \\'Entity is too large\\')\\n        REQUEST_URI_TOO_LONG = _HTTPStatus(414, \\'Request-URI Too Long\\',\\n            \\'URI is too long\\')\\n        UNSUPPORTED_MEDIA_TYPE = _HTTPStatus(415, \\'Unsupported Media Type\\',\\n            \\'Entity body in unsupported format\\')\\n        REQUESTED_RANGE_NOT_SATISFIABLE = _HTTPStatus(416,\\n            \\'Requested Range Not Satisfiable\\',\\n            \\'Cannot satisfy request range\\')\\n        EXPECTATION_FAILED = _HTTPStatus(417, \\'Expectation Failed\\',\\n            \\'Expect condition could not be satisfied\\')\\n        MISDIRECTED_REQUEST = _HTTPStatus(421, \\'Misdirected Request\\',\\n            \\'Server is not able to produce a response\\')\\n        UNPROCESSABLE_ENTITY = _HTTPStatus(422, \\'Unprocessable Entity\\')\\n        LOCKED = _HTTPStatus(423, \\'Locked\\')\\n        FAILED_DEPENDENCY = _HTTPStatus(424, \\'Failed Dependency\\')\\n        UPGRADE_REQUIRED = _HTTPStatus(426, \\'Upgrade Required\\')\\n        PRECONDITION_REQUIRED = _HTTPStatus(428, \\'Precondition Required\\',\\n            \\'The origin server requires the request to be conditional\\')\\n        TOO_MANY_REQUESTS = _HTTPStatus(429, \\'Too Many Requests\\',\\n            \\'The user has sent too many requests in \\'\\n            \\'a given amount of time (\"rate limiting\")\\')\\n        REQUEST_HEADER_FIELDS_TOO_LARGE = _HTTPStatus(431,\\n            \\'Request Header Fields Too Large\\',\\n            \\'The server is unwilling to process the request because its header \\'\\n            \\'fields are too large\\')\\n        UNAVAILABLE_FOR_LEGAL_REASONS = _HTTPStatus(451,\\n            \\'Unavailable For Legal Reasons\\',\\n            \\'The server is denying access to the \\'\\n            \\'resource as a consequence of a legal demand\\')\\n        INTERNAL_SERVER_ERROR = _HTTPStatus(500, \\'Internal Server Error\\',\\n            \\'Server got itself in trouble\\')\\n        NOT_IMPLEMENTED = _HTTPStatus(501, \\'Not Implemented\\',\\n            \\'Server does not support this operation\\')\\n        BAD_GATEWAY = _HTTPStatus(502, \\'Bad Gateway\\',\\n            \\'Invalid responses from another server/proxy\\')\\n        SERVICE_UNAVAILABLE = _HTTPStatus(503, \\'Service Unavailable\\',\\n            \\'The server cannot process the request due to a high load\\')\\n        GATEWAY_TIMEOUT = _HTTPStatus(504, \\'Gateway Timeout\\',\\n            \\'The gateway server did not receive a timely response\\')\\n        HTTP_VERSION_NOT_SUPPORTED = _HTTPStatus(505, \\'HTTP Version Not Supported\\',\\n            \\'Cannot fulfill request\\')\\n        VARIANT_ALSO_NEGOTIATES = _HTTPStatus(506, \\'Variant Also Negotiates\\')\\n        INSUFFICIENT_STORAGE = _HTTPStatus(507, \\'Insufficient Storage\\')\\n        LOOP_DETECTED = _HTTPStatus(508, \\'Loop Detected\\')\\n        NOT_EXTENDED = _HTTPStatus(510, \\'Not Extended\\')\\n        NETWORK_AUTHENTICATION_REQUIRED = _HTTPStatus(511,\\n            \\'Network Authentication Required\\',\\n            \\'The client needs to authenticate to gain network access\\')\\n    ')\n    return AstroidBuilder(AstroidManager()).string_build(code)",
    "label": true
  },
  {
    "code": "def _get_line_with_reprcrash_message(config: Config, rep: BaseReport, tw: TerminalWriter, word_markup: Dict[str, bool]) -> str:\n    verbose_word = rep._get_verbose_word(config)\n    word = tw.markup(verbose_word, **word_markup)\n    node = _get_node_id_with_markup(tw, config, rep)\n    line = f'{word} {node}'\n    line_width = wcswidth(line)\n    try:\n        msg = rep.longrepr.reprcrash.message\n    except AttributeError:\n        pass\n    else:\n        if not running_on_ci():\n            available_width = tw.fullwidth - line_width\n            msg = _format_trimmed(' - {}', msg, available_width)\n        else:\n            msg = f' - {msg}'\n        if msg is not None:\n            line += msg\n    return line",
    "label": true
  },
  {
    "code": "def test_pickled_inner():\n    add5 = adder(x)\n    pinner = pickle.dumps(add5)\n    p5add = pickle.loads(pinner)\n    assert p5add(y) == x + y",
    "label": true
  },
  {
    "code": "def test_x_user_defined():\n    encoded = b'2,\\x0c\\x0b\\x1aO\\xd9#\\xcb\\x0f\\xc9\\xbbt\\xcf\\xa8\\xca'\n    decoded = '2,\\x0c\\x0b\\x1aO\\uf7d9#\\uf7cb\\x0f\\uf7c9\\uf7bbt\\uf7cf\\uf7a8\\uf7ca'\n    encoded = b'aa'\n    decoded = 'aa'\n    assert decode(encoded, 'x-user-defined') == (decoded, lookup('x-user-defined'))\n    assert encode(decoded, 'x-user-defined') == encoded",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def _infer_decorator_callchain(node):\n    if not isinstance(node, FunctionDef):\n        return None\n    if not node.parent:\n        return None\n    try:\n        result = next(node.infer_call_result(node.parent), None)\n    except InferenceError:\n        return None\n    if isinstance(result, bases.Instance):\n        result = result._proxied\n    if isinstance(result, ClassDef):\n        if result.is_subtype_of('builtins.classmethod'):\n            return 'classmethod'\n        if result.is_subtype_of('builtins.staticmethod'):\n            return 'staticmethod'\n    if isinstance(result, FunctionDef):\n        if not result.decorators:\n            return None\n        for decorator in result.decorators.nodes:\n            if isinstance(decorator, node_classes.Name):\n                if decorator.name in BUILTIN_DESCRIPTORS:\n                    return decorator.name\n            if isinstance(decorator, node_classes.Attribute) and isinstance(decorator.expr, node_classes.Name) and (decorator.expr.name == 'builtins') and (decorator.attrname in BUILTIN_DESCRIPTORS):\n                return decorator.attrname\n    return None",
    "label": true
  },
  {
    "code": "def _get_python_inc_posix_prefix(prefix):\n    implementation = 'pypy' if IS_PYPY else 'python'\n    python_dir = implementation + get_python_version() + build_flags\n    return os.path.join(prefix, 'include', python_dir)",
    "label": true
  },
  {
    "code": "def guess_lexer(_text, **options):\n    if not isinstance(_text, str):\n        inencoding = options.get('inencoding', options.get('encoding'))\n        if inencoding:\n            _text = _text.decode(inencoding or 'utf8')\n        else:\n            _text, _ = guess_decode(_text)\n    ft = get_filetype_from_buffer(_text)\n    if ft is not None:\n        try:\n            return get_lexer_by_name(ft, **options)\n        except ClassNotFound:\n            pass\n    best_lexer = [0.0, None]\n    for lexer in _iter_lexerclasses():\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        if rv > best_lexer[0]:\n            best_lexer[:] = (rv, lexer)\n    if not best_lexer[0] or best_lexer[1] is None:\n        raise ClassNotFound('no lexer matching the text found')\n    return best_lexer[1](**options)",
    "label": true
  },
  {
    "code": "def _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n    if len(lines) < 2 or lines[0][:4] != 'musl':\n        return None\n    m = re.match('Version (\\\\d+)\\\\.(\\\\d+)', lines[1])\n    if not m:\n        return None\n    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))",
    "label": true
  },
  {
    "code": "def _parse_version_many(tokenizer: Tokenizer) -> str:\n    parsed_specifiers = ''\n    while tokenizer.check('SPECIFIER'):\n        span_start = tokenizer.position\n        parsed_specifiers += tokenizer.read().text\n        if tokenizer.check('VERSION_PREFIX_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('.* suffix can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position + 1)\n        if tokenizer.check('VERSION_LOCAL_LABEL_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('Local version label can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position)\n        tokenizer.consume('WS')\n        if not tokenizer.check('COMMA'):\n            break\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def set_file_position(body, pos):\n    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except (IOError, OSError):\n            pos = _FAILEDTELL\n    return pos",
    "label": true
  },
  {
    "code": "def check_byteslike(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, (bytearray, bytes, memoryview)):\n        raise TypeCheckError('is not bytes-like')",
    "label": true
  },
  {
    "code": "def _forgiving_version(version):\n    version = version.replace(' ', '.')\n    match = _PEP440_FALLBACK.search(version)\n    if match:\n        safe = match['safe']\n        rest = version[len(safe):]\n    else:\n        safe = '0'\n        rest = version\n    local = f'sanitized.{_safe_segment(rest)}'.strip('.')\n    return f'{safe}.dev0+{local}'",
    "label": true
  },
  {
    "code": "def tags_f(args):\n    from .tags import tags\n    names = (tags(wheel, args.python_tag, args.abi_tag, args.platform_tag, args.build, args.remove) for wheel in args.wheel)\n    for name in names:\n        print(name)",
    "label": true
  },
  {
    "code": "def _check_class_type_annotations(klass: type, instance: Any) -> None:\n    klass_mod = _get_module(klass)\n    cls_annotations = typing.get_type_hints(klass, localns=klass_mod.__dict__)\n    for attr, annotation in cls_annotations.items():\n        value = getattr(instance, attr)\n        try:\n            _debug(f'Checking type of attribute {attr} for {klass.__qualname__} instance')\n            check_type(value, annotation, collection_check_strategy=CollectionCheckStrategy.ALL_ITEMS)\n        except TypeCheckError:\n            raise AssertionError(f'{_display_value(value)} did not match type annotation for attribute {attr}: {_display_annotation(annotation)}')",
    "label": true
  },
  {
    "code": "def get_parser_module(type_comments: bool=True) -> ParserModule:\n    parser_module = ast\n    if type_comments and _ast_py3:\n        parser_module = _ast_py3\n    unary_op_classes = _unary_operators_from_module(parser_module)\n    cmp_op_classes = _compare_operators_from_module(parser_module)\n    bool_op_classes = _bool_operators_from_module(parser_module)\n    bin_op_classes = _binary_operators_from_module(parser_module)\n    context_classes = _contexts_from_module(parser_module)\n    return ParserModule(parser_module, unary_op_classes, cmp_op_classes, bool_op_classes, bin_op_classes, context_classes)",
    "label": true
  },
  {
    "code": "def _zip_equal_generator(iterables):\n    for combo in zip_longest(*iterables, fillvalue=_marker):\n        for val in combo:\n            if val is _marker:\n                raise UnequalIterablesError()\n        yield combo",
    "label": true
  },
  {
    "code": "def create_dict_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot declare {key} twice')\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.get_or_create_nest(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']', pos):\n        raise suffixed_err(src, pos, \"Expected ']' at the end of a table declaration\")\n    return (pos + 1, key)",
    "label": true
  },
  {
    "code": "def derive_importpath(import_path: str, raising: bool) -> Tuple[str, object]:\n    if not isinstance(import_path, str) or '.' not in import_path:\n        raise TypeError(f'must be absolute import path string, not {import_path!r}')\n    module, attr = import_path.rsplit('.', 1)\n    target = resolve(module)\n    if raising:\n        annotated_getattr(target, attr, ann=module)\n    return (attr, target)",
    "label": true
  },
  {
    "code": "def _is_present_dir(path: Traversable) -> bool:\n    with contextlib.suppress(FileNotFoundError):\n        return path.is_dir()\n    return False",
    "label": true
  },
  {
    "code": "def enum(*sequential: Any, **named: Any) -> Type[Any]:\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    reverse = {value: key for key, value in enums.items()}\n    enums['reverse_mapping'] = reverse\n    return type('Enum', (), enums)",
    "label": true
  },
  {
    "code": "def windowed_complete(iterable, n):\n    if n < 0:\n        raise ValueError('n must be >= 0')\n    seq = tuple(iterable)\n    size = len(seq)\n    if n > size:\n        raise ValueError('n must be <= len(seq)')\n    for i in range(size - n + 1):\n        beginning = seq[:i]\n        middle = seq[i:i + n]\n        end = seq[i + n:]\n        yield (beginning, middle, end)",
    "label": true
  },
  {
    "code": "def distinct_combinations(iterable, r):\n    if r < 0:\n        raise ValueError('r must be non-negative')\n    elif r == 0:\n        yield ()\n        return\n    pool = tuple(iterable)\n    generators = [unique_everseen(enumerate(pool), key=itemgetter(1))]\n    current_combo = [None] * r\n    level = 0\n    while generators:\n        try:\n            cur_idx, p = next(generators[-1])\n        except StopIteration:\n            generators.pop()\n            level -= 1\n            continue\n        current_combo[level] = p\n        if level + 1 == r:\n            yield tuple(current_combo)\n        else:\n            generators.append(unique_everseen(enumerate(pool[cur_idx + 1:], cur_idx + 1), key=itemgetter(1)))\n            level += 1",
    "label": true
  },
  {
    "code": "def match_to_number(match: 're.Match', parse_float: 'ParseFloat') -> Any:\n    if match.group('floatpart'):\n        return parse_float(match.group())\n    return int(match.group(), 0)",
    "label": true
  },
  {
    "code": "def _rx_indent(level):\n    tab_width = 8\n    if tab_width == 2:\n        space_repeat = '+'\n    else:\n        space_repeat = '{1,%d}' % (tab_width - 1)\n    if level == 1:\n        level_repeat = ''\n    else:\n        level_repeat = '{%s}' % level\n    return '(?:\\\\t| %s\\\\t| {%s})%s.*\\\\n' % (space_repeat, tab_width, level_repeat)",
    "label": true
  },
  {
    "code": "def wtf(x, y, z):\n\n    def zzz():\n        return x\n\n    def yyy():\n        return y\n\n    def xxx():\n        return z\n    return (zzz, yyy)",
    "label": true
  },
  {
    "code": "def _is_builtin_module(module):\n    if not hasattr(module, '__file__'):\n        return True\n    if module.__file__ is None:\n        return False\n    names = ['base_prefix', 'base_exec_prefix', 'exec_prefix', 'prefix', 'real_prefix']\n    rp = os.path.realpath\n    return any((module.__file__.startswith(getattr(sys, name)) or rp(module.__file__).startswith(rp(getattr(sys, name))) for name in names if hasattr(sys, name))) or module.__file__.endswith(EXTENSION_SUFFIXES) or 'site-packages' in module.__file__",
    "label": true
  },
  {
    "code": "def get_process_umask():\n    result = os.umask(18)\n    os.umask(result)\n    return result",
    "label": true
  },
  {
    "code": "def try_cleanup(path: Path, consider_lock_dead_if_created_before: float) -> None:\n    if ensure_deletable(path, consider_lock_dead_if_created_before):\n        maybe_delete_a_numbered_dir(path)",
    "label": true
  },
  {
    "code": "def evaluate_marker(text, extra=None):\n    try:\n        marker = packaging.markers.Marker(text)\n        return marker.evaluate()\n    except packaging.markers.InvalidMarker as e:\n        raise SyntaxError(e) from e",
    "label": true
  },
  {
    "code": "def parse_name_and_version(p):\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException(\"Ill-formed name/version string: '%s'\" % p)\n    d = m.groupdict()\n    return (d['name'].strip().lower(), d['ver'])",
    "label": true
  },
  {
    "code": "def reset_all():\n    if AnsiToWin32 is not None:\n        AnsiToWin32(orig_stdout).reset_all()",
    "label": true
  },
  {
    "code": "def _generate_tree_from_node(node: ReadNode) -> HuffmanTree:\n    if node.l_type == 1:\n        left_tree = HuffmanTree(None)\n        left_tree.number = node.l_data\n    else:\n        left_tree = HuffmanTree(node.l_data)\n    if node.r_type == 1:\n        right_tree = HuffmanTree(None)\n        right_tree.number = node.r_data\n    else:\n        right_tree = HuffmanTree(node.r_data)\n    return HuffmanTree(None, left_tree, right_tree)",
    "label": true
  },
  {
    "code": "def check_file(filename: Union[str, Path], show_diff: Union[bool, TextIO]=False, config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, disregard_skip: bool=True, extension: Optional[str]=None, **config_kwargs: Any) -> bool:\n    file_config: Config = config\n    if 'config_trie' in config_kwargs:\n        config_trie = config_kwargs.pop('config_trie', None)\n        if config_trie:\n            config_info = config_trie.search(filename)\n            if config.verbose:\n                print(f'{config_info[0]} used for file {filename}')\n            file_config = Config(**config_info[1])\n    with io.File.read(filename) as source_file:\n        return check_stream(source_file.stream, show_diff=show_diff, extension=extension, config=file_config, file_path=file_path or source_file.path, disregard_skip=disregard_skip, **config_kwargs)",
    "label": true
  },
  {
    "code": "def guess_lexer(_text, **options):\n    if not isinstance(_text, str):\n        inencoding = options.get('inencoding', options.get('encoding'))\n        if inencoding:\n            _text = _text.decode(inencoding or 'utf8')\n        else:\n            _text, _ = guess_decode(_text)\n    ft = get_filetype_from_buffer(_text)\n    if ft is not None:\n        try:\n            return get_lexer_by_name(ft, **options)\n        except ClassNotFound:\n            pass\n    best_lexer = [0.0, None]\n    for lexer in _iter_lexerclasses():\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        if rv > best_lexer[0]:\n            best_lexer[:] = (rv, lexer)\n    if not best_lexer[0] or best_lexer[1] is None:\n        raise ClassNotFound('no lexer matching the text found')\n    return best_lexer[1](**options)",
    "label": true
  },
  {
    "code": "def _makeTags(tagStr, xml, suppress_LT=Suppress('<'), suppress_GT=Suppress('>')):\n    if isinstance(tagStr, str_type):\n        resname = tagStr\n        tagStr = Keyword(tagStr, caseless=not xml)\n    else:\n        resname = tagStr.name\n    tagAttrName = Word(alphas, alphanums + '_-:')\n    if xml:\n        tagAttrValue = dbl_quoted_string.copy().set_parse_action(remove_quotes)\n        openTag = suppress_LT + tagStr('tag') + Dict(ZeroOrMore(Group(tagAttrName + Suppress('=') + tagAttrValue))) + Opt('/', default=[False])('empty').set_parse_action(lambda s, l, t: t[0] == '/') + suppress_GT\n    else:\n        tagAttrValue = quoted_string.copy().set_parse_action(remove_quotes) | Word(printables, exclude_chars='>')\n        openTag = suppress_LT + tagStr('tag') + Dict(ZeroOrMore(Group(tagAttrName.set_parse_action(lambda t: t[0].lower()) + Opt(Suppress('=') + tagAttrValue)))) + Opt('/', default=[False])('empty').set_parse_action(lambda s, l, t: t[0] == '/') + suppress_GT\n    closeTag = Combine(Literal('</') + tagStr + '>', adjacent=False)\n    openTag.set_name('<%s>' % resname)\n    openTag.add_parse_action(lambda t: t.__setitem__('start' + ''.join(resname.replace(':', ' ').title().split()), t.copy()))\n    closeTag = closeTag('end' + ''.join(resname.replace(':', ' ').title().split())).set_name('</%s>' % resname)\n    openTag.tag = resname\n    closeTag.tag = resname\n    openTag.tag_body = SkipTo(closeTag())\n    return (openTag, closeTag)",
    "label": true
  },
  {
    "code": "def _raise_for_invalid_entrypoint(specification: str) -> None:\n    entry = get_export_entry(specification)\n    if entry is not None and entry.suffix is None:\n        raise MissingCallableSuffix(str(entry))",
    "label": true
  },
  {
    "code": "def _convert_extras_requirements(extras_require: Dict[str, Dict[str, Requirement]]) -> Mapping[str, _Ordered[Requirement]]:\n    output: Mapping[str, _Ordered[Requirement]] = defaultdict(dict)\n    for section, v in extras_require.items():\n        output[section]\n        for r in v.values():\n            output[section + _suffix_for(r)].setdefault(r)\n    return output",
    "label": true
  },
  {
    "code": "def _dev_pkgs() -> AbstractSet[str]:\n    pkgs = {'pip'}\n    if _should_suppress_build_backends():\n        pkgs |= {'setuptools', 'distribute', 'wheel'}\n    return pkgs",
    "label": true
  },
  {
    "code": "def is_archive_file(name: str) -> bool:\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def load_cdll(name: str, macos10_16_path: str) -> CDLL:\n    try:\n        path: str | None\n        if version_info >= (10, 16):\n            path = macos10_16_path\n        else:\n            path = find_library(name)\n        if not path:\n            raise OSError\n        return CDLL(path, use_errno=True)\n    except OSError:\n        raise ImportError(f'The library {name} failed to load') from None",
    "label": true
  },
  {
    "code": "def path_to_url(path: str) -> str:\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib.parse.urljoin('file:', urllib.request.pathname2url(path))\n    return url",
    "label": true
  },
  {
    "code": "def _is_linux_i686() -> bool:\n    elf_header = _get_elf_header()\n    if elf_header is None:\n        return False\n    result = elf_header.e_ident_class == elf_header.ELFCLASS32\n    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB\n    result &= elf_header.e_machine == elf_header.EM_386\n    return result",
    "label": true
  },
  {
    "code": "def _get_env(environment: Dict[str, str], name: str) -> str:\n    value: Union[str, Undefined] = environment.get(name, _undefined)\n    if isinstance(value, Undefined):\n        raise UndefinedEnvironmentName(f'{name!r} does not exist in evaluation environment.')\n    return value",
    "label": true
  },
  {
    "code": "def add_ext_suffix_39(vars):\n    import _imp\n    ext_suffix = _imp.extension_suffixes()[0]\n    vars.update(EXT_SUFFIX=ext_suffix, SO=ext_suffix)",
    "label": true
  },
  {
    "code": "def all_unique(iterable, key=None):\n    seenset = set()\n    seenset_add = seenset.add\n    seenlist = []\n    seenlist_add = seenlist.append\n    for element in map(key, iterable) if key else iterable:\n        try:\n            if element in seenset:\n                return False\n            seenset_add(element)\n        except TypeError:\n            if element in seenlist:\n                return False\n            seenlist_add(element)\n    return True",
    "label": true
  },
  {
    "code": "def is_hashable(node: nodes.NodeNG) -> bool:\n    try:\n        for inferred in node.infer():\n            if isinstance(inferred, (nodes.ClassDef, util.UninferableBase)):\n                return True\n            if not hasattr(inferred, 'igetattr'):\n                return True\n            hash_fn = next(inferred.igetattr('__hash__'))\n            if hash_fn.parent is inferred:\n                return True\n            if getattr(hash_fn, 'value', True) is not None:\n                return True\n        return False\n    except astroid.InferenceError:\n        return True",
    "label": true
  },
  {
    "code": "def get_extended_length_path_str(path: str) -> str:\n    long_path_prefix = '\\\\\\\\?\\\\'\n    unc_long_path_prefix = '\\\\\\\\?\\\\UNC\\\\'\n    if path.startswith((long_path_prefix, unc_long_path_prefix)):\n        return path\n    if path.startswith('\\\\\\\\'):\n        return unc_long_path_prefix + path[2:]\n    return long_path_prefix + path",
    "label": true
  },
  {
    "code": "def transform_pyqt_signal(node: nodes.FunctionDef) -> None:\n    module = parse('\\n    _UNSET = object()\\n\\n    class pyqtSignal(object):\\n        def connect(self, slot, type=None, no_receiver_check=False):\\n            pass\\n        def disconnect(self, slot=_UNSET):\\n            pass\\n        def emit(self, *args):\\n            pass\\n    ')\n    signal_cls: nodes.ClassDef = module['pyqtSignal']\n    node.instance_attrs['emit'] = [signal_cls['emit']]\n    node.instance_attrs['disconnect'] = [signal_cls['disconnect']]\n    node.instance_attrs['connect'] = [signal_cls['connect']]",
    "label": true
  },
  {
    "code": "def resolve_path(module, name):\n    if isinstance(module, string_types):\n        __import__(module)\n        module = sys.modules[module]\n    parent = module\n    path = name.split('.')\n    attribute = path[0]\n\n    def lookup_attribute(parent, attribute):\n        if inspect.isclass(parent):\n            for cls in inspect.getmro(parent):\n                if attribute in vars(cls):\n                    return vars(cls)[attribute]\n            else:\n                return getattr(parent, attribute)\n        else:\n            return getattr(parent, attribute)\n    original = lookup_attribute(parent, attribute)\n    for attribute in path[1:]:\n        parent = original\n        original = lookup_attribute(parent, attribute)\n    return (parent, attribute, original)",
    "label": true
  },
  {
    "code": "def is_registered_in_singledispatchmethod_function(node: nodes.FunctionDef) -> bool:\n    singledispatchmethod_qnames = ('functools.singledispatchmethod', 'singledispatch.singledispatchmethod')\n    decorators = node.decorators.nodes if node.decorators else []\n    for decorator in decorators:\n        func_def = find_inferred_fn_from_register(decorator)\n        if func_def:\n            return decorated_with(func_def, singledispatchmethod_qnames)\n    return False",
    "label": true
  },
  {
    "code": "def get_path_uid(path: str) -> int:\n    if hasattr(os, 'O_NOFOLLOW'):\n        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)\n        file_uid = os.fstat(fd).st_uid\n        os.close(fd)\n    elif not os.path.islink(path):\n        file_uid = os.stat(path).st_uid\n    else:\n        raise OSError(f'{path} is a symlink; Will not return uid for symlinks')\n    return file_uid",
    "label": true
  },
  {
    "code": "def _combining_class(cp: int) -> int:\n    v = unicodedata.combining(chr(cp))\n    if v == 0:\n        if not unicodedata.name(chr(cp)):\n            raise ValueError('Unknown character in unicodedata')\n    return v",
    "label": true
  },
  {
    "code": "def _load_items_from_file(keychain: SecKeychainRef, path: str) -> tuple[list[CFTypeRef], list[CFTypeRef]]:\n    certificates = []\n    identities = []\n    result_array = None\n    with open(path, 'rb') as f:\n        raw_filedata = f.read()\n    try:\n        filedata = CoreFoundation.CFDataCreate(CoreFoundation.kCFAllocatorDefault, raw_filedata, len(raw_filedata))\n        result_array = CoreFoundation.CFArrayRef()\n        result = Security.SecItemImport(filedata, None, None, None, 0, None, keychain, ctypes.byref(result_array))\n        _assert_no_error(result)\n        result_count = CoreFoundation.CFArrayGetCount(result_array)\n        for index in range(result_count):\n            item = CoreFoundation.CFArrayGetValueAtIndex(result_array, index)\n            item = ctypes.cast(item, CoreFoundation.CFTypeRef)\n            if _is_cert(item):\n                CoreFoundation.CFRetain(item)\n                certificates.append(item)\n            elif _is_identity(item):\n                CoreFoundation.CFRetain(item)\n                identities.append(item)\n    finally:\n        if result_array:\n            CoreFoundation.CFRelease(result_array)\n        CoreFoundation.CFRelease(filedata)\n    return (identities, certificates)",
    "label": true
  },
  {
    "code": "def _get_text_stdout(buffer_stream: t.BinaryIO) -> t.TextIO:\n    text_stream = _NonClosingTextIOWrapper(io.BufferedWriter(_WindowsConsoleWriter(STDOUT_HANDLE)), 'utf-16-le', 'strict', line_buffering=True)\n    return t.cast(t.TextIO, ConsoleStream(text_stream, buffer_stream))",
    "label": true
  },
  {
    "code": "def merge_family(left, right) -> None:\n    result = {}\n    for kl, vl in left.items():\n        for kr, vr in right.items():\n            if not isinstance(vl, list):\n                raise TypeError(type(vl))\n            result[kl] = vl + vr\n    left.update(result)",
    "label": true
  },
  {
    "code": "def _find_parametrized_scope(argnames: Sequence[str], arg2fixturedefs: Mapping[str, Sequence[fixtures.FixtureDef[object]]], indirect: Union[bool, Sequence[str]]) -> Scope:\n    if isinstance(indirect, Sequence):\n        all_arguments_are_fixtures = len(indirect) == len(argnames)\n    else:\n        all_arguments_are_fixtures = bool(indirect)\n    if all_arguments_are_fixtures:\n        fixturedefs = arg2fixturedefs or {}\n        used_scopes = [fixturedef[0]._scope for name, fixturedef in fixturedefs.items() if name in argnames]\n        return min(used_scopes, default=Scope.Function)\n    return Scope.Function",
    "label": true
  },
  {
    "code": "def _is_inside_context_manager(node: nodes.Call) -> bool:\n    frame = node.frame(future=True)\n    if not isinstance(frame, (nodes.FunctionDef, astroid.BoundMethod, astroid.UnboundMethod)):\n        return False\n    return frame.name == '__enter__' or utils.decorated_with(frame, 'contextlib.contextmanager')",
    "label": true
  },
  {
    "code": "def is_python_source(filename: str | None) -> bool:\n    if not filename:\n        return False\n    return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS",
    "label": true
  },
  {
    "code": "def parse_credentials(netloc):\n    username = password = None\n    if '@' in netloc:\n        prefix, netloc = netloc.rsplit('@', 1)\n        if ':' not in prefix:\n            username = prefix\n        else:\n            username, password = prefix.split(':', 1)\n    if username:\n        username = unquote(username)\n    if password:\n        password = unquote(password)\n    return (username, password, netloc)",
    "label": true
  },
  {
    "code": "def _find_packages(dist: Distribution) -> Iterator[str]:\n    yield from iter(dist.packages or [])\n    py_modules = dist.py_modules or []\n    nested_modules = [mod for mod in py_modules if '.' in mod]\n    if dist.ext_package:\n        yield dist.ext_package\n    else:\n        ext_modules = dist.ext_modules or []\n        nested_modules += [x.name for x in ext_modules if '.' in x.name]\n    for module in nested_modules:\n        package, _, _ = module.rpartition('.')\n        yield package",
    "label": true
  },
  {
    "code": "def _get_ansi_code(msg_style: MessageStyle) -> str:\n    ansi_code = [ANSI_STYLES[effect] for effect in msg_style.style]\n    if msg_style.color:\n        if msg_style.color.isdigit():\n            ansi_code.extend(['38', '5'])\n            ansi_code.append(msg_style.color)\n        else:\n            ansi_code.append(ANSI_COLORS[msg_style.color])\n    if ansi_code:\n        return ANSI_PREFIX + ';'.join(ansi_code) + ANSI_END\n    return ''",
    "label": true
  },
  {
    "code": "def table_lines_from_stats(stats: LinterStats, old_stats: LinterStats | None, stat_type: Literal['duplicated_lines', 'message_types']) -> list[str]:\n    lines: list[str] = []\n    if stat_type == 'duplicated_lines':\n        new: list[tuple[str, int | float]] = [('nb_duplicated_lines', stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', stats.duplicated_lines['percent_duplicated_lines'])]\n        if old_stats:\n            old: list[tuple[str, str | int | float]] = [('nb_duplicated_lines', old_stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', old_stats.duplicated_lines['percent_duplicated_lines'])]\n        else:\n            old = [('nb_duplicated_lines', 'NC'), ('percent_duplicated_lines', 'NC')]\n    elif stat_type == 'message_types':\n        new = [('convention', stats.convention), ('refactor', stats.refactor), ('warning', stats.warning), ('error', stats.error)]\n        if old_stats:\n            old = [('convention', old_stats.convention), ('refactor', old_stats.refactor), ('warning', old_stats.warning), ('error', old_stats.error)]\n        else:\n            old = [('convention', 'NC'), ('refactor', 'NC'), ('warning', 'NC'), ('error', 'NC')]\n    for index, value in enumerate(new):\n        new_value = value[1]\n        old_value = old[index][1]\n        diff_str = diff_string(old_value, new_value) if isinstance(old_value, float) else old_value\n        new_str = f'{new_value:.3f}' if isinstance(new_value, float) else str(new_value)\n        old_str = f'{old_value:.3f}' if isinstance(old_value, float) else str(old_value)\n        lines.extend((value[0].replace('_', ' '), new_str, old_str, diff_str))\n    return lines",
    "label": true
  },
  {
    "code": "def ignore_case(value: V) -> V:\n    if isinstance(value, str):\n        return t.cast(V, value.lower())\n    return value",
    "label": true
  },
  {
    "code": "def parse_netloc(netloc: str) -> Tuple[Optional[str], Optional[int]]:\n    url = build_url_from_netloc(netloc)\n    parsed = urllib.parse.urlparse(url)\n    return (parsed.hostname, parsed.port)",
    "label": true
  },
  {
    "code": "def _get_text_stderr(buffer_stream: t.BinaryIO) -> t.TextIO:\n    text_stream = _NonClosingTextIOWrapper(io.BufferedWriter(_WindowsConsoleWriter(STDERR_HANDLE)), 'utf-16-le', 'strict', line_buffering=True)\n    return t.cast(t.TextIO, ConsoleStream(text_stream, buffer_stream))",
    "label": true
  },
  {
    "code": "def _parse(markup: str) -> Iterable[Tuple[int, Optional[str], Optional[Tag]]]:\n    position = 0\n    _divmod = divmod\n    _Tag = Tag\n    for match in RE_TAGS.finditer(markup):\n        full_text, escapes, tag_text = match.groups()\n        start, end = match.span()\n        if start > position:\n            yield (start, markup[position:start], None)\n        if escapes:\n            backslashes, escaped = _divmod(len(escapes), 2)\n            if backslashes:\n                yield (start, '\\\\' * backslashes, None)\n                start += backslashes * 2\n            if escaped:\n                yield (start, full_text[len(escapes):], None)\n                position = end\n                continue\n        text, equals, parameters = tag_text.partition('=')\n        yield (start, None, _Tag(text, parameters if equals else None))\n        position = end\n    if position < len(markup):\n        yield (position, markup[position:], None)",
    "label": true
  },
  {
    "code": "def construct_package_dir(packages: List[str], package_path: _Path) -> Dict[str, str]:\n    parent_pkgs = remove_nested_packages(packages)\n    prefix = Path(package_path).parts\n    return {pkg: '/'.join([*prefix, *pkg.split('.')]) for pkg in parent_pkgs}",
    "label": true
  },
  {
    "code": "def _environment_config_check(environment: 'Environment') -> 'Environment':\n    assert issubclass(environment.undefined, Undefined), \"'undefined' must be a subclass of 'jinja2.Undefined'.\"\n    assert environment.block_start_string != environment.variable_start_string != environment.comment_start_string, 'block, variable and comment start strings must be different.'\n    assert environment.newline_sequence in {'\\r', '\\r\\n', '\\n'}, \"'newline_sequence' must be one of '\\\\n', '\\\\r\\\\n', or '\\\\r'.\"\n    return environment",
    "label": true
  },
  {
    "code": "def run_commands(dist):\n    try:\n        dist.run_commands()\n    except KeyboardInterrupt:\n        raise SystemExit('interrupted')\n    except OSError as exc:\n        if DEBUG:\n            sys.stderr.write('error: {}\\n'.format(exc))\n            raise\n        else:\n            raise SystemExit('error: {}'.format(exc))\n    except (DistutilsError, CCompilerError) as msg:\n        if DEBUG:\n            raise\n        else:\n            raise SystemExit('error: ' + str(msg))\n    return dist",
    "label": true
  },
  {
    "code": "def show_tags(options: Values) -> None:\n    tag_limit = 10\n    target_python = make_target_python(options)\n    tags = target_python.get_tags()\n    formatted_target = target_python.format_given()\n    suffix = ''\n    if formatted_target:\n        suffix = f' (target: {formatted_target})'\n    msg = 'Compatible tags: {}{}'.format(len(tags), suffix)\n    logger.info(msg)\n    if options.verbose < 1 and len(tags) > tag_limit:\n        tags_limited = True\n        tags = tags[:tag_limit]\n    else:\n        tags_limited = False\n    with indent_log():\n        for tag in tags:\n            logger.info(str(tag))\n        if tags_limited:\n            msg = '...\\n[First {tag_limit} tags shown. Pass --verbose to show all.]'.format(tag_limit=tag_limit)\n            logger.info(msg)",
    "label": true
  },
  {
    "code": "def _handle_merge_hash(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    if not parser.values.hashes:\n        parser.values.hashes = {}\n    try:\n        algo, digest = value.split(':', 1)\n    except ValueError:\n        parser.error('Arguments to {} must be a hash name followed by a value, like --hash=sha256:abcde...'.format(opt_str))\n    if algo not in STRONG_HASHES:\n        parser.error('Allowed hash algorithms for {} are {}.'.format(opt_str, ', '.join(STRONG_HASHES)))\n    parser.values.hashes.setdefault(algo, []).append(digest)",
    "label": true
  },
  {
    "code": "def report_raw_stats(sect: Section, stats: LinterStats, old_stats: LinterStats | None) -> None:\n    total_lines = stats.code_type_count['total']\n    sect.insert(0, Paragraph([Text(f'{total_lines} lines have been analyzed\\n')]))\n    lines = ['type', 'number', '%', 'previous', 'difference']\n    for node_type in ('code', 'docstring', 'comment', 'empty'):\n        node_type = cast(Literal['code', 'docstring', 'comment', 'empty'], node_type)\n        total = stats.code_type_count[node_type]\n        percent = float(total * 100) / total_lines if total_lines else None\n        old = old_stats.code_type_count[node_type] if old_stats else None\n        diff_str = diff_string(old, total) if old else None\n        lines += [node_type, str(total), f'{percent:.2f}' if percent is not None else 'NC', str(old) if old else 'NC', diff_str if diff_str else 'NC']\n    sect.append(Table(children=lines, cols=5, rheaders=1))",
    "label": true
  },
  {
    "code": "def create_command(name: str, **kwargs: Any) -> Command:\n    module_path, class_name, summary = commands_dict[name]\n    module = importlib.import_module(module_path)\n    command_class = getattr(module, class_name)\n    command = command_class(name=name, summary=summary, **kwargs)\n    return command",
    "label": true
  },
  {
    "code": "def get_log_level_for_setting(config: Config, *setting_names: str) -> Optional[int]:\n    for setting_name in setting_names:\n        log_level = config.getoption(setting_name)\n        if log_level is None:\n            log_level = config.getini(setting_name)\n        if log_level:\n            break\n    else:\n        return None\n    if isinstance(log_level, str):\n        log_level = log_level.upper()\n    try:\n        return int(getattr(logging, log_level, log_level))\n    except ValueError as e:\n        raise UsageError(\"'{}' is not recognized as a logging level name for '{}'. Please consider passing the logging level num instead.\".format(log_level, setting_name)) from e",
    "label": true
  },
  {
    "code": "def do_items(value: t.Union[t.Mapping[K, V], Undefined]) -> t.Iterator[t.Tuple[K, V]]:\n    if isinstance(value, Undefined):\n        return\n    if not isinstance(value, abc.Mapping):\n        raise TypeError('Can only get item pairs from a mapping.')\n    yield from value.items()",
    "label": true
  },
  {
    "code": "def ensure_no_io(modulename: str) -> None:\n    test_module = sys.modules[modulename]\n    setattr(test_module, 'input', _mock_disallow('input'))\n    setattr(test_module, 'print', _mock_disallow('print'))",
    "label": true
  },
  {
    "code": "def _flat_list(nested_list):\n    ret = []\n    for item in nested_list:\n        if isinstance(item, list):\n            for subitem in item:\n                ret.append(subitem)\n        else:\n            ret.append(item)\n    return ret",
    "label": true
  },
  {
    "code": "def is_attribute_typed_annotation(node: nodes.ClassDef | astroid.Instance, attr_name: str) -> bool:\n    attribute = node.locals.get(attr_name, [None])[0]\n    if attribute and isinstance(attribute, nodes.AssignName) and isinstance(attribute.parent, nodes.AnnAssign):\n        return True\n    for base in node.bases:\n        inferred = safe_infer(base)\n        if inferred and isinstance(inferred, nodes.ClassDef) and is_attribute_typed_annotation(inferred, attr_name):\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def decode(s: Union[str, bytes, bytearray], strict: bool=False, uts46: bool=False, std3_rules: bool=False) -> str:\n    try:\n        if isinstance(s, (bytes, bytearray)):\n            s = s.decode('ascii')\n    except UnicodeDecodeError:\n        raise IDNAError('Invalid ASCII in A-label')\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split('.')\n    if not labels or labels == ['']:\n        raise IDNAError('Empty domain')\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError('Empty label')\n    if trailing_dot:\n        result.append('')\n    return '.'.join(result)",
    "label": true
  },
  {
    "code": "def apply(dist: 'Distribution', config: dict, filename: _Path) -> 'Distribution':\n    if not config:\n        return dist\n    root_dir = os.path.dirname(filename) or '.'\n    _apply_project_table(dist, config, root_dir)\n    _apply_tool_table(dist, config, filename)\n    current_directory = os.getcwd()\n    os.chdir(root_dir)\n    try:\n        dist._finalize_requires()\n        dist._finalize_license_files()\n    finally:\n        os.chdir(current_directory)\n    return dist",
    "label": true
  },
  {
    "code": "def bygroups(*args):\n\n    def callback(lexer, match, ctx=None):\n        for i, action in enumerate(args):\n            if action is None:\n                continue\n            elif type(action) is _TokenType:\n                data = match.group(i + 1)\n                if data:\n                    yield (match.start(i + 1), action, data)\n            else:\n                data = match.group(i + 1)\n                if data is not None:\n                    if ctx:\n                        ctx.pos = match.start(i + 1)\n                    for item in action(lexer, _PseudoMatch(match.start(i + 1), data), ctx):\n                        if item:\n                            yield item\n        if ctx:\n            ctx.pos = match.end()\n    return callback",
    "label": true
  },
  {
    "code": "def _remove_path_dot_segments(path):\n    segments = path.split('/')\n    output = []\n    for segment in segments:\n        if segment == '.':\n            continue\n        elif segment != '..':\n            output.append(segment)\n        elif output:\n            output.pop()\n    if path.startswith('/') and (not output or output[0]):\n        output.insert(0, '')\n    if path.endswith(('/.', '/..')):\n        output.append('')\n    return '/'.join(output)",
    "label": true
  },
  {
    "code": "def test_decode():\n    assert decode(b'\\x80', 'latin1') == ('\u20ac', lookup('latin1'))\n    assert decode(b'\\x80', lookup('latin1')) == ('\u20ac', lookup('latin1'))\n    assert decode(b'\\xc3\\xa9', 'utf8') == ('\u00e9', lookup('utf8'))\n    assert decode(b'\\xc3\\xa9', UTF8) == ('\u00e9', lookup('utf8'))\n    assert decode(b'\\xc3\\xa9', 'ascii') == ('\u00c3\u00a9', lookup('ascii'))\n    assert decode(b'\\xef\\xbb\\xbf\\xc3\\xa9', 'ascii') == ('\u00e9', lookup('utf8'))\n    assert decode(b'\\xfe\\xff\\x00\\xe9', 'ascii') == ('\u00e9', lookup('utf-16be'))\n    assert decode(b'\\xff\\xfe\\xe9\\x00', 'ascii') == ('\u00e9', lookup('utf-16le'))\n    assert decode(b'\\xfe\\xff\\xe9\\x00', 'ascii') == ('\\ue900', lookup('utf-16be'))\n    assert decode(b'\\xff\\xfe\\x00\\xe9', 'ascii') == ('\\ue900', lookup('utf-16le'))\n    assert decode(b'\\x00\\xe9', 'UTF-16BE') == ('\u00e9', lookup('utf-16be'))\n    assert decode(b'\\xe9\\x00', 'UTF-16LE') == ('\u00e9', lookup('utf-16le'))\n    assert decode(b'\\xe9\\x00', 'UTF-16') == ('\u00e9', lookup('utf-16le'))\n    assert decode(b'\\xe9\\x00', 'UTF-16BE') == ('\\ue900', lookup('utf-16be'))\n    assert decode(b'\\x00\\xe9', 'UTF-16LE') == ('\\ue900', lookup('utf-16le'))\n    assert decode(b'\\x00\\xe9', 'UTF-16') == ('\\ue900', lookup('utf-16le'))",
    "label": true
  },
  {
    "code": "def skip_until(src: str, pos: Pos, expect: str, *, error_on: FrozenSet[str], error_on_eof: bool) -> Pos:\n    try:\n        new_pos = src.index(expect, pos)\n    except ValueError:\n        new_pos = len(src)\n        if error_on_eof:\n            raise suffixed_err(src, new_pos, f'Expected \"{expect!r}\"')\n    if not error_on.isdisjoint(src[pos:new_pos]):\n        while src[pos] not in error_on:\n            pos += 1\n        raise suffixed_err(src, pos, f'Found invalid character \"{src[pos]!r}\"')\n    return new_pos",
    "label": true
  },
  {
    "code": "def _check_generate_dataclass_init(node: nodes.ClassDef) -> bool:\n    if '__init__' in node.locals:\n        return False\n    found = None\n    for decorator_attribute in node.decorators.nodes:\n        if not isinstance(decorator_attribute, nodes.Call):\n            continue\n        if _looks_like_dataclass_decorator(decorator_attribute):\n            found = decorator_attribute\n    if found is None:\n        return True\n    return not any((keyword.arg == 'init' and (not keyword.value.bool_value()) for keyword in found.keywords))",
    "label": true
  },
  {
    "code": "def update_counts(s, counts):\n    for char in s:\n        if char in counts:\n            counts[char] += 1",
    "label": true
  },
  {
    "code": "def is_valid_flyer(ticket: str) -> bool:\n    if len(ticket) == 17:\n        return True\n    elif len(ticket) == 21:\n        calculation = int(ticket[FLYER]) + int(ticket[FLYER + 1])\n        calculation2 = calculation + int(ticket[FLYER + 2])\n        return calculation2 % 10 == int(ticket[FLYER + 3])\n    else:\n        return False",
    "label": true
  },
  {
    "code": "def circular_shifts(iterable):\n    lst = list(iterable)\n    return take(len(lst), windowed(cycle(lst), len(lst)))",
    "label": true
  },
  {
    "code": "def pip_self_version_check(session: PipSession, options: optparse.Values) -> None:\n    installed_dist = get_default_environment().get_distribution('pip')\n    if not installed_dist:\n        return\n    try:\n        upgrade_prompt = _self_version_check_logic(state=SelfCheckState(cache_dir=options.cache_dir), current_time=datetime.datetime.utcnow(), local_version=installed_dist.version, get_remote_version=functools.partial(_get_current_remote_pip_version, session, options))\n        if upgrade_prompt is not None:\n            logger.warning('[present-rich] %s', upgrade_prompt)\n    except Exception:\n        logger.warning('There was an error checking the latest version of pip.')\n        logger.debug('See below for error', exc_info=True)",
    "label": true
  },
  {
    "code": "def get_file_paths(rel_path: AnyStr) -> Generator[AnyStr, None, None]:\n    if not os.path.isdir(rel_path):\n        yield rel_path\n    else:\n        for root, _, files in os.walk(rel_path):\n            for filename in (f for f in files if f.endswith('.py')):\n                yield os.path.join(root, filename)",
    "label": true
  },
  {
    "code": "def _infer_copy_method(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[nodes.NodeNG]:\n    assert isinstance(node.func, nodes.Attribute)\n    inferred_orig, inferred_copy = itertools.tee(node.func.expr.infer(context=context))\n    if all((isinstance(inferred_node, (nodes.Dict, nodes.List, nodes.Set, objects.FrozenSet)) for inferred_node in inferred_orig)):\n        return inferred_copy\n    raise UseInferenceDefault()",
    "label": true
  },
  {
    "code": "def uts46_remap(domain: str, std3_rules: bool=True, transitional: bool=False) -> str:\n    from .uts46data import uts46data\n    output = ''\n    for pos, char in enumerate(domain):\n        code_point = ord(char)\n        try:\n            uts46row = uts46data[code_point if code_point < 256 else bisect.bisect_left(uts46data, (code_point, 'Z')) - 1]\n            status = uts46row[1]\n            replacement = None\n            if len(uts46row) == 3:\n                replacement = uts46row[2]\n            if status == 'V' or (status == 'D' and (not transitional)) or (status == '3' and (not std3_rules) and (replacement is None)):\n                output += char\n            elif replacement is not None and (status == 'M' or (status == '3' and (not std3_rules)) or (status == 'D' and transitional)):\n                output += replacement\n            elif status != 'I':\n                raise IndexError()\n        except IndexError:\n            raise InvalidCodepoint('Codepoint {} not allowed at position {} in {}'.format(_unot(code_point), pos + 1, repr(domain)))\n    return unicodedata.normalize('NFC', output)",
    "label": true
  },
  {
    "code": "def _load_messages_config(path: str, default_path: str) -> dict:\n    merge_into = toml.load(default_path)\n    if Path(default_path).resolve() == Path(path).resolve():\n        return merge_into\n    try:\n        merge_from = toml.load(path)\n    except FileNotFoundError:\n        print(f'[WARNING] Could not find messages config file at {str(Path(path).resolve())}. Using default messages config file at {str(Path(default_path).resolve())}.')\n        return merge_into\n    for category in merge_from:\n        if category not in merge_into:\n            merge_into[category] = {}\n        for checker in merge_from[category]:\n            if checker not in merge_into[category]:\n                merge_into[category][checker] = {}\n            for error_code in merge_from[category][checker]:\n                merge_into[category][checker][error_code] = merge_from[category][checker][error_code]\n    return merge_into",
    "label": true
  },
  {
    "code": "def _lookup_style(style):\n    if isinstance(style, str):\n        return get_style_by_name(style)\n    return style",
    "label": true
  },
  {
    "code": "def foo(x):\n\n    def bar(y):\n        return squared(x) + y\n    return bar",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def entry_points(text: str, text_source='entry-points') -> Dict[str, dict]:\n    parser = ConfigParser(default_section=None, delimiters=('=',))\n    parser.optionxform = str\n    parser.read_string(text, text_source)\n    groups = {k: dict(v.items()) for k, v in parser.items()}\n    groups.pop(parser.default_section, None)\n    return groups",
    "label": true
  },
  {
    "code": "def is_object_one_of_types(obj: object, fully_qualified_types_names: Collection[str]) -> bool:\n    for type_name in get_object_types_mro_as_strings(obj):\n        if type_name in fully_qualified_types_names:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _idna_encode(name):\n    if name and any((ord(x) >= 128 for x in name)):\n        try:\n            from pip._vendor import idna\n        except ImportError:\n            six.raise_from(LocationParseError(\"Unable to parse URL without the 'idna' module\"), None)\n        try:\n            return idna.encode(name.lower(), strict=True, std3_rules=True)\n        except idna.IDNAError:\n            six.raise_from(LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name), None)\n    return name.lower().encode('ascii')",
    "label": true
  },
  {
    "code": "def token_map(func, *args) -> ParseAction:\n\n    def pa(s, l, t):\n        return [func(tokn, *args) for tokn in t]\n    func_name = getattr(func, '__name__', getattr(func, '__class__').__name__)\n    pa.__name__ = func_name\n    return pa",
    "label": true
  },
  {
    "code": "def requires_to_requires_dist(requirement: Requirement) -> str:\n    if getattr(requirement, 'url', None):\n        return ' @ ' + requirement.url\n    requires_dist = []\n    for spec in requirement.specifier:\n        requires_dist.append(spec.operator + spec.version)\n    if requires_dist:\n        return ' ' + ','.join(sorted(requires_dist))\n    else:\n        return ''",
    "label": true
  },
  {
    "code": "def find_filter_class(filtername):\n    if filtername in FILTERS:\n        return FILTERS[filtername]\n    for name, cls in find_plugin_filters():\n        if name == filtername:\n            return cls\n    return None",
    "label": true
  },
  {
    "code": "def measure_table(rows: t.Iterable[t.Tuple[str, str]]) -> t.Tuple[int, ...]:\n    widths: t.Dict[int, int] = {}\n    for row in rows:\n        for idx, col in enumerate(row):\n            widths[idx] = max(widths.get(idx, 0), term_len(col))\n    return tuple((y for x, y in sorted(widths.items())))",
    "label": true
  },
  {
    "code": "def load_session(filename=None, main=None, **kwds):\n    warnings.warn('load_session() has been renamed load_module().', PendingDeprecationWarning)\n    load_module(filename, module=main, **kwds)",
    "label": true
  },
  {
    "code": "def wheel_version(wheel_data: Message) -> Tuple[int, ...]:\n    version_text = wheel_data['Wheel-Version']\n    if version_text is None:\n        raise UnsupportedWheel('WHEEL is missing Wheel-Version')\n    version = version_text.strip()\n    try:\n        return tuple(map(int, version.split('.')))\n    except ValueError:\n        raise UnsupportedWheel(f'invalid Wheel-Version: {version!r}')",
    "label": true
  },
  {
    "code": "def _setitems(dest, source):\n    for k, v in source.items():\n        dest[k] = v",
    "label": true
  },
  {
    "code": "def check(obj, *args, **kwds):\n    verbose = kwds.pop('verbose', False)\n    python = kwds.pop('python', None)\n    if python is None:\n        import sys\n        python = sys.executable\n    isinstance(python, str)\n    import subprocess\n    fail = True\n    try:\n        _obj = dumps(obj, *args, **kwds)\n        fail = False\n    finally:\n        if fail and verbose:\n            print('DUMP FAILED')\n    msg = '%s -c import dill; print(dill.loads(%s))' % (python, repr(_obj))\n    msg = 'SUCCESS' if not subprocess.call(msg.split(None, 2)) else 'LOAD FAILED'\n    if verbose:\n        print(msg)\n    return",
    "label": true
  },
  {
    "code": "def nth_product(index, *args):\n    pools = list(map(tuple, reversed(args)))\n    ns = list(map(len, pools))\n    c = reduce(mul, ns)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    result = []\n    for pool, n in zip(pools, ns):\n        result.append(pool[index % n])\n        index //= n\n    return tuple(reversed(result))",
    "label": true
  },
  {
    "code": "def get_build_version():\n    prefix = 'MSC v.'\n    i = sys.version.find(prefix)\n    if i == -1:\n        return 6\n    i = i + len(prefix)\n    s, rest = sys.version[i:].split(' ', 1)\n    majorVersion = int(s[:-2]) - 6\n    if majorVersion >= 13:\n        majorVersion += 1\n    minorVersion = int(s[2:3]) / 10.0\n    if majorVersion == 6:\n        minorVersion = 0\n    if majorVersion >= 6:\n        return majorVersion + minorVersion\n    return None",
    "label": true
  },
  {
    "code": "def read_keys(base, key):\n    try:\n        handle = RegOpenKeyEx(base, key)\n    except RegError:\n        return None\n    L = []\n    i = 0\n    while True:\n        try:\n            k = RegEnumKey(handle, i)\n        except RegError:\n            break\n        L.append(k)\n        i += 1\n    return L",
    "label": true
  },
  {
    "code": "def find_formatter_class(alias):\n    for module_name, name, aliases, _, _ in FORMATTERS.values():\n        if alias in aliases:\n            if name not in _formatter_cache:\n                _load_formatters(module_name)\n            return _formatter_cache[name]\n    for _, cls in find_plugin_formatters():\n        if alias in cls.aliases:\n            return cls",
    "label": true
  },
  {
    "code": "def _is_cert(item):\n    expected = Security.SecCertificateGetTypeID()\n    return CoreFoundation.CFGetTypeID(item) == expected",
    "label": true
  },
  {
    "code": "def _init():\n    for code, titles in _codes.items():\n        for title in titles:\n            setattr(codes, title, code)\n            if not title.startswith(('\\\\', '/')):\n                setattr(codes, title.upper(), code)\n\n    def doc(code):\n        names = ', '.join((f'``{n}``' for n in _codes[code]))\n        return '* %d: %s' % (code, names)\n    global __doc__\n    __doc__ = __doc__ + '\\n' + '\\n'.join((doc(code) for code in sorted(_codes))) if __doc__ is not None else None",
    "label": true
  },
  {
    "code": "def finder(package):\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result",
    "label": true
  },
  {
    "code": "def ircformat(color, text):\n    if len(color) < 1:\n        return text\n    add = sub = ''\n    if '_' in color:\n        add += '\\x1d'\n        sub = '\\x1d' + sub\n        color = color.strip('_')\n    if '*' in color:\n        add += '\\x02'\n        sub = '\\x02' + sub\n        color = color.strip('*')\n    if len(color) > 0:\n        add += '\\x03' + str(IRC_COLOR_MAP[color]).zfill(2)\n        sub = '\\x03' + sub\n    return add + text + sub\n    return '<' + add + '>' + text + '</' + sub + '>'",
    "label": true
  },
  {
    "code": "def interpreter_name() -> str:\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name",
    "label": true
  },
  {
    "code": "def _read(filename):\n    if (2, 5) < sys.version_info < (3, 0):\n        with open(filename, 'rU') as f:\n            return f.read()\n    elif (3, 0) <= sys.version_info < (4, 0):\n        'Read the source code.'\n        try:\n            with open(filename, 'rb') as f:\n                encoding, _ = tokenize.detect_encoding(f.readline)\n        except (LookupError, SyntaxError, UnicodeError):\n            with open(filename, encoding='latin-1') as f:\n                return f.read()\n        with open(filename, 'r', encoding=encoding) as f:\n            return f.read()",
    "label": true
  },
  {
    "code": "def _find_vc2015():\n    try:\n        key = winreg.OpenKeyEx(winreg.HKEY_LOCAL_MACHINE, 'Software\\\\Microsoft\\\\VisualStudio\\\\SxS\\\\VC7', access=winreg.KEY_READ | winreg.KEY_WOW64_32KEY)\n    except OSError:\n        log.debug('Visual C++ is not registered')\n        return (None, None)\n    best_version = 0\n    best_dir = None\n    with key:\n        for i in count():\n            try:\n                v, vc_dir, vt = winreg.EnumValue(key, i)\n            except OSError:\n                break\n            if v and vt == winreg.REG_SZ and os.path.isdir(vc_dir):\n                try:\n                    version = int(float(v))\n                except (ValueError, TypeError):\n                    continue\n                if version >= 14 and version > best_version:\n                    best_version, best_dir = (version, vc_dir)\n    return (best_version, best_dir)",
    "label": true
  },
  {
    "code": "def all_equal(iterable):\n    g = groupby(iterable)\n    return next(g, True) and (not next(g, False))",
    "label": true
  },
  {
    "code": "def parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, list]:\n    pos += 1\n    array: list = []\n    pos = skip_comments_and_array_ws(src, pos)\n    if src.startswith(']', pos):\n        return (pos + 1, array)\n    while True:\n        pos, val = parse_value(src, pos, parse_float)\n        array.append(val)\n        pos = skip_comments_and_array_ws(src, pos)\n        c = src[pos:pos + 1]\n        if c == ']':\n            return (pos + 1, array)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed array')\n        pos += 1\n        pos = skip_comments_and_array_ws(src, pos)\n        if src.startswith(']', pos):\n            return (pos + 1, array)",
    "label": true
  },
  {
    "code": "def reorder_items(items: Sequence[nodes.Item]) -> List[nodes.Item]:\n    argkeys_cache: Dict[Scope, Dict[nodes.Item, Dict[_Key, None]]] = {}\n    items_by_argkey: Dict[Scope, Dict[_Key, Deque[nodes.Item]]] = {}\n    for scope in HIGH_SCOPES:\n        d: Dict[nodes.Item, Dict[_Key, None]] = {}\n        argkeys_cache[scope] = d\n        item_d: Dict[_Key, Deque[nodes.Item]] = defaultdict(deque)\n        items_by_argkey[scope] = item_d\n        for item in items:\n            keys = dict.fromkeys(get_parametrized_fixture_keys(item, scope), None)\n            if keys:\n                d[item] = keys\n                for key in keys:\n                    item_d[key].append(item)\n    items_dict = dict.fromkeys(items, None)\n    return list(reorder_items_atscope(items_dict, argkeys_cache, items_by_argkey, Scope.Session))",
    "label": true
  },
  {
    "code": "def get_with_lines(lines: list[str], num_whitespace: int) -> str:\n    endpoint = len(lines)\n    for i in range(len(lines)):\n        if lines[i].strip() != '' and (not lines[i][num_whitespace].isspace()):\n            endpoint = i\n            break\n    return '\\n'.join(lines[:endpoint])",
    "label": true
  },
  {
    "code": "def with_cleanup(func: Any) -> Any:\n\n    def configure_tempdir_registry(registry: TempDirectoryTypeRegistry) -> None:\n        for t in KEEPABLE_TEMPDIR_TYPES:\n            registry.set_delete(t, False)\n\n    def wrapper(self: RequirementCommand, options: Values, args: List[Any]) -> Optional[int]:\n        assert self.tempdir_registry is not None\n        if options.no_clean:\n            configure_tempdir_registry(self.tempdir_registry)\n        try:\n            return func(self, options, args)\n        except PreviousBuildDirError:\n            configure_tempdir_registry(self.tempdir_registry)\n            raise\n    return wrapper",
    "label": true
  },
  {
    "code": "def iter_fields(fields):\n    if isinstance(fields, dict):\n        return ((k, v) for k, v in six.iteritems(fields))\n    return ((k, v) for k, v in fields)",
    "label": true
  },
  {
    "code": "def test_code_object():\n    import warnings\n    from dill._dill import ALL_CODE_PARAMS, CODE_PARAMS, CODE_VERSION, _create_code\n    code = function_c.__code__\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    LNOTAB = getattr(code, 'co_lnotab', b'')\n    if warnings.filters:\n        del warnings.filters[0]\n    fields = {f: getattr(code, 'co_' + f) for f in CODE_PARAMS}\n    fields.setdefault('posonlyargcount', 0)\n    fields.setdefault('lnotab', LNOTAB)\n    fields.setdefault('linetable', b'')\n    fields.setdefault('qualname', fields['name'])\n    fields.setdefault('exceptiontable', b'')\n    fields.setdefault('endlinetable', None)\n    fields.setdefault('columntable', None)\n    for version, _, params in ALL_CODE_PARAMS:\n        args = tuple((fields[p] for p in params.split()))\n        try:\n            _create_code(*args)\n            if version >= (3, 10):\n                _create_code(fields['lnotab'], *args)\n        except Exception as error:\n            raise Exception('failed to construct code object with format version {}'.format(version)) from error",
    "label": true
  },
  {
    "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)",
    "label": true
  },
  {
    "code": "def _macos_vers(_cache=[]):\n    if not _cache:\n        version = platform.mac_ver()[0]\n        if version == '':\n            plist = '/System/Library/CoreServices/SystemVersion.plist'\n            if os.path.exists(plist):\n                if hasattr(plistlib, 'readPlist'):\n                    plist_content = plistlib.readPlist(plist)\n                    if 'ProductVersion' in plist_content:\n                        version = plist_content['ProductVersion']\n        _cache.append(version.split('.'))\n    return _cache[0]",
    "label": true
  },
  {
    "code": "def run_setup(setup_script, args):\n    setup_dir = os.path.abspath(os.path.dirname(setup_script))\n    with setup_context(setup_dir):\n        try:\n            sys.argv[:] = [setup_script] + list(args)\n            sys.path.insert(0, setup_dir)\n            working_set.__init__()\n            working_set.callbacks.append(lambda dist: dist.activate())\n            with DirectorySandbox(setup_dir):\n                ns = dict(__file__=setup_script, __name__='__main__')\n                _execfile(setup_script, ns)\n        except SystemExit as v:\n            if v.args and v.args[0]:\n                raise",
    "label": true
  },
  {
    "code": "def _normalize_name(name: str) -> str:\n    name = name.lower().replace('_', '-')\n    if name.startswith('--'):\n        name = name[2:]\n    return name",
    "label": true
  },
  {
    "code": "def _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n            if isinstance(lhs, Variable):\n                environment_key = lhs.value\n                lhs_value = environment[environment_key]\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                environment_key = rhs.value\n                rhs_value = environment[environment_key]\n            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in ['and', 'or']\n            if marker == 'or':\n                groups.append([])\n    return any((all(item) for item in groups))",
    "label": true
  },
  {
    "code": "def _byte_to_str_length(codec: str) -> int:\n    if codec.startswith('utf-32'):\n        return 4\n    if codec.startswith('utf-16'):\n        return 2\n    return 1",
    "label": true
  },
  {
    "code": "def hide_setuptools():\n    _distutils_hack = sys.modules.get('_distutils_hack', None)\n    if _distutils_hack is not None:\n        _distutils_hack._remove_shim()\n    modules = filter(_needs_hiding, sys.modules)\n    _clear_modules(modules)",
    "label": true
  },
  {
    "code": "def _write_pyc_fp(fp: IO[bytes], source_stat: os.stat_result, co: types.CodeType) -> None:\n    fp.write(importlib.util.MAGIC_NUMBER)\n    flags = b'\\x00\\x00\\x00\\x00'\n    fp.write(flags)\n    mtime = int(source_stat.st_mtime) & 4294967295\n    size = source_stat.st_size & 4294967295\n    fp.write(struct.pack('<LL', mtime, size))\n    fp.write(marshal.dumps(co))",
    "label": true
  },
  {
    "code": "def enquote_executable(executable):\n    if ' ' in executable:\n        if executable.startswith('/usr/bin/env '):\n            env, _executable = executable.split(' ', 1)\n            if ' ' in _executable and (not _executable.startswith('\"')):\n                executable = '%s \"%s\"' % (env, _executable)\n        elif not executable.startswith('\"'):\n            executable = '\"%s\"' % executable\n    return executable",
    "label": true
  },
  {
    "code": "def wrap(s):\n    paragraphs = s.splitlines()\n    wrapped = ('\\n'.join(textwrap.wrap(para)) for para in paragraphs)\n    return '\\n\\n'.join(wrapped)",
    "label": true
  },
  {
    "code": "def parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:\n    if multiline:\n        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape_multiline\n    else:\n        error_on = ILLEGAL_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape\n    result = ''\n    start_pos = pos\n    while True:\n        try:\n            char = src[pos]\n        except IndexError:\n            raise suffixed_err(src, pos, 'Unterminated string') from None\n        if char == '\"':\n            if not multiline:\n                return (pos + 1, result + src[start_pos:pos])\n            if src.startswith('\"\"\"', pos):\n                return (pos + 3, result + src[start_pos:pos])\n            pos += 1\n            continue\n        if char == '\\\\':\n            result += src[start_pos:pos]\n            pos, parsed_escape = parse_escapes(src, pos)\n            result += parsed_escape\n            start_pos = pos\n            continue\n        if char in error_on:\n            raise suffixed_err(src, pos, f'Illegal character {char!r}')\n        pos += 1",
    "label": true
  },
  {
    "code": "def get_http_url(link: Link, download: Downloader, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> File:\n    temp_dir = TempDirectory(kind='unpack', globally_managed=True)\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n        content_type = None\n    else:\n        from_path, content_type = download(link, temp_dir.path)\n        if hashes:\n            hashes.check_against_path(from_path)\n    return File(from_path, content_type)",
    "label": true
  },
  {
    "code": "def clean_bci_data(bci_years: list[str], start_year: int, bci_scores: list) -> None:\n    for i in range(len(bci_scores)):\n        bci_years += [str(start_year - i)]\n        if bci_scores[i] == '':\n            bci_scores[i] = MISSING_BCI\n        else:\n            bci_scores[i] = float(bci_scores[i])",
    "label": true
  },
  {
    "code": "def _get_custom_platforms(arch: str) -> List[str]:\n    arch_prefix, arch_sep, arch_suffix = arch.partition('_')\n    if arch.startswith('macosx'):\n        arches = _mac_platforms(arch)\n    elif arch_prefix in ['manylinux2014', 'manylinux2010']:\n        arches = _custom_manylinux_platforms(arch)\n    else:\n        arches = [arch]\n    return arches",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def parse_basic_str_escape(src: str, pos: Pos, *, multiline: bool=False) -> tuple[Pos, str]:\n    escape_id = src[pos:pos + 2]\n    pos += 2\n    if multiline and escape_id in {'\\\\ ', '\\\\\\t', '\\\\\\n'}:\n        if escape_id != '\\\\\\n':\n            pos = skip_chars(src, pos, TOML_WS)\n            try:\n                char = src[pos]\n            except IndexError:\n                return (pos, '')\n            if char != '\\n':\n                raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\")\n            pos += 1\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        return (pos, '')\n    if escape_id == '\\\\u':\n        return parse_hex_char(src, pos, 4)\n    if escape_id == '\\\\U':\n        return parse_hex_char(src, pos, 8)\n    try:\n        return (pos, BASIC_STR_ESCAPE_REPLACEMENTS[escape_id])\n    except KeyError:\n        raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\") from None",
    "label": true
  },
  {
    "code": "def _identify_module(file, main=None):\n    from pickletools import genops\n    UNICODE = {'UNICODE', 'BINUNICODE', 'SHORT_BINUNICODE'}\n    found_import = False\n    try:\n        for opcode, arg, pos in genops(file.peek(256)):\n            if not found_import:\n                if opcode.name in ('GLOBAL', 'SHORT_BINUNICODE') and arg.endswith('_import_module'):\n                    found_import = True\n            elif opcode.name in UNICODE:\n                return arg\n        else:\n            raise UnpicklingError('reached STOP without finding main module')\n    except (NotImplementedError, ValueError) as error:\n        if isinstance(error, NotImplementedError) and main is not None:\n            return None\n        raise UnpicklingError('unable to identify main module') from error",
    "label": true
  },
  {
    "code": "def str_eval(token: str) -> str:\n    if token[0:2].lower() in {'fr', 'rf'}:\n        token = token[2:]\n    elif token[0].lower() in {'r', 'u', 'f'}:\n        token = token[1:]\n    if token[0:3] in {'\"\"\"', \"'''\"}:\n        return token[3:-3]\n    return token[1:-1]",
    "label": true
  },
  {
    "code": "def always_reversible(iterable):\n    try:\n        return reversed(iterable)\n    except TypeError:\n        return reversed(list(iterable))",
    "label": true
  },
  {
    "code": "def _dunder_dict(instance, attributes):\n    obj = node_classes.Dict(parent=instance)\n    keys = [node_classes.Const(value=value, parent=obj) for value in list(attributes.keys())]\n    values = [elem[-1] for elem in attributes.values()]\n    obj.postinit(list(zip(keys, values)))\n    return obj",
    "label": true
  },
  {
    "code": "def _bypass_ensure_directory(path):\n    if not WRITE_SUPPORT:\n        raise IOError('\"os.mkdir\" not supported on this platform.')\n    dirname, filename = split(path)\n    if dirname and filename and (not isdir(dirname)):\n        _bypass_ensure_directory(dirname)\n        try:\n            mkdir(dirname, 493)\n        except FileExistsError:\n            pass",
    "label": true
  },
  {
    "code": "def _match_link(link: Link, candidate: 'Candidate') -> bool:\n    if candidate.source_link:\n        return links_equivalent(link, candidate.source_link)\n    return False",
    "label": true
  },
  {
    "code": "def _dnsname_to_stdlib(name):\n\n    def idna_encode(name):\n        \"\"\"\n        Borrowed wholesale from the Python Cryptography Project. It turns out\n        that we can't just safely call `idna.encode`: it can explode for\n        wildcard names. This avoids that problem.\n        \"\"\"\n        from pip._vendor import idna\n        try:\n            for prefix in [u'*.', u'.']:\n                if name.startswith(prefix):\n                    name = name[len(prefix):]\n                    return prefix.encode('ascii') + idna.encode(name)\n            return idna.encode(name)\n        except idna.core.IDNAError:\n            return None\n    if ':' in name:\n        return name\n    name = idna_encode(name)\n    if name is None:\n        return None\n    elif sys.version_info >= (3, 0):\n        name = name.decode('utf-8')\n    return name",
    "label": true
  },
  {
    "code": "def _has_parent_of_type(node: nodes.Call, node_type: nodes.Keyword | nodes.Starred, statement: nodes.Statement) -> bool:\n    parent = node.parent\n    while not isinstance(parent, node_type) and statement.parent_of(parent):\n        parent = parent.parent\n    return isinstance(parent, node_type)",
    "label": true
  },
  {
    "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def python_2_unicode_compatible(klass):\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied to %s because it doesn't define __str__().\" % klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass",
    "label": true
  },
  {
    "code": "def invoke(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f",
    "label": true
  },
  {
    "code": "def _signature_from_arguments(arguments: nodes.Arguments) -> _ParameterSignature:\n    kwarg = arguments.kwarg\n    vararg = arguments.vararg\n    args = [arg.name for arg in chain(arguments.posonlyargs, arguments.args) if arg.name != 'self']\n    kwonlyargs = [arg.name for arg in arguments.kwonlyargs]\n    return _ParameterSignature(args, kwonlyargs, vararg, kwarg)",
    "label": true
  },
  {
    "code": "def _python_requires(dist: 'Distribution', val: dict, _root_dir):\n    from setuptools.extern.packaging.specifiers import SpecifierSet\n    _set_config(dist, 'python_requires', SpecifierSet(val))",
    "label": true
  },
  {
    "code": "def _ipaddress_match(ipname, host_ip):\n    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())\n    return ip == host_ip",
    "label": true
  },
  {
    "code": "def set_partitions(iterable, k=None):\n    L = list(iterable)\n    n = len(L)\n    if k is not None:\n        if k < 1:\n            raise ValueError(\"Can't partition in a negative or zero number of groups\")\n        elif k > n:\n            return\n\n    def set_partitions_helper(L, k):\n        n = len(L)\n        if k == 1:\n            yield [L]\n        elif n == k:\n            yield [[s] for s in L]\n        else:\n            e, *M = L\n            for p in set_partitions_helper(M, k - 1):\n                yield [[e], *p]\n            for p in set_partitions_helper(M, k):\n                for i in range(len(p)):\n                    yield (p[:i] + [[e] + p[i]] + p[i + 1:])\n    if k is None:\n        for k in range(1, n + 1):\n            yield from set_partitions_helper(L, k)\n    else:\n        yield from set_partitions_helper(L, k)",
    "label": true
  },
  {
    "code": "def _get_checkers_infos(linter: PyLinter) -> dict[str, dict[str, Any]]:\n    by_checker: dict[str, dict[str, Any]] = {}\n    for checker in linter.get_checkers():\n        name = checker.name\n        if name != MAIN_CHECKER_NAME:\n            try:\n                by_checker[name]['checker'] = checker\n                with warnings.catch_warnings():\n                    warnings.filterwarnings('ignore', category=DeprecationWarning)\n                    by_checker[name]['options'] += checker.options_and_values()\n                by_checker[name]['msgs'].update(checker.msgs)\n                by_checker[name]['reports'] += checker.reports\n            except KeyError:\n                with warnings.catch_warnings():\n                    warnings.filterwarnings('ignore', category=DeprecationWarning)\n                    by_checker[name] = {'checker': checker, 'options': list(checker.options_and_values()), 'msgs': dict(checker.msgs), 'reports': list(checker.reports)}\n    return by_checker",
    "label": true
  },
  {
    "code": "def function_to_method(n, klass):\n    if isinstance(n, FunctionDef):\n        if n.type == 'classmethod':\n            return bases.BoundMethod(n, klass)\n        if n.type == 'property':\n            return n\n        if n.type != 'staticmethod':\n            return bases.UnboundMethod(n)\n    return n",
    "label": true
  },
  {
    "code": "def _apply(dist: 'Distribution', filepath: _Path, other_files: Iterable[_Path]=(), ignore_option_errors: bool=False) -> Tuple['ConfigHandler', ...]:\n    from setuptools.dist import _Distribution\n    filepath = os.path.abspath(filepath)\n    if not os.path.isfile(filepath):\n        raise FileError(f'Configuration file {filepath} does not exist.')\n    current_directory = os.getcwd()\n    os.chdir(os.path.dirname(filepath))\n    filenames = [*other_files, filepath]\n    try:\n        _Distribution.parse_config_files(dist, filenames=filenames)\n        handlers = parse_configuration(dist, dist.command_options, ignore_option_errors=ignore_option_errors)\n        dist._finalize_license_files()\n    finally:\n        os.chdir(current_directory)\n    return handlers",
    "label": true
  },
  {
    "code": "def split_format_field_names(format_string: str) -> tuple[str, Iterable[tuple[bool, str]]]:\n    try:\n        return _string.formatter_field_name_split(format_string)\n    except ValueError as e:\n        raise IncompleteFormatString() from e",
    "label": true
  },
  {
    "code": "def baditems(obj, exact=False, safe=False):\n    if not hasattr(obj, '__iter__'):\n        return [j for j in (badobjects(obj, 0, exact, safe),) if j is not None]\n    obj = obj.values() if getattr(obj, 'values', None) else obj\n    _obj = []\n    [_obj.append(badobjects(i, 0, exact, safe)) for i in obj if i not in _obj]\n    return [j for j in _obj if j is not None]",
    "label": true
  },
  {
    "code": "def _get_raise_exc(node: nodes.Raise) -> str:\n    exceptions = node.nodes_of_class(nodes.Name)\n    try:\n        return f'{nodes.Raise.__name__} {next(exceptions).name}'\n    except StopIteration:\n        return nodes.Raise.__name__",
    "label": true
  },
  {
    "code": "def dist_factory(path_item, entry, only):\n    lower = entry.lower()\n    is_egg_info = lower.endswith('.egg-info')\n    is_dist_info = lower.endswith('.dist-info') and os.path.isdir(os.path.join(path_item, entry))\n    is_meta = is_egg_info or is_dist_info\n    return distributions_from_metadata if is_meta else find_distributions if not only and _is_egg_path(entry) else resolve_egg_link if not only and lower.endswith('.egg-link') else NoDists()",
    "label": true
  },
  {
    "code": "def _self_version_check_logic(*, state: SelfCheckState, current_time: datetime.datetime, local_version: DistributionVersion, get_remote_version: Callable[[], Optional[str]]) -> Optional[UpgradePrompt]:\n    remote_version_str = state.get(current_time)\n    if remote_version_str is None:\n        remote_version_str = get_remote_version()\n        if remote_version_str is None:\n            logger.debug('No remote pip version found')\n            return None\n        state.set(remote_version_str, current_time)\n    remote_version = parse_version(remote_version_str)\n    logger.debug('Remote version of pip: %s', remote_version)\n    logger.debug('Local version of pip:  %s', local_version)\n    pip_installed_by_pip = was_installed_by_pip('pip')\n    logger.debug('Was pip installed by pip? %s', pip_installed_by_pip)\n    if not pip_installed_by_pip:\n        return None\n    local_version_is_older = local_version < remote_version and local_version.base_version != remote_version.base_version\n    if local_version_is_older:\n        return UpgradePrompt(old=str(local_version), new=remote_version_str)\n    return None",
    "label": true
  },
  {
    "code": "def _get_report_choice(key: str) -> int:\n    import doctest\n    return {DOCTEST_REPORT_CHOICE_UDIFF: doctest.REPORT_UDIFF, DOCTEST_REPORT_CHOICE_CDIFF: doctest.REPORT_CDIFF, DOCTEST_REPORT_CHOICE_NDIFF: doctest.REPORT_NDIFF, DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE: doctest.REPORT_ONLY_FIRST_FAILURE, DOCTEST_REPORT_CHOICE_NONE: 0}[key]",
    "label": true
  },
  {
    "code": "def create_main_parser() -> ConfigOptionParser:\n    parser = ConfigOptionParser(usage='\\n%prog <command> [options]', add_help_option=False, formatter=UpdatingDefaultsHelpFormatter(), name='global', prog=get_prog())\n    parser.disable_interspersed_args()\n    parser.version = get_pip_version()\n    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)\n    parser.add_option_group(gen_opts)\n    parser.main = True\n    description = [''] + [f'{name:27} {command_info.summary}' for name, command_info in commands_dict.items()]\n    parser.description = '\\n'.join(description)\n    return parser",
    "label": true
  },
  {
    "code": "def _ensure_quoted_url(url: str) -> str:\n    result = urllib.parse.urlparse(url)\n    is_local_path = not result.netloc\n    path = _clean_url_path(result.path, is_local_path=is_local_path)\n    return urllib.parse.urlunparse(result._replace(path=path))",
    "label": true
  },
  {
    "code": "def value_chain(*args):\n    for value in args:\n        if isinstance(value, (str, bytes)):\n            yield value\n            continue\n        try:\n            yield from value\n        except TypeError:\n            yield value",
    "label": true
  },
  {
    "code": "def locatedExpr(expr: ParserElement) -> ParserElement:\n    locator = Empty().set_parse_action(lambda ss, ll, tt: ll)\n    return Group(locator('locn_start') + expr('value') + locator.copy().leaveWhitespace()('locn_end'))",
    "label": true
  },
  {
    "code": "def _get_node_id_with_markup(tw: TerminalWriter, config: Config, rep: BaseReport):\n    nodeid = config.cwd_relative_nodeid(rep.nodeid)\n    path, *parts = nodeid.split('::')\n    if parts:\n        parts_markup = tw.markup('::'.join(parts), bold=True)\n        return path + '::' + parts_markup\n    else:\n        return path",
    "label": true
  },
  {
    "code": "def get_exe_prefixes(exe_filename):\n    prefixes = [('PURELIB/', ''), ('PLATLIB/pywin32_system32', ''), ('PLATLIB/', ''), ('SCRIPTS/', 'EGG-INFO/scripts/'), ('DATA/lib/site-packages', '')]\n    z = zipfile.ZipFile(exe_filename)\n    try:\n        for info in z.infolist():\n            name = info.filename\n            parts = name.split('/')\n            if len(parts) == 3 and parts[2] == 'PKG-INFO':\n                if parts[1].endswith('.egg-info'):\n                    prefixes.insert(0, ('/'.join(parts[:2]), 'EGG-INFO/'))\n                    break\n            if len(parts) != 2 or not name.endswith('.pth'):\n                continue\n            if name.endswith('-nspkg.pth'):\n                continue\n            if parts[0].upper() in ('PURELIB', 'PLATLIB'):\n                contents = z.read(name).decode()\n                for pth in yield_lines(contents):\n                    pth = pth.strip().replace('\\\\', '/')\n                    if not pth.startswith('import'):\n                        prefixes.append(('%s/%s/' % (parts[0], pth), ''))\n    finally:\n        z.close()\n    prefixes = [(x.lower(), y) for x, y in prefixes]\n    prefixes.sort()\n    prefixes.reverse()\n    return prefixes",
    "label": true
  },
  {
    "code": "def remove_nested_packages(packages: List[str]) -> List[str]:\n    pkgs = sorted(packages, key=len)\n    top_level = pkgs[:]\n    size = len(pkgs)\n    for i, name in enumerate(reversed(pkgs)):\n        if any((name.startswith(f'{other}.') for other in top_level)):\n            top_level.pop(size - i - 1)\n    return top_level",
    "label": true
  },
  {
    "code": "def _create_weakproxy(obj, callable=False, *args):\n    from weakref import proxy\n    if obj is None:\n        if callable:\n            return proxy(lambda x: x, *args)\n        from collections import UserDict\n        return proxy(UserDict(), *args)\n    return proxy(obj, *args)",
    "label": true
  },
  {
    "code": "def open_file(filename: str, mode: str='r', encoding: t.Optional[str]=None, errors: t.Optional[str]='strict', lazy: bool=False, atomic: bool=False) -> t.IO[t.Any]:\n    if lazy:\n        return t.cast(t.IO[t.Any], LazyFile(filename, mode, encoding, errors, atomic=atomic))\n    f, should_close = open_stream(filename, mode, encoding, errors, atomic=atomic)\n    if not should_close:\n        f = t.cast(t.IO[t.Any], KeepOpenFile(f))\n    return f",
    "label": true
  },
  {
    "code": "def resolve_ssl_version(candidate: None | int | str) -> int:\n    if candidate is None:\n        return PROTOCOL_TLS\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'PROTOCOL_' + candidate)\n        return typing.cast(int, res)\n    return candidate",
    "label": true
  },
  {
    "code": "def _create_cfstring_array(lst):\n    cf_arr = None\n    try:\n        cf_arr = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        if not cf_arr:\n            raise MemoryError('Unable to allocate memory!')\n        for item in lst:\n            cf_str = _cfstr(item)\n            if not cf_str:\n                raise MemoryError('Unable to allocate memory!')\n            try:\n                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)\n            finally:\n                CoreFoundation.CFRelease(cf_str)\n    except BaseException as e:\n        if cf_arr:\n            CoreFoundation.CFRelease(cf_arr)\n        raise ssl.SSLError('Unable to allocate array: %s' % (e,))\n    return cf_arr",
    "label": true
  },
  {
    "code": "def _dump_time(v):\n    utcoffset = v.utcoffset()\n    if utcoffset is None:\n        return v.isoformat()\n    return v.isoformat()[:-6]",
    "label": true
  },
  {
    "code": "def infer_map(self: nodes.Dict, context: InferenceContext | None=None) -> Iterator[nodes.Dict]:\n    if not any((isinstance(k, nodes.DictUnpack) for k, _ in self.items)):\n        yield self\n    else:\n        items = _infer_map(self, context)\n        new_seq = type(self)(self.lineno, self.col_offset, self.parent)\n        new_seq.postinit(list(items.items()))\n        yield new_seq",
    "label": true
  },
  {
    "code": "def resolve_class(qualified_class_name: str, package_dir: Optional[Mapping[str, str]]=None, root_dir: Optional[_Path]=None) -> Callable:\n    root_dir = root_dir or os.getcwd()\n    idx = qualified_class_name.rfind('.')\n    class_name = qualified_class_name[idx + 1:]\n    pkg_name = qualified_class_name[:idx]\n    _parent_path, path, module_name = _find_module(pkg_name, package_dir, root_dir)\n    module = _load_spec(_find_spec(module_name, path), module_name)\n    return getattr(module, class_name)",
    "label": true
  },
  {
    "code": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {'implementation_name': implementation_name, 'implementation_version': iver, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'python_full_version': platform.python_version(), 'platform_python_implementation': platform.python_implementation(), 'python_version': '.'.join(platform.python_version_tuple()[:2]), 'sys_platform': sys.platform}",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable: Iterable[_T], key: Optional[Callable[[_T], _U]]=None) -> Iterator[_T]:\n    seen: Set[Union[_T, _U]] = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def _get_simple_response(url: str, session: PipSession) -> Response:\n    if is_archive_file(Link(url).filename):\n        _ensure_api_response(url, session=session)\n    logger.debug('Getting page %s', redact_auth_from_url(url))\n    resp = session.get(url, headers={'Accept': ', '.join(['application/vnd.pypi.simple.v1+json', 'application/vnd.pypi.simple.v1+html; q=0.1', 'text/html; q=0.01']), 'Cache-Control': 'max-age=0'})\n    raise_for_status(resp)\n    _ensure_api_header(resp)\n    logger.debug('Fetched page %s as %s', redact_auth_from_url(url), resp.headers.get('Content-Type', 'Unknown'))\n    return resp",
    "label": true
  },
  {
    "code": "def with_metaclass(meta, *bases):\n\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})",
    "label": true
  },
  {
    "code": "def iglob(path_glob):\n    if _CHECK_RECURSIVE_GLOB.search(path_glob):\n        msg = 'invalid glob %r: recursive glob \"**\" must be used alone'\n        raise ValueError(msg % path_glob)\n    if _CHECK_MISMATCH_SET.search(path_glob):\n        msg = \"invalid glob %r: mismatching set marker '{' or '}'\"\n        raise ValueError(msg % path_glob)\n    return _iglob(path_glob)",
    "label": true
  },
  {
    "code": "def _simulate_installation_of(to_install: List[InstallRequirement], package_set: PackageSet) -> Set[NormalizedName]:\n    installed = set()\n    for inst_req in to_install:\n        abstract_dist = make_distribution_for_install_requirement(inst_req)\n        dist = abstract_dist.get_metadata_distribution()\n        name = dist.canonical_name\n        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))\n        installed.add(name)\n    return installed",
    "label": true
  },
  {
    "code": "def is_compatible(wheel, tags=None):\n    if not isinstance(wheel, Wheel):\n        wheel = Wheel(wheel)\n    result = False\n    if tags is None:\n        tags = COMPATIBLE_TAGS\n    for ver, abi, arch in tags:\n        if ver in wheel.pyver and abi in wheel.abi and (arch in wheel.arch):\n            result = True\n            break\n    return result",
    "label": true
  },
  {
    "code": "def get_all_distribution_names(url=None):\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()",
    "label": true
  },
  {
    "code": "def _compare_approx(full_object: object, message_data: Sequence[Tuple[str, str, str]], number_of_elements: int, different_ids: Sequence[object], max_abs_diff: float, max_rel_diff: float) -> List[str]:\n    message_list = list(message_data)\n    message_list.insert(0, ('Index', 'Obtained', 'Expected'))\n    max_sizes = [0, 0, 0]\n    for index, obtained, expected in message_list:\n        max_sizes[0] = max(max_sizes[0], len(index))\n        max_sizes[1] = max(max_sizes[1], len(obtained))\n        max_sizes[2] = max(max_sizes[2], len(expected))\n    explanation = [f'comparison failed. Mismatched elements: {len(different_ids)} / {number_of_elements}:', f'Max absolute difference: {max_abs_diff}', f'Max relative difference: {max_rel_diff}'] + [f'{indexes:<{max_sizes[0]}} | {obtained:<{max_sizes[1]}} | {expected:<{max_sizes[2]}}' for indexes, obtained, expected in message_list]\n    return explanation",
    "label": true
  },
  {
    "code": "def _generic_io_transform(node, name, cls):\n    io_module = AstroidManager().ast_from_module_name('_io')\n    attribute_object = io_module[cls]\n    instance = attribute_object.instantiate_class()\n    node.locals[name] = [instance]",
    "label": true
  },
  {
    "code": "def _is_metaclass(klass, seen=None) -> bool:\n    if klass.name == 'type':\n        return True\n    if seen is None:\n        seen = set()\n    for base in klass.bases:\n        try:\n            for baseobj in base.infer():\n                baseobj_name = baseobj.qname()\n                if baseobj_name in seen:\n                    continue\n                seen.add(baseobj_name)\n                if isinstance(baseobj, bases.Instance):\n                    return False\n                if baseobj is klass:\n                    continue\n                if not isinstance(baseobj, ClassDef):\n                    continue\n                if baseobj._type == 'metaclass':\n                    return True\n                if _is_metaclass(baseobj, seen):\n                    return True\n        except InferenceError:\n            continue\n    return False",
    "label": true
  },
  {
    "code": "def get_lexer_by_name(_alias, **options):\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls(**options)\n    raise ClassNotFound('no lexer for alias %r found' % _alias)",
    "label": true
  },
  {
    "code": "def build_wheel(wheel_directory, config_settings, metadata_directory=None):\n    prebuilt_whl = _find_already_built_wheel(metadata_directory)\n    if prebuilt_whl:\n        shutil.copy2(prebuilt_whl, wheel_directory)\n        return os.path.basename(prebuilt_whl)\n    return _build_backend().build_wheel(wheel_directory, config_settings, metadata_directory)",
    "label": true
  },
  {
    "code": "def splitext(path: str) -> Tuple[str, str]:\n    base, ext = posixpath.splitext(path)\n    if base.lower().endswith('.tar'):\n        ext = base[-4:] + ext\n        base = base[:-4]\n    return (base, ext)",
    "label": true
  },
  {
    "code": "def _get_enchant_dict_help(inner_enchant_dicts: list[tuple[Any, enchant.ProviderDesc]], pyenchant_available: bool) -> str:\n    if inner_enchant_dicts:\n        dict_as_str = [f'{d[0]} ({d[1].name})' for d in inner_enchant_dicts]\n        enchant_help = f\"Available dictionaries: {', '.join(dict_as_str)}\"\n    else:\n        enchant_help = 'No available dictionaries : You need to install '\n        if not pyenchant_available:\n            enchant_help += 'both the python package and '\n        enchant_help += 'the system dependency for enchant to work.'\n    return f'Spelling dictionary name. {enchant_help}.'",
    "label": true
  },
  {
    "code": "def setup(**attrs):\n    logging.configure()\n    _install_setup_requires(attrs)\n    return distutils.core.setup(**attrs)",
    "label": true
  },
  {
    "code": "def _correct_article(noun: str) -> str:\n    if noun.lower()[0] in 'aeiou':\n        return 'an ' + noun\n    else:\n        return 'a ' + noun",
    "label": true
  },
  {
    "code": "def _dnsname_match(dn, hostname, max_wildcards=1):\n    pats = []\n    if not dn:\n        return False\n    parts = dn.split('.')\n    leftmost = parts[0]\n    remainder = parts[1:]\n    wildcards = leftmost.count('*')\n    if wildcards > max_wildcards:\n        raise CertificateError('too many wildcards in certificate DNS name: ' + repr(dn))\n    if not wildcards:\n        return dn.lower() == hostname.lower()\n    if leftmost == '*':\n        pats.append('[^.]+')\n    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n        pats.append(re.escape(leftmost))\n    else:\n        pats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n    for frag in remainder:\n        pats.append(re.escape(frag))\n    pat = re.compile('\\\\A' + '\\\\.'.join(pats) + '\\\\Z', re.IGNORECASE)\n    return pat.match(hostname)",
    "label": true
  },
  {
    "code": "def wrap_stream(stream, convert, strip, autoreset, wrap):\n    if wrap:\n        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)\n        if wrapper.should_wrap():\n            stream = wrapper.stream\n    return stream",
    "label": true
  },
  {
    "code": "def consume(iterator, n=None):\n    if n is None:\n        deque(iterator, maxlen=0)\n    else:\n        next(islice(iterator, n, n), None)",
    "label": true
  },
  {
    "code": "def reinit():\n    if wrapped_stdout is not None:\n        sys.stdout = wrapped_stdout\n    if wrapped_stderr is not None:\n        sys.stderr = wrapped_stderr",
    "label": true
  },
  {
    "code": "def _infer_caller():\n\n    def is_this_file(frame_info):\n        return frame_info.filename == __file__\n\n    def is_wrapper(frame_info):\n        return frame_info.function == 'wrapper'\n    not_this_file = itertools.filterfalse(is_this_file, inspect.stack())\n    callers = itertools.filterfalse(is_wrapper, not_this_file)\n    return next(callers).frame",
    "label": true
  },
  {
    "code": "def _find_adapter(registry, ob):\n    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))\n    for t in types:\n        if t in registry:\n            return registry[t]",
    "label": true
  },
  {
    "code": "def _is_has_never_check_common_name_reliable(openssl_version: str, openssl_version_number: int, implementation_name: str, version_info: _TYPE_VERSION_INFO, pypy_version_info: _TYPE_VERSION_INFO | None) -> bool:\n    is_openssl = openssl_version.startswith('OpenSSL ')\n    is_openssl_issue_14579_fixed = openssl_version_number >= 269488335\n    return is_openssl and (is_openssl_issue_14579_fixed or _is_bpo_43522_fixed(implementation_name, version_info, pypy_version_info))",
    "label": true
  },
  {
    "code": "def circular_shifts(iterable):\n    lst = list(iterable)\n    return take(len(lst), windowed(cycle(lst), len(lst)))",
    "label": true
  }
]