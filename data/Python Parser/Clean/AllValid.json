[
  {
    "code": "def approx(expected, rel=None, abs=None, nan_ok: bool=False) -> ApproxBase:\n    __tracebackhide__ = True\n    if isinstance(expected, Decimal):\n        cls: Type[ApproxBase] = ApproxDecimal\n    elif isinstance(expected, Mapping):\n        cls = ApproxMapping\n    elif _is_numpy_array(expected):\n        expected = _as_numpy_array(expected)\n        cls = ApproxNumpy\n    elif hasattr(expected, '__getitem__') and isinstance(expected, Sized) and (not isinstance(expected, STRING_TYPES)):\n        cls = ApproxSequenceLike\n    elif isinstance(expected, Collection) and (not isinstance(expected, STRING_TYPES)):\n        msg = f'pytest.approx() only supports ordered sequences, but got: {repr(expected)}'\n        raise TypeError(msg)\n    else:\n        cls = ApproxScalar\n    return cls(expected, rel, abs, nan_ok)",
    "label": true
  },
  {
    "code": "def get_constraints(expr: _NameNodes, frame: nodes.LocalsDictNodeNG) -> dict[nodes.If, set[Constraint]]:\n    current_node: nodes.NodeNG | None = expr\n    constraints_mapping: dict[nodes.If, set[Constraint]] = {}\n    while current_node is not None and current_node is not frame:\n        parent = current_node.parent\n        if isinstance(parent, nodes.If):\n            branch, _ = parent.locate_child(current_node)\n            constraints: set[Constraint] | None = None\n            if branch == 'body':\n                constraints = set(_match_constraint(expr, parent.test))\n            elif branch == 'orelse':\n                constraints = set(_match_constraint(expr, parent.test, invert=True))\n            if constraints:\n                constraints_mapping[parent] = constraints\n        current_node = parent\n    return constraints_mapping",
    "label": true
  },
  {
    "code": "def show_sys_implementation() -> None:\n    logger.info('sys.implementation:')\n    implementation_name = sys.implementation.name\n    with indent_log():\n        show_value('name', implementation_name)",
    "label": true
  },
  {
    "code": "def encode_multipart_formdata(fields: _TYPE_FIELDS, boundary: str | None=None) -> tuple[bytes, str]:\n    body = BytesIO()\n    if boundary is None:\n        boundary = choose_boundary()\n    for field in iter_field_objects(fields):\n        body.write(f'--{boundary}\\r\\n'.encode('latin-1'))\n        writer(body).write(field.render_headers())\n        data = field.data\n        if isinstance(data, int):\n            data = str(data)\n        if isinstance(data, str):\n            writer(body).write(data)\n        else:\n            body.write(data)\n        body.write(b'\\r\\n')\n    body.write(f'--{boundary}--\\r\\n'.encode('latin-1'))\n    content_type = f'multipart/form-data; boundary={boundary}'\n    return (body.getvalue(), content_type)",
    "label": true
  },
  {
    "code": "def _type_check(type1, type2) -> bool:\n    if not all(map(has_known_bases, (type1, type2))):\n        raise _NonDeducibleTypeHierarchy\n    if not all([type1.newstyle, type2.newstyle]):\n        return False\n    try:\n        return type1 in type2.mro()[:-1]\n    except MroError as e:\n        raise _NonDeducibleTypeHierarchy from e",
    "label": true
  },
  {
    "code": "def _get_quote_delimiter(string_token: str) -> str:\n    match = QUOTE_DELIMITER_REGEX.match(string_token)\n    if not match:\n        raise ValueError(f'string token {string_token} is not a well-formed string')\n    return match.group(2)",
    "label": true
  },
  {
    "code": "def parse_args(argv: Optional[Sequence[str]]=None) -> Dict[str, Any]:\n    argv = sys.argv[1:] if argv is None else list(argv)\n    remapped_deprecated_args = []\n    for index, arg in enumerate(argv):\n        if arg in DEPRECATED_SINGLE_DASH_ARGS:\n            remapped_deprecated_args.append(arg)\n            argv[index] = f'-{arg}'\n    parser = _build_arg_parser()\n    arguments = {key: value for key, value in vars(parser.parse_args(argv)).items() if value}\n    if remapped_deprecated_args:\n        arguments['remapped_deprecated_args'] = remapped_deprecated_args\n    if 'dont_order_by_type' in arguments:\n        arguments['order_by_type'] = False\n        del arguments['dont_order_by_type']\n    if 'dont_follow_links' in arguments:\n        arguments['follow_links'] = False\n        del arguments['dont_follow_links']\n    if 'dont_float_to_top' in arguments:\n        del arguments['dont_float_to_top']\n        if arguments.get('float_to_top', False):\n            sys.exit(\"Can't set both --float-to-top and --dont-float-to-top.\")\n        else:\n            arguments['float_to_top'] = False\n    multi_line_output = arguments.get('multi_line_output', None)\n    if multi_line_output:\n        if multi_line_output.isdigit():\n            arguments['multi_line_output'] = WrapModes(int(multi_line_output))\n        else:\n            arguments['multi_line_output'] = WrapModes[multi_line_output]\n    return arguments",
    "label": true
  },
  {
    "code": "def product_star(integer_queue: Queue) -> int:\n    bhavya = 1\n    skibid_peter = Queue()\n    while not integer_queue.is_empty():\n        skibid_peter.enqueue(integer_queue.dequeue())\n    while not skibid_peter.is_empty():\n        a = skibid_peter.dequeue()\n        bhavya *= a\n        integer_queue.enqueue(a)\n    return bhavya",
    "label": true
  },
  {
    "code": "def _check_link_requires_python(link: Link, version_info: Tuple[int, int, int], ignore_requires_python: bool=False) -> bool:\n    try:\n        is_compatible = check_requires_python(link.requires_python, version_info=version_info)\n    except specifiers.InvalidSpecifier:\n        logger.debug('Ignoring invalid Requires-Python (%r) for link: %s', link.requires_python, link)\n    else:\n        if not is_compatible:\n            version = '.'.join(map(str, version_info))\n            if not ignore_requires_python:\n                logger.verbose('Link requires a different Python (%s not in: %r): %s', version, link.requires_python, link)\n                return False\n            logger.debug('Ignoring failed Requires-Python check (%s not in: %r) for link: %s', version, link.requires_python, link)\n    return True",
    "label": true
  },
  {
    "code": "def _msvc14_find_vc2017():\n    root = environ.get('ProgramFiles(x86)') or environ.get('ProgramFiles')\n    if not root:\n        return (None, None)\n    suitable_components = ('Microsoft.VisualStudio.Component.VC.Tools.x86.x64', 'Microsoft.VisualStudio.Workload.WDExpress')\n    for component in suitable_components:\n        with contextlib.suppress(CalledProcessError, OSError, UnicodeDecodeError):\n            path = subprocess.check_output([join(root, 'Microsoft Visual Studio', 'Installer', 'vswhere.exe'), '-latest', '-prerelease', '-requires', component, '-property', 'installationPath', '-products', '*']).decode(encoding='mbcs', errors='strict').strip()\n            path = join(path, 'VC', 'Auxiliary', 'Build')\n            if isdir(path):\n                return (15, path)\n    return (None, None)",
    "label": true
  },
  {
    "code": "def find_ordinal(pos_num: int) -> str:\n    if pos_num == 0:\n        return 'th'\n    elif pos_num == 1:\n        return 'st'\n    elif pos_num == 2:\n        return 'nd'\n    elif pos_num == 3:\n        return 'rd'\n    elif 4 <= pos_num <= 20:\n        return 'th'\n    else:\n        return find_ordinal(pos_num % 10)",
    "label": true
  },
  {
    "code": "def _collect_type_vars(types, typevar_types=None):\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if isinstance(t, typevar_types) and t not in tvars and (not _is_unpack(t)):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)",
    "label": true
  },
  {
    "code": "def _encode_auth(auth):\n    auth_s = urllib.parse.unquote(auth)\n    auth_bytes = auth_s.encode()\n    encoded_bytes = base64.b64encode(auth_bytes)\n    encoded = encoded_bytes.decode()\n    return encoded.replace('\\n', '')",
    "label": true
  },
  {
    "code": "def _has_option(options: Values, reqs: List[InstallRequirement], option: str) -> bool:\n    if getattr(options, option, None):\n        return True\n    for req in reqs:\n        if getattr(req, option, None):\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _tree_to_bytes_helper(tree: HuffmanTree) -> list:\n    data = []\n    left_leaf = 0\n    left_info = tree.left.symbol\n    if not tree.left.is_leaf():\n        left_leaf = 1\n        left_info = tree.left.number\n        data.extend(_tree_to_bytes_helper(tree.left))\n    right_leaf = 0\n    right_info = tree.right.symbol\n    if not tree.right.is_leaf():\n        right_leaf = 1\n        right_info = tree.right.number\n        data.extend(_tree_to_bytes_helper(tree.right))\n    data.extend([left_leaf, left_info, right_leaf, right_info])\n    return data",
    "label": true
  },
  {
    "code": "def merge_cookies(cookiejar, cookies):\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError('You can only merge into CookieJar')\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n    return cookiejar",
    "label": true
  },
  {
    "code": "def _prepare_download(resp: Response, link: Link, progress_bar: str) -> Iterable[bytes]:\n    total_length = _get_http_response_size(resp)\n    if link.netloc == PyPI.file_storage_domain:\n        url = link.show_url\n    else:\n        url = link.url_without_fragment\n    logged_url = redact_auth_from_url(url)\n    if total_length:\n        logged_url = '{} ({})'.format(logged_url, format_size(total_length))\n    if is_from_cache(resp):\n        logger.info('Using cached %s', logged_url)\n    else:\n        logger.info('Downloading %s', logged_url)\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif is_from_cache(resp):\n        show_progress = False\n    elif not total_length:\n        show_progress = True\n    elif total_length > 40 * 1000:\n        show_progress = True\n    else:\n        show_progress = False\n    chunks = response_chunks(resp, CONTENT_CHUNK_SIZE)\n    if not show_progress:\n        return chunks\n    renderer = get_download_progress_renderer(bar_type=progress_bar, size=total_length)\n    return renderer(chunks)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e306(msg, _node, source_lines=None):\n    line = msg.line - 1\n    yield from render_context(line - 1, line + 1, source_lines)\n    body = source_lines[line]\n    indentation = len(body) - len(body.lstrip())\n    yield (None, slice(None, None), LineType.ERROR, body[:indentation] + NEW_BLANK_LINE_MESSAGE + ' ' * indentation)\n    yield from render_context(msg.line, msg.line + 2, source_lines)",
    "label": true
  },
  {
    "code": "def get_text_stdin(encoding: t.Optional[str]=None, errors: t.Optional[str]=None) -> t.TextIO:\n    rv = _get_windows_console_stream(sys.stdin, encoding, errors)\n    if rv is not None:\n        return rv\n    return _force_correct_text_reader(sys.stdin, encoding, errors, force_readable=True)",
    "label": true
  },
  {
    "code": "def get_attrs(obj):\n    if type(obj) in builtins_types or (type(obj) is type and obj in builtins_types):\n        return\n    return getattr(obj, '__dict__', None)",
    "label": true
  },
  {
    "code": "def patch_messages():\n    old_add_message = PyLinter.add_message\n\n    def new_add_message(self, msg_id, line=None, node=None, args=None, confidence=UNDEFINED, col_offset=None, end_lineno=None, end_col_offset=None):\n        old_add_message(self, msg_id, line, node, args, confidence, col_offset, end_lineno, end_col_offset)\n        msg_info = self.msgs_store.get_message_definitions(msg_id)[0]\n        if hasattr(self.reporter, 'handle_node'):\n            self.reporter.handle_node(msg_info, node)\n    PyLinter.add_message = new_add_message",
    "label": true
  },
  {
    "code": "def _is_cert(item):\n    expected = Security.SecCertificateGetTypeID()\n    return CoreFoundation.CFGetTypeID(item) == expected",
    "label": true
  },
  {
    "code": "def _call_aside(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f",
    "label": true
  },
  {
    "code": "def pick_bool(*values: Optional[bool]) -> bool:\n    assert values, '1 or more values required'\n    for value in values:\n        if value is not None:\n            return value\n    return bool(value)",
    "label": true
  },
  {
    "code": "def file_size(path: str) -> Union[int, float]:\n    if os.path.islink(path):\n        return 0\n    return os.path.getsize(path)",
    "label": true
  },
  {
    "code": "def glob_to_re(pattern):\n    pattern_re = fnmatch.translate(pattern)\n    sep = os.sep\n    if os.sep == '\\\\':\n        sep = '\\\\\\\\\\\\\\\\'\n    escaped = '\\\\1[^%s]' % sep\n    pattern_re = re.sub('((?<!\\\\\\\\)(\\\\\\\\\\\\\\\\)*)\\\\.', escaped, pattern_re)\n    return pattern_re",
    "label": true
  },
  {
    "code": "def glob0(dirname, basename):\n    if not basename:\n        if os.path.isdir(dirname):\n            return [basename]\n    elif os.path.lexists(os.path.join(dirname, basename)):\n        return [basename]\n    return []",
    "label": true
  },
  {
    "code": "def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None, socket_options=None):\n    host, port = address\n    if host.startswith('['):\n        host = host.strip('[]')\n    err = None\n    family = allowed_gai_family()\n    try:\n        host.encode('idna')\n    except UnicodeError:\n        return six.raise_from(LocationParseError(u\"'%s', label empty or too long\" % host), None)\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket.socket(af, socktype, proto)\n            _set_socket_options(sock, socket_options)\n            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n        except socket.error as e:\n            err = e\n            if sock is not None:\n                sock.close()\n                sock = None\n    if err is not None:\n        raise err\n    raise socket.error('getaddrinfo returns an empty list')",
    "label": true
  },
  {
    "code": "def get_python_inc(plat_specific=0, prefix=None):\n    default_prefix = BASE_EXEC_PREFIX if plat_specific else BASE_PREFIX\n    resolved_prefix = prefix if prefix is not None else default_prefix\n    try:\n        getter = globals()[f'_get_python_inc_{os.name}']\n    except KeyError:\n        raise DistutilsPlatformError(\"I don't know where Python installs its C header files on platform '%s'\" % os.name)\n    return getter(resolved_prefix, prefix, plat_specific)",
    "label": true
  },
  {
    "code": "def get_import_name(importnode: ImportNode, modname: str | None) -> str | None:\n    if isinstance(importnode, nodes.ImportFrom) and importnode.level:\n        root = importnode.root()\n        if isinstance(root, nodes.Module):\n            try:\n                return root.relative_to_absolute_name(modname, level=importnode.level)\n            except TooManyLevelsError:\n                return modname\n    return modname",
    "label": true
  },
  {
    "code": "def _glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def show_compilers():\n    from ..ccompiler import show_compilers\n    show_compilers()",
    "label": true
  },
  {
    "code": "def suppress_type_checks(func: Callable[P, T] | None=None) -> Callable[P, T] | ContextManager[None]:\n\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n        global type_checks_suppressed\n        with type_checks_suppress_lock:\n            type_checks_suppressed += 1\n        assert func is not None\n        try:\n            return func(*args, **kwargs)\n        finally:\n            with type_checks_suppress_lock:\n                type_checks_suppressed -= 1\n\n    def cm() -> Generator[None, None, None]:\n        global type_checks_suppressed\n        with type_checks_suppress_lock:\n            type_checks_suppressed += 1\n        try:\n            yield\n        finally:\n            with type_checks_suppress_lock:\n                type_checks_suppressed -= 1\n    if func is None:\n        return contextmanager(cm)()\n    else:\n        update_wrapper(wrapper, func)\n        return wrapper",
    "label": true
  },
  {
    "code": "def _check_link_requires_python(link: Link, version_info: Tuple[int, int, int], ignore_requires_python: bool=False) -> bool:\n    try:\n        is_compatible = check_requires_python(link.requires_python, version_info=version_info)\n    except specifiers.InvalidSpecifier:\n        logger.debug('Ignoring invalid Requires-Python (%r) for link: %s', link.requires_python, link)\n    else:\n        if not is_compatible:\n            version = '.'.join(map(str, version_info))\n            if not ignore_requires_python:\n                logger.verbose('Link requires a different Python (%s not in: %r): %s', version, link.requires_python, link)\n                return False\n            logger.debug('Ignoring failed Requires-Python check (%s not in: %r) for link: %s', version, link.requires_python, link)\n    return True",
    "label": true
  },
  {
    "code": "def _set_socket_options(sock, options):\n    if options is None:\n        return\n    for opt in options:\n        sock.setsockopt(*opt)",
    "label": true
  },
  {
    "code": "def _set_start_from_first_decorator(node):\n    if getattr(node, 'decorators'):\n        first_child = node.decorators\n        node.fromlineno = first_child.fromlineno\n        node.col_offset = first_child.col_offset\n    return node",
    "label": true
  },
  {
    "code": "def iter_slices(string, slice_length):\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos:pos + slice_length]\n        pos += slice_length",
    "label": true
  },
  {
    "code": "def _verifyflags_enum() -> str:\n    enum = '\\n    class VerifyFlags(_IntFlag):\\n        VERIFY_DEFAULT = 0\\n        VERIFY_CRL_CHECK_LEAF = 1\\n        VERIFY_CRL_CHECK_CHAIN = 2\\n        VERIFY_X509_STRICT = 3\\n        VERIFY_X509_TRUSTED_FIRST = 4'\n    if PY310_PLUS:\n        enum += '\\n        VERIFY_ALLOW_PROXY_CERTS = 5\\n        VERIFY_X509_PARTIAL_CHAIN = 6\\n        '\n    return enum",
    "label": true
  },
  {
    "code": "def read_attr(attr_desc: str, package_dir: Optional[Mapping[str, str]]=None, root_dir: Optional[_Path]=None):\n    root_dir = root_dir or os.getcwd()\n    attrs_path = attr_desc.strip().split('.')\n    attr_name = attrs_path.pop()\n    module_name = '.'.join(attrs_path)\n    module_name = module_name or '__init__'\n    _parent_path, path, module_name = _find_module(module_name, package_dir, root_dir)\n    spec = _find_spec(module_name, path)\n    try:\n        return getattr(StaticModule(module_name, spec), attr_name)\n    except Exception:\n        module = _load_spec(spec, module_name)\n        return getattr(module, attr_name)",
    "label": true
  },
  {
    "code": "def railroad_to_html(diagrams: List[NamedDiagram], embed=False, **kwargs) -> str:\n    data = []\n    for diagram in diagrams:\n        if diagram.diagram is None:\n            continue\n        io = StringIO()\n        try:\n            css = kwargs.get('css')\n            diagram.diagram.writeStandalone(io.write, css=css)\n        except AttributeError:\n            diagram.diagram.writeSvg(io.write)\n        title = diagram.name\n        if diagram.index == 0:\n            title += ' (root)'\n        data.append({'title': title, 'text': '', 'svg': io.getvalue()})\n    return template.render(diagrams=data, embed=embed, **kwargs)",
    "label": true
  },
  {
    "code": "def create_list_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 2\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n    out.flags.unset_all(key)\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.append_nest_to_list(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']]', pos):\n        raise suffixed_err(src, pos, \"Expected ']]' at the end of an array declaration\")\n    return (pos + 2, key)",
    "label": true
  },
  {
    "code": "def get_unpacked_marks(obj: Union[object, type], *, consider_mro: bool=True) -> List[Mark]:\n    if isinstance(obj, type):\n        if not consider_mro:\n            mark_lists = [obj.__dict__.get('pytestmark', [])]\n        else:\n            mark_lists = [x.__dict__.get('pytestmark', []) for x in obj.__mro__]\n        mark_list = []\n        for item in mark_lists:\n            if isinstance(item, list):\n                mark_list.extend(item)\n            else:\n                mark_list.append(item)\n    else:\n        mark_attribute = getattr(obj, 'pytestmark', [])\n        if isinstance(mark_attribute, list):\n            mark_list = mark_attribute\n        else:\n            mark_list = [mark_attribute]\n    return list(normalize_mark_list(mark_list))",
    "label": true
  },
  {
    "code": "def check_typed_dict(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, dict):\n        raise TypeCheckError('is not a dict')\n    declared_keys = frozenset(origin_type.__annotations__)\n    if hasattr(origin_type, '__required_keys__'):\n        required_keys = origin_type.__required_keys__\n    else:\n        required_keys = declared_keys if origin_type.__total__ else frozenset()\n    existing_keys = frozenset(value)\n    extra_keys = existing_keys - declared_keys\n    if extra_keys:\n        keys_formatted = ', '.join((f'\"{key}\"' for key in sorted(extra_keys, key=repr)))\n        raise TypeCheckError(f'has unexpected extra key(s): {keys_formatted}')\n    missing_keys = required_keys - existing_keys\n    if missing_keys:\n        keys_formatted = ', '.join((f'\"{key}\"' for key in sorted(missing_keys, key=repr)))\n        raise TypeCheckError(f'is missing required key(s): {keys_formatted}')\n    for key, argtype in get_type_hints(origin_type).items():\n        argvalue = value.get(key, _missing)\n        if argvalue is not _missing:\n            try:\n                check_type_internal(argvalue, argtype, memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f'value of key {key!r}')\n                raise",
    "label": true
  },
  {
    "code": "def _rebuild_mod_path(orig_path, package_name, module):\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path",
    "label": true
  },
  {
    "code": "def unquote_header_value(value, is_filename=False):\n    if value and value[0] == value[-1] == '\"':\n        value = value[1:-1]\n        if not is_filename or value[:2] != '\\\\\\\\':\n            return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n    return value",
    "label": true
  },
  {
    "code": "def splitUp(pred):\n    res = re_splitComparison.match(pred)\n    if not res:\n        raise ValueError('bad package restriction syntax: %r' % pred)\n    comp, verStr = res.groups()\n    with version.suppress_known_deprecation():\n        other = version.StrictVersion(verStr)\n    return (comp, other)",
    "label": true
  },
  {
    "code": "def format_header_param_html5(name, value):\n    if isinstance(value, six.binary_type):\n        value = value.decode('utf-8')\n    value = _replace_multiple(value, _HTML5_REPLACEMENTS)\n    return u'%s=\"%s\"' % (name, value)",
    "label": true
  },
  {
    "code": "def report_by_type_stats(sect: reporter_nodes.Section, stats: LinterStats, old_stats: LinterStats | None) -> None:\n    nice_stats: dict[str, dict[str, str]] = {}\n    for node_type in ('module', 'class', 'method', 'function'):\n        node_type = cast(Literal['function', 'class', 'method', 'module'], node_type)\n        total = stats.get_node_count(node_type)\n        nice_stats[node_type] = {}\n        if total != 0:\n            undocumented_node = stats.get_undocumented(node_type)\n            documented = total - undocumented_node\n            percent = documented * 100.0 / total\n            nice_stats[node_type]['percent_documented'] = f'{percent:.2f}'\n            badname_node = stats.get_bad_names(node_type)\n            percent = badname_node * 100.0 / total\n            nice_stats[node_type]['percent_badname'] = f'{percent:.2f}'\n    lines = ['type', 'number', 'old number', 'difference', '%documented', '%badname']\n    for node_type in ('module', 'class', 'method', 'function'):\n        node_type = cast(Literal['function', 'class', 'method', 'module'], node_type)\n        new = stats.get_node_count(node_type)\n        old = old_stats.get_node_count(node_type) if old_stats else None\n        diff_str = lint_utils.diff_string(old, new) if old else None\n        lines += [node_type, str(new), str(old) if old else 'NC', diff_str if diff_str else 'NC', nice_stats[node_type].get('percent_documented', '0'), nice_stats[node_type].get('percent_badname', '0')]\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))",
    "label": true
  },
  {
    "code": "def _preconvert(item: Any) -> Union[str, List[Any]]:\n    if isinstance(item, (set, frozenset)):\n        return list(item)\n    if isinstance(item, WrapModes):\n        return str(item.name)\n    if isinstance(item, Path):\n        return str(item)\n    if callable(item) and hasattr(item, '__name__'):\n        return str(item.__name__)\n    raise TypeError(f'Unserializable object {item} of type {type(item)}')",
    "label": true
  },
  {
    "code": "def _is_invalid_metaclass(metaclass: nodes.ClassDef) -> bool:\n    try:\n        mro = metaclass.mro()\n    except (astroid.DuplicateBasesError, astroid.InconsistentMroError):\n        return True\n    return not any((is_builtin_object(cls) and cls.name == 'type' for cls in mro))",
    "label": true
  },
  {
    "code": "def _create_truststore_ssl_context() -> Optional['SSLContext']:\n    if sys.version_info < (3, 10):\n        raise CommandError('The truststore feature is only available for Python 3.10+')\n    try:\n        import ssl\n    except ImportError:\n        logger.warning('Disabling truststore since ssl support is missing')\n        return None\n    try:\n        import truststore\n    except ImportError:\n        raise CommandError(\"To use the truststore feature, 'truststore' must be installed into pip's current environment.\")\n    return truststore.SSLContext(ssl.PROTOCOL_TLS_CLIENT)",
    "label": true
  },
  {
    "code": "def code(func):\n    if ismethod(func):\n        func = func.__func__\n    if isfunction(func):\n        func = func.__code__\n    if istraceback(func):\n        func = func.tb_frame\n    if isframe(func):\n        func = func.f_code\n    if iscode(func):\n        return func\n    return",
    "label": true
  },
  {
    "code": "def _can_symlink_files(base_dir: Path) -> bool:\n    with TemporaryDirectory(dir=str(base_dir.resolve())) as tmp:\n        path1, path2 = (Path(tmp, 'file1.txt'), Path(tmp, 'file2.txt'))\n        path1.write_text('file1', encoding='utf-8')\n        with suppress(AttributeError, NotImplementedError, OSError):\n            os.symlink(path1, path2)\n            if path2.is_symlink() and path2.read_text(encoding='utf-8') == 'file1':\n                return True\n        try:\n            os.link(path1, path2)\n        except Exception as ex:\n            msg = 'File system does not seem to support either symlinks or hard links. Strict editable installs require one of them to be supported.'\n            raise LinksNotSupported(msg) from ex\n        return False",
    "label": true
  },
  {
    "code": "def _evaluate_markers(markers: List[Any], environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n            if isinstance(lhs, Variable):\n                lhs_value = _get_env(environment, lhs.value)\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                rhs_value = _get_env(environment, rhs.value)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in ['and', 'or']\n            if marker == 'or':\n                groups.append([])\n    return any((all(item) for item in groups))",
    "label": true
  },
  {
    "code": "def main() -> None:\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n    parser = argparse.ArgumentParser(description='OS distro info tool')\n    parser.add_argument('--json', '-j', help='Output in machine readable format', action='store_true')\n    parser.add_argument('--root-dir', '-r', type=str, dest='root_dir', help='Path to the root filesystem directory (defaults to /)')\n    args = parser.parse_args()\n    if args.root_dir:\n        dist = LinuxDistribution(include_lsb=False, include_uname=False, include_oslevel=False, root_dir=args.root_dir)\n    else:\n        dist = _distro\n    if args.json:\n        logger.info(json.dumps(dist.info(), indent=4, sort_keys=True))\n    else:\n        logger.info('Name: %s', dist.name(pretty=True))\n        distribution_version = dist.version(pretty=True)\n        logger.info('Version: %s', distribution_version)\n        distribution_codename = dist.codename()\n        logger.info('Codename: %s', distribution_codename)",
    "label": true
  },
  {
    "code": "def decode(s: Union[str, bytes, bytearray], strict: bool=False, uts46: bool=False, std3_rules: bool=False) -> str:\n    try:\n        if isinstance(s, (bytes, bytearray)):\n            s = s.decode('ascii')\n    except UnicodeDecodeError:\n        raise IDNAError('Invalid ASCII in A-label')\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split('.')\n    if not labels or labels == ['']:\n        raise IDNAError('Empty domain')\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError('Empty label')\n    if trailing_dot:\n        result.append('')\n    return '.'.join(result)",
    "label": true
  },
  {
    "code": "def inject_into_urllib3() -> None:\n    _validate_dependencies_met()\n    util.SSLContext = PyOpenSSLContext\n    util.ssl_.SSLContext = PyOpenSSLContext\n    util.IS_PYOPENSSL = True\n    util.ssl_.IS_PYOPENSSL = True",
    "label": true
  },
  {
    "code": "def configuration_to_dict(handlers: Tuple['ConfigHandler', ...]) -> dict:\n    config_dict: dict = defaultdict(dict)\n    for handler in handlers:\n        for option in handler.set_options:\n            value = _get_option(handler.target_obj, option)\n            config_dict[handler.section_prefix][option] = value\n    return config_dict",
    "label": true
  },
  {
    "code": "def get_best_invocation_for_this_python() -> str:\n    exe = sys.executable\n    exe_name = os.path.basename(exe)\n    found_executable = shutil.which(exe_name)\n    if found_executable and os.path.samefile(found_executable, exe):\n        return exe_name\n    return exe",
    "label": true
  },
  {
    "code": "def ensure_dir(path: str) -> None:\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST and e.errno != errno.ENOTEMPTY:\n            raise",
    "label": true
  },
  {
    "code": "def get_all_lexers(plugins=True):\n    for item in LEXERS.values():\n        yield item[1:]\n    if plugins:\n        for lexer in find_plugin_lexers():\n            yield (lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes)",
    "label": true
  },
  {
    "code": "def show_unified_diff(*, file_input: str, file_output: str, file_path: Optional[Path], output: Optional[TextIO]=None, color_output: bool=False) -> None:\n    printer = create_terminal_printer(color_output, output)\n    file_name = '' if file_path is None else str(file_path)\n    file_mtime = str(datetime.now() if file_path is None else datetime.fromtimestamp(file_path.stat().st_mtime))\n    unified_diff_lines = unified_diff(file_input.splitlines(keepends=True), file_output.splitlines(keepends=True), fromfile=file_name + ':before', tofile=file_name + ':after', fromfiledate=file_mtime, tofiledate=str(datetime.now()))\n    for line in unified_diff_lines:\n        printer.diff_line(line)",
    "label": true
  },
  {
    "code": "def _get_renamed_namedtuple_attributes(field_names):\n    names = list(field_names)\n    seen = set()\n    for i, name in enumerate(field_names):\n        if not all((c.isalnum() or c == '_' for c in name)) or keyword.iskeyword(name) or (not name) or name[0].isdigit() or name.startswith('_') or (name in seen):\n            names[i] = '_%d' % i\n        seen.add(name)\n    return tuple(names)",
    "label": true
  },
  {
    "code": "def object_type_repr(obj: t.Any) -> str:\n    if obj is None:\n        return 'None'\n    elif obj is Ellipsis:\n        return 'Ellipsis'\n    cls = type(obj)\n    if cls.__module__ == 'builtins':\n        return f'{cls.__name__} object'\n    return f'{cls.__module__}.{cls.__name__} object'",
    "label": true
  },
  {
    "code": "def _indent(text, prefix, predicate=default_predicate) -> str:\n\n    def prefixed_lines():\n        for line in text.splitlines(True):\n            yield (prefix + line if predicate(line) else line)\n    return ''.join(prefixed_lines())",
    "label": true
  },
  {
    "code": "def random_combination_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    indices = sorted((randrange(n) for i in range(r)))\n    return tuple((pool[i] for i in indices))",
    "label": true
  },
  {
    "code": "def makefile(self: socket_cls, mode: Literal['r'] | Literal['w'] | Literal['rw'] | Literal['wr'] | Literal['']='r', buffering: int | None=None, *args: typing.Any, **kwargs: typing.Any) -> typing.BinaryIO | typing.TextIO:\n    buffering = 0\n    return socket_cls.makefile(self, mode, buffering, *args, **kwargs)",
    "label": true
  },
  {
    "code": "def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n    if tokenizer.check('VARIABLE'):\n        return process_env_var(tokenizer.read().text.replace('.', '_'))\n    elif tokenizer.check('QUOTED_STRING'):\n        return process_python_str(tokenizer.read().text)\n    else:\n        tokenizer.raise_syntax_error(message='Expected a marker variable or quoted string')",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:\n    package_set, _ = create_package_set_from_installed()\n    would_be_installed = _simulate_installation_of(to_install, package_set)\n    whitelist = _create_whitelist(would_be_installed, package_set)\n    return (package_set, check_package_set(package_set, should_ignore=lambda name: name not in whitelist))",
    "label": true
  },
  {
    "code": "def _pipe_line_with_colons(colwidths, colaligns):\n    if not colaligns:\n        colaligns = [''] * len(colwidths)\n    segments = [_pipe_segment_with_colons(a, w) for a, w in zip(colaligns, colwidths)]\n    return '|' + '|'.join(segments) + '|'",
    "label": true
  },
  {
    "code": "def unpack_url(link: Link, location: str, download: Downloader, verbosity: int, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> Optional[File]:\n    if link.is_vcs:\n        unpack_vcs_link(link, location, verbosity=verbosity)\n        return None\n    assert not link.is_existing_dir()\n    if link.is_file:\n        file = get_file_url(link, download_dir, hashes=hashes)\n    else:\n        file = get_http_url(link, download, download_dir, hashes=hashes)\n    if not link.is_wheel:\n        unpack_file(file.path, location, file.content_type)\n    return file",
    "label": true
  },
  {
    "code": "def _with_exception(exception_type: _ET) -> Callable[[_F], _WithException[_F, _ET]]:\n\n    def decorate(func: _F) -> _WithException[_F, _ET]:\n        func_with_exception = cast(_WithException[_F, _ET], func)\n        func_with_exception.Exception = exception_type\n        return func_with_exception\n    return decorate",
    "label": true
  },
  {
    "code": "def get_win_folder_via_ctypes(csidl_name: str) -> str:\n    csidl_const = {'CSIDL_APPDATA': 26, 'CSIDL_COMMON_APPDATA': 35, 'CSIDL_LOCAL_APPDATA': 28, 'CSIDL_PERSONAL': 5}.get(csidl_name)\n    if csidl_const is None:\n        raise ValueError(f'Unknown CSIDL name: {csidl_name}')\n    buf = ctypes.create_unicode_buffer(1024)\n    windll = getattr(ctypes, 'windll')\n    windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n    if any((ord(c) > 255 for c in buf)):\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n    return buf.value",
    "label": true
  },
  {
    "code": "def interpreter_version(*, warn: bool=False) -> str:\n    version = _get_config_var('py_version_nodot', warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
    "label": true
  },
  {
    "code": "def process_env_var(env_var: str) -> Variable:\n    if env_var == 'platform_python_implementation' or env_var == 'python_implementation':\n        return Variable('platform_python_implementation')\n    else:\n        return Variable(env_var)",
    "label": true
  },
  {
    "code": "def test_nostrictio_contentsfmode():\n    bench(False, dill.CONTENTS_FMODE, True)\n    teardown_module()",
    "label": true
  },
  {
    "code": "def _handle_no_cache_dir(option: Option, opt: str, value: str, parser: OptionParser) -> None:\n    if value is not None:\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n    parser.values.cache_dir = False",
    "label": true
  },
  {
    "code": "def _build_simple_row(padded_cells, rowfmt):\n    begin, sep, end = rowfmt\n    return (begin + sep.join(padded_cells) + end).rstrip()",
    "label": true
  },
  {
    "code": "def locate(iterable, pred=bool, window_size=None):\n    if window_size is None:\n        return compress(count(), map(pred, iterable))\n    if window_size < 1:\n        raise ValueError('window size must be at least 1')\n    it = windowed(iterable, window_size, fillvalue=_marker)\n    return compress(count(), starmap(pred, it))",
    "label": true
  },
  {
    "code": "def parse_format_method_string(format_string: str) -> tuple[list[tuple[str, list[tuple[bool, str]]]], int, int]:\n    keyword_arguments = []\n    implicit_pos_args_cnt = 0\n    explicit_pos_args = set()\n    for name in collect_string_fields(format_string):\n        if name and str(name).isdigit():\n            explicit_pos_args.add(str(name))\n        elif name:\n            keyname, fielditerator = split_format_field_names(name)\n            if isinstance(keyname, numbers.Number):\n                explicit_pos_args.add(str(keyname))\n            try:\n                keyword_arguments.append((keyname, list(fielditerator)))\n            except ValueError as e:\n                raise IncompleteFormatString() from e\n        else:\n            implicit_pos_args_cnt += 1\n    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))",
    "label": true
  },
  {
    "code": "def _sample_unweighted(iterable, k):\n    reservoir = take(k, iterable)\n    W = exp(log(random()) / k)\n    next_index = k + floor(log(random()) / log(1 - W))\n    for index, element in enumerate(iterable, k):\n        if index == next_index:\n            reservoir[randrange(k)] = element\n            W *= exp(log(random()) / k)\n            next_index += floor(log(random()) / log(1 - W)) + 1\n    return reservoir",
    "label": true
  },
  {
    "code": "def _get_encoding(encoding_or_label):\n    if hasattr(encoding_or_label, 'codec_info'):\n        return encoding_or_label\n    encoding = lookup(encoding_or_label)\n    if encoding is None:\n        raise LookupError('Unknown encoding label: %r' % encoding_or_label)\n    return encoding",
    "label": true
  },
  {
    "code": "def is_compatible(wheel, tags=None):\n    if not isinstance(wheel, Wheel):\n        wheel = Wheel(wheel)\n    result = False\n    if tags is None:\n        tags = COMPATIBLE_TAGS\n    for ver, abi, arch in tags:\n        if ver in wheel.pyver and abi in wheel.abi and (arch in wheel.arch):\n            result = True\n            break\n    return result",
    "label": true
  },
  {
    "code": "def parse_function_type_comment(type_comment: str) -> FunctionType | None:\n    if _ast_py3 is None:\n        return None\n    func_type = _ast_py3.parse(type_comment, '<type_comment>', 'func_type')\n    return FunctionType(argtypes=func_type.argtypes, returns=func_type.returns)",
    "label": true
  },
  {
    "code": "def _print_list_as_json(requested_items):\n    import json\n    result = {}\n    if 'lexer' in requested_items:\n        info = {}\n        for fullname, names, filenames, mimetypes in get_all_lexers():\n            info[fullname] = {'aliases': names, 'filenames': filenames, 'mimetypes': mimetypes}\n        result['lexers'] = info\n    if 'formatter' in requested_items:\n        info = {}\n        for cls in get_all_formatters():\n            doc = docstring_headline(cls)\n            info[cls.name] = {'aliases': cls.aliases, 'filenames': cls.filenames, 'doc': doc}\n        result['formatters'] = info\n    if 'filter' in requested_items:\n        info = {}\n        for name in get_all_filters():\n            cls = find_filter_class(name)\n            info[name] = {'doc': docstring_headline(cls)}\n        result['filters'] = info\n    if 'style' in requested_items:\n        info = {}\n        for name in get_all_styles():\n            cls = get_style_by_name(name)\n            info[name] = {'doc': docstring_headline(cls)}\n        result['styles'] = info\n    json.dump(result, sys.stdout)",
    "label": true
  },
  {
    "code": "def gen_lib_options(compiler, library_dirs, runtime_library_dirs, libraries):\n    lib_opts = []\n    for dir in library_dirs:\n        lib_opts.append(compiler.library_dir_option(dir))\n    for dir in runtime_library_dirs:\n        opt = compiler.runtime_library_dir_option(dir)\n        if isinstance(opt, list):\n            lib_opts = lib_opts + opt\n        else:\n            lib_opts.append(opt)\n    for lib in libraries:\n        lib_dir, lib_name = os.path.split(lib)\n        if lib_dir:\n            lib_file = compiler.find_library_file([lib_dir], lib_name)\n            if lib_file:\n                lib_opts.append(lib_file)\n            else:\n                compiler.warn(\"no library file corresponding to '%s' found (skipping)\" % lib)\n        else:\n            lib_opts.append(compiler.library_option(lib))\n    return lib_opts",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def platform_tags(linux: str, arch: str) -> Iterator[str]:\n    if not _have_compatible_abi(arch):\n        return\n    too_old_glibc2 = _GLibCVersion(2, 16)\n    if arch in {'x86_64', 'i686'}:\n        too_old_glibc2 = _GLibCVersion(2, 4)\n    current_glibc = _GLibCVersion(*_get_glibc_version())\n    glibc_max_list = [current_glibc]\n    for glibc_major in range(current_glibc.major - 1, 1, -1):\n        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n    for glibc_max in glibc_max_list:\n        if glibc_max.major == too_old_glibc2.major:\n            min_minor = too_old_glibc2.minor\n        else:\n            min_minor = -1\n        for glibc_minor in range(glibc_max.minor, min_minor, -1):\n            glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n            tag = 'manylinux_{}_{}'.format(*glibc_version)\n            if _is_compatible(tag, arch, glibc_version):\n                yield linux.replace('linux', tag)\n            if glibc_version in _LEGACY_MANYLINUX_MAP:\n                legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n                if _is_compatible(legacy_tag, arch, glibc_version):\n                    yield linux.replace('linux', legacy_tag)",
    "label": true
  },
  {
    "code": "def __getstate__():\n    state = {}\n    g = globals()\n    for k, v in _state_vars.items():\n        state[k] = g['_sget_' + v](g[k])\n    return state",
    "label": true
  },
  {
    "code": "def _check_no_input(message: str) -> None:\n    if os.environ.get('PIP_NO_INPUT'):\n        raise Exception(f'No input was expected ($PIP_NO_INPUT set); question: {message}')",
    "label": true
  },
  {
    "code": "def _is_owner_ignored(owner: SuccessfulInferenceResult, attrname: str | None, ignored_classes: Iterable[str], ignored_modules: Iterable[str]) -> bool:\n    if is_module_ignored(owner.root(), ignored_modules):\n        return True\n    ignored_classes = set(ignored_classes)\n    qname = owner.qname() if hasattr(owner, 'qname') else ''\n    return any((ignore in (attrname, qname) for ignore in ignored_classes))",
    "label": true
  },
  {
    "code": "def make_distribution_for_install_requirement(install_req: InstallRequirement) -> AbstractDistribution:\n    if install_req.editable:\n        return SourceDistribution(install_req)\n    if install_req.is_wheel:\n        return WheelDistribution(install_req)\n    return SourceDistribution(install_req)",
    "label": true
  },
  {
    "code": "def get_stderr_fileno() -> int:\n    try:\n        fileno = sys.stderr.fileno()\n        if fileno == -1:\n            raise AttributeError()\n        return fileno\n    except (AttributeError, io.UnsupportedOperation):\n        return sys.__stderr__.fileno()",
    "label": true
  },
  {
    "code": "def _get_previous_scripts(dist: 'Distribution') -> Optional[list]:\n    value = getattr(dist, 'entry_points', None) or {}\n    return value.get('console_scripts')",
    "label": true
  },
  {
    "code": "def parse_key(src: str, pos: Pos) -> tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: str | None = src[pos]\n        except IndexError:\n            char = None\n        if char != '.':\n            return (pos, key)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def _remove_and_clear_zip_directory_cache_data(normalized_path):\n\n    def clear_and_remove_cached_zip_archive_directory_data(path, old_entry):\n        old_entry.clear()\n    _update_zipimporter_cache(normalized_path, zipimport._zip_directory_cache, updater=clear_and_remove_cached_zip_archive_directory_data)",
    "label": true
  },
  {
    "code": "def _annotated_unpack_infer(stmt: nodes.NodeNG, context: InferenceContext | None=None) -> Generator[tuple[nodes.NodeNG, SuccessfulInferenceResult], None, None]:\n    if isinstance(stmt, (nodes.List, nodes.Tuple)):\n        for elt in stmt.elts:\n            inferred = utils.safe_infer(elt)\n            if inferred and (not isinstance(inferred, util.UninferableBase)):\n                yield (elt, inferred)\n        return\n    for inferred in stmt.infer(context):\n        if isinstance(inferred, util.UninferableBase):\n            continue\n        yield (stmt, inferred)",
    "label": true
  },
  {
    "code": "def _get_python_inc_posix(prefix, spec_prefix, plat_specific):\n    if IS_PYPY and sys.version_info < (3, 8):\n        return os.path.join(prefix, 'include')\n    return _get_python_inc_posix_python(plat_specific) or _extant(_get_python_inc_from_config(plat_specific, spec_prefix)) or _get_python_inc_posix_prefix(prefix)",
    "label": true
  },
  {
    "code": "def is_generator(func: object) -> bool:\n    genfunc = inspect.isgeneratorfunction(func)\n    return genfunc and (not iscoroutinefunction(func))",
    "label": true
  },
  {
    "code": "def write_docstring(tw: TerminalWriter, doc: str, indent: str='    ') -> None:\n    for line in doc.split('\\n'):\n        tw.line(indent + line)",
    "label": true
  },
  {
    "code": "def _is_binary_reader(stream: t.IO[t.Any], default: bool=False) -> bool:\n    try:\n        return isinstance(stream.read(0), bytes)\n    except Exception:\n        return default",
    "label": true
  },
  {
    "code": "def _get_url_from_path(path: str, name: str) -> Optional[str]:\n    if _looks_like_path(name) and os.path.isdir(path):\n        if is_installable_dir(path):\n            return path_to_url(path)\n        raise InstallationError(f\"Directory {name!r} is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\")\n    if not is_archive_file(path):\n        return None\n    if os.path.isfile(path):\n        return path_to_url(path)\n    urlreq_parts = name.split('@', 1)\n    if len(urlreq_parts) >= 2 and (not _looks_like_path(urlreq_parts[0])):\n        return None\n    logger.warning('Requirement %r looks like a filename, but the file does not exist', name)\n    return path_to_url(path)",
    "label": true
  },
  {
    "code": "def _looks_like_subscriptable(node: ClassDef) -> bool:\n    if node.qname().startswith('_collections') or node.qname().startswith('collections'):\n        try:\n            node.getattr('__class_getitem__')\n            return True\n        except AttributeInferenceError:\n            pass\n    return False",
    "label": true
  },
  {
    "code": "def _have_cython():\n    cython_impl = 'Cython.Distutils.build_ext'\n    try:\n        __import__(cython_impl, fromlist=['build_ext']).build_ext\n        return True\n    except Exception:\n        pass\n    return False",
    "label": true
  },
  {
    "code": "def parse(code: str, module_name: str='', path: str | None=None, apply_transforms: bool=True) -> nodes.Module:\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(manager=AstroidManager(), apply_transforms=apply_transforms)\n    return builder.string_build(code, modname=module_name, path=path)",
    "label": true
  },
  {
    "code": "def is_from_fallback_block(node: nodes.NodeNG) -> bool:\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n    if isinstance(context, nodes.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable((handler.body for handler in context.handlers))\n        handlers = context.handlers\n    has_fallback_imports = any((isinstance(import_node, (nodes.ImportFrom, nodes.Import)) for import_node in other_body))\n    ignores_import_error = _except_handlers_ignores_exceptions(handlers, (ImportError, ModuleNotFoundError))\n    return ignores_import_error or has_fallback_imports",
    "label": true
  },
  {
    "code": "def redact_netloc(netloc: str) -> str:\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    if password is None:\n        user = '****'\n        password = ''\n    else:\n        user = urllib.parse.quote(user)\n        password = ':****'\n    return '{user}{password}@{netloc}'.format(user=user, password=password, netloc=netloc)",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def insert_default_options() -> None:\n    options = get_default_options()\n    options.reverse()\n    for arg in options:\n        sys.argv.insert(1, arg)",
    "label": true
  },
  {
    "code": "def format_command_result(command_args: List[str], command_output: str) -> str:\n    command_desc = format_command_args(command_args)\n    text = f'Command arguments: {command_desc}\\n'\n    if not command_output:\n        text += 'Command output: None'\n    elif logger.getEffectiveLevel() > logging.DEBUG:\n        text += 'Command output: [use --verbose to show]'\n    else:\n        if not command_output.endswith('\\n'):\n            command_output += '\\n'\n        text += f'Command output:\\n{command_output}'\n    return text",
    "label": true
  },
  {
    "code": "def _get_leafs_and_levels(tree: HuffmanTree) -> tuple[list[HuffmanTree], list[int]]:\n    leafs = []\n    nodes = [tree]\n    levels = []\n    while nodes:\n        n = len(nodes)\n        leaves_counter = 0\n        for _ in range(0, n):\n            tree_curr = nodes.pop(0)\n            if tree_curr.left.is_leaf():\n                leafs.append(tree_curr.left)\n                leaves_counter += 1\n            else:\n                nodes.append(tree_curr.left)\n            if tree_curr.right.is_leaf():\n                leafs.append(tree_curr.right)\n                leaves_counter += 1\n            else:\n                nodes.append(tree_curr.right)\n        levels.append(leaves_counter)\n    return (leafs, levels)",
    "label": true
  },
  {
    "code": "def _print_help(what, name):\n    try:\n        if what == 'lexer':\n            cls = get_lexer_by_name(name)\n            print('Help on the %s lexer:' % cls.name)\n            print(dedent(cls.__doc__))\n        elif what == 'formatter':\n            cls = find_formatter_class(name)\n            print('Help on the %s formatter:' % cls.name)\n            print(dedent(cls.__doc__))\n        elif what == 'filter':\n            cls = find_filter_class(name)\n            print('Help on the %s filter:' % name)\n            print(dedent(cls.__doc__))\n        return 0\n    except (AttributeError, ValueError):\n        print('%s not found!' % what, file=sys.stderr)\n        return 1",
    "label": true
  },
  {
    "code": "def looks_like_numpy_member(member_name: str, node: NodeNG) -> bool:\n    if isinstance(node, Attribute) and node.attrname == member_name and isinstance(node.expr, Name) and _is_a_numpy_module(node.expr):\n        return True\n    if isinstance(node, Name) and node.name == member_name and node.root().name.startswith('numpy'):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def test_enummeta():\n    from http import HTTPStatus\n    import enum\n    assert dill.copy(HTTPStatus.OK) is HTTPStatus.OK\n    assert dill.copy(enum.EnumMeta) is enum.EnumMeta",
    "label": true
  },
  {
    "code": "def generic_tags(interpreter: Optional[str]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not interpreter:\n        interp_name = interpreter_name()\n        interp_version = interpreter_version(warn=warn)\n        interpreter = ''.join([interp_name, interp_version])\n    if abis is None:\n        abis = _generic_abi()\n    else:\n        abis = list(abis)\n    platforms = list(platforms or platform_tags())\n    if 'none' not in abis:\n        abis.append('none')\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)",
    "label": true
  },
  {
    "code": "def _has_ipv6(host: str) -> bool:\n    sock = None\n    has_ipv6 = False\n    if socket.has_ipv6:\n        try:\n            sock = socket.socket(socket.AF_INET6)\n            sock.bind((host, 0))\n            has_ipv6 = True\n        except Exception:\n            pass\n    if sock:\n        sock.close()\n    return has_ipv6",
    "label": true
  },
  {
    "code": "def _lookup_style(style):\n    if isinstance(style, str):\n        return get_style_by_name(style)\n    return style",
    "label": true
  },
  {
    "code": "def _get_allow_unicode_flag() -> int:\n    import doctest\n    return doctest.register_optionflag('ALLOW_UNICODE')",
    "label": true
  },
  {
    "code": "def _encode_invalid_chars(component: str | None, allowed_chars: typing.Container[str]) -> str | None:\n    if component is None:\n        return component\n    component = to_str(component)\n    component, percent_encodings = _PERCENT_RE.subn(lambda match: match.group(0).upper(), component)\n    uri_bytes = component.encode('utf-8', 'surrogatepass')\n    is_percent_encoded = percent_encodings == uri_bytes.count(b'%')\n    encoded_component = bytearray()\n    for i in range(0, len(uri_bytes)):\n        byte = uri_bytes[i:i + 1]\n        byte_ord = ord(byte)\n        if is_percent_encoded and byte == b'%' or (byte_ord < 128 and byte.decode() in allowed_chars):\n            encoded_component += byte\n            continue\n        encoded_component.extend(b'%' + hex(byte_ord)[2:].encode().zfill(2).upper())\n    return encoded_component.decode()",
    "label": true
  },
  {
    "code": "def _regexp_paths_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(re.compile(str(pathlib.PureWindowsPath(pattern)).replace('\\\\', '\\\\\\\\') + '|' + pathlib.PureWindowsPath(pattern).as_posix()))\n    return patterns",
    "label": true
  },
  {
    "code": "def _file_with_extension(directory, extension):\n    matching = (f for f in os.listdir(directory) if f.endswith(extension))\n    try:\n        file, = matching\n    except ValueError:\n        raise ValueError('No distribution was found. Ensure that `setup.py` is not empty and that it calls `setup()`.')\n    return file",
    "label": true
  },
  {
    "code": "def register_cleanup_lock_removal(lock_path: Path, register=atexit.register):\n    pid = os.getpid()\n\n    def cleanup_on_exit(lock_path: Path=lock_path, original_pid: int=pid) -> None:\n        current_pid = os.getpid()\n        if current_pid != original_pid:\n            return\n        try:\n            lock_path.unlink()\n        except OSError:\n            pass\n    return register(cleanup_on_exit)",
    "label": true
  },
  {
    "code": "def _check_typevar_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_pascal_case(name):\n        error_msgs.append(f'Type variable name \"{name}\" should be in PascalCase format. Type variable names should have the first letter of each word capitalized with no separation between each word.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def _verify_pre_check(filepath: AnyStr) -> bool:\n    try:\n        with tokenize.open(os.path.expanduser(filepath)) as f:\n            for tok_type, content, _, _, _ in tokenize.generate_tokens(f.readline):\n                if tok_type != tokenize.COMMENT:\n                    continue\n                match = OPTION_PO.search(content)\n                if match is not None:\n                    print('[ERROR] String \"pylint:\" found in comment. ' + 'No check run on file `{}.`\\n'.format(filepath))\n                    return False\n    except IndentationError as e:\n        print('[ERROR] python_ta could not check your code due to an ' + 'indentation error at line {}.'.format(e.lineno))\n        return False\n    except tokenize.TokenError as e:\n        print('[ERROR] python_ta could not check your code due to a ' + 'syntax error in your file.')\n        return False\n    except UnicodeDecodeError:\n        print('[ERROR] python_ta could not check your code due to an ' + 'invalid character. Please check the following lines in your file and all characters that are marked with a \ufffd.')\n        with open(os.path.expanduser(filepath), encoding='utf-8', errors='replace') as f:\n            for i, line in enumerate(f):\n                if '\ufffd' in line:\n                    print(f'  Line {i}: {line}', end='')\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _repr_tree_defs(data: _ImportTree, indent_str: str | None=None) -> str:\n    lines = []\n    nodes_items = data.items()\n    for i, (mod, (sub, files)) in enumerate(sorted(nodes_items, key=lambda x: x[0])):\n        files_list = '' if not files else f\"({','.join(sorted(files))})\"\n        if indent_str is None:\n            lines.append(f'{mod} {files_list}')\n            sub_indent_str = '  '\n        else:\n            lines.append(f'{indent_str}\\\\-{mod} {files_list}')\n            if i == len(nodes_items) - 1:\n                sub_indent_str = f'{indent_str}  '\n            else:\n                sub_indent_str = f'{indent_str}| '\n        if sub and isinstance(sub, dict):\n            lines.append(_repr_tree_defs(sub, sub_indent_str))\n    return '\\n'.join(lines)",
    "label": true
  },
  {
    "code": "def get_lexer_for_filename(_fn, code=None, **options):\n    res = find_lexer_class_for_filename(_fn, code)\n    if not res:\n        raise ClassNotFound('no lexer for filename %r found' % _fn)\n    return res(**options)",
    "label": true
  },
  {
    "code": "def _read_payload_from_msg(msg: Message) -> Optional[str]:\n    value = msg.get_payload().strip()\n    if value == 'UNKNOWN' or not value:\n        return None\n    return value",
    "label": true
  },
  {
    "code": "def _load_client_cert_chain(keychain, *paths):\n    certificates = []\n    identities = []\n    paths = (path for path in paths if path)\n    try:\n        for file_path in paths:\n            new_identities, new_certs = _load_items_from_file(keychain, file_path)\n            identities.extend(new_identities)\n            certificates.extend(new_certs)\n        if not identities:\n            new_identity = Security.SecIdentityRef()\n            status = Security.SecIdentityCreateWithCertificate(keychain, certificates[0], ctypes.byref(new_identity))\n            _assert_no_error(status)\n            identities.append(new_identity)\n            CoreFoundation.CFRelease(certificates.pop(0))\n        trust_chain = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        for item in itertools.chain(identities, certificates):\n            CoreFoundation.CFArrayAppendValue(trust_chain, item)\n        return trust_chain\n    finally:\n        for obj in itertools.chain(identities, certificates):\n            CoreFoundation.CFRelease(obj)",
    "label": true
  },
  {
    "code": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    tags = set()\n    interpreters, abis, platforms = tag.split('-')\n    for interpreter in interpreters.split('.'):\n        for abi in abis.split('.'):\n            for platform_ in platforms.split('.'):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)",
    "label": true
  },
  {
    "code": "def assignment(code: str, sort_type: str, extension: str, config: Config=DEFAULT_CONFIG) -> str:\n    if sort_type == 'assignments':\n        return assignments(code)\n    if sort_type not in type_mapping:\n        raise ValueError(f\"Trying to sort using an undefined sort_type. Defined sort types are {', '.join(type_mapping.keys())}.\")\n    variable_name, literal = code.split('=')\n    variable_name = variable_name.strip()\n    literal = literal.lstrip()\n    try:\n        value = ast.literal_eval(literal)\n    except Exception as error:\n        raise LiteralParsingFailure(code, error)\n    expected_type, sort_function = type_mapping[sort_type]\n    if type(value) != expected_type:\n        raise LiteralSortTypeMismatch(type(value), expected_type)\n    printer = ISortPrettyPrinter(config)\n    sorted_value_code = f'{variable_name} = {sort_function(value, printer)}'\n    if config.formatting_function:\n        sorted_value_code = config.formatting_function(sorted_value_code, extension, config).rstrip()\n    sorted_value_code += code[len(code.rstrip()):]\n    return sorted_value_code",
    "label": true
  },
  {
    "code": "def _error_line(error, obj, refimported):\n    import traceback\n    line = traceback.format_exc().splitlines()[-2].replace('[obj]', '[' + repr(obj) + ']')\n    return 'while testing (with refimported=%s):  %s' % (refimported, line.lstrip())",
    "label": true
  },
  {
    "code": "def find_path_to_project_root_from_repo_root(location: str, repo_root: str) -> Optional[str]:\n    orig_location = location\n    while not is_installable_dir(location):\n        last_location = location\n        location = os.path.dirname(location)\n        if location == last_location:\n            logger.warning('Could not find a Python project for directory %s (tried all parent directories)', orig_location)\n            return None\n    if os.path.samefile(repo_root, location):\n        return None\n    return os.path.relpath(location, repo_root)",
    "label": true
  },
  {
    "code": "def interpreter_version(*, warn: bool=False) -> str:\n    version = _get_config_var('py_version_nodot', warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
    "label": true
  },
  {
    "code": "def format_section(stream: TextIO, section: str, options: list[tuple[str, OptionDict, Any]], doc: str | None=None) -> None:\n    warnings.warn('format_section has been deprecated. It will be removed in pylint 3.0.', DeprecationWarning, stacklevel=2)\n    if doc:\n        print(_comment(doc), file=stream)\n    print(f'[{section}]', file=stream)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=DeprecationWarning)\n        _ini_format(stream, options)",
    "label": true
  },
  {
    "code": "def check_packages(dist, attr, value):\n    for pkgname in value:\n        if not re.match('\\\\w+(\\\\.\\\\w+)*', pkgname):\n            distutils.log.warn('WARNING: %r not a valid package name; please use only .-separated package names in setup.py', pkgname)",
    "label": true
  },
  {
    "code": "def _build_prompt(text: str, suffix: str, show_default: bool=False, default: t.Optional[t.Any]=None, show_choices: bool=True, type: t.Optional[ParamType]=None) -> str:\n    prompt = text\n    if type is not None and show_choices and isinstance(type, Choice):\n        prompt += f\" ({', '.join(map(str, type.choices))})\"\n    if default is not None and show_default:\n        prompt = f'{prompt} [{_format_default(default)}]'\n    return f'{prompt}{suffix}'",
    "label": true
  },
  {
    "code": "def _is_incomplete_option(ctx: Context, args: t.List[str], param: Parameter) -> bool:\n    if not isinstance(param, Option):\n        return False\n    if param.is_flag or param.count:\n        return False\n    last_option = None\n    for index, arg in enumerate(reversed(args)):\n        if index + 1 > param.nargs:\n            break\n        if _start_of_option(ctx, arg):\n            last_option = arg\n    return last_option is not None and last_option in param.opts",
    "label": true
  },
  {
    "code": "def show_vendor_versions() -> None:\n    logger.info('vendored library versions:')\n    vendor_txt_versions = create_vendor_txt_map()\n    with indent_log():\n        show_actual_vendor_versions(vendor_txt_versions)",
    "label": true
  },
  {
    "code": "def _get_pyvenv_cfg_lines() -> Optional[List[str]]:\n    pyvenv_cfg_file = os.path.join(sys.prefix, 'pyvenv.cfg')\n    try:\n        with open(pyvenv_cfg_file, encoding='utf-8') as f:\n            return f.read().splitlines()\n    except OSError:\n        return None",
    "label": true
  },
  {
    "code": "def _build_cmdtuple(path, cmdtuples):\n    for f in os.listdir(path):\n        real_f = os.path.join(path, f)\n        if os.path.isdir(real_f) and (not os.path.islink(real_f)):\n            _build_cmdtuple(real_f, cmdtuples)\n        else:\n            cmdtuples.append((os.remove, real_f))\n    cmdtuples.append((os.rmdir, path))",
    "label": true
  },
  {
    "code": "def _isrecursive(pattern):\n    if isinstance(pattern, bytes):\n        return pattern == b'**'\n    else:\n        return pattern == '**'",
    "label": true
  },
  {
    "code": "def quiet_subprocess_runner(cmd, cwd=None, extra_environ=None):\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    check_output(cmd, cwd=cwd, env=env, stderr=STDOUT)",
    "label": true
  },
  {
    "code": "def except_(*exceptions, replace=None, use=None):\n\n    def decorate(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except exceptions:\n                try:\n                    return eval(use)\n                except TypeError:\n                    return replace\n        return wrapper\n    return decorate",
    "label": true
  },
  {
    "code": "def get_module_constant(module, symbol, default=-1, paths=None):\n    try:\n        f, path, (suffix, mode, kind) = info = find_module(module, paths)\n    except ImportError:\n        return None\n    with maybe_close(f):\n        if kind == PY_COMPILED:\n            f.read(8)\n            code = marshal.load(f)\n        elif kind == PY_FROZEN:\n            code = _imp.get_frozen_object(module, paths)\n        elif kind == PY_SOURCE:\n            code = compile(f.read(), path, 'exec')\n        else:\n            imported = _imp.get_module(module, paths, info)\n            return getattr(imported, symbol, None)\n    return extract_constant(code, symbol, default)",
    "label": true
  },
  {
    "code": "def main() -> None:\n    app_name = 'MyApp'\n    app_author = 'MyCompany'\n    print(f'-- platformdirs {__version__} --')\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author, version='1.0')\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = PlatformDirs(app_name)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = PlatformDirs(app_name, appauthor=False)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')",
    "label": true
  },
  {
    "code": "def _make_text_stream(stream: t.BinaryIO, encoding: t.Optional[str], errors: t.Optional[str], force_readable: bool=False, force_writable: bool=False) -> t.TextIO:\n    if encoding is None:\n        encoding = get_best_encoding(stream)\n    if errors is None:\n        errors = 'replace'\n    return _NonClosingTextIOWrapper(stream, encoding, errors, line_buffering=True, force_readable=force_readable, force_writable=force_writable)",
    "label": true
  },
  {
    "code": "def get_package_data(name, version):\n    url = '%s/%s/package-%s.json' % (name[0].upper(), name, version)\n    url = urljoin(_external_data_base_url, url)\n    return _get_external_data(url)",
    "label": true
  },
  {
    "code": "def dump(object, **kwds):\n    import dill as pickle\n    import tempfile\n    kwds.setdefault('delete', True)\n    file = tempfile.NamedTemporaryFile(**kwds)\n    pickle.dump(object, file)\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def get_items_helper(self: BinarySearchTree) -> list[Any]:\n    if self.is_empty():\n        return []\n    else:\n        return get_items_helper(self._left) + [self._root] + get_items_helper(self._right)",
    "label": true
  },
  {
    "code": "def _get_versions(s):\n    result = []\n    for m in _VERSION_PATTERN.finditer(s):\n        result.append(NV(m.groups()[0]))\n    return set(result)",
    "label": true
  },
  {
    "code": "def _should_truncate_item(item: Item) -> bool:\n    verbose = item.config.option.verbose\n    return verbose < 2 and (not util.running_on_ci())",
    "label": true
  },
  {
    "code": "def transform_hits(hits: List[Dict[str, str]]) -> List['TransformedHit']:\n    packages: Dict[str, 'TransformedHit'] = OrderedDict()\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary']\n        version = hit['version']\n        if name not in packages.keys():\n            packages[name] = {'name': name, 'summary': summary, 'versions': [version]}\n        else:\n            packages[name]['versions'].append(version)\n            if version == highest_version(packages[name]['versions']):\n                packages[name]['summary'] = summary\n    return list(packages.values())",
    "label": true
  },
  {
    "code": "def powerset(iterable):\n    s = list(iterable)\n    return chain.from_iterable((combinations(s, r) for r in range(len(s) + 1)))",
    "label": true
  },
  {
    "code": "def join_continuation(lines):\n    lines = iter(lines)\n    for item in lines:\n        while item.endswith('\\\\'):\n            try:\n                item = item[:-2].strip() + next(lines)\n            except StopIteration:\n                return\n        yield item",
    "label": true
  },
  {
    "code": "def _load_schemes():\n    sysconfig_schemes = _load_sysconfig_schemes() or {}\n    return {scheme: {**INSTALL_SCHEMES.get(scheme, {}), **sysconfig_schemes.get(scheme, {})} for scheme in set(itertools.chain(INSTALL_SCHEMES, sysconfig_schemes))}",
    "label": true
  },
  {
    "code": "def validate_project_dynamic(pyproject: T) -> T:\n    project_table = pyproject.get('project', {})\n    dynamic = project_table.get('dynamic', [])\n    for field in dynamic:\n        if field in project_table:\n            msg = f'You cannot provide a value for `project.{field}` and '\n            msg += 'list it under `project.dynamic` at the same time'\n            name = f'data.project.{field}'\n            value = {field: project_table[field], '...': ' # ...', 'dynamic': dynamic}\n            raise RedefiningStaticFieldAsDynamic(msg, value, name, rule='PEP 621')\n    return pyproject",
    "label": true
  },
  {
    "code": "def validate(data, custom_formats={}, name_prefix=None):\n    validate_https___packaging_python_org_en_latest_specifications_declaring_build_dependencies(data, custom_formats, (name_prefix or 'data') + '')\n    return data",
    "label": true
  },
  {
    "code": "def run_pyreverse(argv: Sequence[str] | None=None) -> NoReturn:\n    from pylint.pyreverse.main import Run as PyreverseRun\n    PyreverseRun(argv or sys.argv[1:])",
    "label": true
  },
  {
    "code": "def infer_bool(node, context: InferenceContext | None=None):\n    if len(node.args) > 1:\n        raise UseInferenceDefault\n    if not node.args:\n        return nodes.Const(False)\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if isinstance(inferred, util.UninferableBase):\n        return util.Uninferable\n    bool_value = inferred.bool_value(context=context)\n    if isinstance(bool_value, util.UninferableBase):\n        return util.Uninferable\n    return nodes.Const(bool_value)",
    "label": true
  },
  {
    "code": "def function_name(func: Callable[..., Any]) -> str:\n    module = getattr(func, '__module__', '')\n    qualname = module + '.' if module not in ('builtins', '') else ''\n    return qualname + getattr(func, '__qualname__', repr(func))",
    "label": true
  },
  {
    "code": "def skip_comments_and_array_ws(src: str, pos: Pos) -> Pos:\n    while True:\n        pos_before_skip = pos\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        pos = skip_comment(src, pos)\n        if pos == pos_before_skip:\n            return pos",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def normalize_and_reduce_paths(paths):\n    reduced_paths = []\n    for p in paths:\n        np = os.path.normpath(p)\n        if np not in reduced_paths:\n            reduced_paths.append(np)\n    return reduced_paths",
    "label": true
  },
  {
    "code": "def _get_checker() -> 'doctest.OutputChecker':\n    global CHECKER_CLASS\n    if CHECKER_CLASS is None:\n        CHECKER_CLASS = _init_checker_class()\n    return CHECKER_CLASS()",
    "label": true
  },
  {
    "code": "def get_distribution(dist):\n    if isinstance(dist, str):\n        dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement):\n        dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError('Expected string, Requirement, or Distribution', dist)\n    return dist",
    "label": true
  },
  {
    "code": "def inspect(obj: Any, *, console: Optional['Console']=None, title: Optional[str]=None, help: bool=False, methods: bool=False, docs: bool=True, private: bool=False, dunder: bool=False, sort: bool=True, all: bool=False, value: bool=True) -> None:\n    _console = console or get_console()\n    from pip._vendor.rich._inspect import Inspect\n    is_inspect = obj is inspect\n    _inspect = Inspect(obj, title=title, help=is_inspect or help, methods=is_inspect or methods, docs=is_inspect or docs, private=private, dunder=dunder, sort=sort, all=all, value=value)\n    _console.print(_inspect)",
    "label": true
  },
  {
    "code": "def set_logging_handler(name: str='charset_normalizer', level: int=logging.INFO, format_string: str='%(asctime)s | %(levelname)s | %(message)s') -> None:\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(format_string))\n    logger.addHandler(handler)",
    "label": true
  },
  {
    "code": "def unpackb(packed, **kwargs):\n    unpacker = Unpacker(None, max_buffer_size=len(packed), **kwargs)\n    unpacker.feed(packed)\n    try:\n        ret = unpacker._unpack()\n    except OutOfData:\n        raise ValueError('Unpack failed: incomplete input')\n    except RecursionError as e:\n        if _is_recursionerror(e):\n            raise StackError\n        raise\n    if unpacker._got_extradata():\n        raise ExtraData(ret, unpacker._get_extradata())\n    return ret",
    "label": true
  },
  {
    "code": "def pep517_backend_reference(value: str) -> bool:\n    module, _, obj = value.partition(':')\n    identifiers = (i.strip() for i in _chain(module.split('.'), obj.split('.')))\n    return all((python_identifier(i) for i in identifiers if i))",
    "label": true
  },
  {
    "code": "def libc_ver() -> Tuple[str, str]:\n    glibc_version = glibc_version_string()\n    if glibc_version is None:\n        return ('', '')\n    else:\n        return ('glibc', glibc_version)",
    "label": true
  },
  {
    "code": "def binop_error_message(node: nodes.BinOp) -> str:\n    op_name = BINOP_TO_ENGLISH[node.op]\n    left_type = _get_name(node.left.inf_type.getValue())\n    right_type = _get_name(node.right.inf_type.getValue())\n    hint = binary_op_hints(node.op, [left_type, right_type]) or ''\n    return f'You cannot {op_name} {_correct_article(left_type)}, {node.left.as_string()}, and {_correct_article(right_type)}, {node.right.as_string()}. {hint}'",
    "label": true
  },
  {
    "code": "def isatty(stream: t.IO[t.Any]) -> bool:\n    try:\n        return stream.isatty()\n    except Exception:\n        return False",
    "label": true
  },
  {
    "code": "def _infer_old_style_string_formatting(instance: nodes.Const, other: nodes.NodeNG, context: InferenceContext) -> tuple[util.UninferableBase | nodes.Const]:\n    if isinstance(other, nodes.Tuple):\n        if util.Uninferable in other.elts:\n            return (util.Uninferable,)\n        inferred_positional = [helpers.safe_infer(i, context) for i in other.elts]\n        if all((isinstance(i, nodes.Const) for i in inferred_positional)):\n            values = tuple((i.value for i in inferred_positional))\n        else:\n            values = None\n    elif isinstance(other, nodes.Dict):\n        values: dict[Any, Any] = {}\n        for pair in other.items:\n            key = helpers.safe_infer(pair[0], context)\n            if not isinstance(key, nodes.Const):\n                return (util.Uninferable,)\n            value = helpers.safe_infer(pair[1], context)\n            if not isinstance(value, nodes.Const):\n                return (util.Uninferable,)\n            values[key.value] = value.value\n    elif isinstance(other, nodes.Const):\n        values = other.value\n    else:\n        return (util.Uninferable,)\n    try:\n        return (nodes.const_factory(instance.value % values),)\n    except (TypeError, KeyError, ValueError):\n        return (util.Uninferable,)",
    "label": true
  },
  {
    "code": "def _is_part_of_with_items(node: nodes.Call) -> bool:\n    frame = node.frame(future=True)\n    current = node\n    while current != frame:\n        if isinstance(current, nodes.With):\n            items_start = current.items[0][0].lineno\n            items_end = current.items[-1][0].tolineno\n            return items_start <= node.lineno <= items_end\n        current = current.parent\n    return False",
    "label": true
  },
  {
    "code": "def skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:\n    try:\n        while src[pos] in chars:\n            pos += 1\n    except IndexError:\n        pass\n    return pos",
    "label": true
  },
  {
    "code": "def distributions_from_metadata(path):\n    root = os.path.dirname(path)\n    if os.path.isdir(path):\n        if len(os.listdir(path)) == 0:\n            return\n        metadata = PathMetadata(root, path)\n    else:\n        metadata = FileMetadata(path)\n    entry = os.path.basename(path)\n    yield Distribution.from_location(root, entry, metadata, precedence=DEVELOP_DIST)",
    "label": true
  },
  {
    "code": "def is_line_commented(line: bytes) -> bool:\n    comment_idx = line.find(b'#')\n    if comment_idx == -1:\n        return False\n    if comment_part_of_string(line, comment_idx):\n        return is_line_commented(line[:comment_idx] + line[comment_idx + 1:])\n    return True",
    "label": true
  },
  {
    "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = ('', '')\n    return auth",
    "label": true
  },
  {
    "code": "def _compare_eq_iterable(left: Iterable[Any], right: Iterable[Any], verbose: int=0) -> List[str]:\n    if verbose <= 0 and (not running_on_ci()):\n        return ['Use -v to get more diff']\n    import difflib\n    left_formatting = pprint.pformat(left).splitlines()\n    right_formatting = pprint.pformat(right).splitlines()\n    lines_left = len(left_formatting)\n    lines_right = len(right_formatting)\n    if lines_left != lines_right:\n        left_formatting = _pformat_dispatch(left).splitlines()\n        right_formatting = _pformat_dispatch(right).splitlines()\n    if lines_left > 1 or lines_right > 1:\n        _surrounding_parens_on_own_lines(left_formatting)\n        _surrounding_parens_on_own_lines(right_formatting)\n    explanation = ['Full diff:']\n    explanation.extend((line.rstrip() for line in difflib.ndiff(right_formatting, left_formatting)))\n    return explanation",
    "label": true
  },
  {
    "code": "def register_encoder(encoder: E) -> E:\n    CUSTOM_ENCODERS.append(encoder)\n    return encoder",
    "label": true
  },
  {
    "code": "def parse_key_part(src: str, pos: Pos) -> tuple[Pos, str]:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char in BARE_KEY_CHARS:\n        start_pos = pos\n        pos = skip_chars(src, pos, BARE_KEY_CHARS)\n        return (pos, src[start_pos:pos])\n    if char == \"'\":\n        return parse_literal_str(src, pos)\n    if char == '\"':\n        return parse_one_line_basic_str(src, pos)\n    raise suffixed_err(src, pos, 'Invalid initial character for a key part')",
    "label": true
  },
  {
    "code": "def unwrap(s):\n    paragraphs = re.split('\\\\n\\\\n+', s)\n    cleaned = (para.replace('\\n', ' ') for para in paragraphs)\n    return '\\n'.join(cleaned)",
    "label": true
  },
  {
    "code": "def random_combination_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    indices = sorted((randrange(n) for i in range(r)))\n    return tuple((pool[i] for i in indices))",
    "label": true
  },
  {
    "code": "def remove_move(name):\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError('no such move, %r' % (name,))",
    "label": true
  },
  {
    "code": "def _check_multicommand(base_command: 'MultiCommand', cmd_name: str, cmd: 'Command', register: bool=False) -> None:\n    if not base_command.chain or not isinstance(cmd, MultiCommand):\n        return\n    if register:\n        hint = 'It is not possible to add multi commands as children to another multi command that is in chain mode.'\n    else:\n        hint = 'Found a multi command as subcommand to a multi command that is in chain mode. This is not supported.'\n    raise RuntimeError(f'{hint}. Command {base_command.name!r} is set to chain and {cmd_name!r} was added as a subcommand but it in itself is a multi command. ({cmd_name!r} is a {type(cmd).__name__} within a chained {type(base_command).__name__} named {base_command.name!r}).')",
    "label": true
  },
  {
    "code": "def confirmation_option(*param_decls: str, **kwargs: t.Any) -> t.Callable[[FC], FC]:\n\n    def callback(ctx: Context, param: Parameter, value: bool) -> None:\n        if not value:\n            ctx.abort()\n    if not param_decls:\n        param_decls = ('--yes',)\n    kwargs.setdefault('is_flag', True)\n    kwargs.setdefault('callback', callback)\n    kwargs.setdefault('expose_value', False)\n    kwargs.setdefault('prompt', 'Do you want to continue?')\n    kwargs.setdefault('help', 'Confirm the action without prompting.')\n    return option(*param_decls, **kwargs)",
    "label": true
  },
  {
    "code": "def progressbar(iterable: t.Optional[t.Iterable[V]]=None, length: t.Optional[int]=None, label: t.Optional[str]=None, show_eta: bool=True, show_percent: t.Optional[bool]=None, show_pos: bool=False, item_show_func: t.Optional[t.Callable[[t.Optional[V]], t.Optional[str]]]=None, fill_char: str='#', empty_char: str='-', bar_template: str='%(label)s  [%(bar)s]  %(info)s', info_sep: str='  ', width: int=36, file: t.Optional[t.TextIO]=None, color: t.Optional[bool]=None, update_min_steps: int=1) -> 'ProgressBar[V]':\n    from ._termui_impl import ProgressBar\n    color = resolve_color_default(color)\n    return ProgressBar(iterable=iterable, length=length, show_eta=show_eta, show_percent=show_percent, show_pos=show_pos, item_show_func=item_show_func, fill_char=fill_char, empty_char=empty_char, bar_template=bar_template, info_sep=info_sep, file=file, label=label, width=width, color=color, update_min_steps=update_min_steps)",
    "label": true
  },
  {
    "code": "def _http_get_download(session: PipSession, link: Link) -> Response:\n    target_url = link.url.split('#', 1)[0]\n    resp = session.get(target_url, headers=HEADERS, stream=True)\n    raise_for_status(resp)\n    return resp",
    "label": true
  },
  {
    "code": "def main() -> None:\n    app_name = 'MyApp'\n    app_author = 'MyCompany'\n    print(f'-- platformdirs {__version__} --')\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author, version='1.0')\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = PlatformDirs(app_name, app_author)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = PlatformDirs(app_name)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = PlatformDirs(app_name, appauthor=False)\n    for prop in PROPS:\n        print(f'{prop}: {getattr(dirs, prop)}')",
    "label": true
  },
  {
    "code": "def _cf_string_to_unicode(value: CFString) -> str | None:\n    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))\n    string = CoreFoundation.CFStringGetCStringPtr(value_as_void_p, CFConst.kCFStringEncodingUTF8)\n    if string is None:\n        buffer = ctypes.create_string_buffer(1024)\n        result = CoreFoundation.CFStringGetCString(value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8)\n        if not result:\n            raise OSError('Error copying C string from CFStringRef')\n        string = buffer.value\n    if string is not None:\n        string = string.decode('utf-8')\n    return string",
    "label": true
  },
  {
    "code": "def parse_wheel(wheel_zip: ZipFile, name: str) -> Tuple[str, Message]:\n    try:\n        info_dir = wheel_dist_info_dir(wheel_zip, name)\n        metadata = wheel_metadata(wheel_zip, info_dir)\n        version = wheel_version(metadata)\n    except UnsupportedWheel as e:\n        raise UnsupportedWheel('{} has an invalid wheel, {}'.format(name, str(e)))\n    check_compatibility(version, name)\n    return (info_dir, metadata)",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], Tuple[Node, ...], str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def mark_ends(iterable):\n    it = iter(iterable)\n    try:\n        b = next(it)\n    except StopIteration:\n        return\n    try:\n        for i in count():\n            a = b\n            b = next(it)\n            yield (i == 0, False, a)\n    except StopIteration:\n        yield (i == 0, True, a)",
    "label": true
  },
  {
    "code": "def _compare_eq_set(left: AbstractSet[Any], right: AbstractSet[Any], verbose: int=0) -> List[str]:\n    explanation = []\n    diff_left = left - right\n    diff_right = right - left\n    if diff_left:\n        explanation.append('Extra items in the left set:')\n        for item in diff_left:\n            explanation.append(saferepr(item))\n    if diff_right:\n        explanation.append('Extra items in the right set:')\n        for item in diff_right:\n            explanation.append(saferepr(item))\n    return explanation",
    "label": true
  },
  {
    "code": "def create_package_set_from_installed() -> Tuple[PackageSet, bool]:\n    package_set = {}\n    problems = False\n    env = get_default_environment()\n    for dist in env.iter_installed_distributions(local_only=False, skip=()):\n        name = dist.canonical_name\n        try:\n            dependencies = list(dist.iter_dependencies())\n            package_set[name] = PackageDetails(dist.version, dependencies)\n        except (OSError, ValueError) as e:\n            logger.warning('Error parsing requirements for %s: %s', name, e)\n            problems = True\n    return (package_set, problems)",
    "label": true
  },
  {
    "code": "def _infer_name_module(node: nodes.Import, name: str) -> Generator[InferenceResult, None, None]:\n    context = astroid.context.InferenceContext()\n    context.lookupname = name\n    return node.infer(context, asname=False)",
    "label": true
  },
  {
    "code": "def _is_a_numpy_module(node: Name) -> bool:\n    module_nickname = node.name\n    potential_import_target = [x for x in node.lookup(module_nickname)[1] if isinstance(x, Import)]\n    return any((('numpy', module_nickname) in target.names or ('numpy', None) in target.names for target in potential_import_target))",
    "label": true
  },
  {
    "code": "def _infer_prefix() -> str:\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API('prefix')\n    if _should_use_osx_framework_prefix():\n        return 'osx_framework_library'\n    implementation_suffixed = f'{sys.implementation.name}_{os.name}'\n    if implementation_suffixed in _AVAILABLE_SCHEMES:\n        return implementation_suffixed\n    if sys.implementation.name in _AVAILABLE_SCHEMES:\n        return sys.implementation.name\n    suffixed = f'{os.name}_prefix'\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if os.name in _AVAILABLE_SCHEMES:\n        return os.name\n    return 'posix_prefix'",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable: Iterable[_T], key: Optional[Callable[[_T], _U]]=None) -> Iterator[_T]:\n    seen: Set[Union[_T, _U]] = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def _cf_string_to_unicode(value):\n    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))\n    string = CoreFoundation.CFStringGetCStringPtr(value_as_void_p, CFConst.kCFStringEncodingUTF8)\n    if string is None:\n        buffer = ctypes.create_string_buffer(1024)\n        result = CoreFoundation.CFStringGetCString(value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8)\n        if not result:\n            raise OSError('Error copying C string from CFStringRef')\n        string = buffer.value\n    if string is not None:\n        string = string.decode('utf-8')\n    return string",
    "label": true
  },
  {
    "code": "def check_nsp(dist, attr, value):\n    ns_packages = value\n    assert_string_list(dist, attr, ns_packages)\n    for nsp in ns_packages:\n        if not dist.has_contents_for(nsp):\n            raise DistutilsSetupError('Distribution contains no modules or packages for ' + 'namespace package %r' % nsp)\n        parent, sep, child = nsp.rpartition('.')\n        if parent and parent not in ns_packages:\n            distutils.log.warn('WARNING: %r is declared as a package namespace, but %r is not: please correct this in setup.py', nsp, parent)\n        SetuptoolsDeprecationWarning.emit('The namespace_packages parameter is deprecated.', 'Please replace its usage with implicit namespaces (PEP 420).', see_docs='references/keywords.html#keyword-namespace-packages')",
    "label": true
  },
  {
    "code": "def scandir(path: Union[str, 'os.PathLike[str]']) -> List['os.DirEntry[str]']:\n    entries = []\n    with os.scandir(path) as s:\n        for entry in s:\n            try:\n                entry.is_file()\n            except OSError as err:\n                if _ignore_error(err):\n                    continue\n                raise\n            entries.append(entry)\n    entries.sort(key=lambda entry: entry.name)\n    return entries",
    "label": true
  },
  {
    "code": "def main(args: Optional[List[str]]=None) -> int:\n    if args is None:\n        args = sys.argv[1:]\n    warnings.filterwarnings(action='ignore', category=DeprecationWarning, module='.*pkg_resources')\n    deprecation.install_warning_logger()\n    autocomplete()\n    try:\n        cmd_name, cmd_args = parse_command(args)\n    except PipError as exc:\n        sys.stderr.write(f'ERROR: {exc}')\n        sys.stderr.write(os.linesep)\n        sys.exit(1)\n    try:\n        locale.setlocale(locale.LC_ALL, '')\n    except locale.Error as e:\n        logger.debug('Ignoring error %s when setting locale', e)\n    command = create_command(cmd_name, isolated='--isolated' in cmd_args)\n    return command.main(cmd_args)",
    "label": true
  },
  {
    "code": "def select_wait_for_socket(sock: socket.socket, read: bool=False, write: bool=False, timeout: float | None=None) -> bool:\n    if not read and (not write):\n        raise RuntimeError('must specify at least one of read=True, write=True')\n    rcheck = []\n    wcheck = []\n    if read:\n        rcheck.append(sock)\n    if write:\n        wcheck.append(sock)\n    fn = partial(select.select, rcheck, wcheck, wcheck)\n    rready, wready, xready = fn(timeout)\n    return bool(rready or wready or xready)",
    "label": true
  },
  {
    "code": "def product_star(integer_queue: Queue) -> int:\n    prod = 1\n    for i in range(len(integer_queue)):\n        item = integer_queue.dequeue()\n        prod *= item\n        integer_queue.enqueue(item)\n    return prod",
    "label": true
  },
  {
    "code": "def filter_except(validator, iterable, *exceptions):\n    for item in iterable:\n        try:\n            validator(item)\n        except exceptions:\n            pass\n        else:\n            yield item",
    "label": true
  },
  {
    "code": "def _infer_prefix() -> str:\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API('prefix')\n    if _should_use_osx_framework_prefix():\n        return 'osx_framework_library'\n    implementation_suffixed = f'{sys.implementation.name}_{os.name}'\n    if implementation_suffixed in _AVAILABLE_SCHEMES:\n        return implementation_suffixed\n    if sys.implementation.name in _AVAILABLE_SCHEMES:\n        return sys.implementation.name\n    suffixed = f'{os.name}_prefix'\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if os.name in _AVAILABLE_SCHEMES:\n        return os.name\n    return 'posix_prefix'",
    "label": true
  },
  {
    "code": "def dumpIO(object, **kwds):\n    import dill as pickle\n    from io import BytesIO as StringIO\n    file = StringIO()\n    pickle.dump(object, file)\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def extract_cookies_to_jar(jar, request, response):\n    if not (hasattr(response, '_original_response') and response._original_response):\n        return\n    req = MockRequest(request)\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)",
    "label": true
  },
  {
    "code": "def morsel_to_cookie(morsel):\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(time.strptime(morsel['expires'], time_template))\n    return create_cookie(comment=morsel['comment'], comment_url=bool(morsel['comment']), discard=False, domain=morsel['domain'], expires=expires, name=morsel.key, path=morsel['path'], port=None, rest={'HttpOnly': morsel['httponly']}, rfc2109=False, secure=bool(morsel['secure']), value=morsel.value, version=morsel['version'] or 0)",
    "label": true
  },
  {
    "code": "def get_requires_for_build_sdist(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_sdist\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)",
    "label": true
  },
  {
    "code": "def _parse(markup: str) -> Iterable[Tuple[int, Optional[str], Optional[Tag]]]:\n    position = 0\n    _divmod = divmod\n    _Tag = Tag\n    for match in RE_TAGS.finditer(markup):\n        full_text, escapes, tag_text = match.groups()\n        start, end = match.span()\n        if start > position:\n            yield (start, markup[position:start], None)\n        if escapes:\n            backslashes, escaped = _divmod(len(escapes), 2)\n            if backslashes:\n                yield (start, '\\\\' * backslashes, None)\n                start += backslashes * 2\n            if escaped:\n                yield (start, full_text[len(escapes):], None)\n                position = end\n                continue\n        text, equals, parameters = tag_text.partition('=')\n        yield (start, None, _Tag(text, parameters if equals else None))\n        position = end\n    if position < len(markup):\n        yield (position, markup[position:], None)",
    "label": true
  },
  {
    "code": "def test_dictproxy():\n    from dill._dill import DictProxyType\n    try:\n        m = DictProxyType({'foo': 'bar'})\n    except Exception:\n        m = type.__dict__\n    mp = dill.copy(m)\n    assert mp.items() == m.items()",
    "label": true
  },
  {
    "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def is_decorated_with_st_composite(node) -> bool:\n    if node.decorators and node.args.args and (node.args.args[0].name == 'draw'):\n        for decorator_attribute in node.decorators.nodes:\n            if decorator_attribute.as_string() in COMPOSITE_NAMES:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def inspect(obj: Any, *, console: Optional['Console']=None, title: Optional[str]=None, help: bool=False, methods: bool=False, docs: bool=True, private: bool=False, dunder: bool=False, sort: bool=True, all: bool=False, value: bool=True) -> None:\n    _console = console or get_console()\n    from pip._vendor.rich._inspect import Inspect\n    is_inspect = obj is inspect\n    _inspect = Inspect(obj, title=title, help=is_inspect or help, methods=is_inspect or methods, docs=is_inspect or docs, private=private, dunder=dunder, sort=sort, all=all, value=value)\n    _console.print(_inspect)",
    "label": true
  },
  {
    "code": "def compatible_platforms(provided, required):\n    if provided is None or required is None or provided == required:\n        return True\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n        if not provMac:\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = '%s.%s' % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= '10.3' or (dversion == 8 and macosversion >= '10.4'):\n                    return True\n            return False\n        if provMac.group(1) != reqMac.group(1) or provMac.group(3) != reqMac.group(3):\n            return False\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def get_cache_base(suffix=None):\n    if suffix is None:\n        suffix = '.distlib'\n    if os.name == 'nt' and 'LOCALAPPDATA' in os.environ:\n        result = os.path.expandvars('$localappdata')\n    else:\n        result = os.path.expanduser('~')\n    if os.path.isdir(result):\n        usable = os.access(result, os.W_OK)\n        if not usable:\n            logger.warning('Directory exists but is not writable: %s', result)\n    else:\n        try:\n            os.makedirs(result)\n            usable = True\n        except OSError:\n            logger.warning('Unable to create %s', result, exc_info=True)\n            usable = False\n    if not usable:\n        result = tempfile.mkdtemp()\n        logger.warning('Default location unusable, using %s', result)\n    return os.path.join(result, suffix)",
    "label": true
  },
  {
    "code": "def test_dict_contents():\n    c = type.__dict__\n    for i, j in c.items():\n        ok = dill.pickles(j)\n        if verbose:\n            print('%s: %s, %s' % (ok, type(j), j))\n        assert ok\n    if verbose:\n        print('')",
    "label": true
  },
  {
    "code": "def _ini_format(stream: TextIO, options: list[tuple[str, OptionDict, Any]]) -> None:\n    warnings.warn('_ini_format has been deprecated. It will be removed in pylint 3.0.', DeprecationWarning, stacklevel=2)\n    for optname, optdict, value in options:\n        if 'kwargs' in optdict:\n            assert isinstance(optdict['kwargs'], dict)\n            if 'new_names' in optdict['kwargs']:\n                continue\n        value = _format_option_value(optdict, value)\n        help_opt = optdict.get('help')\n        if help_opt:\n            assert isinstance(help_opt, str)\n            help_opt = normalize_text(help_opt, indent='# ')\n            print(file=stream)\n            print(help_opt, file=stream)\n        else:\n            print(file=stream)\n        if value in {'None', 'False'}:\n            print(f'#{optname}=', file=stream)\n        else:\n            value = str(value).strip()\n            if re.match('^([\\\\w-]+,)+[\\\\w-]+$', str(value)):\n                separator = '\\n ' + ' ' * len(optname)\n                value = separator.join((x + ',' for x in str(value).split(',')))\n                value = value[:-1]\n            print(f'{optname}={value}', file=stream)",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], MarkerAtom, str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def test_module_is_none():\n    assert obj.__module__ is None\n    assert dill.copy(obj)(3) == obj(3)",
    "label": true
  },
  {
    "code": "def _vertical_grid_common(need_trailing_char: bool, **interface: Any) -> str:\n    if not interface['imports']:\n        return ''\n    interface['statement'] += isort.comments.add_to_line(interface['comments'], '(', removed=interface['remove_comments'], comment_prefix=interface['comment_prefix']) + interface['line_separator'] + interface['indent'] + interface['imports'].pop(0)\n    while interface['imports']:\n        next_import = interface['imports'].pop(0)\n        next_statement = f\"{interface['statement']}, {next_import}\"\n        current_line_length = len(next_statement.split(interface['line_separator'])[-1])\n        if interface['imports'] or interface['include_trailing_comma']:\n            current_line_length += 1\n        if not interface['imports'] and need_trailing_char:\n            current_line_length += 1\n        if current_line_length > interface['line_length']:\n            next_statement = f\"{interface['statement']},{interface['line_separator']}{interface['indent']}{next_import}\"\n        interface['statement'] = next_statement\n    if interface['include_trailing_comma']:\n        interface['statement'] += ','\n    return str(interface['statement'])",
    "label": true
  },
  {
    "code": "def make_install_req_from_link(link: Link, template: InstallRequirement) -> InstallRequirement:\n    assert not template.editable, 'template is editable'\n    if template.req:\n        line = str(template.req)\n    else:\n        line = link.url\n    ireq = install_req_from_line(line, user_supplied=template.user_supplied, comes_from=template.comes_from, use_pep517=template.use_pep517, isolated=template.isolated, constraint=template.constraint, global_options=template.global_options, hash_options=template.hash_options, config_settings=template.config_settings)\n    ireq.original_link = template.original_link\n    ireq.link = link\n    ireq.extras = template.extras\n    return ireq",
    "label": true
  },
  {
    "code": "def nested_expr(opener: Union[str, ParserElement]='(', closer: Union[str, ParserElement]=')', content: typing.Optional[ParserElement]=None, ignore_expr: ParserElement=quoted_string(), *, ignoreExpr: ParserElement=quoted_string()) -> ParserElement:\n    if ignoreExpr != ignore_expr:\n        ignoreExpr = ignore_expr if ignoreExpr == quoted_string() else ignoreExpr\n    if opener == closer:\n        raise ValueError('opening and closing strings cannot be the same')\n    if content is None:\n        if isinstance(opener, str_type) and isinstance(closer, str_type):\n            opener = typing.cast(str, opener)\n            closer = typing.cast(str, closer)\n            if len(opener) == 1 and len(closer) == 1:\n                if ignoreExpr is not None:\n                    content = Combine(OneOrMore(~ignoreExpr + CharsNotIn(opener + closer + ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n                else:\n                    content = empty.copy() + CharsNotIn(opener + closer + ParserElement.DEFAULT_WHITE_CHARS).set_parse_action(lambda t: t[0].strip())\n            elif ignoreExpr is not None:\n                content = Combine(OneOrMore(~ignoreExpr + ~Literal(opener) + ~Literal(closer) + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n            else:\n                content = Combine(OneOrMore(~Literal(opener) + ~Literal(closer) + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n        else:\n            raise ValueError('opening and closing arguments must be strings if no content expression is given')\n    ret = Forward()\n    if ignoreExpr is not None:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer))\n    else:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ret | content) + Suppress(closer))\n    ret.set_name('nested %s%s expression' % (opener, closer))\n    return ret",
    "label": true
  },
  {
    "code": "def await_(obj):\n    obj_type = type(obj)\n    if obj_type is CoroutineType or (obj_type is GeneratorType and bool(obj.gi_code.co_flags & CO_ITERABLE_COROUTINE)) or isinstance(obj, Awaitable):\n        return do_await(obj).__await__()\n    else:\n        return do_yield_from(obj)",
    "label": true
  },
  {
    "code": "def to_railroad(element: pyparsing.ParserElement, diagram_kwargs: typing.Optional[dict]=None, vertical: int=3, show_results_names: bool=False, show_groups: bool=False) -> List[NamedDiagram]:\n    lookup = ConverterState(diagram_kwargs=diagram_kwargs or {})\n    _to_diagram_element(element, lookup=lookup, parent=None, vertical=vertical, show_results_names=show_results_names, show_groups=show_groups)\n    root_id = id(element)\n    if root_id in lookup:\n        if not element.customName:\n            lookup[root_id].name = ''\n        lookup[root_id].mark_for_extraction(root_id, lookup, force=True)\n    diags = list(lookup.diagrams.values())\n    if len(diags) > 1:\n        seen = set()\n        deduped_diags = []\n        for d in diags:\n            if d.name == '...':\n                continue\n            if d.name is not None and d.name not in seen:\n                seen.add(d.name)\n                deduped_diags.append(d)\n        resolved = [resolve_partial(partial) for partial in deduped_diags]\n    else:\n        resolved = [resolve_partial(partial) for partial in diags]\n    return sorted(resolved, key=lambda diag: diag.index)",
    "label": true
  },
  {
    "code": "def test_frame_related():\n    g = _g(1)\n    f = g.gi_frame\n    e, t = _f()\n    _is = lambda ok: ok\n    ok = dill.pickles(f)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(f), f))\n    assert not ok\n    ok = dill.pickles(g)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(g), g))\n    assert _is(not ok)\n    ok = dill.pickles(t)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(t), t))\n    assert not ok\n    ok = dill.pickles(e)\n    if verbose:\n        print('%s: %s, %s' % (ok, type(e), e))\n    assert ok\n    if verbose:\n        print('')",
    "label": true
  },
  {
    "code": "def get_configuration_files() -> Dict[Kind, List[str]]:\n    global_config_files = [os.path.join(path, CONFIG_BASENAME) for path in appdirs.site_config_dirs('pip')]\n    site_config_file = os.path.join(sys.prefix, CONFIG_BASENAME)\n    legacy_config_file = os.path.join(os.path.expanduser('~'), 'pip' if WINDOWS else '.pip', CONFIG_BASENAME)\n    new_config_file = os.path.join(appdirs.user_config_dir('pip'), CONFIG_BASENAME)\n    return {kinds.GLOBAL: global_config_files, kinds.SITE: [site_config_file], kinds.USER: [legacy_config_file, new_config_file]}",
    "label": true
  },
  {
    "code": "def _version_from_file(lines):\n\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None",
    "label": true
  },
  {
    "code": "def is_comprehension(node: nodes.NodeNG) -> bool:\n    comprehensions = (nodes.ListComp, nodes.SetComp, nodes.DictComp, nodes.GeneratorExp)\n    return isinstance(node, comprehensions)",
    "label": true
  },
  {
    "code": "def ensure_relative(path):\n    drive, path = os.path.splitdrive(path)\n    if path[0:1] == os.sep:\n        path = drive + path[1:]\n    return path",
    "label": true
  },
  {
    "code": "def find_bridges_in_radius(bridges: list[list], lat: float, lon: float, radius: int, exclusions: list[int]) -> list[int]:\n    valid_bridges = []\n    for bridge in bridges:\n        if bridge[COLUMN_ID] not in exclusions:\n            bridge_lon = bridge[COLUMN_LON]\n            bridge_lat = bridge[COLUMN_LAT]\n            if calculate_distance(bridge_lat, bridge_lon, lat, lon) <= radius:\n                valid_bridges.append(bridge[COLUMN_ID])\n    return valid_bridges",
    "label": true
  },
  {
    "code": "def inherit_from_std_ex(node: nodes.NodeNG | astroid.Instance) -> bool:\n    ancestors = node.ancestors() if hasattr(node, 'ancestors') else []\n    return any((ancestor.name in {'Exception', 'BaseException'} and ancestor.root().name == EXCEPTIONS_MODULE for ancestor in itertools.chain([node], ancestors)))",
    "label": true
  },
  {
    "code": "def _apply_diagram_item_enhancements(fn):\n\n    def _inner(element: pyparsing.ParserElement, parent: typing.Optional[EditablePartial], lookup: ConverterState=None, vertical: int=None, index: int=0, name_hint: str=None, show_results_names: bool=False, show_groups: bool=False) -> typing.Optional[EditablePartial]:\n        ret = fn(element, parent, lookup, vertical, index, name_hint, show_results_names, show_groups)\n        if show_results_names and ret is not None:\n            element_results_name = element.resultsName\n            if element_results_name:\n                element_results_name += '' if element.modalResults else '*'\n                ret = EditablePartial.from_call(railroad.Group, item=ret, label=element_results_name)\n        return ret\n    return _inner",
    "label": true
  },
  {
    "code": "def _normalize_extra_values(results: Any) -> Any:\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == 'extra':\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == 'extra':\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = (lhs, op, rhs)\n    return results",
    "label": true
  },
  {
    "code": "def exists_case_sensitive(path: str) -> bool:\n    result = os.path.exists(path)\n    if (sys.platform.startswith('win') or sys.platform == 'darwin') and result:\n        directory, basename = os.path.split(path)\n        result = basename in os.listdir(directory)\n    return result",
    "label": true
  },
  {
    "code": "def get_resource_reader(package: types.ModuleType) -> Optional[ResourceReader]:\n    spec = package.__spec__\n    reader = getattr(spec.loader, 'get_resource_reader', None)\n    if reader is None:\n        return None\n    return reader(spec.name)",
    "label": true
  },
  {
    "code": "def pytest_collect_file(file_path: Path, parent: nodes.Collector) -> Optional['Module']:\n    if file_path.suffix == '.py':\n        if not parent.session.isinitpath(file_path):\n            if not path_matches_patterns(file_path, parent.config.getini('python_files') + ['__init__.py']):\n                return None\n        ihook = parent.session.gethookproxy(file_path)\n        module: Module = ihook.pytest_pycollect_makemodule(module_path=file_path, parent=parent)\n        return module\n    return None",
    "label": true
  },
  {
    "code": "def render_missing_return_type(_msg, node, source_lines=None):\n    start_line, start_col = (node.fromlineno, node.parent.col_offset)\n    end_line, end_col = (node.end_lineno, node.end_col_offset)\n    yield from render_context(start_line - 2, start_line, source_lines)\n    yield from ((line, slice(None, end_col + 1), LineType.ERROR, source_lines[line - 1]) for line in range(start_line, end_line + 1))\n    yield from render_context(end_line + 1, end_line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def add_n(obj: Union[int, list], n: int) -> Union[int, list]:\n    lst = []\n    if isinstance(obj, int):\n        return obj + n\n    else:\n        for i in range(len(obj)):\n            lst.append(add_n(obj[i], n))\n    return lst",
    "label": true
  },
  {
    "code": "def _get_encoding(encoding_or_label):\n    if hasattr(encoding_or_label, 'codec_info'):\n        return encoding_or_label\n    encoding = lookup(encoding_or_label)\n    if encoding is None:\n        raise LookupError('Unknown encoding label: %r' % encoding_or_label)\n    return encoding",
    "label": true
  },
  {
    "code": "def _module_map():\n    from collections import defaultdict\n    from types import SimpleNamespace\n    modmap = SimpleNamespace(by_name=defaultdict(list), by_id=defaultdict(list), top_level={})\n    for modname, module in sys.modules.items():\n        if modname in ('__main__', '__mp_main__') or not isinstance(module, ModuleType):\n            continue\n        if '.' not in modname:\n            modmap.top_level[id(module)] = modname\n        for objname, modobj in module.__dict__.items():\n            modmap.by_name[objname].append((modobj, modname))\n            modmap.by_id[id(modobj)].append((modobj, objname, modname))\n    return modmap",
    "label": true
  },
  {
    "code": "def _get_plugin_specs_as_list(specs: Union[None, types.ModuleType, str, Sequence[str]]) -> List[str]:\n    if specs is None:\n        return []\n    if isinstance(specs, types.ModuleType):\n        return []\n    if isinstance(specs, str):\n        return specs.split(',') if specs else []\n    if isinstance(specs, collections.abc.Sequence):\n        return list(specs)\n    raise UsageError(\"Plugins may be specified as a sequence or a ','-separated string of plugin names. Got: %r\" % specs)",
    "label": true
  },
  {
    "code": "def check_sequence(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, collections.abc.Sequence):\n        raise TypeCheckError('is not a sequence')\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for i, v in enumerate(samples):\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f'item {i}')\n                raise",
    "label": true
  },
  {
    "code": "def _linux_platforms(is_32bit: bool=_32_BIT_INTERPRETER) -> Iterator[str]:\n    linux = _normalize_string(sysconfig.get_platform())\n    if is_32bit:\n        if linux == 'linux_x86_64':\n            linux = 'linux_i686'\n        elif linux == 'linux_aarch64':\n            linux = 'linux_armv7l'\n    _, arch = linux.split('_', 1)\n    yield from _manylinux.platform_tags(linux, arch)\n    yield from _musllinux.platform_tags(arch)\n    yield linux",
    "label": true
  },
  {
    "code": "def is_node_in_type_annotation_context(node: nodes.NodeNG) -> bool:\n    current_node, parent_node = (node, node.parent)\n    while True:\n        if isinstance(parent_node, nodes.AnnAssign) and parent_node.annotation == current_node or (isinstance(parent_node, nodes.Arguments) and current_node in (*parent_node.annotations, *parent_node.posonlyargs_annotations, *parent_node.kwonlyargs_annotations, parent_node.varargannotation, parent_node.kwargannotation)) or (isinstance(parent_node, nodes.FunctionDef) and parent_node.returns == current_node):\n            return True\n        current_node, parent_node = (parent_node, parent_node.parent)\n        if isinstance(parent_node, nodes.Module):\n            return False",
    "label": true
  },
  {
    "code": "def register(linter: PyLinter) -> None:\n    linter.register_checker(TypeChecker(linter))\n    linter.register_checker(IterableChecker(linter))",
    "label": true
  },
  {
    "code": "def make_numbered_dir(root: Path, prefix: str, mode: int=448) -> Path:\n    for i in range(10):\n        max_existing = max(map(parse_num, find_suffixes(root, prefix)), default=-1)\n        new_number = max_existing + 1\n        new_path = root.joinpath(f'{prefix}{new_number}')\n        try:\n            new_path.mkdir(mode=mode)\n        except Exception:\n            pass\n        else:\n            _force_symlink(root, prefix + 'current', new_path)\n            return new_path\n    else:\n        raise OSError('could not create numbered dir with prefix {prefix} in {root} after 10 tries'.format(prefix=prefix, root=root))",
    "label": true
  },
  {
    "code": "def generate_metadata(build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str) -> str:\n    metadata_tmpdir = TempDirectory(kind='modern-metadata', globally_managed=True)\n    metadata_dir = metadata_tmpdir.path\n    with build_env:\n        runner = runner_with_spinner_message('Preparing metadata (pyproject.toml)')\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n    return os.path.join(metadata_dir, distinfo_dir)",
    "label": true
  },
  {
    "code": "def doctype_matches(text, regex):\n    m = doctype_lookup_re.search(text)\n    if m is None:\n        return False\n    doctype = m.group(1)\n    return re.compile(regex, re.I).match(doctype.strip()) is not None",
    "label": true
  },
  {
    "code": "def skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:\n    try:\n        while src[pos] in chars:\n            pos += 1\n    except IndexError:\n        pass\n    return pos",
    "label": true
  },
  {
    "code": "def builtin_words(names, backslash, suffix=NAME_END_RE):\n    prefix = '[\\\\-_^]?'\n    if backslash == 'mandatory':\n        prefix += '\\\\\\\\'\n    elif backslash == 'optional':\n        prefix += '\\\\\\\\?'\n    else:\n        assert backslash == 'disallowed'\n    return words(names, prefix, suffix)",
    "label": true
  },
  {
    "code": "def get_default_compiler(osname=None, platform=None):\n    if osname is None:\n        osname = os.name\n    if platform is None:\n        platform = sys.platform\n    for pattern, compiler in _default_compilers:\n        if re.match(pattern, platform) is not None or re.match(pattern, osname) is not None:\n            return compiler\n    return 'unix'",
    "label": true
  },
  {
    "code": "def print_yielded(func):\n    print_all = functools.partial(map, print)\n    print_results = compose(more_itertools.consume, print_all, func)\n    return functools.wraps(func)(print_results)",
    "label": true
  },
  {
    "code": "def find_on_path(importer, path_item, only=False):\n    path_item = _normalize_cached(path_item)\n    if _is_unpacked_egg(path_item):\n        yield Distribution.from_filename(path_item, metadata=PathMetadata(path_item, os.path.join(path_item, 'EGG-INFO')))\n        return\n    entries = (os.path.join(path_item, child) for child in safe_listdir(path_item))\n    for entry in sorted(entries):\n        fullpath = os.path.join(path_item, entry)\n        factory = dist_factory(path_item, entry, only)\n        for dist in factory(fullpath):\n            yield dist",
    "label": true
  },
  {
    "code": "def _looks_like_wheel(location: str) -> bool:\n    if not location.endswith(WHEEL_EXTENSION):\n        return False\n    if not os.path.isfile(location):\n        return False\n    if not Wheel.wheel_file_re.match(os.path.basename(location)):\n        return False\n    return zipfile.is_zipfile(location)",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str, *, validate: bool=False) -> NormalizedName:\n    if validate and (not _validate_regex.match(name)):\n        raise InvalidName(f'name is invalid: {name!r}')\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def _get_config_var(name: str, warn: bool=False) -> Union[int, str, None]:\n    value = sysconfig.get_config_var(name)\n    if value is None and warn:\n        logger.debug(\"Config variable '%s' is unset, Python ABI tag may be incorrect\", name)\n    return value",
    "label": true
  },
  {
    "code": "def raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:\n    msg = f'{option} error: {msg}'\n    msg = textwrap.fill(' '.join(msg.split()))\n    parser.error(msg)",
    "label": true
  },
  {
    "code": "def unpack_url(link: Link, location: str, download: Downloader, verbosity: int, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> Optional[File]:\n    if link.is_vcs:\n        unpack_vcs_link(link, location, verbosity=verbosity)\n        return None\n    assert not link.is_existing_dir()\n    if link.is_file:\n        file = get_file_url(link, download_dir, hashes=hashes)\n    else:\n        file = get_http_url(link, download, download_dir, hashes=hashes)\n    if not link.is_wheel:\n        unpack_file(file.path, location, file.content_type)\n    return file",
    "label": true
  },
  {
    "code": "def get_url_scheme(url: str) -> Optional[str]:\n    if ':' not in url:\n        return None\n    return url.split(':', 1)[0].lower()",
    "label": true
  },
  {
    "code": "def _set_module(typevarlike):\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod",
    "label": true
  },
  {
    "code": "def _legacy_key(s):\n\n    def get_parts(s):\n        result = []\n        for p in _VERSION_PART.split(s.lower()):\n            p = _VERSION_REPLACE.get(p, p)\n            if p:\n                if '0' <= p[:1] <= '9':\n                    p = p.zfill(8)\n                else:\n                    p = '*' + p\n                result.append(p)\n        result.append('*final')\n        return result\n    result = []\n    for p in get_parts(s):\n        if p.startswith('*'):\n            if p < '*final':\n                while result and result[-1] == '*final-':\n                    result.pop()\n            while result and result[-1] == '00000000':\n                result.pop()\n        result.append(p)\n    return tuple(result)",
    "label": true
  },
  {
    "code": "def get_console_script_specs(console: Dict[str, str]) -> List[str]:\n    console = console.copy()\n    scripts_to_generate = []\n    pip_script = console.pop('pip', None)\n    if pip_script:\n        if 'ENSUREPIP_OPTIONS' not in os.environ:\n            scripts_to_generate.append('pip = ' + pip_script)\n        if os.environ.get('ENSUREPIP_OPTIONS', '') != 'altinstall':\n            scripts_to_generate.append('pip{} = {}'.format(sys.version_info[0], pip_script))\n        scripts_to_generate.append(f'pip{get_major_minor_version()} = {pip_script}')\n        pip_ep = [k for k in console if re.match('pip(\\\\d+(\\\\.\\\\d+)?)?$', k)]\n        for k in pip_ep:\n            del console[k]\n    easy_install_script = console.pop('easy_install', None)\n    if easy_install_script:\n        if 'ENSUREPIP_OPTIONS' not in os.environ:\n            scripts_to_generate.append('easy_install = ' + easy_install_script)\n        scripts_to_generate.append('easy_install-{} = {}'.format(get_major_minor_version(), easy_install_script))\n        easy_install_ep = [k for k in console if re.match('easy_install(-\\\\d+\\\\.\\\\d+)?$', k)]\n        for k in easy_install_ep:\n            del console[k]\n    scripts_to_generate.extend(starmap('{} = {}'.format, console.items()))\n    return scripts_to_generate",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def __getattr__(name: str) -> Any:\n    if name == 'config':\n        from ._config import global_config\n        return global_config\n    raise AttributeError(f'module {__name__!r} has no attribute {name!r}')",
    "label": true
  },
  {
    "code": "def platform_tags(arch: str) -> Iterator[str]:\n    sys_musl = _get_musl_version(sys.executable)\n    if sys_musl is None:\n        return\n    for minor in range(sys_musl.minor, -1, -1):\n        yield f'musllinux_{sys_musl.major}_{minor}_{arch}'",
    "label": true
  },
  {
    "code": "def _create_dtypemeta(scalar_type):\n    if NumpyDType is True:\n        __hook__()\n    if scalar_type is None:\n        return NumpyDType\n    return type(NumpyDType(scalar_type))",
    "label": true
  },
  {
    "code": "def connection_requires_http_tunnel(proxy_url: Url | None=None, proxy_config: ProxyConfig | None=None, destination_scheme: str | None=None) -> bool:\n    if proxy_url is None:\n        return False\n    if destination_scheme == 'http':\n        return False\n    if proxy_url.scheme == 'https' and proxy_config and proxy_config.use_forwarding_for_https:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _get_system_sitepackages() -> Set[str]:\n    if hasattr(site, 'getsitepackages'):\n        system_sites = site.getsitepackages()\n    else:\n        system_sites = [get_purelib(), get_platlib()]\n    return {os.path.normcase(path) for path in system_sites}",
    "label": true
  },
  {
    "code": "def get_win_launcher(type):\n    launcher_fn = '%s.exe' % type\n    if is_64bit():\n        if get_platform() == 'win-arm64':\n            launcher_fn = launcher_fn.replace('.', '-arm64.')\n        else:\n            launcher_fn = launcher_fn.replace('.', '-64.')\n    else:\n        launcher_fn = launcher_fn.replace('.', '-32.')\n    return resource_string('setuptools', launcher_fn)",
    "label": true
  },
  {
    "code": "def _default_key_normalizer(key_class, request_context):\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n    return key_class(**context)",
    "label": true
  },
  {
    "code": "def load_source(file, **kwds):\n    alias = kwds.pop('alias', None)\n    mode = kwds.pop('mode', 'r')\n    fname = getattr(file, 'name', file)\n    source = open(fname, mode=mode, **kwds).read()\n    if not alias:\n        tag = source.strip().splitlines()[-1].split()\n        if tag[0] != '#NAME:':\n            stub = source.splitlines()[0]\n            raise IOError('unknown name for code: %s' % stub)\n        alias = tag[-1]\n    local = {}\n    exec(source, local)\n    _ = eval('%s' % alias, local)\n    return _",
    "label": true
  },
  {
    "code": "def assign_params(func, namespace):\n    sig = inspect.signature(func)\n    params = sig.parameters.keys()\n    call_ns = {k: namespace[k] for k in params if k in namespace}\n    return functools.partial(func, **call_ns)",
    "label": true
  },
  {
    "code": "def _complete_visible_commands(ctx: 'Context', incomplete: str) -> t.Iterator[t.Tuple[str, 'Command']]:\n    multi = t.cast(MultiCommand, ctx.command)\n    for name in multi.list_commands(ctx):\n        if name.startswith(incomplete):\n            command = multi.get_command(ctx, name)\n            if command is not None and (not command.hidden):\n                yield (name, command)",
    "label": true
  },
  {
    "code": "def create_proxy_ssl_context(ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None):\n    ssl_context = create_urllib3_context(ssl_version=resolve_ssl_version(ssl_version), cert_reqs=resolve_cert_reqs(cert_reqs))\n    if not ca_certs and (not ca_cert_dir) and (not ca_cert_data) and hasattr(ssl_context, 'load_default_certs'):\n        ssl_context.load_default_certs()\n    return ssl_context",
    "label": true
  },
  {
    "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if '=' not in item:\n            result[item] = None\n            continue\n        name, value = item.split('=', 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result",
    "label": true
  },
  {
    "code": "def first_invoke(func1, func2):\n\n    def wrapper(*args, **kwargs):\n        func1()\n        return func2(*args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def ulabel(label: Union[str, bytes, bytearray]) -> str:\n    if not isinstance(label, (bytes, bytearray)):\n        try:\n            label_bytes = label.encode('ascii')\n        except UnicodeEncodeError:\n            check_label(label)\n            return label\n    else:\n        label_bytes = label\n    label_bytes = label_bytes.lower()\n    if label_bytes.startswith(_alabel_prefix):\n        label_bytes = label_bytes[len(_alabel_prefix):]\n        if not label_bytes:\n            raise IDNAError('Malformed A-label, no Punycode eligible content found')\n        if label_bytes.decode('ascii')[-1] == '-':\n            raise IDNAError('A-label must not end with a hyphen')\n    else:\n        check_label(label_bytes)\n        return label_bytes.decode('ascii')\n    try:\n        label = label_bytes.decode('punycode')\n    except UnicodeError:\n        raise IDNAError('Invalid A-label')\n    check_label(label)\n    return label",
    "label": true
  },
  {
    "code": "def encoding_unicode_range(iana_name: str) -> List[str]:\n    if is_multi_byte_encoding(iana_name):\n        raise IOError('Function not supported on multi-byte code page')\n    decoder = importlib.import_module('encodings.{}'.format(iana_name)).IncrementalDecoder\n    p: IncrementalDecoder = decoder(errors='ignore')\n    seen_ranges: Dict[str, int] = {}\n    character_count: int = 0\n    for i in range(64, 255):\n        chunk: str = p.decode(bytes([i]))\n        if chunk:\n            character_range: Optional[str] = unicode_range(chunk)\n            if character_range is None:\n                continue\n            if is_unicode_range_secondary(character_range) is False:\n                if character_range not in seen_ranges:\n                    seen_ranges[character_range] = 0\n                seen_ranges[character_range] += 1\n                character_count += 1\n    return sorted([character_range for character_range in seen_ranges if seen_ranges[character_range] / character_count >= 0.15])",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], MarkerAtom, str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def assert_header_parsing(headers: httplib.HTTPMessage) -> None:\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError(f'expected httplib.Message, got {type(headers)}.')\n    unparsed_data = None\n    if not headers.is_multipart():\n        payload = headers.get_payload()\n        if isinstance(payload, (bytes, str)):\n            unparsed_data = payload\n    defects = [defect for defect in headers.defects if not isinstance(defect, (StartBoundaryNotFoundDefect, MultipartInvariantViolationDefect))]\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)",
    "label": true
  },
  {
    "code": "def _normalize_extra_values(results: Any) -> Any:\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == 'extra':\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == 'extra':\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = (lhs, op, rhs)\n    return results",
    "label": true
  },
  {
    "code": "def _validate_requirements(requirements: List[InstallRequirement]) -> Generator[Tuple[str, InstallRequirement], None, None]:\n    for req in requirements:\n        assert req.name, f'invalid to-be-installed requirement: {req}'\n        yield (req.name, req)",
    "label": true
  },
  {
    "code": "def get_help(parser: argparse.ArgumentParser) -> str:\n    formatter = parser._get_formatter()\n    formatter.add_usage(parser.usage, parser._actions, parser._mutually_exclusive_groups)\n    formatter.add_text(parser.description)\n    for action_group in parser._action_groups:\n        if action_group.title == 'Subcommands':\n            formatter.start_section(action_group.title)\n            formatter.add_text(action_group.description)\n            formatter.add_arguments(action_group._group_actions)\n            formatter.end_section()\n    formatter.add_text(parser.epilog)\n    return formatter.format_help()",
    "label": true
  },
  {
    "code": "def _dist_info_files(whl_zip):\n    res = []\n    for path in whl_zip.namelist():\n        m = re.match('[^/\\\\\\\\]+-[^/\\\\\\\\]+\\\\.dist-info/', path)\n        if m:\n            res.append(path)\n    if res:\n        return res\n    raise Exception('No .dist-info folder found in wheel')",
    "label": true
  },
  {
    "code": "def _search_zip(modpath: Sequence[str]) -> tuple[Literal[ModuleType.PY_ZIPMODULE], str, str]:\n    for filepath, importer in _get_zipimporters():\n        if PY310_PLUS:\n            found: Any = importer.find_spec(modpath[0])\n        else:\n            found = importer.find_module(modpath[0])\n        if found:\n            if PY310_PLUS:\n                if not importer.find_spec(os.path.sep.join(modpath)):\n                    raise ImportError('No module named %s in %s/%s' % ('.'.join(modpath[1:]), filepath, modpath))\n            elif not importer.find_module(os.path.sep.join(modpath)):\n                raise ImportError('No module named %s in %s/%s' % ('.'.join(modpath[1:]), filepath, modpath))\n            return (ModuleType.PY_ZIPMODULE, os.path.abspath(filepath) + os.path.sep + os.path.sep.join(modpath), filepath)\n    raise ImportError(f\"No module named {'.'.join(modpath)}\")",
    "label": true
  },
  {
    "code": "def infer_slice(node, context: InferenceContext | None=None):\n    args = node.args\n    if not 0 < len(args) <= 3:\n        raise UseInferenceDefault\n    infer_func = partial(helpers.safe_infer, context=context)\n    args = [infer_func(arg) for arg in args]\n    for arg in args:\n        if not arg or isinstance(arg, util.UninferableBase):\n            raise UseInferenceDefault\n        if not isinstance(arg, nodes.Const):\n            raise UseInferenceDefault\n        if not isinstance(arg.value, (type(None), int)):\n            raise UseInferenceDefault\n    if len(args) < 3:\n        args.extend([None] * (3 - len(args)))\n    slice_node = nodes.Slice(lineno=node.lineno, col_offset=node.col_offset, parent=node.parent)\n    slice_node.postinit(*args)\n    return slice_node",
    "label": true
  },
  {
    "code": "def evaluate_skip_marks(item: Item) -> Optional[Skip]:\n    for mark in item.iter_markers(name='skipif'):\n        if 'condition' not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs['condition'],)\n        if not conditions:\n            reason = mark.kwargs.get('reason', '')\n            return Skip(reason)\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Skip(reason)\n    for mark in item.iter_markers(name='skip'):\n        try:\n            return Skip(*mark.args, **mark.kwargs)\n        except TypeError as e:\n            raise TypeError(str(e) + ' - maybe you meant pytest.mark.skipif?') from None\n    return None",
    "label": true
  },
  {
    "code": "def in_type_checking_block(node: nodes.NodeNG) -> bool:\n    for ancestor in node.node_ancestors():\n        if not isinstance(ancestor, nodes.If):\n            continue\n        if isinstance(ancestor.test, nodes.Name):\n            if ancestor.test.name != 'TYPE_CHECKING':\n                continue\n            lookup_result = ancestor.test.lookup(ancestor.test.name)[1]\n            if not lookup_result:\n                return False\n            maybe_import_from = lookup_result[0]\n            if isinstance(maybe_import_from, nodes.ImportFrom) and maybe_import_from.modname == 'typing':\n                return True\n            inferred = safe_infer(ancestor.test)\n            if isinstance(inferred, nodes.Const) and inferred.value is False:\n                return True\n        elif isinstance(ancestor.test, nodes.Attribute):\n            if ancestor.test.attrname != 'TYPE_CHECKING':\n                continue\n            inferred_module = safe_infer(ancestor.test.expr)\n            if isinstance(inferred_module, nodes.Module) and inferred_module.name == 'typing':\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def install_wheel(name: str, wheel_path: str, scheme: Scheme, req_description: str, pycompile: bool=True, warn_script_location: bool=True, direct_url: Optional[DirectUrl]=None, requested: bool=False) -> None:\n    with ZipFile(wheel_path, allowZip64=True) as z:\n        with req_error_context(req_description):\n            _install_wheel(name=name, wheel_zip=z, wheel_path=wheel_path, scheme=scheme, pycompile=pycompile, warn_script_location=warn_script_location, direct_url=direct_url, requested=requested)",
    "label": true
  },
  {
    "code": "def cell_len(text: str, _cell_len: Callable[[str], int]=cached_cell_len) -> int:\n    if len(text) < 512:\n        return _cell_len(text)\n    _get_size = get_character_cell_size\n    total_size = sum((_get_size(character) for character in text))\n    return total_size",
    "label": true
  },
  {
    "code": "def get_related_files(tested_configuration_file: str | Path, suffix_filter: str) -> list[Path]:\n    conf_path = Path(tested_configuration_file)\n    return [p for p in conf_path.parent.iterdir() if str(p.stem).startswith(conf_path.stem) and str(p).endswith(suffix_filter)]",
    "label": true
  },
  {
    "code": "def unwrap(s):\n    paragraphs = re.split('\\\\n\\\\n+', s)\n    cleaned = (para.replace('\\n', ' ') for para in paragraphs)\n    return '\\n'.join(cleaned)",
    "label": true
  },
  {
    "code": "def show_compilers():\n    from ..ccompiler import show_compilers\n    show_compilers()",
    "label": true
  },
  {
    "code": "def _mediawiki_row_with_attrs(separator, cell_values, colwidths, colaligns):\n    alignment = {'left': '', 'right': 'align=\"right\"| ', 'center': 'align=\"center\"| ', 'decimal': 'align=\"right\"| '}\n    values_with_attrs = [' ' + alignment.get(a, '') + c + ' ' for c, a in zip(cell_values, colaligns)]\n    colsep = separator * 2\n    return (separator + colsep.join(values_with_attrs)).rstrip()",
    "label": true
  },
  {
    "code": "def _simulate_installation_of(to_install: List[InstallRequirement], package_set: PackageSet) -> Set[NormalizedName]:\n    installed = set()\n    for inst_req in to_install:\n        abstract_dist = make_distribution_for_install_requirement(inst_req)\n        dist = abstract_dist.get_metadata_distribution()\n        name = dist.canonical_name\n        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))\n        installed.add(name)\n    return installed",
    "label": true
  },
  {
    "code": "def do_int(value: t.Any, default: int=0, base: int=10) -> int:\n    try:\n        if isinstance(value, str):\n            return int(value, base)\n        return int(value)\n    except (TypeError, ValueError):\n        try:\n            return int(float(value))\n        except (TypeError, ValueError):\n            return default",
    "label": true
  },
  {
    "code": "def _cert_array_from_pem(pem_bundle):\n    pem_bundle = pem_bundle.replace(b'\\r\\n', b'\\n')\n    der_certs = [base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)]\n    if not der_certs:\n        raise ssl.SSLError('No root certificates specified')\n    cert_array = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n    if not cert_array:\n        raise ssl.SSLError('Unable to allocate memory!')\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError('Unable to allocate memory!')\n            cert = Security.SecCertificateCreateWithData(CoreFoundation.kCFAllocatorDefault, certdata)\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError('Unable to build cert object!')\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        CoreFoundation.CFRelease(cert_array)\n        raise\n    return cert_array",
    "label": true
  },
  {
    "code": "def parse_rgb_hex(hex_color: str) -> ColorTriplet:\n    assert len(hex_color) == 6, 'must be 6 characters'\n    color = ColorTriplet(int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16))\n    return color",
    "label": true
  },
  {
    "code": "def words(text: str) -> Iterable[Tuple[int, int, str]]:\n    position = 0\n    word_match = re_word.match(text, position)\n    while word_match is not None:\n        start, end = word_match.span()\n        word = word_match.group(0)\n        yield (start, end, word)\n        word_match = re_word.match(text, end)",
    "label": true
  },
  {
    "code": "def _encode_pth(content: str) -> bytes:\n    encoding = 'locale' if sys.version_info >= (3, 10) else None\n    with io.BytesIO() as buffer:\n        wrapper = io.TextIOWrapper(buffer, encoding)\n        wrapper.write(content)\n        wrapper.flush()\n        buffer.seek(0)\n        return buffer.read()",
    "label": true
  },
  {
    "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)",
    "label": true
  },
  {
    "code": "def make_proxy_method(code):\n\n    def proxy_wrapper(self, *args):\n        return code(self.__wrapped__, *args)\n    return proxy_wrapper",
    "label": true
  },
  {
    "code": "def check_none(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if value is not None:\n        raise TypeCheckError('is not None')",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e128(msg, _node, source_lines):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(0, col if col != 0 else None), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def ensure_binary(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def _add_plugins(run: Run, value: str | None) -> None:\n    assert value is not None\n    run._plugins.extend(utils._splitstrip(value))",
    "label": true
  },
  {
    "code": "def make_attrgetter(environment: 'Environment', attribute: t.Optional[t.Union[str, int]], postprocess: t.Optional[t.Callable[[t.Any], t.Any]]=None, default: t.Optional[t.Any]=None) -> t.Callable[[t.Any], t.Any]:\n    parts = _prepare_attribute_parts(attribute)\n\n    def attrgetter(item: t.Any) -> t.Any:\n        for part in parts:\n            item = environment.getitem(item, part)\n            if default is not None and isinstance(item, Undefined):\n                item = default\n        if postprocess is not None:\n            item = postprocess(item)\n        return item\n    return attrgetter",
    "label": true
  },
  {
    "code": "def modify_sys_path() -> None:\n    cwd = os.getcwd()\n    if sys.path[0] in ('', '.', cwd):\n        sys.path.pop(0)\n    env_pythonpath = os.environ.get('PYTHONPATH', '')\n    if env_pythonpath.startswith(':') and env_pythonpath not in (f':{cwd}', ':.'):\n        sys.path.pop(0)\n    elif env_pythonpath.endswith(':') and env_pythonpath not in (f'{cwd}:', '.:'):\n        sys.path.pop(1)",
    "label": true
  },
  {
    "code": "def import_type(line: str, config: Config=DEFAULT_CONFIG) -> Optional[str]:\n    if config.honor_noqa and line.lower().rstrip().endswith('noqa'):\n        return None\n    if 'isort:skip' in line or 'isort: skip' in line or 'isort: split' in line:\n        return None\n    if line.startswith(('import ', 'cimport ')):\n        return 'straight'\n    if line.startswith('from '):\n        return 'from'\n    return None",
    "label": true
  },
  {
    "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv",
    "label": true
  },
  {
    "code": "def _idna_encode(name: str) -> bytes:\n    if not name.isascii():\n        try:\n            import idna\n        except ImportError:\n            raise LocationParseError(\"Unable to parse URL without the 'idna' module\") from None\n        try:\n            return idna.encode(name.lower(), strict=True, std3_rules=True)\n        except idna.IDNAError:\n            raise LocationParseError(f\"Name '{name}' is not a valid IDNA label\") from None\n    return name.lower().encode('ascii')",
    "label": true
  },
  {
    "code": "def varnames(func):\n    func = code(func)\n    if not iscode(func):\n        return ()\n    return (func.co_varnames, func.co_cellvars)",
    "label": true
  },
  {
    "code": "def assert_header_parsing(headers):\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError('expected httplib.Message, got {0}.'.format(type(headers)))\n    defects = getattr(headers, 'defects', None)\n    get_payload = getattr(headers, 'get_payload', None)\n    unparsed_data = None\n    if get_payload:\n        if not headers.is_multipart():\n            payload = get_payload()\n            if isinstance(payload, (bytes, str)):\n                unparsed_data = payload\n    if defects:\n        defects = [defect for defect in defects if not isinstance(defect, (StartBoundaryNotFoundDefect, MultipartInvariantViolationDefect))]\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)",
    "label": true
  },
  {
    "code": "def get_filetype_from_line(l):\n    m = modeline_re.search(l)\n    if m:\n        return m.group(1)",
    "label": true
  },
  {
    "code": "def railroad_to_html(diagrams: List[NamedDiagram], embed=False, **kwargs) -> str:\n    data = []\n    for diagram in diagrams:\n        if diagram.diagram is None:\n            continue\n        io = StringIO()\n        try:\n            css = kwargs.get('css')\n            diagram.diagram.writeStandalone(io.write, css=css)\n        except AttributeError:\n            diagram.diagram.writeSvg(io.write)\n        title = diagram.name\n        if diagram.index == 0:\n            title += ' (root)'\n        data.append({'title': title, 'text': '', 'svg': io.getvalue()})\n    return template.render(diagrams=data, embed=embed, **kwargs)",
    "label": true
  },
  {
    "code": "def pick_bool(*values: Optional[bool]) -> bool:\n    assert values, '1 or more values required'\n    for value in values:\n        if value is not None:\n            return value\n    return bool(value)",
    "label": true
  },
  {
    "code": "def glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def token_map(func, *args) -> ParseAction:\n\n    def pa(s, l, t):\n        return [func(tokn, *args) for tokn in t]\n    func_name = getattr(func, '__name__', getattr(func, '__class__').__name__)\n    pa.__name__ = func_name\n    return pa",
    "label": true
  },
  {
    "code": "def norm_and_check(source_tree, requested):\n    if os.path.isabs(requested):\n        raise ValueError('paths must be relative')\n    abs_source = os.path.abspath(source_tree)\n    abs_requested = os.path.normpath(os.path.join(abs_source, requested))\n    norm_source = os.path.normcase(abs_source)\n    norm_requested = os.path.normcase(abs_requested)\n    if os.path.commonprefix([norm_source, norm_requested]) != norm_source:\n        raise ValueError('paths must be inside source tree')\n    return abs_requested",
    "label": true
  },
  {
    "code": "def valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:\n    if len(label) > (254 if trailing_dot else 253):\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def process_env_var(env_var: str) -> Variable:\n    if env_var == 'platform_python_implementation' or env_var == 'python_implementation':\n        return Variable('platform_python_implementation')\n    else:\n        return Variable(env_var)",
    "label": true
  },
  {
    "code": "def _prepare_download(resp: Response, link: Link, progress_bar: str) -> Iterable[bytes]:\n    total_length = _get_http_response_size(resp)\n    if link.netloc == PyPI.file_storage_domain:\n        url = link.show_url\n    else:\n        url = link.url_without_fragment\n    logged_url = redact_auth_from_url(url)\n    if total_length:\n        logged_url = '{} ({})'.format(logged_url, format_size(total_length))\n    if is_from_cache(resp):\n        logger.info('Using cached %s', logged_url)\n    else:\n        logger.info('Downloading %s', logged_url)\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif is_from_cache(resp):\n        show_progress = False\n    elif not total_length:\n        show_progress = True\n    elif total_length > 40 * 1000:\n        show_progress = True\n    else:\n        show_progress = False\n    chunks = response_chunks(resp, CONTENT_CHUNK_SIZE)\n    if not show_progress:\n        return chunks\n    renderer = get_download_progress_renderer(bar_type=progress_bar, size=total_length)\n    return renderer(chunks)",
    "label": true
  },
  {
    "code": "def check_package_data(dist, attr, value):\n    if not isinstance(value, dict):\n        raise DistutilsSetupError('{!r} must be a dictionary mapping package names to lists of string wildcard patterns'.format(attr))\n    for k, v in value.items():\n        if not isinstance(k, str):\n            raise DistutilsSetupError('keys of {!r} dict must be strings (got {!r})'.format(attr, k))\n        assert_string_list(dist, 'values of {!r} dict'.format(attr), v)",
    "label": true
  },
  {
    "code": "def get_list_opt(options, optname, default=None):\n    val = options.get(optname, default)\n    if isinstance(val, str):\n        return val.split()\n    elif isinstance(val, (list, tuple)):\n        return list(val)\n    else:\n        raise OptionError('Invalid type %r for option %s; you must give a list value' % (val, optname))",
    "label": true
  },
  {
    "code": "def install_req_from_line(name: str, comes_from: Optional[Union[str, InstallRequirement]]=None, *, use_pep517: Optional[bool]=None, isolated: bool=False, global_options: Optional[List[str]]=None, hash_options: Optional[Dict[str, List[str]]]=None, constraint: bool=False, line_source: Optional[str]=None, user_supplied: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    parts = parse_req_from_line(name, line_source)\n    return InstallRequirement(parts.requirement, comes_from, link=parts.link, markers=parts.markers, use_pep517=use_pep517, isolated=isolated, global_options=global_options, hash_options=hash_options, config_settings=config_settings, constraint=constraint, extras=parts.extras, user_supplied=user_supplied)",
    "label": true
  },
  {
    "code": "def triplewise(iterable):\n    for (a, _), (b, c) in pairwise(pairwise(iterable)):\n        yield (a, b, c)",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def intersperse(e, iterable, n=1):\n    if n == 0:\n        raise ValueError('n must be > 0')\n    elif n == 1:\n        return islice(interleave(repeat(e), iterable), 1, None)\n    else:\n        filler = repeat([e])\n        chunks = chunked(iterable, n)\n        return flatten(islice(interleave(filler, chunks), 1, None))",
    "label": true
  },
  {
    "code": "def getreportopt(config: Config) -> str:\n    reportchars: str = config.option.reportchars\n    old_aliases = {'F', 'S'}\n    reportopts = ''\n    for char in reportchars:\n        if char in old_aliases:\n            char = char.lower()\n        if char == 'a':\n            reportopts = 'sxXEf'\n        elif char == 'A':\n            reportopts = 'PpsxXEf'\n        elif char == 'N':\n            reportopts = ''\n        elif char not in reportopts:\n            reportopts += char\n    if not config.option.disable_warnings and 'w' not in reportopts:\n        reportopts = 'w' + reportopts\n    elif config.option.disable_warnings and 'w' in reportopts:\n        reportopts = reportopts.replace('w', '')\n    return reportopts",
    "label": true
  },
  {
    "code": "def parse_version(version):\n    x = (version & 4294901760) >> 16\n    y = (version & 65280) >> 8\n    z = version & 255\n    return (x, y, z)",
    "label": true
  },
  {
    "code": "def skip_line(line: str, in_quote: str, index: int, section_comments: Tuple[str, ...], needs_import: bool=True) -> Tuple[bool, str]:\n    should_skip = bool(in_quote)\n    if '\"' in line or \"'\" in line:\n        char_index = 0\n        while char_index < len(line):\n            if line[char_index] == '\\\\':\n                char_index += 1\n            elif in_quote:\n                if line[char_index:char_index + len(in_quote)] == in_quote:\n                    in_quote = ''\n            elif line[char_index] in (\"'\", '\"'):\n                long_quote = line[char_index:char_index + 3]\n                if long_quote in ('\"\"\"', \"'''\"):\n                    in_quote = long_quote\n                    char_index += 2\n                else:\n                    in_quote = line[char_index]\n            elif line[char_index] == '#':\n                break\n            char_index += 1\n    if ';' in line.split('#')[0] and needs_import:\n        for part in (part.strip() for part in line.split(';')):\n            if part and (not part.startswith('from ')) and (not part.startswith(('import ', 'cimport '))):\n                should_skip = True\n    return (bool(should_skip or in_quote), in_quote)",
    "label": true
  },
  {
    "code": "def _safe_segment(segment):\n    segment = re.sub('[^A-Za-z0-9.]+', '-', segment)\n    segment = re.sub('-[^A-Za-z0-9]+', '-', segment)\n    return re.sub('\\\\.[^A-Za-z0-9]+', '.', segment).strip('.-')",
    "label": true
  },
  {
    "code": "def _should_vertical(specification: int, exprs: Iterable[pyparsing.ParserElement]) -> bool:\n    if specification is None:\n        return False\n    else:\n        return len(_visible_exprs(exprs)) >= specification",
    "label": true
  },
  {
    "code": "def _find_vc2017():\n    root = os.environ.get('ProgramFiles(x86)') or os.environ.get('ProgramFiles')\n    if not root:\n        return (None, None)\n    try:\n        path = subprocess.check_output([os.path.join(root, 'Microsoft Visual Studio', 'Installer', 'vswhere.exe'), '-latest', '-prerelease', '-requires', 'Microsoft.VisualStudio.Component.VC.Tools.x86.x64', '-property', 'installationPath', '-products', '*'], encoding='mbcs', errors='strict').strip()\n    except (subprocess.CalledProcessError, OSError, UnicodeDecodeError):\n        return (None, None)\n    path = os.path.join(path, 'VC', 'Auxiliary', 'Build')\n    if os.path.isdir(path):\n        return (15, path)\n    return (None, None)",
    "label": true
  },
  {
    "code": "def _check_namedtuple_attributes(typename, attributes, rename=False):\n    attributes = tuple(attributes)\n    if rename:\n        attributes = _get_renamed_namedtuple_attributes(attributes)\n    for name in (typename,) + attributes:\n        if not isinstance(name, str):\n            raise AstroidTypeError('Type names and field names must be strings')\n        if not name.isidentifier():\n            raise AstroidValueError('Type names and field names must be valid' + f'identifiers: {name!r}')\n        if keyword.iskeyword(name):\n            raise AstroidValueError(f'Type names and field names cannot be a keyword: {name!r}')\n    seen = set()\n    for name in attributes:\n        if name.startswith('_') and (not rename):\n            raise AstroidValueError(f'Field names cannot start with an underscore: {name!r}')\n        if name in seen:\n            raise AstroidValueError(f'Encountered duplicate field name: {name!r}')\n        seen.add(name)\n    return attributes",
    "label": true
  },
  {
    "code": "def normalize_path(path: str, resolve_symlinks: bool=True) -> str:\n    path = os.path.expanduser(path)\n    if resolve_symlinks:\n        path = os.path.realpath(path)\n    else:\n        path = os.path.abspath(path)\n    return os.path.normcase(path)",
    "label": true
  },
  {
    "code": "def _validate_dependencies_met():\n    from cryptography.x509.extensions import Extensions\n    if getattr(Extensions, 'get_extension_for_class', None) is None:\n        raise ImportError(\"'cryptography' module missing required functionality.  Try upgrading to v1.3.4 or newer.\")\n    from OpenSSL.crypto import X509\n    x509 = X509()\n    if getattr(x509, '_x509', None) is None:\n        raise ImportError(\"'pyOpenSSL' module missing required functionality. Try upgrading to v0.14 or newer.\")",
    "label": true
  },
  {
    "code": "def get_windows_console_features() -> 'WindowsConsoleFeatures':\n    global _windows_console_features\n    if _windows_console_features is not None:\n        return _windows_console_features\n    from ._windows import get_windows_console_features\n    _windows_console_features = get_windows_console_features()\n    return _windows_console_features",
    "label": true
  },
  {
    "code": "def get_win_folder_if_csidl_name_not_env_var(csidl_name: str) -> str | None:\n    if csidl_name == 'CSIDL_PERSONAL':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Documents')\n    if csidl_name == 'CSIDL_DOWNLOADS':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Downloads')\n    if csidl_name == 'CSIDL_MYPICTURES':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Pictures')\n    if csidl_name == 'CSIDL_MYVIDEO':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Videos')\n    if csidl_name == 'CSIDL_MYMUSIC':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Music')\n    return None",
    "label": true
  },
  {
    "code": "def find_parent_package(packages: List[str], package_dir: Mapping[str, str], root_dir: _Path) -> Optional[str]:\n    packages = sorted(packages, key=len)\n    common_ancestors = []\n    for i, name in enumerate(packages):\n        if not all((n.startswith(f'{name}.') for n in packages[i + 1:])):\n            break\n        common_ancestors.append(name)\n    for name in common_ancestors:\n        pkg_path = find_package_path(name, package_dir, root_dir)\n        init = os.path.join(pkg_path, '__init__.py')\n        if os.path.isfile(init):\n            return name\n    return None",
    "label": true
  },
  {
    "code": "def _dump_str(v):\n    if sys.version_info < (3,) and hasattr(v, 'decode') and isinstance(v, str):\n        v = v.decode('utf-8')\n    v = '%r' % v\n    if v[0] == 'u':\n        v = v[1:]\n    singlequote = v.startswith(\"'\")\n    if singlequote or v.startswith('\"'):\n        v = v[1:-1]\n    if singlequote:\n        v = v.replace(\"\\\\'\", \"'\")\n        v = v.replace('\"', '\\\\\"')\n    v = v.split('\\\\x')\n    while len(v) > 1:\n        i = -1\n        if not v[0]:\n            v = v[1:]\n        v[0] = v[0].replace('\\\\\\\\', '\\\\')\n        joinx = v[0][i] != '\\\\'\n        while v[0][:i] and v[0][i] == '\\\\':\n            joinx = not joinx\n            i -= 1\n        if joinx:\n            joiner = 'x'\n        else:\n            joiner = 'u00'\n        v = [v[0] + joiner + v[1]] + v[2:]\n    return unicode('\"' + v[0] + '\"')",
    "label": true
  },
  {
    "code": "def is_installable_dir(path: str) -> bool:\n    if not os.path.isdir(path):\n        return False\n    if os.path.isfile(os.path.join(path, 'pyproject.toml')):\n        return True\n    if os.path.isfile(os.path.join(path, 'setup.py')):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def getsourcelines(object, lstrip=False, enclosing=False):\n    code, n = getblocks(object, lstrip=lstrip, enclosing=enclosing, locate=True)\n    return (code[-1], n[-1])",
    "label": true
  },
  {
    "code": "def infer_compression(url):\n    compression_indicator = url[-2:]\n    mapping = dict(gz='z', bz='j', xz='J')\n    return mapping.get(compression_indicator, 'z')",
    "label": true
  },
  {
    "code": "def get_iterating_dictionary_name(node: nodes.For | nodes.Comprehension) -> str | None:\n    if isinstance(node.iter, nodes.Call) and isinstance(node.iter.func, nodes.Attribute) and (node.iter.func.attrname == 'keys'):\n        inferred = safe_infer(node.iter.func)\n        if not isinstance(inferred, astroid.BoundMethod):\n            return None\n        return node.iter.as_string().rpartition('.keys')[0]\n    if isinstance(node.iter, (nodes.Name, nodes.Attribute)):\n        inferred = safe_infer(node.iter)\n        if not isinstance(inferred, nodes.Dict):\n            return None\n        return node.iter.as_string()\n    return None",
    "label": true
  },
  {
    "code": "def _mac_platforms(arch: str) -> List[str]:\n    match = _osx_arch_pat.match(arch)\n    if match:\n        name, major, minor, actual_arch = match.groups()\n        mac_version = (int(major), int(minor))\n        arches = ['{}_{}'.format(name, arch[len('macosx_'):]) for arch in mac_platforms(mac_version, actual_arch)]\n    else:\n        arches = [arch]\n    return arches",
    "label": true
  },
  {
    "code": "def load(__fp: BinaryIO, *, parse_float: ParseFloat=float) -> dict[str, Any]:\n    b = __fp.read()\n    try:\n        s = b.decode()\n    except AttributeError:\n        raise TypeError(\"File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`\") from None\n    return loads(s, parse_float=parse_float)",
    "label": true
  },
  {
    "code": "def ircformat(color, text):\n    if len(color) < 1:\n        return text\n    add = sub = ''\n    if '_' in color:\n        add += '\\x1d'\n        sub = '\\x1d' + sub\n        color = color.strip('_')\n    if '*' in color:\n        add += '\\x02'\n        sub = '\\x02' + sub\n        color = color.strip('*')\n    if len(color) > 0:\n        add += '\\x03' + str(IRC_COLOR_MAP[color]).zfill(2)\n        sub = '\\x03' + sub\n    return add + text + sub\n    return '<' + add + '>' + text + '</' + sub + '>'",
    "label": true
  },
  {
    "code": "def test_pickled_cadder():\n    pcadder = pickle.dumps(cadder)\n    pcadd5 = pickle.loads(pcadder)(x)\n    assert pcadd5(y) == x + y",
    "label": true
  },
  {
    "code": "def _is_bpo_43522_fixed(implementation_name: str, version_info: _TYPE_VERSION_INFO, pypy_version_info: _TYPE_VERSION_INFO | None) -> bool:\n    if implementation_name == 'pypy':\n        return pypy_version_info >= (7, 3, 8) and version_info >= (3, 8)\n    elif implementation_name == 'cpython':\n        major_minor = version_info[:2]\n        micro = version_info[2]\n        return major_minor == (3, 8) and micro >= 9 or (major_minor == (3, 9) and micro >= 3) or major_minor >= (3, 10)\n    else:\n        return False",
    "label": true
  },
  {
    "code": "def disable_importlib_metadata_finder(metadata):\n    try:\n        import importlib_metadata\n    except ImportError:\n        return\n    except AttributeError:\n        from .warnings import SetuptoolsWarning\n        SetuptoolsWarning.emit('Incompatibility problem.', '\\n            `importlib-metadata` version is incompatible with `setuptools`.\\n            This problem is likely to be solved by installing an updated version of\\n            `importlib-metadata`.\\n            ', see_url='https://github.com/python/importlib_metadata/issues/396')\n        raise\n    if importlib_metadata is metadata:\n        return\n    to_remove = [ob for ob in sys.meta_path if isinstance(ob, importlib_metadata.MetadataPathFinder)]\n    for item in to_remove:\n        sys.meta_path.remove(item)",
    "label": true
  },
  {
    "code": "def normalize_path(path: Any) -> str:\n    str_path = str(path)\n    parent, file_name = os.path.split(str_path)\n    if parent:\n        raise ValueError(f'{path!r} must be only a file name')\n    return file_name",
    "label": true
  },
  {
    "code": "def nth_permutation(iterable, r, index):\n    pool = list(iterable)\n    n = len(pool)\n    if r is None or r == n:\n        r, c = (n, factorial(n))\n    elif not 0 <= r < n:\n        raise ValueError\n    else:\n        c = factorial(n) // factorial(n - r)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    if c == 0:\n        return tuple()\n    result = [0] * r\n    q = index * factorial(n) // c if r < n else index\n    for d in range(1, n + 1):\n        q, i = divmod(q, d)\n        if 0 <= n - d < r:\n            result[n - d] = i\n        if q == 0:\n            break\n    return tuple(map(pool.pop, result))",
    "label": true
  },
  {
    "code": "def to_native_string(string, encoding='ascii'):\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n    return out",
    "label": true
  },
  {
    "code": "def file_ns_handler(importer, path_item, packageName, module):\n    subpath = os.path.join(path_item, packageName.split('.')[-1])\n    normalized = _normalize_cached(subpath)\n    for item in module.__path__:\n        if _normalize_cached(item) == normalized:\n            break\n    else:\n        return subpath",
    "label": true
  },
  {
    "code": "def _build_one(req: InstallRequirement, output_dir: str, verify: bool, build_options: List[str], global_options: List[str], editable: bool) -> Optional[str]:\n    artifact = 'editable' if editable else 'wheel'\n    try:\n        ensure_dir(output_dir)\n    except OSError as e:\n        logger.warning('Building %s for %s failed: %s', artifact, req.name, e)\n        return None\n    with req.build_env:\n        wheel_path = _build_one_inside_env(req, output_dir, build_options, global_options, editable)\n    if wheel_path and verify:\n        try:\n            _verify_one(req, wheel_path)\n        except (InvalidWheelFilename, UnsupportedWheel) as e:\n            logger.warning('Built %s for %s is invalid: %s', artifact, req.name, e)\n            return None\n    return wheel_path",
    "label": true
  },
  {
    "code": "def guess_decode(text):\n    try:\n        text = text.decode('utf-8')\n        return (text, 'utf-8')\n    except UnicodeDecodeError:\n        try:\n            import locale\n            prefencoding = locale.getpreferredencoding()\n            text = text.decode()\n            return (text, prefencoding)\n        except (UnicodeDecodeError, LookupError):\n            text = text.decode('latin1')\n            return (text, 'latin1')",
    "label": true
  },
  {
    "code": "def requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        return quote(uri, safe=safe_without_percent)",
    "label": true
  },
  {
    "code": "def get_encodings_from_content(content):\n    warnings.warn('In requests 3.0, get_encodings_from_content will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)', DeprecationWarning)\n    charset_re = re.compile('<meta.*?charset=[\"\\\\\\']*(.+?)[\"\\\\\\'>]', flags=re.I)\n    pragma_re = re.compile('<meta.*?content=[\"\\\\\\']*;?charset=(.+?)[\"\\\\\\'>]', flags=re.I)\n    xml_re = re.compile('^<\\\\?xml.*?encoding=[\"\\\\\\']*(.+?)[\"\\\\\\'>]')\n    return charset_re.findall(content) + pragma_re.findall(content) + xml_re.findall(content)",
    "label": true
  },
  {
    "code": "def new_context(environment: 'Environment', template_name: t.Optional[str], blocks: t.Dict[str, t.Callable[['Context'], t.Iterator[str]]], vars: t.Optional[t.Dict[str, t.Any]]=None, shared: bool=False, globals: t.Optional[t.MutableMapping[str, t.Any]]=None, locals: t.Optional[t.Mapping[str, t.Any]]=None) -> 'Context':\n    if vars is None:\n        vars = {}\n    if shared:\n        parent = vars\n    else:\n        parent = dict(globals or (), **vars)\n    if locals:\n        if shared:\n            parent = dict(parent)\n        for key, value in locals.items():\n            if value is not missing:\n                parent[key] = value\n    return environment.context_class(environment, parent, template_name, blocks, globals=globals)",
    "label": true
  },
  {
    "code": "def _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n    try:\n        spec = Specifier(''.join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs)\n    oper: Optional[Operator] = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(f'Undefined {op!r} on {lhs!r} and {rhs!r}.')\n    return oper(lhs, rhs)",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def _needs_hiding(mod_name):\n    base_module = mod_name.split('.', 1)[0]\n    return base_module in _MODULES_TO_HIDE",
    "label": true
  },
  {
    "code": "def roundrobin(*iterables):\n    pending = len(iterables)\n    nexts = cycle((iter(it).__next__ for it in iterables))\n    while pending:\n        try:\n            for next in nexts:\n                yield next()\n        except StopIteration:\n            pending -= 1\n            nexts = cycle(islice(nexts, pending))",
    "label": true
  },
  {
    "code": "def find_plugin_styles():\n    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):\n        yield (entrypoint.name, entrypoint.load())",
    "label": true
  },
  {
    "code": "def check_modpath_has_init(path: str, mod_path: list[str]) -> bool:\n    modpath: list[str] = []\n    for part in mod_path:\n        modpath.append(part)\n        path = os.path.join(path, part)\n        if not _has_init(path):\n            old_namespace = util.is_namespace('.'.join(modpath))\n            if not old_namespace:\n                return False\n    return True",
    "label": true
  },
  {
    "code": "def check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:\n    package_set, _ = create_package_set_from_installed()\n    would_be_installed = _simulate_installation_of(to_install, package_set)\n    whitelist = _create_whitelist(would_be_installed, package_set)\n    return (package_set, check_package_set(package_set, should_ignore=lambda name: name not in whitelist))",
    "label": true
  },
  {
    "code": "def patch_func(replacement, target_mod, func_name):\n    original = getattr(target_mod, func_name)\n    vars(replacement).setdefault('unpatched', original)\n    setattr(target_mod, func_name, replacement)",
    "label": true
  },
  {
    "code": "def collect_string_fields(format_string: str) -> Iterable[str | None]:\n    formatter = string.Formatter()\n    try:\n        parseiterator = formatter.parse(format_string)\n        for result in parseiterator:\n            if all((item is None for item in result[1:])):\n                continue\n            name = result[1]\n            nested = result[2]\n            yield name\n            if nested:\n                yield from collect_string_fields(nested)\n    except ValueError as exc:\n        if exc.args[0].startswith('cannot switch from manual'):\n            yield ''\n            yield '1'\n            return\n        raise IncompleteFormatString(format_string) from exc",
    "label": true
  },
  {
    "code": "def avg_length(tree: HuffmanTree, freq_dict: dict[int, int]) -> float:\n    accumulator = 0\n    total = 0\n    codes_f = get_codes(tree)\n    for symbol in freq_dict:\n        tmp = freq_dict[symbol]\n        accumulator += tmp * len(codes_f[symbol])\n        total += tmp\n    if total:\n        return accumulator / total\n    else:\n        return 0.0",
    "label": true
  },
  {
    "code": "def _cache_normalize_path(path: str) -> str:\n    if not path:\n        return _normalize_path(path)\n    return _cache_normalize_path_(path)",
    "label": true
  },
  {
    "code": "def get_annotation_label(ann: nodes.Name | nodes.NodeNG) -> str:\n    if isinstance(ann, nodes.Name) and ann.name is not None:\n        return ann.name\n    if isinstance(ann, nodes.NodeNG):\n        return ann.as_string()\n    return ''",
    "label": true
  },
  {
    "code": "def _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:\n    if not value:\n        return (None, None)\n    parts = value.split('.')\n    if len(parts) > 3:\n        return ((), 'at most three version parts are allowed')\n    if len(parts) == 1:\n        value = parts[0]\n        if len(value) > 1:\n            parts = [value[0], value[1:]]\n    try:\n        version_info = tuple((int(part) for part in parts))\n    except ValueError:\n        return ((), 'each version part must be an integer')\n    return (version_info, None)",
    "label": true
  },
  {
    "code": "def map_except(function, iterable, *exceptions):\n    for item in iterable:\n        try:\n            yield function(item)\n        except exceptions:\n            pass",
    "label": true
  },
  {
    "code": "def morsel_to_cookie(morsel):\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(time.strptime(morsel['expires'], time_template))\n    return create_cookie(comment=morsel['comment'], comment_url=bool(morsel['comment']), discard=False, domain=morsel['domain'], expires=expires, name=morsel.key, path=morsel['path'], port=None, rest={'HttpOnly': morsel['httponly']}, rfc2109=False, secure=bool(morsel['secure']), value=morsel.value, version=morsel['version'] or 0)",
    "label": true
  },
  {
    "code": "def key_value_rule(src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat) -> Pos:\n    pos, key, value = parse_key_value_pair(src, pos, parse_float)\n    key_parent, key_stem = (key[:-1], key[-1])\n    abs_key_parent = header + key_parent\n    if out.flags.is_(abs_key_parent, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Can not mutate immutable namespace {abs_key_parent}')\n    out.flags.set_for_relative_key(header, key, Flags.EXPLICIT_NEST)\n    try:\n        nest = out.data.get_or_create_nest(abs_key_parent)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Can not overwrite a value')\n    if key_stem in nest:\n        raise suffixed_err(src, pos, 'Can not overwrite a value')\n    if isinstance(value, (dict, list)):\n        out.flags.set(header + key, Flags.FROZEN, recursive=True)\n    nest[key_stem] = value\n    return pos",
    "label": true
  },
  {
    "code": "def _forgiving_version(version):\n    version = version.replace(' ', '.')\n    match = _PEP440_FALLBACK.search(version)\n    if match:\n        safe = match['safe']\n        rest = version[len(safe):]\n    else:\n        safe = '0'\n        rest = version\n    local = f'sanitized.{_safe_segment(rest)}'.strip('.')\n    return f'{safe}.dev0+{local}'",
    "label": true
  },
  {
    "code": "def _import_module(name):\n    __import__(name)\n    return sys.modules[name]",
    "label": true
  },
  {
    "code": "def _non_empty_string_transformer(value: str) -> str:\n    if not value:\n        raise argparse.ArgumentTypeError('Option cannot be an empty string.')\n    return pylint_utils._unquote(value)",
    "label": true
  },
  {
    "code": "def remove_whitespace(content: str, line_separator: str='\\n') -> str:\n    content = content.replace(line_separator, '').replace(' ', '').replace('\\x0c', '')\n    return content",
    "label": true
  },
  {
    "code": "def _dnsname_match(dn, hostname, max_wildcards=1):\n    pats = []\n    if not dn:\n        return False\n    parts = dn.split('.')\n    leftmost = parts[0]\n    remainder = parts[1:]\n    wildcards = leftmost.count('*')\n    if wildcards > max_wildcards:\n        raise CertificateError('too many wildcards in certificate DNS name: ' + repr(dn))\n    if not wildcards:\n        return dn.lower() == hostname.lower()\n    if leftmost == '*':\n        pats.append('[^.]+')\n    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n        pats.append(re.escape(leftmost))\n    else:\n        pats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n    for frag in remainder:\n        pats.append(re.escape(frag))\n    pat = re.compile('\\\\A' + '\\\\.'.join(pats) + '\\\\Z', re.IGNORECASE)\n    return pat.match(hostname)",
    "label": true
  },
  {
    "code": "def _prepend_row_index(rows, index):\n    if index is None or index is False:\n        return rows\n    if isinstance(index, Sized) and len(index) != len(rows):\n        raise ValueError('index must be as long as the number of data rows: ' + 'len(index)={} len(rows)={}'.format(len(index), len(rows)))\n    sans_rows, separating_lines = _remove_separating_lines(rows)\n    new_rows = []\n    index_iter = iter(index)\n    for row in sans_rows:\n        index_v = next(index_iter)\n        new_rows.append([index_v] + list(row))\n    rows = new_rows\n    _reinsert_separating_lines(rows, separating_lines)\n    return rows",
    "label": true
  },
  {
    "code": "def get_app_dir(app_name: str, roaming: bool=True, force_posix: bool=False) -> str:\n    if WIN:\n        key = 'APPDATA' if roaming else 'LOCALAPPDATA'\n        folder = os.environ.get(key)\n        if folder is None:\n            folder = os.path.expanduser('~')\n        return os.path.join(folder, app_name)\n    if force_posix:\n        return os.path.join(os.path.expanduser(f'~/.{_posixify(app_name)}'))\n    if sys.platform == 'darwin':\n        return os.path.join(os.path.expanduser('~/Library/Application Support'), app_name)\n    return os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')), _posixify(app_name))",
    "label": true
  },
  {
    "code": "def hash_file(path: str, blocksize: int=1 << 20) -> Tuple[Any, int]:\n    h = hashlib.sha256()\n    length = 0\n    with open(path, 'rb') as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    return (h, length)",
    "label": true
  },
  {
    "code": "def _ensure_immutable_ids(ids: Optional[Union[Sequence[Optional[object]], Callable[[Any], Optional[object]]]]) -> Optional[Union[Tuple[Optional[object], ...], Callable[[Any], Optional[object]]]]:\n    if ids is None:\n        return None\n    if callable(ids):\n        return ids\n    return tuple(ids)",
    "label": true
  },
  {
    "code": "def test_sequence(value: t.Any) -> bool:\n    try:\n        len(value)\n        value.__getitem__\n    except Exception:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def get_unicode_from_response(r):\n    warnings.warn('In requests 3.0, get_unicode_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)', DeprecationWarning)\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n    try:\n        return str(r.content, encoding, errors='replace')\n    except TypeError:\n        return r.content",
    "label": true
  },
  {
    "code": "def _collect_zipimporter_cache_entries(normalized_path, cache):\n    result = []\n    prefix_len = len(normalized_path)\n    for p in cache:\n        np = normalize_path(p)\n        if np.startswith(normalized_path) and np[prefix_len:prefix_len + 1] in (os.sep, ''):\n            result.append(p)\n    return result",
    "label": true
  },
  {
    "code": "def path_to_url(path: str) -> str:\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib.parse.urljoin('file:', urllib.request.pathname2url(path))\n    return url",
    "label": true
  },
  {
    "code": "def try_encode(string, enc):\n    try:\n        return string.encode(enc)\n    except UnicodeEncodeError:\n        return None",
    "label": true
  },
  {
    "code": "def check_invalid_constraint_type(req: InstallRequirement) -> str:\n    problem = ''\n    if not req.name:\n        problem = 'Unnamed requirements are not allowed as constraints'\n    elif req.editable:\n        problem = 'Editable requirements are not allowed as constraints'\n    elif req.extras:\n        problem = 'Constraints cannot have extras'\n    if problem:\n        deprecated(reason='Constraints are only allowed to take the form of a package name and a version specifier. Other forms were originally permitted as an accident of the implementation, but were undocumented. The new implementation of the resolver no longer supports these forms.', replacement='replacing the constraint with a requirement', gone_in=None, issue=8210)\n    return problem",
    "label": true
  },
  {
    "code": "def get_project_data(name):\n    url = '%s/%s/project.json' % (name[0].upper(), name)\n    url = urljoin(_external_data_base_url, url)\n    result = _get_external_data(url)\n    return result",
    "label": true
  },
  {
    "code": "def literal_substitute(t: type, type_map: Dict[str, type]) -> type:\n    if isinstance(t, TypeVar) and t.__name__ in type_map:\n        return type_map[t.__name__]\n    elif isinstance(t, TypeVar):\n        return TypeVar(t.__name__)\n    elif isinstance(t, ForwardRef):\n        return ForwardRef(literal_substitute(t.__forward_arg__, type_map))\n    elif isinstance(t, TuplePlus):\n        subbed_args = [literal_substitute(t1, type_map) for t1 in t.__constraints__]\n        return TuplePlus('tup+', *subbed_args)\n    elif is_callable(t):\n        args = list((literal_substitute(t1, type_map) for t1 in t.__args__[:-1]))\n        res = literal_substitute(t.__args__[-1], type_map)\n        new_t = Callable[args, res]\n        if hasattr(t, '__polymorphic_tvars__'):\n            new_t.__polymorphic_tvars__ = t.__polymorphic_tvars__.copy()\n        return new_t\n    elif isinstance(t, _GenericAlias) and t.__args__ is not None:\n        return t.copy_with(tuple((literal_substitute(t1, type_map) for t1 in t.__args__)))\n    else:\n        return t",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def dump(o, f, encoder=None):\n    if not f.write:\n        raise TypeError('You can only dump an object to a file descriptor')\n    d = dumps(o, encoder=encoder)\n    f.write(d)\n    return d",
    "label": true
  },
  {
    "code": "def format(tokens, formatter, outfile=None):\n    try:\n        if not outfile:\n            realoutfile = getattr(formatter, 'encoding', None) and BytesIO() or StringIO()\n            formatter.format(tokens, realoutfile)\n            return realoutfile.getvalue()\n        else:\n            formatter.format(tokens, outfile)\n    except TypeError:\n        from pygments.formatter import Formatter\n        if isinstance(formatter, type) and issubclass(formatter, Formatter):\n            raise TypeError('format() argument must be a formatter instance, not a class')\n        raise",
    "label": true
  },
  {
    "code": "def remove_big(s: Stack) -> None:\n    sigma_stack = Stack()\n    while not s.is_empty():\n        sigma_stack.push(s.pop())\n    while not sigma_stack.is_empty():\n        skibidi = sigma_stack.pop()\n        if skibidi > 5:\n            s.push(skibidi)",
    "label": true
  },
  {
    "code": "def cleanup_numbered_dir(root: Path, prefix: str, keep: int, consider_lock_dead_if_created_before: float) -> None:\n    if not root.exists():\n        return\n    for path in cleanup_candidates(root, prefix, keep):\n        try_cleanup(path, consider_lock_dead_if_created_before)\n    for path in root.glob('garbage-*'):\n        try_cleanup(path, consider_lock_dead_if_created_before)\n    cleanup_dead_symlinks(root)",
    "label": true
  },
  {
    "code": "def current_umask() -> int:\n    mask = os.umask(0)\n    os.umask(mask)\n    return mask",
    "label": true
  },
  {
    "code": "def create_cache(size: int) -> t.Optional[t.MutableMapping[t.Tuple[weakref.ref, str], 'Template']]:\n    if size == 0:\n        return None\n    if size < 0:\n        return {}\n    return LRUCache(size)",
    "label": true
  },
  {
    "code": "def _pythonlib_compat():\n    from distutils import sysconfig\n    if not sysconfig.get_config_var('Py_ENABLED_SHARED'):\n        return\n    yield 'python{}.{}{}'.format(sys.hexversion >> 24, sys.hexversion >> 16 & 255, sysconfig.get_config_var('ABIFLAGS'))",
    "label": true
  },
  {
    "code": "def _match_hostname(cert, asserted_hostname):\n    stripped_hostname = asserted_hostname.strip('u[]')\n    if is_ipaddress(stripped_hostname):\n        asserted_hostname = stripped_hostname\n    try:\n        match_hostname(cert, asserted_hostname)\n    except CertificateError as e:\n        log.warning('Certificate did not match expected hostname: %s. Certificate: %s', asserted_hostname, cert)\n        e._peer_cert = cert\n        raise",
    "label": true
  },
  {
    "code": "def get_similar_commands(name: str) -> Optional[str]:\n    from difflib import get_close_matches\n    name = name.lower()\n    close_commands = get_close_matches(name, commands_dict.keys())\n    if close_commands:\n        return close_commands[0]\n    else:\n        return None",
    "label": true
  },
  {
    "code": "def get_dist_name(dist: importlib.metadata.Distribution) -> str:\n    name = cast(Any, dist).name\n    if not isinstance(name, str):\n        raise BadMetadata(dist, reason=\"invalid metadata entry 'name'\")\n    return name",
    "label": true
  },
  {
    "code": "def nth_combination(iterable, r, index):\n    pool = tuple(iterable)\n    n = len(pool)\n    if r < 0 or r > n:\n        raise ValueError\n    c = 1\n    k = min(r, n - r)\n    for i in range(1, k + 1):\n        c = c * (n - k + i) // i\n    if index < 0:\n        index += c\n    if index < 0 or index >= c:\n        raise IndexError\n    result = []\n    while r:\n        c, n, r = (c * r // n, n - 1, r - 1)\n        while index >= c:\n            index -= c\n            c, n = (c * (n - r) // n, n - 1)\n        result.append(pool[-1 - n])\n    return tuple(result)",
    "label": true
  },
  {
    "code": "def dump_file(filename, head=None):\n    if head is None:\n        log.info('%s', filename)\n    else:\n        log.info(head)\n    file = open(filename)\n    try:\n        log.info(file.read())\n    finally:\n        file.close()",
    "label": true
  },
  {
    "code": "def always_reversible(iterable):\n    try:\n        return reversed(iterable)\n    except TypeError:\n        return reversed(list(iterable))",
    "label": true
  },
  {
    "code": "def run_script(dist_spec, script_name):\n    ns = sys._getframe(1).f_globals\n    name = ns['__name__']\n    ns.clear()\n    ns['__name__'] = name\n    require(dist_spec)[0].run_script(script_name, ns)",
    "label": true
  },
  {
    "code": "def stagger(iterable, offsets=(-1, 0, 1), longest=False, fillvalue=None):\n    children = tee(iterable, len(offsets))\n    return zip_offset(*children, offsets=offsets, longest=longest, fillvalue=fillvalue)",
    "label": true
  },
  {
    "code": "def make_command(*args: Union[str, HiddenText, CommandArgs]) -> CommandArgs:\n    command_args: CommandArgs = []\n    for arg in args:\n        if isinstance(arg, list):\n            command_args.extend(arg)\n        else:\n            command_args.append(arg)\n    return command_args",
    "label": true
  },
  {
    "code": "def scan_module(egg_dir, base, name, stubs):\n    filename = os.path.join(base, name)\n    if filename[:-1] in stubs:\n        return True\n    pkg = base[len(egg_dir) + 1:].replace(os.sep, '.')\n    module = pkg + (pkg and '.' or '') + os.path.splitext(name)[0]\n    if sys.version_info < (3, 7):\n        skip = 12\n    else:\n        skip = 16\n    f = open(filename, 'rb')\n    f.read(skip)\n    code = marshal.load(f)\n    f.close()\n    safe = True\n    symbols = dict.fromkeys(iter_symbols(code))\n    for bad in ['__file__', '__path__']:\n        if bad in symbols:\n            log.warn('%s: module references %s', module, bad)\n            safe = False\n    if 'inspect' in symbols:\n        for bad in ['getsource', 'getabsfile', 'getsourcefile', 'getfilegetsourcelines', 'findsource', 'getcomments', 'getframeinfo', 'getinnerframes', 'getouterframes', 'stack', 'trace']:\n            if bad in symbols:\n                log.warn('%s: module MAY be using inspect.%s', module, bad)\n                safe = False\n    return safe",
    "label": true
  },
  {
    "code": "def ratio_reduce(total: int, ratios: List[int], maximums: List[int], values: List[int]) -> List[int]:\n    ratios = [ratio if _max else 0 for ratio, _max in zip(ratios, maximums)]\n    total_ratio = sum(ratios)\n    if not total_ratio:\n        return values[:]\n    total_remaining = total\n    result: List[int] = []\n    append = result.append\n    for ratio, maximum, value in zip(ratios, maximums, values):\n        if ratio and total_ratio > 0:\n            distributed = min(maximum, round(ratio * total_remaining / total_ratio))\n            append(value - distributed)\n            total_remaining -= distributed\n            total_ratio -= ratio\n        else:\n            append(value)\n    return result",
    "label": true
  },
  {
    "code": "def validate(data: Any) -> bool:\n    with detailed_errors():\n        _validate(data, custom_formats=FORMAT_FUNCTIONS)\n    reduce(lambda acc, fn: fn(acc), EXTRA_VALIDATIONS, data)\n    return True",
    "label": true
  },
  {
    "code": "def find_eggs_in_zip(importer, path_item, only=False):\n    if importer.archive.endswith('.whl'):\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        return\n    for subitem in metadata.resource_listdir(''):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith(('.dist-info', '.egg-info')):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('terminal reporting', 'Reporting', after='general')\n    group.addoption('--durations', action='store', type=int, default=None, metavar='N', help='Show N slowest setup/test durations (N=0 for all)')\n    group.addoption('--durations-min', action='store', type=float, default=0.005, metavar='N', help='Minimal duration in seconds for inclusion in slowest list. Default: 0.005.')",
    "label": true
  },
  {
    "code": "def _encode_invalid_chars(component, allowed_chars, encoding='utf-8'):\n    if component is None:\n        return component\n    component = six.ensure_text(component)\n    component, percent_encodings = PERCENT_RE.subn(lambda match: match.group(0).upper(), component)\n    uri_bytes = component.encode('utf-8', 'surrogatepass')\n    is_percent_encoded = percent_encodings == uri_bytes.count(b'%')\n    encoded_component = bytearray()\n    for i in range(0, len(uri_bytes)):\n        byte = uri_bytes[i:i + 1]\n        byte_ord = ord(byte)\n        if is_percent_encoded and byte == b'%' or (byte_ord < 128 and byte.decode() in allowed_chars):\n            encoded_component += byte\n            continue\n        encoded_component.extend(b'%' + hex(byte_ord)[2:].encode().zfill(2).upper())\n    return encoded_component.decode(encoding)",
    "label": true
  },
  {
    "code": "def _get_option(target_obj: Target, key: str):\n    getter_name = f'get_{key}'\n    by_attribute = functools.partial(getattr, target_obj, key)\n    getter = getattr(target_obj, getter_name, by_attribute)\n    return getter()",
    "label": true
  },
  {
    "code": "def _cmpkey(epoch: int, release: Tuple[int, ...], pre: Optional[Tuple[str, int]], post: Optional[Tuple[str, int]], dev: Optional[Tuple[str, int]], local: Optional[Tuple[SubLocalType]]) -> CmpKey:\n    _release = tuple(reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release)))))\n    if pre is None and post is None and (dev is not None):\n        _pre: PrePostDevType = NegativeInfinity\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n    if post is None:\n        _post: PrePostDevType = NegativeInfinity\n    else:\n        _post = post\n    if dev is None:\n        _dev: PrePostDevType = Infinity\n    else:\n        _dev = dev\n    if local is None:\n        _local: LocalType = NegativeInfinity\n    else:\n        _local = tuple(((i, '') if isinstance(i, int) else (NegativeInfinity, i) for i in local))\n    return (epoch, _release, _pre, _post, _dev, _local)",
    "label": true
  },
  {
    "code": "def clear_caches() -> None:\n    from .environment import get_spontaneous_environment\n    from .lexer import _lexer_cache\n    get_spontaneous_environment.cache_clear()\n    _lexer_cache.clear()",
    "label": true
  },
  {
    "code": "def _check_generic(cls, parameters, elen=_marker):\n    if not elen:\n        raise TypeError(f'{cls} is not a generic class')\n    if elen is _marker:\n        if not hasattr(cls, '__parameters__') or not cls.__parameters__:\n            raise TypeError(f'{cls} is not a generic class')\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, '__parameters__'):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum((isinstance(p, TypeVarTuple) for p in parameters))\n            if num_tv_tuples > 0 and alen >= elen - num_tv_tuples:\n                return\n        raise TypeError(f\"Too {('many' if alen > elen else 'few')} parameters for {cls}; actual {alen}, expected {elen}\")",
    "label": true
  },
  {
    "code": "def rehash(path: str, blocksize: int=1 << 20) -> Tuple[str, str]:\n    h, length = hash_file(path, blocksize)\n    digest = 'sha256=' + urlsafe_b64encode(h.digest()).decode('latin1').rstrip('=')\n    return (digest, str(length))",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    xmlpath = config.option.xmlpath\n    if xmlpath and (not hasattr(config, 'workerinput')):\n        junit_family = config.getini('junit_family')\n        config.stash[xml_key] = LogXML(xmlpath, config.option.junitprefix, config.getini('junit_suite_name'), config.getini('junit_logging'), config.getini('junit_duration_report'), junit_family, config.getini('junit_log_passing_tests'))\n        config.pluginmanager.register(config.stash[xml_key])",
    "label": true
  },
  {
    "code": "def _handle_no_use_pep517(option: Option, opt: str, value: str, parser: OptionParser) -> None:\n    if value is not None:\n        msg = 'A value was passed for --no-use-pep517,\\n        probably using either the PIP_NO_USE_PEP517 environment variable\\n        or the \"no-use-pep517\" config file option. Use an appropriate value\\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\\n        config file option instead.\\n        '\n        raise_option_error(parser, option=option, msg=msg)\n    packages = ('setuptools', 'wheel')\n    if not all((importlib.util.find_spec(package) for package in packages)):\n        msg = f\"It is not possible to use --no-use-pep517 without {' and '.join(packages)} installed.\"\n        raise_option_error(parser, option=option, msg=msg)\n    parser.values.use_pep517 = False",
    "label": true
  },
  {
    "code": "def _make_index_content(response: Response, cache_link_parsing: bool=True) -> IndexContent:\n    encoding = _get_encoding_from_headers(response.headers)\n    return IndexContent(response.content, response.headers['Content-Type'], encoding=encoding, url=response.url, cache_link_parsing=cache_link_parsing)",
    "label": true
  },
  {
    "code": "def _definition_equivalent_to_call(definition: _ParameterSignature, call: _CallSignature) -> bool:\n    if definition.kwargs:\n        if definition.kwargs not in call.starred_kws:\n            return False\n    elif call.starred_kws:\n        return False\n    if definition.varargs:\n        if definition.varargs not in call.starred_args:\n            return False\n    elif call.starred_args:\n        return False\n    if any((kw not in call.kws for kw in definition.kwonlyargs)):\n        return False\n    if definition.args != call.args:\n        return False\n    return all((kw in call.args or kw in definition.kwonlyargs for kw in call.kws))",
    "label": true
  },
  {
    "code": "def add_metaclass(metaclass):\n\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper",
    "label": true
  },
  {
    "code": "def repeatfunc(func, times=None, *args):\n    if times is None:\n        return starmap(func, repeat(args))\n    return starmap(func, repeat(args, times))",
    "label": true
  },
  {
    "code": "def morsel_to_cookie(morsel):\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(time.strptime(morsel['expires'], time_template))\n    return create_cookie(comment=morsel['comment'], comment_url=bool(morsel['comment']), discard=False, domain=morsel['domain'], expires=expires, name=morsel.key, path=morsel['path'], port=None, rest={'HttpOnly': morsel['httponly']}, rfc2109=False, secure=bool(morsel['secure']), value=morsel.value, version=morsel['version'] or 0)",
    "label": true
  },
  {
    "code": "def check_callable(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not callable(value):\n        raise TypeCheckError('is not callable')\n    if args:\n        try:\n            signature = inspect.signature(value)\n        except (TypeError, ValueError):\n            return\n        argument_types = args[0]\n        if isinstance(argument_types, list) and (not any((type(item) is ParamSpec for item in argument_types))):\n            unfulfilled_kwonlyargs = [param.name for param in signature.parameters.values() if param.kind == Parameter.KEYWORD_ONLY and param.default == Parameter.empty]\n            if unfulfilled_kwonlyargs:\n                raise TypeCheckError(f\"has mandatory keyword-only arguments in its declaration: {', '.join(unfulfilled_kwonlyargs)}\")\n            num_positional_args = num_mandatory_pos_args = 0\n            has_varargs = False\n            for param in signature.parameters.values():\n                if param.kind in (Parameter.POSITIONAL_ONLY, Parameter.POSITIONAL_OR_KEYWORD):\n                    num_positional_args += 1\n                    if param.default is Parameter.empty:\n                        num_mandatory_pos_args += 1\n                elif param.kind == Parameter.VAR_POSITIONAL:\n                    has_varargs = True\n            if num_mandatory_pos_args > len(argument_types):\n                raise TypeCheckError(f'has too many mandatory positional arguments in its declaration; expected {len(argument_types)} but {num_mandatory_pos_args} mandatory positional argument(s) declared')\n            elif not has_varargs and num_positional_args < len(argument_types):\n                raise TypeCheckError(f'has too few arguments in its declaration; expected {len(argument_types)} but {num_positional_args} argument(s) declared')",
    "label": true
  },
  {
    "code": "def _make_new_npgettext(func: t.Callable[[str, str, str, int], str]) -> t.Callable[..., str]:\n\n    @pass_context\n    def npgettext(__context: Context, __string_ctx: str, __singular: str, __plural: str, __num: int, **variables: t.Any) -> str:\n        variables.setdefault('context', __string_ctx)\n        variables.setdefault('num', __num)\n        rv = __context.call(func, __string_ctx, __singular, __plural, __num)\n        if __context.eval_ctx.autoescape:\n            rv = Markup(rv)\n        return rv % variables\n    return npgettext",
    "label": true
  },
  {
    "code": "def run():\n    __builtins__\n    script_name = sys.argv[1]\n    namespace = dict(__file__=script_name, __name__='__main__', __doc__=None)\n    sys.argv[:] = sys.argv[1:]\n    open_ = getattr(tokenize, 'open', open)\n    with open_(script_name) as fid:\n        script = fid.read()\n    norm_script = script.replace('\\\\r\\\\n', '\\\\n')\n    code = compile(norm_script, script_name, 'exec')\n    exec(code, namespace)",
    "label": true
  },
  {
    "code": "def _dnsname_match(dn: typing.Any, hostname: str, max_wildcards: int=1) -> typing.Match[str] | None | bool:\n    pats = []\n    if not dn:\n        return False\n    parts = dn.split('.')\n    leftmost = parts[0]\n    remainder = parts[1:]\n    wildcards = leftmost.count('*')\n    if wildcards > max_wildcards:\n        raise CertificateError('too many wildcards in certificate DNS name: ' + repr(dn))\n    if not wildcards:\n        return bool(dn.lower() == hostname.lower())\n    if leftmost == '*':\n        pats.append('[^.]+')\n    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n        pats.append(re.escape(leftmost))\n    else:\n        pats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n    for frag in remainder:\n        pats.append(re.escape(frag))\n    pat = re.compile('\\\\A' + '\\\\.'.join(pats) + '\\\\Z', re.IGNORECASE)\n    return pat.match(hostname)",
    "label": true
  },
  {
    "code": "def build_wheel_legacy(name: str, setup_py_path: str, source_dir: str, global_options: List[str], build_options: List[str], tempd: str) -> Optional[str]:\n    wheel_args = make_setuptools_bdist_wheel_args(setup_py_path, global_options=global_options, build_options=build_options, destination_dir=tempd)\n    spin_message = f'Building wheel for {name} (setup.py)'\n    with open_spinner(spin_message) as spinner:\n        logger.debug('Destination directory: %s', tempd)\n        try:\n            output = call_subprocess(wheel_args, command_desc='python setup.py bdist_wheel', cwd=source_dir, spinner=spinner)\n        except Exception:\n            spinner.finish('error')\n            logger.error('Failed building wheel for %s', name)\n            return None\n        names = os.listdir(tempd)\n        wheel_path = get_legacy_build_wheel_path(names=names, temp_dir=tempd, name=name, command_args=wheel_args, command_output=output)\n        return wheel_path",
    "label": true
  },
  {
    "code": "def patch_error_messages(patch_data: dict):\n    for file_name, file_data in patch_data.items():\n        for checker_name, checker_data in file_data.items():\n            checker = getattr(import_module(file_name), checker_name)\n            if hasattr(checker, 'msgs'):\n                for error_id, new_msg in checker_data.items():\n                    lst_msg = list(checker.msgs[error_id])\n                    lst_msg[0] = new_msg\n                    checker.msgs[error_id] = tuple(lst_msg)\n            else:\n                print('no msgs attribute!')",
    "label": true
  },
  {
    "code": "def _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version",
    "label": true
  },
  {
    "code": "def _infer_str_format_call(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[nodes.Const | util.UninferableBase]:\n    call = arguments.CallSite.from_call(node, context=context)\n    if isinstance(node.func.expr, nodes.Name):\n        value: nodes.Const | None = helpers.safe_infer(node.func.expr)\n        if value is None:\n            return iter([util.Uninferable])\n    else:\n        value = node.func.expr\n    format_template = value.value\n    inferred_positional = [helpers.safe_infer(i, context) for i in call.positional_arguments]\n    if not all((isinstance(i, nodes.Const) for i in inferred_positional)):\n        return iter([util.Uninferable])\n    pos_values: list[str] = [i.value for i in inferred_positional]\n    inferred_keyword = {k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()}\n    if not all((isinstance(i, nodes.Const) for i in inferred_keyword.values())):\n        return iter([util.Uninferable])\n    keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}\n    try:\n        formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n        return iter([util.Uninferable])\n    return iter([nodes.const_factory(formatted_string)])",
    "label": true
  },
  {
    "code": "def canonicalize_version(version: Union[Version, str], *, strip_trailing_zero: bool=True) -> str:\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            return version\n    else:\n        parsed = version\n    parts = []\n    if parsed.epoch != 0:\n        parts.append(f'{parsed.epoch}!')\n    release_segment = '.'.join((str(x) for x in parsed.release))\n    if strip_trailing_zero:\n        release_segment = re.sub('(\\\\.0)+$', '', release_segment)\n    parts.append(release_segment)\n    if parsed.pre is not None:\n        parts.append(''.join((str(x) for x in parsed.pre)))\n    if parsed.post is not None:\n        parts.append(f'.post{parsed.post}')\n    if parsed.dev is not None:\n        parts.append(f'.dev{parsed.dev}')\n    if parsed.local is not None:\n        parts.append(f'+{parsed.local}')\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def pretty_repr(_object: Any, *, max_width: int=80, indent_size: int=4, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> str:\n    if _safe_isinstance(_object, Node):\n        node = _object\n    else:\n        node = traverse(_object, max_length=max_length, max_string=max_string, max_depth=max_depth)\n    repr_str: str = node.render(max_width=max_width, indent_size=indent_size, expand_all=expand_all)\n    return repr_str",
    "label": true
  },
  {
    "code": "def _check_method_and_attr_name(node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not (_is_in_snake_case(name) or (name.startswith('__') and _is_in_snake_case(name[2:]))):\n        error_msgs.append(f'''{node_type.capitalize()} name \"{name}\" should be in snake_case format. {node_type.capitalize()} names should be lowercase, with words separated by underscores. A single leading underscore can be used to denote a private {node_type} while a double leading underscore invokes Python's name-mangling rules.''')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def _clear_cached_macosx_ver():\n    global _syscfg_macosx_ver\n    _syscfg_macosx_ver = None",
    "label": true
  },
  {
    "code": "def _is_linux_armhf(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.Arm) and (f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5) and (f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD)",
    "label": true
  },
  {
    "code": "def process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))",
    "label": true
  },
  {
    "code": "def _override_config(linter: PyLinter, config_location: AnyStr) -> None:\n    linter.set_current_module(config_location)\n    config_file_parser = _ConfigurationFileParser(verbose=True, linter=linter)\n    try:\n        _, config_args = config_file_parser.parse_config_file(file_path=config_location)\n    except OSError as ex:\n        print(ex, file=sys.stderr)\n        sys.exit(32)\n    try:\n        linter._parse_configuration_file(config_args)\n    except _UnrecognizedOptionError as exc:\n        unrecognized_options_message = ', '.join(exc.options)\n        linter.add_message('unrecognized-option', args=unrecognized_options_message, line=0)\n    linter._emit_stashed_messages()\n    linter.config_file = config_location",
    "label": true
  },
  {
    "code": "def extract_constant(code, symbol, default=-1):\n    if symbol not in code.co_names:\n        return None\n    name_idx = list(code.co_names).index(symbol)\n    STORE_NAME = 90\n    STORE_GLOBAL = 97\n    LOAD_CONST = 100\n    const = default\n    for byte_code in dis.Bytecode(code):\n        op = byte_code.opcode\n        arg = byte_code.arg\n        if op == LOAD_CONST:\n            const = code.co_consts[arg]\n        elif arg == name_idx and (op == STORE_NAME or op == STORE_GLOBAL):\n            return const\n        else:\n            const = default",
    "label": true
  },
  {
    "code": "def get_scope_node(node: nodes.Node, scope: Scope) -> Optional[Union[nodes.Item, nodes.Collector]]:\n    import _pytest.python\n    if scope is Scope.Function:\n        return node.getparent(nodes.Item)\n    elif scope is Scope.Class:\n        return node.getparent(_pytest.python.Class)\n    elif scope is Scope.Module:\n        return node.getparent(_pytest.python.Module)\n    elif scope is Scope.Package:\n        return node.getparent(_pytest.python.Package)\n    elif scope is Scope.Session:\n        return node.getparent(_pytest.main.Session)\n    else:\n        assert_never(scope)",
    "label": true
  },
  {
    "code": "def non_empty_lines(path):\n    with open(path) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line",
    "label": true
  },
  {
    "code": "def _python_build():\n    if _sys_home:\n        return _is_python_source_dir(_sys_home)\n    return _is_python_source_dir(project_base)",
    "label": true
  },
  {
    "code": "def _debug(msg: str) -> None:\n    if not DEBUG_CONTRACTS:\n        return\n    print('[PyTA]', msg, file=sys.stderr)",
    "label": true
  },
  {
    "code": "def _get_pdata_path(base_name: Path, recurs: int, pylint_home: Path=PYLINT_HOME_AS_PATH) -> Path:\n    underscored_name = '_'.join((str(p.replace(':', '_').replace('/', '_').replace('\\\\', '_')) for p in base_name.parts))\n    return pylint_home / f'{underscored_name}_{recurs}.stats'",
    "label": true
  },
  {
    "code": "def split_after(iterable, pred, maxsplit=-1):\n    if maxsplit == 0:\n        yield list(iterable)\n        return\n    buf = []\n    it = iter(iterable)\n    for item in it:\n        buf.append(item)\n        if pred(item) and buf:\n            yield buf\n            if maxsplit == 1:\n                buf = list(it)\n                if buf:\n                    yield buf\n                return\n            buf = []\n            maxsplit -= 1\n    if buf:\n        yield buf",
    "label": true
  },
  {
    "code": "def _py_version_validator(_: Any, name: str, value: Any) -> tuple[int, int, int]:\n    if not isinstance(value, tuple):\n        try:\n            value = tuple((int(val) for val in value.split('.')))\n        except (ValueError, AttributeError):\n            raise optparse.OptionValueError(f\"Invalid format for {name}, should be version string. E.g., '3.8'\") from None\n    return value",
    "label": true
  },
  {
    "code": "def _iter_lexerclasses(plugins=True):\n    for key in sorted(LEXERS):\n        module_name, name = LEXERS[key][:2]\n        if name not in _lexer_cache:\n            _load_lexers(module_name)\n        yield _lexer_cache[name]\n    if plugins:\n        yield from find_plugin_lexers()",
    "label": true
  },
  {
    "code": "def _base_class_object_build(node: nodes.Module | nodes.ClassDef, member: type, basenames: list[str], name: str | None=None, localname: str | None=None) -> nodes.ClassDef:\n    class_name = name or getattr(member, '__name__', None) or localname\n    assert isinstance(class_name, str)\n    klass = build_class(class_name, basenames, member.__doc__)\n    klass._newstyle = isinstance(member, type)\n    node.add_local_node(klass, localname)\n    try:\n        if issubclass(member, Exception):\n            instdict = member().__dict__\n        else:\n            raise TypeError\n    except TypeError:\n        pass\n    else:\n        for item_name, obj in instdict.items():\n            valnode = nodes.EmptyNode()\n            valnode.object = obj\n            valnode.parent = klass\n            valnode.lineno = 1\n            klass.instance_attrs[item_name] = [valnode]\n    return klass",
    "label": true
  },
  {
    "code": "def as_base_candidate(candidate: Candidate) -> Optional[BaseCandidate]:\n    base_candidate_classes = (AlreadyInstalledCandidate, EditableCandidate, LinkCandidate)\n    if isinstance(candidate, base_candidate_classes):\n        return candidate\n    return None",
    "label": true
  },
  {
    "code": "def wrap_file(file: BinaryIO, total: int, *, description: str='Reading...', auto_refresh: bool=True, console: Optional[Console]=None, transient: bool=False, get_time: Optional[Callable[[], float]]=None, refresh_per_second: float=10, style: StyleType='bar.back', complete_style: StyleType='bar.complete', finished_style: StyleType='bar.finished', pulse_style: StyleType='bar.pulse', disable: bool=False) -> ContextManager[BinaryIO]:\n    columns: List['ProgressColumn'] = [TextColumn('[progress.description]{task.description}')] if description else []\n    columns.extend((BarColumn(style=style, complete_style=complete_style, finished_style=finished_style, pulse_style=pulse_style), DownloadColumn(), TimeRemainingColumn()))\n    progress = Progress(*columns, auto_refresh=auto_refresh, console=console, transient=transient, get_time=get_time, refresh_per_second=refresh_per_second or 10, disable=disable)\n    reader = progress.wrap_file(file, total=total, description=description)\n    return _ReadContext(progress, reader)",
    "label": true
  },
  {
    "code": "def maybe_delete_a_numbered_dir(path: Path) -> None:\n    path = ensure_extended_length_path(path)\n    lock_path = None\n    try:\n        lock_path = create_cleanup_lock(path)\n        parent = path.parent\n        garbage = parent.joinpath(f'garbage-{uuid.uuid4()}')\n        path.rename(garbage)\n        rm_rf(garbage)\n    except OSError:\n        return\n    finally:\n        if lock_path is not None:\n            try:\n                lock_path.unlink()\n            except OSError:\n                pass",
    "label": true
  },
  {
    "code": "def _selection_sort_by_len(lst: list[str]) -> None:\n    for i in range(len(lst)):\n        mini = i\n        for j in range(i, len(lst)):\n            if len(lst[j]) < len(lst[mini]):\n                mini = j\n        lst[i], lst[mini] = (lst[mini], lst[i])",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def const_factory(value: Any) -> ConstFactoryResult:\n    assert not isinstance(value, NodeNG)\n    if value.__class__ not in CONST_CLS:\n        node = EmptyNode()\n        node.object = value\n        return node\n    instance: List | Set | Tuple | Dict\n    initializer_cls = CONST_CLS[value.__class__]\n    if issubclass(initializer_cls, (List, Set, Tuple)):\n        instance = initializer_cls()\n        instance.postinit(_create_basic_elements(value, instance))\n        return instance\n    if issubclass(initializer_cls, Dict):\n        instance = initializer_cls()\n        instance.postinit(_create_dict_items(value, instance))\n        return instance\n    return Const(value)",
    "label": true
  },
  {
    "code": "def suffixed_err(src: str, pos: Pos, msg: str) -> TOMLDecodeError:\n\n    def coord_repr(src: str, pos: Pos) -> str:\n        if pos >= len(src):\n            return 'end of document'\n        line = src.count('\\n', 0, pos) + 1\n        if line == 1:\n            column = pos + 1\n        else:\n            column = pos - src.rindex('\\n', 0, pos)\n        return f'line {line}, column {column}'\n    return TOMLDecodeError(f'{msg} (at {coord_repr(src, pos)})')",
    "label": true
  },
  {
    "code": "def _resolve_context(cli: BaseCommand, ctx_args: t.MutableMapping[str, t.Any], prog_name: str, args: t.List[str]) -> Context:\n    ctx_args['resilient_parsing'] = True\n    ctx = cli.make_context(prog_name, args.copy(), **ctx_args)\n    args = ctx.protected_args + ctx.args\n    while args:\n        command = ctx.command\n        if isinstance(command, MultiCommand):\n            if not command.chain:\n                name, cmd, args = command.resolve_command(ctx, args)\n                if cmd is None:\n                    return ctx\n                ctx = cmd.make_context(name, args, parent=ctx, resilient_parsing=True)\n                args = ctx.protected_args + ctx.args\n            else:\n                sub_ctx = ctx\n                while args:\n                    name, cmd, args = command.resolve_command(ctx, args)\n                    if cmd is None:\n                        return ctx\n                    sub_ctx = cmd.make_context(name, args, parent=ctx, allow_extra_args=True, allow_interspersed_args=False, resilient_parsing=True)\n                    args = sub_ctx.args\n                ctx = sub_ctx\n                args = [*sub_ctx.protected_args, *sub_ctx.args]\n        else:\n            break\n    return ctx",
    "label": true
  },
  {
    "code": "def remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)",
    "label": true
  },
  {
    "code": "def _expand_numparse(disable_numparse, column_count):\n    if isinstance(disable_numparse, Iterable):\n        numparses = [True] * column_count\n        for index in disable_numparse:\n            numparses[index] = False\n        return numparses\n    else:\n        return [not disable_numparse] * column_count",
    "label": true
  },
  {
    "code": "def _infer_map(node: nodes.Dict, context: InferenceContext | None) -> dict[SuccessfulInferenceResult, SuccessfulInferenceResult]:\n    values: dict[SuccessfulInferenceResult, SuccessfulInferenceResult] = {}\n    for name, value in node.items:\n        if isinstance(name, nodes.DictUnpack):\n            double_starred = helpers.safe_infer(value, context)\n            if not double_starred:\n                raise InferenceError\n            if not isinstance(double_starred, nodes.Dict):\n                raise InferenceError(node=node, context=context)\n            unpack_items = _infer_map(double_starred, context)\n            values = _update_with_replacement(values, unpack_items)\n        else:\n            key = helpers.safe_infer(name, context=context)\n            safe_value = helpers.safe_infer(value, context=context)\n            if any((not elem for elem in (key, safe_value))):\n                raise InferenceError(node=node, context=context)\n            values = _update_with_replacement(values, {key: safe_value})\n    return values",
    "label": true
  },
  {
    "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = ('', '')\n    return auth",
    "label": true
  },
  {
    "code": "def is_response_to_head(response):\n    method = response._method\n    if isinstance(method, int):\n        return method == 3\n    return method.upper() == 'HEAD'",
    "label": true
  },
  {
    "code": "def check_dist_restriction(options: Values, check_target: bool=False) -> None:\n    dist_restriction_set = any([options.python_version, options.platforms, options.abis, options.implementation])\n    binary_only = FormatControl(set(), {':all:'})\n    sdist_dependencies_allowed = options.format_control != binary_only and (not options.ignore_dependencies)\n    if dist_restriction_set and sdist_dependencies_allowed:\n        raise CommandError('When restricting platform and interpreter constraints using --python-version, --platform, --abi, or --implementation, either --no-deps must be set, or --only-binary=:all: must be set and --no-binary must not be set (or must be set to :none:).')\n    if check_target:\n        if dist_restriction_set and (not options.target_dir):\n            raise CommandError(\"Can not use any platform or abi specific options unless installing via '--target'\")",
    "label": true
  },
  {
    "code": "def build_sdist(sdist_directory, config_settings):\n    backend = _build_backend()\n    try:\n        return backend.build_sdist(sdist_directory, config_settings)\n    except getattr(backend, 'UnsupportedOperation', _DummyException):\n        raise GotUnsupportedOperation(traceback.format_exc())",
    "label": true
  },
  {
    "code": "def _get_env(environment: Dict[str, str], name: str) -> str:\n    value: Union[str, Undefined] = environment.get(name, _undefined)\n    if isinstance(value, Undefined):\n        raise UndefinedEnvironmentName(f'{name!r} does not exist in evaluation environment.')\n    return value",
    "label": true
  },
  {
    "code": "def skip_until(src: str, pos: Pos, expect: str, *, error_on: frozenset[str], error_on_eof: bool) -> Pos:\n    try:\n        new_pos = src.index(expect, pos)\n    except ValueError:\n        new_pos = len(src)\n        if error_on_eof:\n            raise suffixed_err(src, new_pos, f'Expected {expect!r}') from None\n    if not error_on.isdisjoint(src[pos:new_pos]):\n        while src[pos] not in error_on:\n            pos += 1\n        raise suffixed_err(src, pos, f'Found invalid character {src[pos]!r}')\n    return new_pos",
    "label": true
  },
  {
    "code": "def get_filter(unicode: str) -> Optional[Filter]:\n    unicode = unicode.lower()\n    if unicode == 'd':\n        return DurationFilter()\n    elif unicode == 'l':\n        return LocationFilter()\n    elif unicode == 'c':\n        return CustomerFilter()\n    elif unicode == 'r':\n        return ResetFilter()\n    return None",
    "label": true
  },
  {
    "code": "def main(argv: Optional[List[str]]=None) -> None:\n    parser = argparse.ArgumentParser(description='Takes one or more file paths and reports their detected encodings')\n    parser.add_argument('input', help='File whose encoding we would like to determine. (default: stdin)', type=argparse.FileType('rb'), nargs='*', default=[sys.stdin.buffer])\n    parser.add_argument('--minimal', help='Print only the encoding to standard output', action='store_true')\n    parser.add_argument('-l', '--legacy', help='Rename legacy encodings to more modern ones.', action='store_true')\n    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')\n    args = parser.parse_args(argv)\n    for f in args.input:\n        if f.isatty():\n            print('You are running chardetect interactively. Press CTRL-D twice at the start of a blank line to signal the end of your input. If you want help, run chardetect --help\\n', file=sys.stderr)\n        print(description_of(f, f.name, minimal=args.minimal, should_rename_legacy=args.legacy))",
    "label": true
  },
  {
    "code": "def validate_invariants(obj: object) -> None:\n    klass = obj.__class__\n    klass_mod = _get_module(klass)\n    try:\n        _check_invariants(obj, klass, klass_mod.__dict__)\n    except PyTAContractError as e:\n        raise AssertionError(str(e)) from None",
    "label": true
  },
  {
    "code": "def pytest_generate_tests(metafunc: 'Metafunc') -> None:\n    for marker in metafunc.definition.iter_markers(name='parametrize'):\n        metafunc.parametrize(*marker.args, **marker.kwargs, _param_mark=marker)",
    "label": true
  },
  {
    "code": "def _req_set_item_sorter(item: Tuple[str, InstallRequirement], weights: Dict[Optional[str], int]) -> Tuple[int, str]:\n    name = canonicalize_name(item[0])\n    return (weights[name], name)",
    "label": true
  },
  {
    "code": "def _get_python_version(version: str) -> PythonVersion:\n    if len(version) > 1:\n        return (int(version[0]), int(version[1:]))\n    else:\n        return (int(version[0]),)",
    "label": true
  },
  {
    "code": "def make_setuptools_egg_info_args(setup_py_path: str, egg_info_dir: Optional[str], no_user_config: bool) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, no_user_config=no_user_config)\n    args += ['egg_info']\n    if egg_info_dir:\n        args += ['--egg-base', egg_info_dir]\n    return args",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e266(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip('#'))\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1] + \"  # THERE SHOULD ONLY BE ONE '#'\")\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def parse_header_links(value):\n    links = []\n    replace_chars = ' \\'\"'\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(', *<', value):\n        try:\n            url, params = val.split(';', 1)\n        except ValueError:\n            url, params = (val, '')\n        link = {'url': url.strip('<> \\'\"')}\n        for param in params.split(';'):\n            try:\n                key, value = param.split('=')\n            except ValueError:\n                break\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n        links.append(link)\n    return links",
    "label": true
  },
  {
    "code": "def remove_successive(all_couples: CplIndexToCplLines_T) -> None:\n    couple: LineSetStartCouple\n    for couple in tuple(all_couples.keys()):\n        to_remove = []\n        test = couple.increment(Index(1))\n        while test in all_couples:\n            all_couples[couple].first_file.end = all_couples[test].first_file.end\n            all_couples[couple].second_file.end = all_couples[test].second_file.end\n            all_couples[couple].effective_cmn_lines_nb += 1\n            to_remove.append(test)\n            test = test.increment(Index(1))\n        for target in to_remove:\n            try:\n                all_couples.pop(target)\n            except KeyError:\n                pass",
    "label": true
  },
  {
    "code": "def _const_compare_digest_backport(a, b):\n    result = abs(len(a) - len(b))\n    for left, right in zip(bytearray(a), bytearray(b)):\n        result |= left ^ right\n    return result == 0",
    "label": true
  },
  {
    "code": "def _simple_escaping_wrapper(func: 't.Callable[_P, str]') -> 't.Callable[_P, Markup]':\n\n    @functools.wraps(func)\n    def wrapped(self: 'Markup', *args: '_P.args', **kwargs: '_P.kwargs') -> 'Markup':\n        arg_list = _escape_argspec(list(args), enumerate(args), self.escape)\n        _escape_argspec(kwargs, kwargs.items(), self.escape)\n        return self.__class__(func(self, *arg_list, **kwargs))\n    return wrapped",
    "label": true
  },
  {
    "code": "def _getoption():\n    var = (('acd', 'acd'), ('ai', 'ai'), ('akm', 'akm'), ('al', 'al'), ('aleph', 'aleph'), ('allowrevins', 'allowrevins'), ('altkeymap', 'altkeymap'), ('ambiwidth', 'ambiwidth'), ('ambw', 'ambw'), ('anti', 'anti'), ('antialias', 'antialias'), ('ar', 'ar'), ('arab', 'arab'), ('arabic', 'arabic'), ('arabicshape', 'arabicshape'), ('ari', 'ari'), ('arshape', 'arshape'), ('autochdir', 'autochdir'), ('autoindent', 'autoindent'), ('autoread', 'autoread'), ('autowrite', 'autowrite'), ('autowriteall', 'autowriteall'), ('aw', 'aw'), ('awa', 'awa'), ('background', 'background'), ('backspace', 'backspace'), ('backup', 'backup'), ('backupcopy', 'backupcopy'), ('backupdir', 'backupdir'), ('backupext', 'backupext'), ('backupskip', 'backupskip'), ('balloondelay', 'balloondelay'), ('ballooneval', 'ballooneval'), ('balloonexpr', 'balloonexpr'), ('bdir', 'bdir'), ('bdlay', 'bdlay'), ('beval', 'beval'), ('bex', 'bex'), ('bexpr', 'bexpr'), ('bg', 'bg'), ('bh', 'bh'), ('bin', 'bin'), ('binary', 'binary'), ('biosk', 'biosk'), ('bioskey', 'bioskey'), ('bk', 'bk'), ('bkc', 'bkc'), ('bl', 'bl'), ('bomb', 'bomb'), ('breakat', 'breakat'), ('brk', 'brk'), ('browsedir', 'browsedir'), ('bs', 'bs'), ('bsdir', 'bsdir'), ('bsk', 'bsk'), ('bt', 'bt'), ('bufhidden', 'bufhidden'), ('buflisted', 'buflisted'), ('buftype', 'buftype'), ('casemap', 'casemap'), ('cb', 'cb'), ('cc', 'cc'), ('ccv', 'ccv'), ('cd', 'cd'), ('cdpath', 'cdpath'), ('cedit', 'cedit'), ('cf', 'cf'), ('cfu', 'cfu'), ('ch', 'ch'), ('charconvert', 'charconvert'), ('ci', 'ci'), ('cin', 'cin'), ('cindent', 'cindent'), ('cink', 'cink'), ('cinkeys', 'cinkeys'), ('cino', 'cino'), ('cinoptions', 'cinoptions'), ('cinw', 'cinw'), ('cinwords', 'cinwords'), ('clipboard', 'clipboard'), ('cmdheight', 'cmdheight'), ('cmdwinheight', 'cmdwinheight'), ('cmp', 'cmp'), ('cms', 'cms'), ('co', 'co'), ('cocu', 'cocu'), ('cole', 'cole'), ('colorcolumn', 'colorcolumn'), ('columns', 'columns'), ('com', 'com'), ('comments', 'comments'), ('commentstring', 'commentstring'), ('compatible', 'compatible'), ('complete', 'complete'), ('completefunc', 'completefunc'), ('completeopt', 'completeopt'), ('concealcursor', 'concealcursor'), ('conceallevel', 'conceallevel'), ('confirm', 'confirm'), ('consk', 'consk'), ('conskey', 'conskey'), ('copyindent', 'copyindent'), ('cot', 'cot'), ('cp', 'cp'), ('cpo', 'cpo'), ('cpoptions', 'cpoptions'), ('cpt', 'cpt'), ('crb', 'crb'), ('cryptmethod', 'cryptmethod'), ('cscopepathcomp', 'cscopepathcomp'), ('cscopeprg', 'cscopeprg'), ('cscopequickfix', 'cscopequickfix'), ('cscoperelative', 'cscoperelative'), ('cscopetag', 'cscopetag'), ('cscopetagorder', 'cscopetagorder'), ('cscopeverbose', 'cscopeverbose'), ('cspc', 'cspc'), ('csprg', 'csprg'), ('csqf', 'csqf'), ('csre', 'csre'), ('cst', 'cst'), ('csto', 'csto'), ('csverb', 'csverb'), ('cuc', 'cuc'), ('cul', 'cul'), ('cursorbind', 'cursorbind'), ('cursorcolumn', 'cursorcolumn'), ('cursorline', 'cursorline'), ('cwh', 'cwh'), ('debug', 'debug'), ('deco', 'deco'), ('def', 'def'), ('define', 'define'), ('delcombine', 'delcombine'), ('dex', 'dex'), ('dg', 'dg'), ('dict', 'dict'), ('dictionary', 'dictionary'), ('diff', 'diff'), ('diffexpr', 'diffexpr'), ('diffopt', 'diffopt'), ('digraph', 'digraph'), ('dip', 'dip'), ('dir', 'dir'), ('directory', 'directory'), ('display', 'display'), ('dy', 'dy'), ('ea', 'ea'), ('ead', 'ead'), ('eadirection', 'eadirection'), ('eb', 'eb'), ('ed', 'ed'), ('edcompatible', 'edcompatible'), ('ef', 'ef'), ('efm', 'efm'), ('ei', 'ei'), ('ek', 'ek'), ('enc', 'enc'), ('encoding', 'encoding'), ('endofline', 'endofline'), ('eol', 'eol'), ('ep', 'ep'), ('equalalways', 'equalalways'), ('equalprg', 'equalprg'), ('errorbells', 'errorbells'), ('errorfile', 'errorfile'), ('errorformat', 'errorformat'), ('esckeys', 'esckeys'), ('et', 'et'), ('eventignore', 'eventignore'), ('ex', 'ex'), ('expandtab', 'expandtab'), ('exrc', 'exrc'), ('fcl', 'fcl'), ('fcs', 'fcs'), ('fdc', 'fdc'), ('fde', 'fde'), ('fdi', 'fdi'), ('fdl', 'fdl'), ('fdls', 'fdls'), ('fdm', 'fdm'), ('fdn', 'fdn'), ('fdo', 'fdo'), ('fdt', 'fdt'), ('fen', 'fen'), ('fenc', 'fenc'), ('fencs', 'fencs'), ('fex', 'fex'), ('ff', 'ff'), ('ffs', 'ffs'), ('fic', 'fic'), ('fileencoding', 'fileencoding'), ('fileencodings', 'fileencodings'), ('fileformat', 'fileformat'), ('fileformats', 'fileformats'), ('fileignorecase', 'fileignorecase'), ('filetype', 'filetype'), ('fillchars', 'fillchars'), ('fk', 'fk'), ('fkmap', 'fkmap'), ('flp', 'flp'), ('fml', 'fml'), ('fmr', 'fmr'), ('fo', 'fo'), ('foldclose', 'foldclose'), ('foldcolumn', 'foldcolumn'), ('foldenable', 'foldenable'), ('foldexpr', 'foldexpr'), ('foldignore', 'foldignore'), ('foldlevel', 'foldlevel'), ('foldlevelstart', 'foldlevelstart'), ('foldmarker', 'foldmarker'), ('foldmethod', 'foldmethod'), ('foldminlines', 'foldminlines'), ('foldnestmax', 'foldnestmax'), ('foldopen', 'foldopen'), ('foldtext', 'foldtext'), ('formatexpr', 'formatexpr'), ('formatlistpat', 'formatlistpat'), ('formatoptions', 'formatoptions'), ('formatprg', 'formatprg'), ('fp', 'fp'), ('fs', 'fs'), ('fsync', 'fsync'), ('ft', 'ft'), ('gcr', 'gcr'), ('gd', 'gd'), ('gdefault', 'gdefault'), ('gfm', 'gfm'), ('gfn', 'gfn'), ('gfs', 'gfs'), ('gfw', 'gfw'), ('ghr', 'ghr'), ('go', 'go'), ('gp', 'gp'), ('grepformat', 'grepformat'), ('grepprg', 'grepprg'), ('gtl', 'gtl'), ('gtt', 'gtt'), ('guicursor', 'guicursor'), ('guifont', 'guifont'), ('guifontset', 'guifontset'), ('guifontwide', 'guifontwide'), ('guiheadroom', 'guiheadroom'), ('guioptions', 'guioptions'), ('guipty', 'guipty'), ('guitablabel', 'guitablabel'), ('guitabtooltip', 'guitabtooltip'), ('helpfile', 'helpfile'), ('helpheight', 'helpheight'), ('helplang', 'helplang'), ('hf', 'hf'), ('hh', 'hh'), ('hi', 'hi'), ('hid', 'hid'), ('hidden', 'hidden'), ('highlight', 'highlight'), ('history', 'history'), ('hk', 'hk'), ('hkmap', 'hkmap'), ('hkmapp', 'hkmapp'), ('hkp', 'hkp'), ('hl', 'hl'), ('hlg', 'hlg'), ('hls', 'hls'), ('hlsearch', 'hlsearch'), ('ic', 'ic'), ('icon', 'icon'), ('iconstring', 'iconstring'), ('ignorecase', 'ignorecase'), ('im', 'im'), ('imactivatefunc', 'imactivatefunc'), ('imactivatekey', 'imactivatekey'), ('imaf', 'imaf'), ('imak', 'imak'), ('imc', 'imc'), ('imcmdline', 'imcmdline'), ('imd', 'imd'), ('imdisable', 'imdisable'), ('imi', 'imi'), ('iminsert', 'iminsert'), ('ims', 'ims'), ('imsearch', 'imsearch'), ('imsf', 'imsf'), ('imstatusfunc', 'imstatusfunc'), ('inc', 'inc'), ('include', 'include'), ('includeexpr', 'includeexpr'), ('incsearch', 'incsearch'), ('inde', 'inde'), ('indentexpr', 'indentexpr'), ('indentkeys', 'indentkeys'), ('indk', 'indk'), ('inex', 'inex'), ('inf', 'inf'), ('infercase', 'infercase'), ('inoremap', 'inoremap'), ('insertmode', 'insertmode'), ('invacd', 'invacd'), ('invai', 'invai'), ('invakm', 'invakm'), ('invallowrevins', 'invallowrevins'), ('invaltkeymap', 'invaltkeymap'), ('invanti', 'invanti'), ('invantialias', 'invantialias'), ('invar', 'invar'), ('invarab', 'invarab'), ('invarabic', 'invarabic'), ('invarabicshape', 'invarabicshape'), ('invari', 'invari'), ('invarshape', 'invarshape'), ('invautochdir', 'invautochdir'), ('invautoindent', 'invautoindent'), ('invautoread', 'invautoread'), ('invautowrite', 'invautowrite'), ('invautowriteall', 'invautowriteall'), ('invaw', 'invaw'), ('invawa', 'invawa'), ('invbackup', 'invbackup'), ('invballooneval', 'invballooneval'), ('invbeval', 'invbeval'), ('invbin', 'invbin'), ('invbinary', 'invbinary'), ('invbiosk', 'invbiosk'), ('invbioskey', 'invbioskey'), ('invbk', 'invbk'), ('invbl', 'invbl'), ('invbomb', 'invbomb'), ('invbuflisted', 'invbuflisted'), ('invcf', 'invcf'), ('invci', 'invci'), ('invcin', 'invcin'), ('invcindent', 'invcindent'), ('invcompatible', 'invcompatible'), ('invconfirm', 'invconfirm'), ('invconsk', 'invconsk'), ('invconskey', 'invconskey'), ('invcopyindent', 'invcopyindent'), ('invcp', 'invcp'), ('invcrb', 'invcrb'), ('invcscoperelative', 'invcscoperelative'), ('invcscopetag', 'invcscopetag'), ('invcscopeverbose', 'invcscopeverbose'), ('invcsre', 'invcsre'), ('invcst', 'invcst'), ('invcsverb', 'invcsverb'), ('invcuc', 'invcuc'), ('invcul', 'invcul'), ('invcursorbind', 'invcursorbind'), ('invcursorcolumn', 'invcursorcolumn'), ('invcursorline', 'invcursorline'), ('invdeco', 'invdeco'), ('invdelcombine', 'invdelcombine'), ('invdg', 'invdg'), ('invdiff', 'invdiff'), ('invdigraph', 'invdigraph'), ('invea', 'invea'), ('inveb', 'inveb'), ('inved', 'inved'), ('invedcompatible', 'invedcompatible'), ('invek', 'invek'), ('invendofline', 'invendofline'), ('inveol', 'inveol'), ('invequalalways', 'invequalalways'), ('inverrorbells', 'inverrorbells'), ('invesckeys', 'invesckeys'), ('invet', 'invet'), ('invex', 'invex'), ('invexpandtab', 'invexpandtab'), ('invexrc', 'invexrc'), ('invfen', 'invfen'), ('invfic', 'invfic'), ('invfileignorecase', 'invfileignorecase'), ('invfk', 'invfk'), ('invfkmap', 'invfkmap'), ('invfoldenable', 'invfoldenable'), ('invgd', 'invgd'), ('invgdefault', 'invgdefault'), ('invguipty', 'invguipty'), ('invhid', 'invhid'), ('invhidden', 'invhidden'), ('invhk', 'invhk'), ('invhkmap', 'invhkmap'), ('invhkmapp', 'invhkmapp'), ('invhkp', 'invhkp'), ('invhls', 'invhls'), ('invhlsearch', 'invhlsearch'), ('invic', 'invic'), ('invicon', 'invicon'), ('invignorecase', 'invignorecase'), ('invim', 'invim'), ('invimc', 'invimc'), ('invimcmdline', 'invimcmdline'), ('invimd', 'invimd'), ('invimdisable', 'invimdisable'), ('invincsearch', 'invincsearch'), ('invinf', 'invinf'), ('invinfercase', 'invinfercase'), ('invinsertmode', 'invinsertmode'), ('invis', 'invis'), ('invjoinspaces', 'invjoinspaces'), ('invjs', 'invjs'), ('invlazyredraw', 'invlazyredraw'), ('invlbr', 'invlbr'), ('invlinebreak', 'invlinebreak'), ('invlisp', 'invlisp'), ('invlist', 'invlist'), ('invloadplugins', 'invloadplugins'), ('invlpl', 'invlpl'), ('invlz', 'invlz'), ('invma', 'invma'), ('invmacatsui', 'invmacatsui'), ('invmagic', 'invmagic'), ('invmh', 'invmh'), ('invml', 'invml'), ('invmod', 'invmod'), ('invmodeline', 'invmodeline'), ('invmodifiable', 'invmodifiable'), ('invmodified', 'invmodified'), ('invmore', 'invmore'), ('invmousef', 'invmousef'), ('invmousefocus', 'invmousefocus'), ('invmousehide', 'invmousehide'), ('invnu', 'invnu'), ('invnumber', 'invnumber'), ('invodev', 'invodev'), ('invopendevice', 'invopendevice'), ('invpaste', 'invpaste'), ('invpi', 'invpi'), ('invpreserveindent', 'invpreserveindent'), ('invpreviewwindow', 'invpreviewwindow'), ('invprompt', 'invprompt'), ('invpvw', 'invpvw'), ('invreadonly', 'invreadonly'), ('invrelativenumber', 'invrelativenumber'), ('invremap', 'invremap'), ('invrestorescreen', 'invrestorescreen'), ('invrevins', 'invrevins'), ('invri', 'invri'), ('invrightleft', 'invrightleft'), ('invrl', 'invrl'), ('invrnu', 'invrnu'), ('invro', 'invro'), ('invrs', 'invrs'), ('invru', 'invru'), ('invruler', 'invruler'), ('invsb', 'invsb'), ('invsc', 'invsc'), ('invscb', 'invscb'), ('invscrollbind', 'invscrollbind'), ('invscs', 'invscs'), ('invsecure', 'invsecure'), ('invsft', 'invsft'), ('invshellslash', 'invshellslash'), ('invshelltemp', 'invshelltemp'), ('invshiftround', 'invshiftround'), ('invshortname', 'invshortname'), ('invshowcmd', 'invshowcmd'), ('invshowfulltag', 'invshowfulltag'), ('invshowmatch', 'invshowmatch'), ('invshowmode', 'invshowmode'), ('invsi', 'invsi'), ('invsm', 'invsm'), ('invsmartcase', 'invsmartcase'), ('invsmartindent', 'invsmartindent'), ('invsmarttab', 'invsmarttab'), ('invsmd', 'invsmd'), ('invsn', 'invsn'), ('invsol', 'invsol'), ('invspell', 'invspell'), ('invsplitbelow', 'invsplitbelow'), ('invsplitright', 'invsplitright'), ('invspr', 'invspr'), ('invsr', 'invsr'), ('invssl', 'invssl'), ('invsta', 'invsta'), ('invstartofline', 'invstartofline'), ('invstmp', 'invstmp'), ('invswapfile', 'invswapfile'), ('invswf', 'invswf'), ('invta', 'invta'), ('invtagbsearch', 'invtagbsearch'), ('invtagrelative', 'invtagrelative'), ('invtagstack', 'invtagstack'), ('invtbi', 'invtbi'), ('invtbidi', 'invtbidi'), ('invtbs', 'invtbs'), ('invtermbidi', 'invtermbidi'), ('invterse', 'invterse'), ('invtextauto', 'invtextauto'), ('invtextmode', 'invtextmode'), ('invtf', 'invtf'), ('invtgst', 'invtgst'), ('invtildeop', 'invtildeop'), ('invtimeout', 'invtimeout'), ('invtitle', 'invtitle'), ('invto', 'invto'), ('invtop', 'invtop'), ('invtr', 'invtr'), ('invttimeout', 'invttimeout'), ('invttybuiltin', 'invttybuiltin'), ('invttyfast', 'invttyfast'), ('invtx', 'invtx'), ('invudf', 'invudf'), ('invundofile', 'invundofile'), ('invvb', 'invvb'), ('invvisualbell', 'invvisualbell'), ('invwa', 'invwa'), ('invwarn', 'invwarn'), ('invwb', 'invwb'), ('invweirdinvert', 'invweirdinvert'), ('invwfh', 'invwfh'), ('invwfw', 'invwfw'), ('invwic', 'invwic'), ('invwildignorecase', 'invwildignorecase'), ('invwildmenu', 'invwildmenu'), ('invwinfixheight', 'invwinfixheight'), ('invwinfixwidth', 'invwinfixwidth'), ('invwiv', 'invwiv'), ('invwmnu', 'invwmnu'), ('invwrap', 'invwrap'), ('invwrapscan', 'invwrapscan'), ('invwrite', 'invwrite'), ('invwriteany', 'invwriteany'), ('invwritebackup', 'invwritebackup'), ('invws', 'invws'), ('is', 'is'), ('isf', 'isf'), ('isfname', 'isfname'), ('isi', 'isi'), ('isident', 'isident'), ('isk', 'isk'), ('iskeyword', 'iskeyword'), ('isp', 'isp'), ('isprint', 'isprint'), ('joinspaces', 'joinspaces'), ('js', 'js'), ('key', 'key'), ('keymap', 'keymap'), ('keymodel', 'keymodel'), ('keywordprg', 'keywordprg'), ('km', 'km'), ('kmp', 'kmp'), ('kp', 'kp'), ('langmap', 'langmap'), ('langmenu', 'langmenu'), ('laststatus', 'laststatus'), ('lazyredraw', 'lazyredraw'), ('lbr', 'lbr'), ('lcs', 'lcs'), ('linebreak', 'linebreak'), ('lines', 'lines'), ('linespace', 'linespace'), ('lisp', 'lisp'), ('lispwords', 'lispwords'), ('list', 'list'), ('listchars', 'listchars'), ('lm', 'lm'), ('lmap', 'lmap'), ('loadplugins', 'loadplugins'), ('lpl', 'lpl'), ('ls', 'ls'), ('lsp', 'lsp'), ('lw', 'lw'), ('lz', 'lz'), ('ma', 'ma'), ('macatsui', 'macatsui'), ('magic', 'magic'), ('makeef', 'makeef'), ('makeprg', 'makeprg'), ('mat', 'mat'), ('matchpairs', 'matchpairs'), ('matchtime', 'matchtime'), ('maxcombine', 'maxcombine'), ('maxfuncdepth', 'maxfuncdepth'), ('maxmapdepth', 'maxmapdepth'), ('maxmem', 'maxmem'), ('maxmempattern', 'maxmempattern'), ('maxmemtot', 'maxmemtot'), ('mco', 'mco'), ('mef', 'mef'), ('menuitems', 'menuitems'), ('mfd', 'mfd'), ('mh', 'mh'), ('mis', 'mis'), ('mkspellmem', 'mkspellmem'), ('ml', 'ml'), ('mls', 'mls'), ('mm', 'mm'), ('mmd', 'mmd'), ('mmp', 'mmp'), ('mmt', 'mmt'), ('mod', 'mod'), ('modeline', 'modeline'), ('modelines', 'modelines'), ('modifiable', 'modifiable'), ('modified', 'modified'), ('more', 'more'), ('mouse', 'mouse'), ('mousef', 'mousef'), ('mousefocus', 'mousefocus'), ('mousehide', 'mousehide'), ('mousem', 'mousem'), ('mousemodel', 'mousemodel'), ('mouses', 'mouses'), ('mouseshape', 'mouseshape'), ('mouset', 'mouset'), ('mousetime', 'mousetime'), ('mp', 'mp'), ('mps', 'mps'), ('msm', 'msm'), ('mzq', 'mzq'), ('mzquantum', 'mzquantum'), ('nf', 'nf'), ('nnoremap', 'nnoremap'), ('noacd', 'noacd'), ('noai', 'noai'), ('noakm', 'noakm'), ('noallowrevins', 'noallowrevins'), ('noaltkeymap', 'noaltkeymap'), ('noanti', 'noanti'), ('noantialias', 'noantialias'), ('noar', 'noar'), ('noarab', 'noarab'), ('noarabic', 'noarabic'), ('noarabicshape', 'noarabicshape'), ('noari', 'noari'), ('noarshape', 'noarshape'), ('noautochdir', 'noautochdir'), ('noautoindent', 'noautoindent'), ('noautoread', 'noautoread'), ('noautowrite', 'noautowrite'), ('noautowriteall', 'noautowriteall'), ('noaw', 'noaw'), ('noawa', 'noawa'), ('nobackup', 'nobackup'), ('noballooneval', 'noballooneval'), ('nobeval', 'nobeval'), ('nobin', 'nobin'), ('nobinary', 'nobinary'), ('nobiosk', 'nobiosk'), ('nobioskey', 'nobioskey'), ('nobk', 'nobk'), ('nobl', 'nobl'), ('nobomb', 'nobomb'), ('nobuflisted', 'nobuflisted'), ('nocf', 'nocf'), ('noci', 'noci'), ('nocin', 'nocin'), ('nocindent', 'nocindent'), ('nocompatible', 'nocompatible'), ('noconfirm', 'noconfirm'), ('noconsk', 'noconsk'), ('noconskey', 'noconskey'), ('nocopyindent', 'nocopyindent'), ('nocp', 'nocp'), ('nocrb', 'nocrb'), ('nocscoperelative', 'nocscoperelative'), ('nocscopetag', 'nocscopetag'), ('nocscopeverbose', 'nocscopeverbose'), ('nocsre', 'nocsre'), ('nocst', 'nocst'), ('nocsverb', 'nocsverb'), ('nocuc', 'nocuc'), ('nocul', 'nocul'), ('nocursorbind', 'nocursorbind'), ('nocursorcolumn', 'nocursorcolumn'), ('nocursorline', 'nocursorline'), ('nodeco', 'nodeco'), ('nodelcombine', 'nodelcombine'), ('nodg', 'nodg'), ('nodiff', 'nodiff'), ('nodigraph', 'nodigraph'), ('noea', 'noea'), ('noeb', 'noeb'), ('noed', 'noed'), ('noedcompatible', 'noedcompatible'), ('noek', 'noek'), ('noendofline', 'noendofline'), ('noeol', 'noeol'), ('noequalalways', 'noequalalways'), ('noerrorbells', 'noerrorbells'), ('noesckeys', 'noesckeys'), ('noet', 'noet'), ('noex', 'noex'), ('noexpandtab', 'noexpandtab'), ('noexrc', 'noexrc'), ('nofen', 'nofen'), ('nofic', 'nofic'), ('nofileignorecase', 'nofileignorecase'), ('nofk', 'nofk'), ('nofkmap', 'nofkmap'), ('nofoldenable', 'nofoldenable'), ('nogd', 'nogd'), ('nogdefault', 'nogdefault'), ('noguipty', 'noguipty'), ('nohid', 'nohid'), ('nohidden', 'nohidden'), ('nohk', 'nohk'), ('nohkmap', 'nohkmap'), ('nohkmapp', 'nohkmapp'), ('nohkp', 'nohkp'), ('nohls', 'nohls'), ('nohlsearch', 'nohlsearch'), ('noic', 'noic'), ('noicon', 'noicon'), ('noignorecase', 'noignorecase'), ('noim', 'noim'), ('noimc', 'noimc'), ('noimcmdline', 'noimcmdline'), ('noimd', 'noimd'), ('noimdisable', 'noimdisable'), ('noincsearch', 'noincsearch'), ('noinf', 'noinf'), ('noinfercase', 'noinfercase'), ('noinsertmode', 'noinsertmode'), ('nois', 'nois'), ('nojoinspaces', 'nojoinspaces'), ('nojs', 'nojs'), ('nolazyredraw', 'nolazyredraw'), ('nolbr', 'nolbr'), ('nolinebreak', 'nolinebreak'), ('nolisp', 'nolisp'), ('nolist', 'nolist'), ('noloadplugins', 'noloadplugins'), ('nolpl', 'nolpl'), ('nolz', 'nolz'), ('noma', 'noma'), ('nomacatsui', 'nomacatsui'), ('nomagic', 'nomagic'), ('nomh', 'nomh'), ('noml', 'noml'), ('nomod', 'nomod'), ('nomodeline', 'nomodeline'), ('nomodifiable', 'nomodifiable'), ('nomodified', 'nomodified'), ('nomore', 'nomore'), ('nomousef', 'nomousef'), ('nomousefocus', 'nomousefocus'), ('nomousehide', 'nomousehide'), ('nonu', 'nonu'), ('nonumber', 'nonumber'), ('noodev', 'noodev'), ('noopendevice', 'noopendevice'), ('nopaste', 'nopaste'), ('nopi', 'nopi'), ('nopreserveindent', 'nopreserveindent'), ('nopreviewwindow', 'nopreviewwindow'), ('noprompt', 'noprompt'), ('nopvw', 'nopvw'), ('noreadonly', 'noreadonly'), ('norelativenumber', 'norelativenumber'), ('noremap', 'noremap'), ('norestorescreen', 'norestorescreen'), ('norevins', 'norevins'), ('nori', 'nori'), ('norightleft', 'norightleft'), ('norl', 'norl'), ('nornu', 'nornu'), ('noro', 'noro'), ('nors', 'nors'), ('noru', 'noru'), ('noruler', 'noruler'), ('nosb', 'nosb'), ('nosc', 'nosc'), ('noscb', 'noscb'), ('noscrollbind', 'noscrollbind'), ('noscs', 'noscs'), ('nosecure', 'nosecure'), ('nosft', 'nosft'), ('noshellslash', 'noshellslash'), ('noshelltemp', 'noshelltemp'), ('noshiftround', 'noshiftround'), ('noshortname', 'noshortname'), ('noshowcmd', 'noshowcmd'), ('noshowfulltag', 'noshowfulltag'), ('noshowmatch', 'noshowmatch'), ('noshowmode', 'noshowmode'), ('nosi', 'nosi'), ('nosm', 'nosm'), ('nosmartcase', 'nosmartcase'), ('nosmartindent', 'nosmartindent'), ('nosmarttab', 'nosmarttab'), ('nosmd', 'nosmd'), ('nosn', 'nosn'), ('nosol', 'nosol'), ('nospell', 'nospell'), ('nosplitbelow', 'nosplitbelow'), ('nosplitright', 'nosplitright'), ('nospr', 'nospr'), ('nosr', 'nosr'), ('nossl', 'nossl'), ('nosta', 'nosta'), ('nostartofline', 'nostartofline'), ('nostmp', 'nostmp'), ('noswapfile', 'noswapfile'), ('noswf', 'noswf'), ('nota', 'nota'), ('notagbsearch', 'notagbsearch'), ('notagrelative', 'notagrelative'), ('notagstack', 'notagstack'), ('notbi', 'notbi'), ('notbidi', 'notbidi'), ('notbs', 'notbs'), ('notermbidi', 'notermbidi'), ('noterse', 'noterse'), ('notextauto', 'notextauto'), ('notextmode', 'notextmode'), ('notf', 'notf'), ('notgst', 'notgst'), ('notildeop', 'notildeop'), ('notimeout', 'notimeout'), ('notitle', 'notitle'), ('noto', 'noto'), ('notop', 'notop'), ('notr', 'notr'), ('nottimeout', 'nottimeout'), ('nottybuiltin', 'nottybuiltin'), ('nottyfast', 'nottyfast'), ('notx', 'notx'), ('noudf', 'noudf'), ('noundofile', 'noundofile'), ('novb', 'novb'), ('novisualbell', 'novisualbell'), ('nowa', 'nowa'), ('nowarn', 'nowarn'), ('nowb', 'nowb'), ('noweirdinvert', 'noweirdinvert'), ('nowfh', 'nowfh'), ('nowfw', 'nowfw'), ('nowic', 'nowic'), ('nowildignorecase', 'nowildignorecase'), ('nowildmenu', 'nowildmenu'), ('nowinfixheight', 'nowinfixheight'), ('nowinfixwidth', 'nowinfixwidth'), ('nowiv', 'nowiv'), ('nowmnu', 'nowmnu'), ('nowrap', 'nowrap'), ('nowrapscan', 'nowrapscan'), ('nowrite', 'nowrite'), ('nowriteany', 'nowriteany'), ('nowritebackup', 'nowritebackup'), ('nows', 'nows'), ('nrformats', 'nrformats'), ('nu', 'nu'), ('number', 'number'), ('numberwidth', 'numberwidth'), ('nuw', 'nuw'), ('odev', 'odev'), ('oft', 'oft'), ('ofu', 'ofu'), ('omnifunc', 'omnifunc'), ('opendevice', 'opendevice'), ('operatorfunc', 'operatorfunc'), ('opfunc', 'opfunc'), ('osfiletype', 'osfiletype'), ('pa', 'pa'), ('para', 'para'), ('paragraphs', 'paragraphs'), ('paste', 'paste'), ('pastetoggle', 'pastetoggle'), ('patchexpr', 'patchexpr'), ('patchmode', 'patchmode'), ('path', 'path'), ('pdev', 'pdev'), ('penc', 'penc'), ('pex', 'pex'), ('pexpr', 'pexpr'), ('pfn', 'pfn'), ('ph', 'ph'), ('pheader', 'pheader'), ('pi', 'pi'), ('pm', 'pm'), ('pmbcs', 'pmbcs'), ('pmbfn', 'pmbfn'), ('popt', 'popt'), ('preserveindent', 'preserveindent'), ('previewheight', 'previewheight'), ('previewwindow', 'previewwindow'), ('printdevice', 'printdevice'), ('printencoding', 'printencoding'), ('printexpr', 'printexpr'), ('printfont', 'printfont'), ('printheader', 'printheader'), ('printmbcharset', 'printmbcharset'), ('printmbfont', 'printmbfont'), ('printoptions', 'printoptions'), ('prompt', 'prompt'), ('pt', 'pt'), ('pumheight', 'pumheight'), ('pvh', 'pvh'), ('pvw', 'pvw'), ('qe', 'qe'), ('quoteescape', 'quoteescape'), ('rdt', 'rdt'), ('re', 're'), ('readonly', 'readonly'), ('redrawtime', 'redrawtime'), ('regexpengine', 'regexpengine'), ('relativenumber', 'relativenumber'), ('remap', 'remap'), ('report', 'report'), ('restorescreen', 'restorescreen'), ('revins', 'revins'), ('ri', 'ri'), ('rightleft', 'rightleft'), ('rightleftcmd', 'rightleftcmd'), ('rl', 'rl'), ('rlc', 'rlc'), ('rnu', 'rnu'), ('ro', 'ro'), ('rs', 'rs'), ('rtp', 'rtp'), ('ru', 'ru'), ('ruf', 'ruf'), ('ruler', 'ruler'), ('rulerformat', 'rulerformat'), ('runtimepath', 'runtimepath'), ('sb', 'sb'), ('sbo', 'sbo'), ('sbr', 'sbr'), ('sc', 'sc'), ('scb', 'scb'), ('scr', 'scr'), ('scroll', 'scroll'), ('scrollbind', 'scrollbind'), ('scrolljump', 'scrolljump'), ('scrolloff', 'scrolloff'), ('scrollopt', 'scrollopt'), ('scs', 'scs'), ('sect', 'sect'), ('sections', 'sections'), ('secure', 'secure'), ('sel', 'sel'), ('selection', 'selection'), ('selectmode', 'selectmode'), ('sessionoptions', 'sessionoptions'), ('sft', 'sft'), ('sh', 'sh'), ('shcf', 'shcf'), ('shell', 'shell'), ('shellcmdflag', 'shellcmdflag'), ('shellpipe', 'shellpipe'), ('shellquote', 'shellquote'), ('shellredir', 'shellredir'), ('shellslash', 'shellslash'), ('shelltemp', 'shelltemp'), ('shelltype', 'shelltype'), ('shellxescape', 'shellxescape'), ('shellxquote', 'shellxquote'), ('shiftround', 'shiftround'), ('shiftwidth', 'shiftwidth'), ('shm', 'shm'), ('shortmess', 'shortmess'), ('shortname', 'shortname'), ('showbreak', 'showbreak'), ('showcmd', 'showcmd'), ('showfulltag', 'showfulltag'), ('showmatch', 'showmatch'), ('showmode', 'showmode'), ('showtabline', 'showtabline'), ('shq', 'shq'), ('si', 'si'), ('sidescroll', 'sidescroll'), ('sidescrolloff', 'sidescrolloff'), ('siso', 'siso'), ('sj', 'sj'), ('slm', 'slm'), ('sm', 'sm'), ('smartcase', 'smartcase'), ('smartindent', 'smartindent'), ('smarttab', 'smarttab'), ('smc', 'smc'), ('smd', 'smd'), ('sn', 'sn'), ('so', 'so'), ('softtabstop', 'softtabstop'), ('sol', 'sol'), ('sp', 'sp'), ('spc', 'spc'), ('spell', 'spell'), ('spellcapcheck', 'spellcapcheck'), ('spellfile', 'spellfile'), ('spelllang', 'spelllang'), ('spellsuggest', 'spellsuggest'), ('spf', 'spf'), ('spl', 'spl'), ('splitbelow', 'splitbelow'), ('splitright', 'splitright'), ('spr', 'spr'), ('sps', 'sps'), ('sr', 'sr'), ('srr', 'srr'), ('ss', 'ss'), ('ssl', 'ssl'), ('ssop', 'ssop'), ('st', 'st'), ('sta', 'sta'), ('stal', 'stal'), ('startofline', 'startofline'), ('statusline', 'statusline'), ('stl', 'stl'), ('stmp', 'stmp'), ('sts', 'sts'), ('su', 'su'), ('sua', 'sua'), ('suffixes', 'suffixes'), ('suffixesadd', 'suffixesadd'), ('sw', 'sw'), ('swapfile', 'swapfile'), ('swapsync', 'swapsync'), ('swb', 'swb'), ('swf', 'swf'), ('switchbuf', 'switchbuf'), ('sws', 'sws'), ('sxe', 'sxe'), ('sxq', 'sxq'), ('syn', 'syn'), ('synmaxcol', 'synmaxcol'), ('syntax', 'syntax'), ('t_AB', 't_AB'), ('t_AF', 't_AF'), ('t_AL', 't_AL'), ('t_CS', 't_CS'), ('t_CV', 't_CV'), ('t_Ce', 't_Ce'), ('t_Co', 't_Co'), ('t_Cs', 't_Cs'), ('t_DL', 't_DL'), ('t_EI', 't_EI'), ('t_F1', 't_F1'), ('t_F2', 't_F2'), ('t_F3', 't_F3'), ('t_F4', 't_F4'), ('t_F5', 't_F5'), ('t_F6', 't_F6'), ('t_F7', 't_F7'), ('t_F8', 't_F8'), ('t_F9', 't_F9'), ('t_IE', 't_IE'), ('t_IS', 't_IS'), ('t_K1', 't_K1'), ('t_K3', 't_K3'), ('t_K4', 't_K4'), ('t_K5', 't_K5'), ('t_K6', 't_K6'), ('t_K7', 't_K7'), ('t_K8', 't_K8'), ('t_K9', 't_K9'), ('t_KA', 't_KA'), ('t_KB', 't_KB'), ('t_KC', 't_KC'), ('t_KD', 't_KD'), ('t_KE', 't_KE'), ('t_KF', 't_KF'), ('t_KG', 't_KG'), ('t_KH', 't_KH'), ('t_KI', 't_KI'), ('t_KJ', 't_KJ'), ('t_KK', 't_KK'), ('t_KL', 't_KL'), ('t_RI', 't_RI'), ('t_RV', 't_RV'), ('t_SI', 't_SI'), ('t_Sb', 't_Sb'), ('t_Sf', 't_Sf'), ('t_WP', 't_WP'), ('t_WS', 't_WS'), ('t_ZH', 't_ZH'), ('t_ZR', 't_ZR'), ('t_al', 't_al'), ('t_bc', 't_bc'), ('t_cd', 't_cd'), ('t_ce', 't_ce'), ('t_cl', 't_cl'), ('t_cm', 't_cm'), ('t_cs', 't_cs'), ('t_da', 't_da'), ('t_db', 't_db'), ('t_dl', 't_dl'), ('t_fs', 't_fs'), ('t_k1', 't_k1'), ('t_k2', 't_k2'), ('t_k3', 't_k3'), ('t_k4', 't_k4'), ('t_k5', 't_k5'), ('t_k6', 't_k6'), ('t_k7', 't_k7'), ('t_k8', 't_k8'), ('t_k9', 't_k9'), ('t_kB', 't_kB'), ('t_kD', 't_kD'), ('t_kI', 't_kI'), ('t_kN', 't_kN'), ('t_kP', 't_kP'), ('t_kb', 't_kb'), ('t_kd', 't_kd'), ('t_ke', 't_ke'), ('t_kh', 't_kh'), ('t_kl', 't_kl'), ('t_kr', 't_kr'), ('t_ks', 't_ks'), ('t_ku', 't_ku'), ('t_le', 't_le'), ('t_mb', 't_mb'), ('t_md', 't_md'), ('t_me', 't_me'), ('t_mr', 't_mr'), ('t_ms', 't_ms'), ('t_nd', 't_nd'), ('t_op', 't_op'), ('t_se', 't_se'), ('t_so', 't_so'), ('t_sr', 't_sr'), ('t_te', 't_te'), ('t_ti', 't_ti'), ('t_ts', 't_ts'), ('t_u7', 't_u7'), ('t_ue', 't_ue'), ('t_us', 't_us'), ('t_ut', 't_ut'), ('t_vb', 't_vb'), ('t_ve', 't_ve'), ('t_vi', 't_vi'), ('t_vs', 't_vs'), ('t_xs', 't_xs'), ('ta', 'ta'), ('tabline', 'tabline'), ('tabpagemax', 'tabpagemax'), ('tabstop', 'tabstop'), ('tag', 'tag'), ('tagbsearch', 'tagbsearch'), ('taglength', 'taglength'), ('tagrelative', 'tagrelative'), ('tags', 'tags'), ('tagstack', 'tagstack'), ('tal', 'tal'), ('tb', 'tb'), ('tbi', 'tbi'), ('tbidi', 'tbidi'), ('tbis', 'tbis'), ('tbs', 'tbs'), ('tenc', 'tenc'), ('term', 'term'), ('termbidi', 'termbidi'), ('termencoding', 'termencoding'), ('terse', 'terse'), ('textauto', 'textauto'), ('textmode', 'textmode'), ('textwidth', 'textwidth'), ('tf', 'tf'), ('tgst', 'tgst'), ('thesaurus', 'thesaurus'), ('tildeop', 'tildeop'), ('timeout', 'timeout'), ('timeoutlen', 'timeoutlen'), ('title', 'title'), ('titlelen', 'titlelen'), ('titleold', 'titleold'), ('titlestring', 'titlestring'), ('tl', 'tl'), ('tm', 'tm'), ('to', 'to'), ('toolbar', 'toolbar'), ('toolbariconsize', 'toolbariconsize'), ('top', 'top'), ('tpm', 'tpm'), ('tr', 'tr'), ('ts', 'ts'), ('tsl', 'tsl'), ('tsr', 'tsr'), ('ttimeout', 'ttimeout'), ('ttimeoutlen', 'ttimeoutlen'), ('ttm', 'ttm'), ('tty', 'tty'), ('ttybuiltin', 'ttybuiltin'), ('ttyfast', 'ttyfast'), ('ttym', 'ttym'), ('ttymouse', 'ttymouse'), ('ttyscroll', 'ttyscroll'), ('ttytype', 'ttytype'), ('tw', 'tw'), ('tx', 'tx'), ('uc', 'uc'), ('udf', 'udf'), ('udir', 'udir'), ('ul', 'ul'), ('undodir', 'undodir'), ('undofile', 'undofile'), ('undolevels', 'undolevels'), ('undoreload', 'undoreload'), ('updatecount', 'updatecount'), ('updatetime', 'updatetime'), ('ur', 'ur'), ('ut', 'ut'), ('vb', 'vb'), ('vbs', 'vbs'), ('vdir', 'vdir'), ('ve', 've'), ('verbose', 'verbose'), ('verbosefile', 'verbosefile'), ('vfile', 'vfile'), ('vi', 'vi'), ('viewdir', 'viewdir'), ('viewoptions', 'viewoptions'), ('viminfo', 'viminfo'), ('virtualedit', 'virtualedit'), ('visualbell', 'visualbell'), ('vnoremap', 'vnoremap'), ('vop', 'vop'), ('wa', 'wa'), ('wak', 'wak'), ('warn', 'warn'), ('wb', 'wb'), ('wc', 'wc'), ('wcm', 'wcm'), ('wd', 'wd'), ('weirdinvert', 'weirdinvert'), ('wfh', 'wfh'), ('wfw', 'wfw'), ('wh', 'wh'), ('whichwrap', 'whichwrap'), ('wi', 'wi'), ('wic', 'wic'), ('wig', 'wig'), ('wildchar', 'wildchar'), ('wildcharm', 'wildcharm'), ('wildignore', 'wildignore'), ('wildignorecase', 'wildignorecase'), ('wildmenu', 'wildmenu'), ('wildmode', 'wildmode'), ('wildoptions', 'wildoptions'), ('wim', 'wim'), ('winaltkeys', 'winaltkeys'), ('window', 'window'), ('winfixheight', 'winfixheight'), ('winfixwidth', 'winfixwidth'), ('winheight', 'winheight'), ('winminheight', 'winminheight'), ('winminwidth', 'winminwidth'), ('winwidth', 'winwidth'), ('wiv', 'wiv'), ('wiw', 'wiw'), ('wm', 'wm'), ('wmh', 'wmh'), ('wmnu', 'wmnu'), ('wmw', 'wmw'), ('wop', 'wop'), ('wrap', 'wrap'), ('wrapmargin', 'wrapmargin'), ('wrapscan', 'wrapscan'), ('write', 'write'), ('writeany', 'writeany'), ('writebackup', 'writebackup'), ('writedelay', 'writedelay'), ('ws', 'ws'), ('ww', 'ww'))\n    return var",
    "label": true
  },
  {
    "code": "def _make_cached_stream_func(src_func: t.Callable[[], t.Optional[t.TextIO]], wrapper_func: t.Callable[[], t.TextIO]) -> t.Callable[[], t.Optional[t.TextIO]]:\n    cache: t.MutableMapping[t.TextIO, t.TextIO] = WeakKeyDictionary()\n\n    def func() -> t.Optional[t.TextIO]:\n        stream = src_func()\n        if stream is None:\n            return None\n        try:\n            rv = cache.get(stream)\n        except Exception:\n            rv = None\n        if rv is not None:\n            return rv\n        rv = wrapper_func()\n        try:\n            cache[stream] = rv\n        except Exception:\n            pass\n        return rv\n    return func",
    "label": true
  },
  {
    "code": "def clean_duplicates_mro(sequences, cls, context):\n    for sequence in sequences:\n        names = [(node.lineno, node.qname()) if node.name else None for node in sequence]\n        last_index = dict(map(reversed, enumerate(names)))\n        if names and names[0] is not None and (last_index[names[0]] != 0):\n            raise DuplicateBasesError(message='Duplicates found in MROs {mros} for {cls!r}.', mros=sequences, cls=cls, context=context)\n        yield [node for i, (node, name) in enumerate(zip(sequence, names)) if name is None or last_index[name] == i]",
    "label": true
  },
  {
    "code": "def get_subscript_const_value(node: nodes.Subscript) -> nodes.Const:\n    inferred = safe_infer(node.slice)\n    if not isinstance(inferred, nodes.Const):\n        raise InferredTypeError('Subscript.slice cannot be inferred as a nodes.Const')\n    return inferred",
    "label": true
  },
  {
    "code": "def sieve(n):\n    isqrt = getattr(math, 'isqrt', lambda x: int(math.sqrt(x)))\n    data = bytearray((0, 1)) * (n // 2)\n    data[:3] = (0, 0, 0)\n    limit = isqrt(n) + 1\n    for p in compress(range(limit), data):\n        data[p * p:n:p + p] = bytes(len(range(p * p, n, p + p)))\n    data[2] = 1\n    return iter_index(data, 1) if n > 2 else iter([])",
    "label": true
  },
  {
    "code": "def _cert_array_from_pem(pem_bundle: bytes) -> CFArray:\n    pem_bundle = pem_bundle.replace(b'\\r\\n', b'\\n')\n    der_certs = [base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)]\n    if not der_certs:\n        raise ssl.SSLError('No root certificates specified')\n    cert_array = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n    if not cert_array:\n        raise ssl.SSLError('Unable to allocate memory!')\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError('Unable to allocate memory!')\n            cert = Security.SecCertificateCreateWithData(CoreFoundation.kCFAllocatorDefault, certdata)\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError('Unable to build cert object!')\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        CoreFoundation.CFRelease(cert_array)\n        raise\n    return cert_array",
    "label": true
  },
  {
    "code": "def random_product(*args, repeat=1):\n    pools = [tuple(pool) for pool in args] * repeat\n    return tuple((choice(pool) for pool in pools))",
    "label": true
  },
  {
    "code": "def check_config_h():\n    from distutils import sysconfig\n    if 'GCC' in sys.version:\n        return (CONFIG_H_OK, \"sys.version mentions 'GCC'\")\n    if 'Clang' in sys.version:\n        return (CONFIG_H_OK, \"sys.version mentions 'Clang'\")\n    fn = sysconfig.get_config_h_filename()\n    try:\n        config_h = open(fn)\n        try:\n            if '__GNUC__' in config_h.read():\n                return (CONFIG_H_OK, \"'%s' mentions '__GNUC__'\" % fn)\n            else:\n                return (CONFIG_H_NOTOK, \"'%s' does not mention '__GNUC__'\" % fn)\n        finally:\n            config_h.close()\n    except OSError as exc:\n        return (CONFIG_H_UNCERTAIN, \"couldn't read '{}': {}\".format(fn, exc.strerror))",
    "label": true
  },
  {
    "code": "def _get_python_inc_posix_python(plat_specific):\n    if not python_build:\n        return\n    if plat_specific:\n        return _sys_home or project_base\n    incdir = os.path.join(get_config_var('srcdir'), 'Include')\n    return os.path.normpath(incdir)",
    "label": true
  },
  {
    "code": "def _append_line(lines, colwidths, colaligns, linefmt):\n    lines.append(_build_line(colwidths, colaligns, linefmt))\n    return lines",
    "label": true
  },
  {
    "code": "def enable_vt_processing(fd):\n    if win32.windll is None or not win32.winapi_test():\n        return False\n    try:\n        handle = get_osfhandle(fd)\n        mode = win32.GetConsoleMode(handle)\n        win32.SetConsoleMode(handle, mode | win32.ENABLE_VIRTUAL_TERMINAL_PROCESSING)\n        mode = win32.GetConsoleMode(handle)\n        if mode & win32.ENABLE_VIRTUAL_TERMINAL_PROCESSING:\n            return True\n    except (OSError, TypeError):\n        return False",
    "label": true
  },
  {
    "code": "def infer_name(self: nodes.Name | nodes.AssignName, context: InferenceContext | None=None, **kwargs: Any) -> Generator[InferenceResult, None, None]:\n    frame, stmts = self.lookup(self.name)\n    if not stmts:\n        parent_function = _higher_function_scope(self.scope())\n        if parent_function:\n            _, stmts = parent_function.lookup(self.name)\n        if not stmts:\n            raise NameInferenceError(name=self.name, scope=self.scope(), context=context)\n    context = copy_context(context)\n    context.lookupname = self.name\n    context.constraints[self.name] = constraint.get_constraints(self, frame)\n    return bases._infer_stmts(stmts, context, frame)",
    "label": true
  },
  {
    "code": "def _handle_config_settings(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    key, sep, val = value.partition('=')\n    if sep != '=':\n        parser.error(f'Arguments to {opt_str} must be of the form KEY=VAL')\n    dest = getattr(parser.values, option.dest)\n    if dest is None:\n        dest = {}\n        setattr(parser.values, option.dest, dest)\n    if key in dest:\n        if isinstance(dest[key], list):\n            dest[key].append(val)\n        else:\n            dest[key] = [dest[key], val]\n    else:\n        dest[key] = val",
    "label": true
  },
  {
    "code": "def infer_property(node: nodes.Call, context: InferenceContext | None=None) -> objects.Property:\n    if len(node.args) < 1:\n        raise UseInferenceDefault\n    getter = node.args[0]\n    try:\n        inferred = next(getter.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n        raise UseInferenceDefault\n    prop_func = objects.Property(function=inferred, name=inferred.name, lineno=node.lineno, parent=node, col_offset=node.col_offset)\n    prop_func.postinit(body=[], args=inferred.args, doc_node=getattr(inferred, 'doc_node', None))\n    return prop_func",
    "label": true
  },
  {
    "code": "def ratio_distribute(total: int, ratios: List[int], minimums: Optional[List[int]]=None) -> List[int]:\n    if minimums:\n        ratios = [ratio if _min else 0 for ratio, _min in zip(ratios, minimums)]\n    total_ratio = sum(ratios)\n    assert total_ratio > 0, 'Sum of ratios must be > 0'\n    total_remaining = total\n    distributed_total: List[int] = []\n    append = distributed_total.append\n    if minimums is None:\n        _minimums = [0] * len(ratios)\n    else:\n        _minimums = minimums\n    for ratio, minimum in zip(ratios, _minimums):\n        if total_ratio > 0:\n            distributed = max(minimum, ceil(ratio * total_remaining / total_ratio))\n        else:\n            distributed = total_remaining\n        append(distributed)\n        total_ratio -= ratio\n        total_remaining -= distributed\n    return distributed_total",
    "label": true
  },
  {
    "code": "def adjacent(predicate, iterable, distance=1):\n    if distance < 0:\n        raise ValueError('distance must be at least 0')\n    i1, i2 = tee(iterable)\n    padding = [False] * distance\n    selected = chain(padding, map(predicate, i1), padding)\n    adjacent_to_selected = map(any, windowed(selected, 2 * distance + 1))\n    return zip(adjacent_to_selected, i2)",
    "label": true
  },
  {
    "code": "def test_code_to_tempfile():\n    if not WINDOWS:\n        pyfile = dump_source(f, alias='_f')\n        _f = load_source(pyfile)\n        assert _f(4) == f(4)",
    "label": true
  },
  {
    "code": "def _custom_manylinux_platforms(arch: str) -> List[str]:\n    arches = [arch]\n    arch_prefix, arch_sep, arch_suffix = arch.partition('_')\n    if arch_prefix == 'manylinux2014':\n        if arch_suffix in {'i686', 'x86_64'}:\n            arches.append('manylinux2010' + arch_sep + arch_suffix)\n            arches.append('manylinux1' + arch_sep + arch_suffix)\n    elif arch_prefix == 'manylinux2010':\n        arches.append('manylinux1' + arch_sep + arch_suffix)\n    return arches",
    "label": true
  },
  {
    "code": "def get_all_distribution_names(url=None):\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()",
    "label": true
  },
  {
    "code": "def set_partitions(iterable, k=None):\n    L = list(iterable)\n    n = len(L)\n    if k is not None:\n        if k < 1:\n            raise ValueError(\"Can't partition in a negative or zero number of groups\")\n        elif k > n:\n            return\n\n    def set_partitions_helper(L, k):\n        n = len(L)\n        if k == 1:\n            yield [L]\n        elif n == k:\n            yield [[s] for s in L]\n        else:\n            e, *M = L\n            for p in set_partitions_helper(M, k - 1):\n                yield [[e], *p]\n            for p in set_partitions_helper(M, k):\n                for i in range(len(p)):\n                    yield (p[:i] + [[e] + p[i]] + p[i + 1:])\n    if k is None:\n        for k in range(1, n + 1):\n            yield from set_partitions_helper(L, k)\n    else:\n        yield from set_partitions_helper(L, k)",
    "label": true
  },
  {
    "code": "def _print_hard_fail(config: Config, offending_file: Optional[str]=None, message: Optional[str]=None) -> None:\n    message = message or f\"Unrecoverable exception thrown when parsing {offending_file or ''}! This should NEVER happen.\\nIf encountered, please open an issue: https://github.com/PyCQA/isort/issues/new\"\n    printer = create_terminal_printer(color=config.color_output, error=config.format_error, success=config.format_success)\n    printer.error(message)",
    "label": true
  },
  {
    "code": "def _format_marker(marker: Union[List[str], Tuple[Node, ...], str], first: Optional[bool]=True) -> str:\n    assert isinstance(marker, (list, tuple, str))\n    if isinstance(marker, list) and len(marker) == 1 and isinstance(marker[0], (list, tuple)):\n        return _format_marker(marker[0])\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return ' '.join(inner)\n        else:\n            return '(' + ' '.join(inner) + ')'\n    elif isinstance(marker, tuple):\n        return ' '.join([m.serialize() for m in marker])\n    else:\n        return marker",
    "label": true
  },
  {
    "code": "def get_win_folder_from_registry(csidl_name: str) -> str:\n    shell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': 'Common AppData', 'CSIDL_LOCAL_APPDATA': 'Local AppData', 'CSIDL_PERSONAL': 'Personal'}.get(csidl_name)\n    if shell_folder_name is None:\n        raise ValueError(f'Unknown CSIDL name: {csidl_name}')\n    if sys.platform != 'win32':\n        raise NotImplementedError\n    import winreg\n    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, 'Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Explorer\\\\Shell Folders')\n    directory, _ = winreg.QueryValueEx(key, shell_folder_name)\n    return str(directory)",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        interp = 'pp3'\n    elif interp_name == 'cp':\n        interp = 'cp' + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)",
    "label": true
  },
  {
    "code": "def loadIO_source(buffer, **kwds):\n    alias = kwds.pop('alias', None)\n    source = getattr(buffer, 'getvalue', buffer)\n    if source != buffer:\n        source = source()\n    source = source.decode()\n    if not alias:\n        tag = source.strip().splitlines()[-1].split()\n        if tag[0] != '#NAME:':\n            stub = source.splitlines()[0]\n            raise IOError('unknown name for code: %s' % stub)\n        alias = tag[-1]\n    local = {}\n    exec(source, local)\n    _ = eval('%s' % alias, local)\n    return _",
    "label": true
  },
  {
    "code": "def key_value_rule(src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat) -> Pos:\n    pos, key, value = parse_key_value_pair(src, pos, parse_float)\n    key_parent, key_stem = (key[:-1], key[-1])\n    abs_key_parent = header + key_parent\n    relative_path_cont_keys = (header + key[:i] for i in range(1, len(key)))\n    for cont_key in relative_path_cont_keys:\n        if out.flags.is_(cont_key, Flags.EXPLICIT_NEST):\n            raise suffixed_err(src, pos, f'Cannot redefine namespace {cont_key}')\n        out.flags.add_pending(cont_key, Flags.EXPLICIT_NEST)\n    if out.flags.is_(abs_key_parent, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {abs_key_parent}')\n    try:\n        nest = out.data.get_or_create_nest(abs_key_parent)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if key_stem in nest:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value')\n    if isinstance(value, (dict, list)):\n        out.flags.set(header + key, Flags.FROZEN, recursive=True)\n    nest[key_stem] = value\n    return pos",
    "label": true
  },
  {
    "code": "def _sample_unweighted(iterable, k):\n    reservoir = take(k, iterable)\n    W = exp(log(random()) / k)\n    next_index = k + floor(log(random()) / log(1 - W))\n    for index, element in enumerate(iterable, k):\n        if index == next_index:\n            reservoir[randrange(k)] = element\n            W *= exp(log(random()) / k)\n            next_index += floor(log(random()) / log(1 - W)) + 1\n    return reservoir",
    "label": true
  },
  {
    "code": "def check_graphviz_availability() -> None:\n    if shutil.which('dot') is None:\n        print(\"'Graphviz' needs to be installed for your chosen output format.\")\n        sys.exit(32)",
    "label": true
  },
  {
    "code": "def safeformat(obj: object) -> str:\n    try:\n        return pprint.pformat(obj)\n    except Exception as exc:\n        return _format_repr_exception(exc, obj)",
    "label": true
  },
  {
    "code": "def type_check_simple(func: callable, args: list, expected: Union[type, tuple]) -> tuple[bool, object]:\n    try:\n        args_copy = deepcopy(args)\n        returned = func(*args_copy)\n    except Exception as exn:\n        return (False, error_message(func, args, exn))\n    if isinstance(returned, expected):\n        return (True, returned)\n    return (False, type_error_message(func.__name__, expected.__name__, returned))",
    "label": true
  },
  {
    "code": "def default_subprocess_runner(cmd, cwd=None, extra_environ=None):\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    check_call(cmd, cwd=cwd, env=env)",
    "label": true
  },
  {
    "code": "def _infer_dunder_doc_attribute(node: nodes.Module | nodes.ClassDef | nodes.FunctionDef) -> str | None:\n    try:\n        docstring = node['__doc__']\n    except KeyError:\n        return None\n    docstring = utils.safe_infer(docstring)\n    if not docstring:\n        return None\n    if not isinstance(docstring, nodes.Const):\n        return None\n    return str(docstring.value)",
    "label": true
  },
  {
    "code": "def filter_except(validator, iterable, *exceptions):\n    for item in iterable:\n        try:\n            validator(item)\n        except exceptions:\n            pass\n        else:\n            yield item",
    "label": true
  },
  {
    "code": "def write_toplevel_names(cmd, basename, filename):\n    pkgs = dict.fromkeys([k.split('.', 1)[0] for k in cmd.distribution.iter_distribution_names()])\n    cmd.write_file('top-level names', filename, '\\n'.join(sorted(pkgs)) + '\\n')",
    "label": true
  },
  {
    "code": "def build_source(location: str, *, candidates_from_page: CandidatesFromPage, page_validator: PageValidator, expand_dir: bool, cache_link_parsing: bool) -> Tuple[Optional[str], Optional[LinkSource]]:\n    path: Optional[str] = None\n    url: Optional[str] = None\n    if os.path.exists(location):\n        url = path_to_url(location)\n        path = location\n    elif location.startswith('file:'):\n        url = location\n        path = url_to_path(location)\n    elif is_url(location):\n        url = location\n    if url is None:\n        msg = \"Location '%s' is ignored: it is either a non-existing path or lacks a specific scheme.\"\n        logger.warning(msg, location)\n        return (None, None)\n    if path is None:\n        source: LinkSource = _RemoteFileSource(candidates_from_page=candidates_from_page, page_validator=page_validator, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    if os.path.isdir(path):\n        if expand_dir:\n            source = _FlatDirectorySource(candidates_from_page=candidates_from_page, path=path)\n        else:\n            source = _IndexDirectorySource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    elif os.path.isfile(path):\n        source = _LocalFileSource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    logger.warning(\"Location '%s' is ignored: it is neither a file nor a directory.\", location)\n    return (url, None)",
    "label": true
  },
  {
    "code": "def load_formatter_from_file(filename, formattername='CustomFormatter', **options):\n    try:\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        if formattername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' % (formattername, filename))\n        formatter_class = custom_namespace[formattername]\n        return formatter_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom formatter: %s' % err)",
    "label": true
  },
  {
    "code": "def extract_cookies_to_jar(jar, request, response):\n    if not (hasattr(response, '_original_response') and response._original_response):\n        return\n    req = MockRequest(request)\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)",
    "label": true
  },
  {
    "code": "def _version_from_file(lines):\n\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None",
    "label": true
  },
  {
    "code": "def _compare_eq_dict(left: Mapping[Any, Any], right: Mapping[Any, Any], verbose: int=0) -> List[str]:\n    explanation: List[str] = []\n    set_left = set(left)\n    set_right = set(right)\n    common = set_left.intersection(set_right)\n    same = {k: left[k] for k in common if left[k] == right[k]}\n    if same and verbose < 2:\n        explanation += ['Omitting %s identical items, use -vv to show' % len(same)]\n    elif same:\n        explanation += ['Common items:']\n        explanation += pprint.pformat(same).splitlines()\n    diff = {k for k in common if left[k] != right[k]}\n    if diff:\n        explanation += ['Differing items:']\n        for k in diff:\n            explanation += [saferepr({k: left[k]}) + ' != ' + saferepr({k: right[k]})]\n    extra_left = set_left - set_right\n    len_extra_left = len(extra_left)\n    if len_extra_left:\n        explanation.append('Left contains %d more item%s:' % (len_extra_left, '' if len_extra_left == 1 else 's'))\n        explanation.extend(pprint.pformat({k: left[k] for k in extra_left}).splitlines())\n    extra_right = set_right - set_left\n    len_extra_right = len(extra_right)\n    if len_extra_right:\n        explanation.append('Right contains %d more item%s:' % (len_extra_right, '' if len_extra_right == 1 else 's'))\n        explanation.extend(pprint.pformat({k: right[k] for k in extra_right}).splitlines())\n    return explanation",
    "label": true
  },
  {
    "code": "def _check_no_input(message: str) -> None:\n    if os.environ.get('PIP_NO_INPUT'):\n        raise Exception(f'No input was expected ($PIP_NO_INPUT set); question: {message}')",
    "label": true
  },
  {
    "code": "def get_entrypoints(dist: BaseDistribution) -> Tuple[Dict[str, str], Dict[str, str]]:\n    console_scripts = {}\n    gui_scripts = {}\n    for entry_point in dist.iter_entry_points():\n        if entry_point.group == 'console_scripts':\n            console_scripts[entry_point.name] = entry_point.value\n        elif entry_point.group == 'gui_scripts':\n            gui_scripts[entry_point.name] = entry_point.value\n    return (console_scripts, gui_scripts)",
    "label": true
  },
  {
    "code": "def build_function(name: str, args: list[str] | None=None, posonlyargs: list[str] | None=None, defaults: list[Any] | None=None, doc: str | None=None, kwonlyargs: list[str] | None=None, kwonlydefaults: list[Any] | None=None) -> nodes.FunctionDef:\n    func = nodes.FunctionDef(name)\n    argsnode = nodes.Arguments(parent=func)\n    if args is not None:\n        arguments = [nodes.AssignName(name=arg, parent=argsnode) for arg in args]\n    else:\n        arguments = None\n    default_nodes: list[nodes.NodeNG] | None = []\n    if defaults is not None:\n        for default in defaults:\n            default_node = nodes.const_factory(default)\n            default_node.parent = argsnode\n            default_nodes.append(default_node)\n    else:\n        default_nodes = None\n    kwonlydefault_nodes: list[nodes.NodeNG | None] | None = []\n    if kwonlydefaults is not None:\n        for kwonlydefault in kwonlydefaults:\n            kwonlydefault_node = nodes.const_factory(kwonlydefault)\n            kwonlydefault_node.parent = argsnode\n            kwonlydefault_nodes.append(kwonlydefault_node)\n    else:\n        kwonlydefault_nodes = None\n    argsnode.postinit(args=arguments, defaults=default_nodes, kwonlyargs=[nodes.AssignName(name=arg, parent=argsnode) for arg in kwonlyargs or ()], kw_defaults=kwonlydefault_nodes, annotations=[], posonlyargs=[nodes.AssignName(name=arg, parent=argsnode) for arg in posonlyargs or ()])\n    func.postinit(args=argsnode, body=[], doc_node=nodes.Const(value=doc) if doc else None)\n    if args:\n        register_arguments(func)\n    return func",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    reporter = TerminalReporter(config, sys.stdout)\n    config.pluginmanager.register(reporter, 'terminalreporter')\n    if config.option.debug or config.option.traceconfig:\n\n        def mywriter(tags, args):\n            msg = ' '.join(map(str, args))\n            reporter.write_line('[traceconfig] ' + msg)\n        config.trace.root.setprocessor('pytest:config', mywriter)",
    "label": true
  },
  {
    "code": "def get_required_dists(dists, dist):\n    if dist not in dists:\n        raise DistlibException('given distribution %r is not a member of the list' % dist.name)\n    graph = make_graph(dists)\n    req = set()\n    todo = graph.adjacency_list[dist]\n    seen = set((t[0] for t in todo))\n    while todo:\n        d = todo.pop()[0]\n        req.add(d)\n        pred_list = graph.adjacency_list[d]\n        for pred in pred_list:\n            d = pred[0]\n            if d not in req and d not in seen:\n                seen.add(d)\n                todo.append(pred)\n    return req",
    "label": true
  },
  {
    "code": "def _default_success_debug_action(instring: str, startloc: int, endloc: int, expr: 'ParserElement', toks: ParseResults, cache_hit: bool=False):\n    cache_hit_str = '*' if cache_hit else ''\n    print(f'{cache_hit_str}Matched {expr} -> {toks.as_list()}')",
    "label": true
  },
  {
    "code": "def create_Callable(args: Iterable[type], rtype: type, class_poly_vars: Set[type]=None) -> Callable:\n    poly_vars = set(class_poly_vars) if class_poly_vars else set()\n    c = Callable.copy_with(tuple([*args, rtype]))\n    poly_vars.update(_get_poly_vars(c))\n    c.__polymorphic_tvars__ = frozenset(poly_vars)\n    return c",
    "label": true
  },
  {
    "code": "def safe_range(*args: int) -> range:\n    rng = range(*args)\n    if len(rng) > MAX_RANGE:\n        raise OverflowError(f'Range too big. The sandbox blocks ranges larger than MAX_RANGE ({MAX_RANGE}).')\n    return rng",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable, key=None):\n    seen = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def _make_new_gettext(func: t.Callable[[str], str]) -> t.Callable[..., str]:\n\n    @pass_context\n    def gettext(__context: Context, __string: str, **variables: t.Any) -> str:\n        rv = __context.call(func, __string)\n        if __context.eval_ctx.autoescape:\n            rv = Markup(rv)\n        return rv % variables\n    return gettext",
    "label": true
  },
  {
    "code": "def create_tree(base_dir, files, mode=511, verbose=1, dry_run=0):\n    need_dir = set()\n    for file in files:\n        need_dir.add(os.path.join(base_dir, os.path.dirname(file)))\n    for dir in sorted(need_dir):\n        mkpath(dir, mode, verbose=verbose, dry_run=dry_run)",
    "label": true
  },
  {
    "code": "def get_build_platform():\n    from sysconfig import get_platform\n    plat = get_platform()\n    if sys.platform == 'darwin' and (not plat.startswith('macosx-')):\n        try:\n            version = _macos_vers()\n            machine = os.uname()[4].replace(' ', '_')\n            return 'macosx-%d.%d-%s' % (int(version[0]), int(version[1]), _macos_arch(machine))\n        except ValueError:\n            pass\n    return plat",
    "label": true
  },
  {
    "code": "def get_requires_for_build_sdist(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_sdist\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)",
    "label": true
  },
  {
    "code": "def generate_metadata(build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str) -> str:\n    metadata_tmpdir = TempDirectory(kind='modern-metadata', globally_managed=True)\n    metadata_dir = metadata_tmpdir.path\n    with build_env:\n        runner = runner_with_spinner_message('Preparing metadata (pyproject.toml)')\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n    return os.path.join(metadata_dir, distinfo_dir)",
    "label": true
  },
  {
    "code": "def is_postponed_evaluation_enabled(node: nodes.NodeNG) -> bool:\n    module = node.root()\n    return 'annotations' in module.future_imports",
    "label": true
  },
  {
    "code": "def _ancestors_to_call(klass_node: nodes.ClassDef, method_name: str='__init__') -> dict[nodes.ClassDef, bases.UnboundMethod]:\n    to_call: dict[nodes.ClassDef, bases.UnboundMethod] = {}\n    for base_node in klass_node.ancestors(recurs=False):\n        try:\n            init_node = next(base_node.igetattr(method_name))\n            if not isinstance(init_node, astroid.UnboundMethod):\n                continue\n            if init_node.is_abstract():\n                continue\n            to_call[base_node] = init_node\n        except astroid.InferenceError:\n            continue\n    return to_call",
    "label": true
  },
  {
    "code": "def _create_weakref(obj, *args):\n    from weakref import ref\n    if obj is None:\n        from collections import UserDict\n        return ref(UserDict(), *args)\n    return ref(obj, *args)",
    "label": true
  },
  {
    "code": "def _set_platform_dir_class() -> type[PlatformDirsABC]:\n    if sys.platform == 'win32':\n        from .windows import Windows as Result\n    elif sys.platform == 'darwin':\n        from .macos import MacOS as Result\n    else:\n        from .unix import Unix as Result\n    if os.getenv('ANDROID_DATA') == '/data' and os.getenv('ANDROID_ROOT') == '/system':\n        if os.getenv('SHELL') or os.getenv('PREFIX'):\n            return Result\n        from .android import _android_folder\n        if _android_folder() is not None:\n            from .android import Android\n            return Android\n    return Result",
    "label": true
  },
  {
    "code": "def with_class(classname, namespace=''):\n    classattr = f'{namespace}:class' if namespace else 'class'\n    return with_attribute(**{classattr: classname})",
    "label": true
  },
  {
    "code": "def _assert_no_error(error, exception_class=None):\n    if error == 0:\n        return\n    cf_error_string = Security.SecCopyErrorMessageString(error, None)\n    output = _cf_string_to_unicode(cf_error_string)\n    CoreFoundation.CFRelease(cf_error_string)\n    if output is None or output == u'':\n        output = u'OSStatus %s' % error\n    if exception_class is None:\n        exception_class = ssl.SSLError\n    raise exception_class(output)",
    "label": true
  },
  {
    "code": "def _require_version_compare(fn: Callable[['Specifier', ParsedVersion, str], bool]) -> Callable[['Specifier', ParsedVersion, str], bool]:\n\n    @functools.wraps(fn)\n    def wrapped(self: 'Specifier', prospective: ParsedVersion, spec: str) -> bool:\n        if not isinstance(prospective, Version):\n            return False\n        return fn(self, prospective, spec)\n    return wrapped",
    "label": true
  },
  {
    "code": "def platform_tags(linux: str, arch: str) -> Iterator[str]:\n    if not _have_compatible_abi(arch):\n        return\n    too_old_glibc2 = _GLibCVersion(2, 16)\n    if arch in {'x86_64', 'i686'}:\n        too_old_glibc2 = _GLibCVersion(2, 4)\n    current_glibc = _GLibCVersion(*_get_glibc_version())\n    glibc_max_list = [current_glibc]\n    for glibc_major in range(current_glibc.major - 1, 1, -1):\n        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n    for glibc_max in glibc_max_list:\n        if glibc_max.major == too_old_glibc2.major:\n            min_minor = too_old_glibc2.minor\n        else:\n            min_minor = -1\n        for glibc_minor in range(glibc_max.minor, min_minor, -1):\n            glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n            tag = 'manylinux_{}_{}'.format(*glibc_version)\n            if _is_compatible(tag, arch, glibc_version):\n                yield linux.replace('linux', tag)\n            if glibc_version in _LEGACY_MANYLINUX_MAP:\n                legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n                if _is_compatible(legacy_tag, arch, glibc_version):\n                    yield linux.replace('linux', legacy_tag)",
    "label": true
  },
  {
    "code": "def _function_type(function: nodes.Lambda | bases.UnboundMethod, builtins: nodes.Module) -> nodes.ClassDef:\n    if isinstance(function, scoped_nodes.Lambda):\n        if function.root().name == 'builtins':\n            cls_name = 'builtin_function_or_method'\n        else:\n            cls_name = 'function'\n    elif isinstance(function, bases.BoundMethod):\n        cls_name = 'method'\n    else:\n        cls_name = 'function'\n    return _build_proxy_class(cls_name, builtins)",
    "label": true
  },
  {
    "code": "def nth_product(index, *args):\n    pools = list(map(tuple, reversed(args)))\n    ns = list(map(len, pools))\n    c = reduce(mul, ns)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    result = []\n    for pool, n in zip(pools, ns):\n        result.append(pool[index % n])\n        index //= n\n    return tuple(reversed(result))",
    "label": true
  },
  {
    "code": "def process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))",
    "label": true
  },
  {
    "code": "def get_python_path(filepath: str) -> str:\n    warnings.warn(\"get_python_path has been deprecated because assumption that there's always an __init__.py is not true since python 3.3 and is causing problems, particularly with PEP 420.Use discover_package_path and pass source root(s).\", DeprecationWarning, stacklevel=2)\n    return discover_package_path(filepath, [])",
    "label": true
  },
  {
    "code": "def _looks_like_dataclass_decorator(node: nodes.NodeNG, decorator_names: frozenset[str]=DATACLASSES_DECORATORS) -> bool:\n    if isinstance(node, nodes.Call):\n        node = node.func\n    try:\n        inferred = next(node.infer())\n    except (InferenceError, StopIteration):\n        inferred = Uninferable\n    if isinstance(inferred, UninferableBase):\n        if isinstance(node, nodes.Name):\n            return node.name in decorator_names\n        if isinstance(node, nodes.Attribute):\n            return node.attrname in decorator_names\n        return False\n    return isinstance(inferred, nodes.FunctionDef) and inferred.name in decorator_names and (inferred.root().name in DATACLASS_MODULES)",
    "label": true
  },
  {
    "code": "def _collect_type_vars(types, typevar_types=None):\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if isinstance(t, typevar_types) and t not in tvars and (not _is_unpack(t)):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)",
    "label": true
  },
  {
    "code": "def _check_argument_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_snake_case(name):\n        error_msgs.append(f'Argument name \"{name}\" should be in snake_case format. Argument names should be lowercase, with words separated by underscores. A single leading underscore can be used to indicate that the argument is not being used but is still needed somehow.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def unzip_file(filename: str, location: str, flatten: bool=True) -> None:\n    ensure_dir(location)\n    zipfp = open(filename, 'rb')\n    try:\n        zip = zipfile.ZipFile(zipfp, allowZip64=True)\n        leading = has_leading_dir(zip.namelist()) and flatten\n        for info in zip.infolist():\n            name = info.filename\n            fn = name\n            if leading:\n                fn = split_leading_dir(name)[1]\n            fn = os.path.join(location, fn)\n            dir = os.path.dirname(fn)\n            if not is_within_directory(location, fn):\n                message = 'The zip file ({}) has a file ({}) trying to install outside target directory ({})'\n                raise InstallationError(message.format(filename, fn, location))\n            if fn.endswith('/') or fn.endswith('\\\\'):\n                ensure_dir(fn)\n            else:\n                ensure_dir(dir)\n                fp = zip.open(name)\n                try:\n                    with open(fn, 'wb') as destfp:\n                        shutil.copyfileobj(fp, destfp)\n                finally:\n                    fp.close()\n                    if zip_item_is_executable(info):\n                        set_extracted_file_to_default_mode_plus_executable(fn)\n    finally:\n        zipfp.close()",
    "label": true
  },
  {
    "code": "def node_frame_class(node: nodes.NodeNG) -> nodes.ClassDef | None:\n    klass = node.frame(future=True)\n    nodes_to_check = (nodes.NodeNG, astroid.UnboundMethod, astroid.BaseInstance)\n    while klass and isinstance(klass, nodes_to_check) and (not isinstance(klass, nodes.ClassDef)):\n        if klass.parent is None:\n            return None\n        klass = klass.parent.frame(future=True)\n    return klass",
    "label": true
  },
  {
    "code": "def _augment_exception(exc, version, arch=''):\n    message = exc.args[0]\n    if 'vcvarsall' in message.lower() or 'visual c' in message.lower():\n        tmpl = 'Microsoft Visual C++ {version:0.1f} or greater is required.'\n        message = tmpl.format(**locals())\n        msdownload = 'www.microsoft.com/download/details.aspx?id=%d'\n        if version == 9.0:\n            if arch.lower().find('ia64') > -1:\n                message += ' Get it with \"Microsoft Windows SDK 7.0\"'\n            else:\n                message += ' Get it from http://aka.ms/vcpython27'\n        elif version == 10.0:\n            message += ' Get it with \"Microsoft Windows SDK 7.1\": '\n            message += msdownload % 8279\n        elif version >= 14.0:\n            message += ' Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/'\n    exc.args = (message,)",
    "label": true
  },
  {
    "code": "def merge_dicts(d1: dict, d2: dict) -> dict:\n    for k, v in d2.items():\n        if k in d1 and isinstance(d1[k], dict) and isinstance(v, Mapping):\n            merge_dicts(d1[k], v)\n        else:\n            d1[k] = d2[k]",
    "label": true
  },
  {
    "code": "def build_sdist(sdist_directory, config_settings):\n    backend = _build_backend()\n    try:\n        return backend.build_sdist(sdist_directory, config_settings)\n    except getattr(backend, 'UnsupportedOperation', _DummyException):\n        raise GotUnsupportedOperation(traceback.format_exc())",
    "label": true
  },
  {
    "code": "def implements(obj: BaseChecker, interface: type[Interface] | tuple[type[Interface], ...]) -> bool:\n    warnings.warn('implements has been deprecated in favour of using basic inheritance patterns without using __implements__.', DeprecationWarning, stacklevel=2)\n    implements_ = getattr(obj, '__implements__', ())\n    if not isinstance(implements_, (list, tuple)):\n        implements_ = (implements_,)\n    return any((issubclass(i, interface) for i in implements_))",
    "label": true
  },
  {
    "code": "def docstringify(docstring: nodes.Const | None, default_type: str='default') -> Docstring:\n    best_match = (0, DOCSTRING_TYPES.get(default_type, Docstring)(docstring))\n    for docstring_type in (SphinxDocstring, EpytextDocstring, GoogleDocstring, NumpyDocstring):\n        instance = docstring_type(docstring)\n        matching_sections = instance.matching_sections()\n        if matching_sections > best_match[0]:\n            best_match = (matching_sections, instance)\n    return best_match[1]",
    "label": true
  },
  {
    "code": "def _write_contents(target, source):\n    child = target.joinpath(source.name)\n    if source.is_dir():\n        child.mkdir()\n        for item in source.iterdir():\n            _write_contents(child, item)\n    else:\n        child.write_bytes(source.read_bytes())\n    return child",
    "label": true
  },
  {
    "code": "def _looks_like_dataclass_field_call(node: nodes.Call, check_scope: bool=True) -> bool:\n    if check_scope:\n        stmt = node.statement(future=True)\n        scope = stmt.scope()\n        if not (isinstance(stmt, nodes.AnnAssign) and stmt.value is not None and isinstance(scope, nodes.ClassDef) and is_decorated_with_dataclass(scope)):\n            return False\n    try:\n        inferred = next(node.func.infer())\n    except (InferenceError, StopIteration):\n        return False\n    if not isinstance(inferred, nodes.FunctionDef):\n        return False\n    return inferred.name == FIELD_NAME and inferred.root().name in DATACLASS_MODULES",
    "label": true
  },
  {
    "code": "def _parse_version_parts(s: str) -> Iterator[str]:\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n        if not part or part == '.':\n            continue\n        if part[:1] in '0123456789':\n            yield part.zfill(8)\n        else:\n            yield ('*' + part)\n    yield '*final'",
    "label": true
  },
  {
    "code": "def _ssl_wrap_socket_impl(sock, ssl_context, tls_in_tls, server_hostname=None):\n    if tls_in_tls:\n        if not SSLTransport:\n            raise ProxySchemeUnsupported(\"TLS in TLS requires support for the 'ssl' module\")\n        SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n        return SSLTransport(sock, ssl_context, server_hostname)\n    if server_hostname:\n        return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n    else:\n        return ssl_context.wrap_socket(sock)",
    "label": true
  },
  {
    "code": "def rewind_body(body: typing.IO[typing.AnyStr], body_pos: _TYPE_BODY_POSITION) -> None:\n    body_seek = getattr(body, 'seek', None)\n    if body_seek is not None and isinstance(body_pos, int):\n        try:\n            body_seek(body_pos)\n        except OSError as e:\n            raise UnrewindableBodyError('An error occurred when rewinding request body for redirect/retry.') from e\n    elif body_pos is _FAILEDTELL:\n        raise UnrewindableBodyError('Unable to record file position for rewinding request body during a redirect/retry.')\n    else:\n        raise ValueError(f'body_pos must be of type integer, instead it was {type(body_pos)}.')",
    "label": true
  },
  {
    "code": "def match_only_at_col(n):\n\n    def verify_col(strg, locn, toks):\n        if col(locn, strg) != n:\n            raise ParseException(strg, locn, f'matched token not at column {n}')\n    return verify_col",
    "label": true
  },
  {
    "code": "def parse_inline_table(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, dict]:\n    pos += 1\n    nested_dict = NestedDict()\n    flags = Flags()\n    pos = skip_chars(src, pos, TOML_WS)\n    if src.startswith('}', pos):\n        return (pos + 1, nested_dict.dict)\n    while True:\n        pos, key, value = parse_key_value_pair(src, pos, parse_float)\n        key_parent, key_stem = (key[:-1], key[-1])\n        if flags.is_(key, Flags.FROZEN):\n            raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n        try:\n            nest = nested_dict.get_or_create_nest(key_parent, access_lists=False)\n        except KeyError:\n            raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n        if key_stem in nest:\n            raise suffixed_err(src, pos, f'Duplicate inline table key {key_stem!r}')\n        nest[key_stem] = value\n        pos = skip_chars(src, pos, TOML_WS)\n        c = src[pos:pos + 1]\n        if c == '}':\n            return (pos + 1, nested_dict.dict)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed inline table')\n        if isinstance(value, (dict, list)):\n            flags.set(key, Flags.FROZEN, recursive=True)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def check_legacy_setup_py_options(options: Values, reqs: List[InstallRequirement]) -> None:\n    has_build_options = _has_option(options, reqs, 'build_options')\n    has_global_options = _has_option(options, reqs, 'global_options')\n    if has_build_options or has_global_options:\n        deprecated(reason='--build-option and --global-option are deprecated.', issue=11859, replacement='to use --config-settings', gone_in='23.3')\n        logger.warning('Implying --no-binary=:all: due to the presence of --build-option / --global-option. ')\n        options.format_control.disallow_binaries()",
    "label": true
  },
  {
    "code": "def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    if session_hooks is None or session_hooks.get('response') == []:\n        return request_hooks\n    if request_hooks is None or request_hooks.get('response') == []:\n        return session_hooks\n    return merge_setting(request_hooks, session_hooks, dict_class)",
    "label": true
  },
  {
    "code": "def unpack(path: str, dest: str='.') -> None:\n    with WheelFile(path) as wf:\n        namever = wf.parsed_filename.group('namever')\n        destination = Path(dest) / namever\n        print(f'Unpacking to: {destination}...', end='', flush=True)\n        for zinfo in wf.filelist:\n            wf.extract(zinfo, destination)\n            permissions = zinfo.external_attr >> 16 & 511\n            destination.joinpath(zinfo.filename).chmod(permissions)\n    print('OK')",
    "label": true
  },
  {
    "code": "def doubler(f):\n\n    def inner(*args, **kwds):\n        fx = f(*args, **kwds)\n        return 2 * fx\n    return inner",
    "label": true
  },
  {
    "code": "def fancy_getopt(options, negative_opt, object, args):\n    parser = FancyGetopt(options)\n    parser.set_negative_aliases(negative_opt)\n    return parser.getopt(args, object)",
    "label": true
  },
  {
    "code": "def parse_key_value_pair(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, Key, Any]:\n    pos, key = parse_key(src, pos)\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char != '=':\n        raise suffixed_err(src, pos, \"Expected '=' after a key in a key/value pair\")\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, value = parse_value(src, pos, parse_float)\n    return (pos, key, value)",
    "label": true
  },
  {
    "code": "def setup_logging(verbosity: int, no_color: bool, user_log_file: Optional[str]) -> int:\n    if verbosity >= 2:\n        level_number = logging.DEBUG\n    elif verbosity == 1:\n        level_number = VERBOSE\n    elif verbosity == -1:\n        level_number = logging.WARNING\n    elif verbosity == -2:\n        level_number = logging.ERROR\n    elif verbosity <= -3:\n        level_number = logging.CRITICAL\n    else:\n        level_number = logging.INFO\n    level = logging.getLevelName(level_number)\n    include_user_log = user_log_file is not None\n    if include_user_log:\n        additional_log_file = user_log_file\n        root_level = 'DEBUG'\n    else:\n        additional_log_file = '/dev/null'\n        root_level = level\n    vendored_log_level = 'WARNING' if level in ['INFO', 'ERROR'] else 'DEBUG'\n    log_streams = {'stdout': 'ext://sys.stdout', 'stderr': 'ext://sys.stderr'}\n    handler_classes = {'stream': 'pip._internal.utils.logging.RichPipStreamHandler', 'file': 'pip._internal.utils.logging.BetterRotatingFileHandler'}\n    handlers = ['console', 'console_errors', 'console_subprocess'] + (['user_log'] if include_user_log else [])\n    logging.config.dictConfig({'version': 1, 'disable_existing_loggers': False, 'filters': {'exclude_warnings': {'()': 'pip._internal.utils.logging.MaxLevelFilter', 'level': logging.WARNING}, 'restrict_to_subprocess': {'()': 'logging.Filter', 'name': subprocess_logger.name}, 'exclude_subprocess': {'()': 'pip._internal.utils.logging.ExcludeLoggerFilter', 'name': subprocess_logger.name}}, 'formatters': {'indent': {'()': IndentingFormatter, 'format': '%(message)s'}, 'indent_with_timestamp': {'()': IndentingFormatter, 'format': '%(message)s', 'add_timestamp': True}}, 'handlers': {'console': {'level': level, 'class': handler_classes['stream'], 'no_color': no_color, 'stream': log_streams['stdout'], 'filters': ['exclude_subprocess', 'exclude_warnings'], 'formatter': 'indent'}, 'console_errors': {'level': 'WARNING', 'class': handler_classes['stream'], 'no_color': no_color, 'stream': log_streams['stderr'], 'filters': ['exclude_subprocess'], 'formatter': 'indent'}, 'console_subprocess': {'level': level, 'class': handler_classes['stream'], 'stream': log_streams['stderr'], 'no_color': no_color, 'filters': ['restrict_to_subprocess'], 'formatter': 'indent'}, 'user_log': {'level': 'DEBUG', 'class': handler_classes['file'], 'filename': additional_log_file, 'encoding': 'utf-8', 'delay': True, 'formatter': 'indent_with_timestamp'}}, 'root': {'level': root_level, 'handlers': handlers}, 'loggers': {'pip._vendor': {'level': vendored_log_level}}})\n    return level_number",
    "label": true
  },
  {
    "code": "def unpack(src_dir, dst_dir):\n    for dirpath, dirnames, filenames in os.walk(src_dir):\n        subdir = os.path.relpath(dirpath, src_dir)\n        for f in filenames:\n            src = os.path.join(dirpath, f)\n            dst = os.path.join(dst_dir, subdir, f)\n            os.renames(src, dst)\n        for n, d in reversed(list(enumerate(dirnames))):\n            src = os.path.join(dirpath, d)\n            dst = os.path.join(dst_dir, subdir, d)\n            if not os.path.exists(dst):\n                os.renames(src, dst)\n                del dirnames[n]\n    for dirpath, dirnames, filenames in os.walk(src_dir, topdown=True):\n        assert not filenames\n        os.rmdir(dirpath)",
    "label": true
  },
  {
    "code": "def _have_compatible_abi(executable: str, archs: Sequence[str]) -> bool:\n    if 'armv7l' in archs:\n        return _is_linux_armhf(executable)\n    if 'i686' in archs:\n        return _is_linux_i686(executable)\n    allowed_archs = {'x86_64', 'aarch64', 'ppc64', 'ppc64le', 's390x', 'loongarch64'}\n    return any((arch in allowed_archs for arch in archs))",
    "label": true
  },
  {
    "code": "def load(fp: IO, *, parse_float: ParseFloat=float) -> Dict[str, Any]:\n    s = fp.read()\n    if isinstance(s, bytes):\n        s = s.decode()\n    else:\n        warnings.warn('Text file object support is deprecated in favor of binary file objects. Use `open(\"foo.toml\", \"rb\")` to open the file in binary mode.', DeprecationWarning)\n    return loads(s, parse_float=parse_float)",
    "label": true
  },
  {
    "code": "def get_optionflags(parent):\n    optionflags_str = parent.config.getini('doctest_optionflags')\n    flag_lookup_table = _get_flag_lookup()\n    flag_acc = 0\n    for flag in optionflags_str:\n        flag_acc |= flag_lookup_table[flag]\n    return flag_acc",
    "label": true
  },
  {
    "code": "def is_decorated_with_attrs(node, decorator_names=ATTRS_NAMES) -> bool:\n    if not node.decorators:\n        return False\n    for decorator_attribute in node.decorators.nodes:\n        if isinstance(decorator_attribute, Call):\n            decorator_attribute = decorator_attribute.func\n        if decorator_attribute.as_string() in decorator_names:\n            return True\n        inferred = safe_infer(decorator_attribute)\n        if inferred and inferred.root().name == 'attr._next_gen':\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _get_codes_helper(tree: HuffmanTree, path: str) -> dict[int, str]:\n    if tree.is_leaf():\n        return {tree.symbol: path}\n    else:\n        left = _get_codes_helper(tree.left, path + '0')\n        right = _get_codes_helper(tree.right, path + '1')\n        left.update(right)\n        return left",
    "label": true
  },
  {
    "code": "def _infer_user() -> str:\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API('user')\n    if is_osx_framework() and (not running_under_virtualenv()):\n        suffixed = 'osx_framework_user'\n    else:\n        suffixed = f'{os.name}_user'\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if 'posix_user' not in _AVAILABLE_SCHEMES:\n        raise UserInstallationInvalid()\n    return 'posix_user'",
    "label": true
  },
  {
    "code": "def get_bin_prefix() -> str:\n    prefix = os.path.normpath(sys.prefix)\n    if WINDOWS:\n        bin_py = os.path.join(prefix, 'Scripts')\n        if not os.path.exists(bin_py):\n            bin_py = os.path.join(prefix, 'bin')\n        return bin_py\n    if sys.platform[:6] == 'darwin' and prefix[:16] == '/System/Library/':\n        return '/usr/local/bin'\n    return os.path.join(prefix, 'bin')",
    "label": true
  },
  {
    "code": "def escape(pathname):\n    drive, pathname = os.path.splitdrive(pathname)\n    if isinstance(pathname, bytes):\n        pathname = magic_check_bytes.sub(b'[\\\\1]', pathname)\n    else:\n        pathname = magic_check.sub('[\\\\1]', pathname)\n    return drive + pathname",
    "label": true
  },
  {
    "code": "def constrained_batches(iterable, max_size, max_count=None, get_len=len, strict=True):\n    if max_size <= 0:\n        raise ValueError('maximum size must be greater than zero')\n    batch = []\n    batch_size = 0\n    batch_count = 0\n    for item in iterable:\n        item_len = get_len(item)\n        if strict and item_len > max_size:\n            raise ValueError('item size exceeds maximum size')\n        reached_count = batch_count == max_count\n        reached_size = item_len + batch_size > max_size\n        if batch_count and (reached_size or reached_count):\n            yield tuple(batch)\n            batch.clear()\n            batch_size = 0\n            batch_count = 0\n        batch.append(item)\n        batch_size += item_len\n        batch_count += 1\n    if batch:\n        yield tuple(batch)",
    "label": true
  },
  {
    "code": "def _hash_dict(d: Dict[str, str]) -> str:\n    s = json.dumps(d, sort_keys=True, separators=(',', ':'), ensure_ascii=True)\n    return hashlib.sha224(s.encode('ascii')).hexdigest()",
    "label": true
  },
  {
    "code": "def optimize(node: nodes.Node, environment: 'Environment') -> nodes.Node:\n    optimizer = Optimizer(environment)\n    return t.cast(nodes.Node, optimizer.visit(node))",
    "label": true
  },
  {
    "code": "def filesys_decode(path):\n    if isinstance(path, str):\n        return path\n    fs_enc = sys.getfilesystemencoding() or 'utf-8'\n    candidates = (fs_enc, 'utf-8')\n    for enc in candidates:\n        try:\n            return path.decode(enc)\n        except UnicodeDecodeError:\n            continue",
    "label": true
  },
  {
    "code": "def condition_as_parse_action(fn: ParseCondition, message: typing.Optional[str]=None, fatal: bool=False) -> ParseAction:\n    msg = message if message is not None else 'failed user-defined condition'\n    exc_type = ParseFatalException if fatal else ParseException\n    fn = _trim_arity(fn)\n\n    @wraps(fn)\n    def pa(s, l, t):\n        if not bool(fn(s, l, t)):\n            raise exc_type(s, l, msg)\n    return pa",
    "label": true
  },
  {
    "code": "def _is_in_upper_case_with_underscores(name: str) -> bool:\n    pattern = '(_?[A-Z][A-Z0-9_]*)$'\n    return re.match(pattern, name) is not None",
    "label": true
  },
  {
    "code": "def _get_external_data(url):\n    result = {}\n    try:\n        resp = urlopen(url)\n        headers = resp.info()\n        ct = headers.get('Content-Type')\n        if not ct.startswith('application/json'):\n            logger.debug('Unexpected response for JSON request: %s', ct)\n        else:\n            reader = codecs.getreader('utf-8')(resp)\n            result = json.load(reader)\n    except Exception as e:\n        logger.exception('Failed to get external data for %s: %s', url, e)\n    return result",
    "label": true
  },
  {
    "code": "def unsafe(f: F) -> F:\n    f.unsafe_callable = True\n    return f",
    "label": true
  },
  {
    "code": "def urlsafe_b64decode(data: bytes) -> bytes:\n    pad = b'=' * (4 - (len(data) & 3))\n    return base64.urlsafe_b64decode(data + pad)",
    "label": true
  },
  {
    "code": "def _build_result(state):\n    mapping = state.mapping\n    all_keys = {id(v): k for k, v in mapping.items()}\n    all_keys[id(None)] = None\n    graph = DirectedGraph()\n    graph.add(None)\n    connected = {None}\n    for key, criterion in state.criteria.items():\n        if not _has_route_to_root(state.criteria, key, all_keys, connected):\n            continue\n        if key not in graph:\n            graph.add(key)\n        for p in criterion.iter_parent():\n            try:\n                pkey = all_keys[id(p)]\n            except KeyError:\n                continue\n            if pkey not in graph:\n                graph.add(pkey)\n            graph.connect(pkey, key)\n    return Result(mapping={k: v for k, v in mapping.items() if k in connected}, graph=graph, criteria=state.criteria)",
    "label": true
  },
  {
    "code": "def direct_url_as_pep440_direct_reference(direct_url: DirectUrl, name: str) -> str:\n    direct_url.validate()\n    requirement = name + ' @ '\n    fragments = []\n    if isinstance(direct_url.info, VcsInfo):\n        requirement += '{}+{}@{}'.format(direct_url.info.vcs, direct_url.url, direct_url.info.commit_id)\n    elif isinstance(direct_url.info, ArchiveInfo):\n        requirement += direct_url.url\n        if direct_url.info.hash:\n            fragments.append(direct_url.info.hash)\n    else:\n        assert isinstance(direct_url.info, DirInfo)\n        requirement += direct_url.url\n    if direct_url.subdirectory:\n        fragments.append('subdirectory=' + direct_url.subdirectory)\n    if fragments:\n        requirement += '#' + '&'.join(fragments)\n    return requirement",
    "label": true
  },
  {
    "code": "def connection_requires_http_tunnel(proxy_url=None, proxy_config=None, destination_scheme=None):\n    if proxy_url is None:\n        return False\n    if destination_scheme == 'http':\n        return False\n    if proxy_url.scheme == 'https' and proxy_config and proxy_config.use_forwarding_for_https:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _set_config(dist: 'Distribution', field: str, value: Any):\n    setter = getattr(dist.metadata, f'set_{field}', None)\n    if setter:\n        setter(value)\n    elif hasattr(dist.metadata, field) or field in SETUPTOOLS_PATCHES:\n        setattr(dist.metadata, field, value)\n    else:\n        setattr(dist, field, value)",
    "label": true
  },
  {
    "code": "def _looks_like_wheel(location: str) -> bool:\n    if not location.endswith(WHEEL_EXTENSION):\n        return False\n    if not os.path.isfile(location):\n        return False\n    if not Wheel.wheel_file_re.match(os.path.basename(location)):\n        return False\n    return zipfile.is_zipfile(location)",
    "label": true
  },
  {
    "code": "def _expand_default(self: optparse.HelpFormatter, option: Option) -> str:\n    if self.parser is None or not self.default_tag:\n        return str(option.help)\n    optname = option._long_opts[0][2:]\n    try:\n        provider = self.parser.options_manager._all_options[optname]\n    except KeyError:\n        value = None\n    else:\n        optdict = provider.get_option_def(optname)\n        optname = provider.option_attrname(optname, optdict)\n        value = getattr(provider.config, optname, optdict)\n        value = utils._format_option_value(optdict, value)\n    if value is optparse.NO_DEFAULT or not value:\n        value = self.NO_DEFAULT_VALUE\n    return option.help.replace(self.default_tag, str(value))",
    "label": true
  },
  {
    "code": "def is_class_attr(name: str, klass: nodes.ClassDef) -> bool:\n    try:\n        klass.getattr(name)\n        return True\n    except astroid.NotFoundError:\n        return False",
    "label": true
  },
  {
    "code": "def _remove_path_dot_segments(path: str) -> str:\n    segments = path.split('/')\n    output = []\n    for segment in segments:\n        if segment == '.':\n            continue\n        if segment != '..':\n            output.append(segment)\n        elif output:\n            output.pop()\n    if path.startswith('/') and (not output or output[0]):\n        output.insert(0, '')\n    if path.endswith(('/.', '/..')):\n        output.append('')\n    return '/'.join(output)",
    "label": true
  },
  {
    "code": "def _replace_return_val_assertion(assertion: str, return_val_var_name: Optional[str]) -> str:\n    if FUNCTION_RETURN_VALUE in assertion:\n        return assertion.replace(FUNCTION_RETURN_VALUE, return_val_var_name)\n    return assertion",
    "label": true
  },
  {
    "code": "def report_total_messages_stats(sect: Section, stats: LinterStats, previous_stats: LinterStats | None) -> None:\n    lines = ['type', 'number', 'previous', 'difference']\n    lines += checkers.table_lines_from_stats(stats, previous_stats, 'message_types')\n    sect.append(Table(children=lines, cols=4, rheaders=1))",
    "label": true
  },
  {
    "code": "def emit_pragma_representer(action: str, messages: list[str]) -> PragmaRepresenter:\n    if not messages and action in MESSAGE_KEYWORDS:\n        raise InvalidPragmaError('The keyword is not followed by message identifier', action)\n    return PragmaRepresenter(action, messages)",
    "label": true
  },
  {
    "code": "def _detect_program_name(path: t.Optional[str]=None, _main: t.Optional[ModuleType]=None) -> str:\n    if _main is None:\n        _main = sys.modules['__main__']\n    if not path:\n        path = sys.argv[0]\n    if getattr(_main, '__package__', None) in {None, ''} or (os.name == 'nt' and _main.__package__ == '' and (not os.path.exists(path)) and os.path.exists(f'{path}.exe')):\n        return os.path.basename(path)\n    py_module = t.cast(str, _main.__package__)\n    name = os.path.splitext(os.path.basename(path))[0]\n    if name != '__main__':\n        py_module = f'{py_module}.{name}'\n    return f\"python -m {py_module.lstrip('.')}\"",
    "label": true
  },
  {
    "code": "def _qualified_names(modname: str | None) -> list[str]:\n    names = modname.split('.') if modname is not None else ''\n    return ['.'.join(names[0:i + 1]) for i in range(len(names))]",
    "label": true
  },
  {
    "code": "def original_text_for(expr: ParserElement, as_string: bool=True, *, asString: bool=True) -> ParserElement:\n    asString = asString and as_string\n    locMarker = Empty().set_parse_action(lambda s, loc, t: loc)\n    endlocMarker = locMarker.copy()\n    endlocMarker.callPreparse = False\n    matchExpr = locMarker('_original_start') + expr + endlocMarker('_original_end')\n    if asString:\n        extractText = lambda s, l, t: s[t._original_start:t._original_end]\n    else:\n\n        def extractText(s, l, t):\n            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]\n    matchExpr.set_parse_action(extractText)\n    matchExpr.ignoreExprs = expr.ignoreExprs\n    matchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)\n    return matchExpr",
    "label": true
  },
  {
    "code": "def _nose_tools_trivial_transform():\n    stub = _BUILDER.string_build('__all__ = []')\n    all_entries = ['ok_', 'eq_']\n    for pep8_name, method in _nose_tools_functions():\n        all_entries.append(pep8_name)\n        stub[pep8_name] = method\n    all_assign = stub['__all__'].parent\n    all_object = astroid.List(all_entries)\n    all_object.parent = all_assign\n    all_assign.value = all_object\n    return stub",
    "label": true
  },
  {
    "code": "def _index_name_nodes(index: str, loop_node: Union[nodes.For, nodes.Comprehension]) -> List[Union[nodes.AssignName, nodes.Name]]:\n    scope = loop_node.scope()\n    if isinstance(loop_node, nodes.For):\n        body = loop_node\n    else:\n        body = loop_node.parent\n    return [name_node for name_node in body.nodes_of_class((nodes.AssignName, nodes.Name)) if name_node.name == index and name_node != loop_node.target and (name_node.lookup(name_node.name)[0] == scope)]",
    "label": true
  },
  {
    "code": "def _get_user_dirs_folder(key: str) -> str | None:\n    user_dirs_config_path = Path(Unix().user_config_dir) / 'user-dirs.dirs'\n    if user_dirs_config_path.exists():\n        parser = ConfigParser()\n        with user_dirs_config_path.open() as stream:\n            parser.read_string(f'[top]\\n{stream.read()}')\n        if key not in parser['top']:\n            return None\n        path = parser['top'][key].strip('\"')\n        return path.replace('$HOME', os.path.expanduser('~'))\n    return None",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def do_override():\n    if enabled():\n        warn_distutils_present()\n        ensure_local_distutils()",
    "label": true
  },
  {
    "code": "def _csv_open(fn, mode, **kwargs):\n    if sys.version_info[0] < 3:\n        mode += 'b'\n    else:\n        kwargs['newline'] = ''\n        kwargs['encoding'] = 'utf-8'\n    return open(fn, mode, **kwargs)",
    "label": true
  },
  {
    "code": "def unpack_directory(filename, extract_dir, progress_filter=default_filter):\n    if not os.path.isdir(filename):\n        raise UnrecognizedFormat('%s is not a directory' % filename)\n    paths = {filename: ('', extract_dir)}\n    for base, dirs, files in os.walk(filename):\n        src, dst = paths[base]\n        for d in dirs:\n            paths[os.path.join(base, d)] = (src + d + '/', os.path.join(dst, d))\n        for f in files:\n            target = os.path.join(dst, f)\n            target = progress_filter(src + f, target)\n            if not target:\n                continue\n            ensure_directory(target)\n            f = os.path.join(base, f)\n            shutil.copyfile(f, target)\n            shutil.copystat(f, target)",
    "label": true
  },
  {
    "code": "def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n    formats = [cpu_arch]\n    if cpu_arch == 'x86_64':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat64', 'fat32'])\n    elif cpu_arch == 'i386':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat32', 'fat'])\n    elif cpu_arch == 'ppc64':\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append('fat64')\n    elif cpu_arch == 'ppc':\n        if version > (10, 6):\n            return []\n        formats.extend(['fat32', 'fat'])\n    if cpu_arch in {'arm64', 'x86_64'}:\n        formats.append('universal2')\n    if cpu_arch in {'x86_64', 'i386', 'ppc64', 'ppc', 'intel'}:\n        formats.append('universal')\n    return formats",
    "label": true
  },
  {
    "code": "def read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:\n    try:\n        return source.read(path)\n    except (BadZipFile, KeyError, RuntimeError) as e:\n        raise UnsupportedWheel(f'could not read {path!r} file: {e!r}')",
    "label": true
  },
  {
    "code": "def get_supported_platform():\n    plat = get_build_platform()\n    m = macosVersionString.match(plat)\n    if m is not None and sys.platform == 'darwin':\n        try:\n            plat = 'macosx-%s-%s' % ('.'.join(_macos_vers()[:2]), m.group(3))\n        except ValueError:\n            pass\n    return plat",
    "label": true
  },
  {
    "code": "def from_package(package: types.ModuleType):\n    spec = wrap_spec(package)\n    reader = spec.loader.get_resource_reader(spec.name)\n    return reader.files()",
    "label": true
  },
  {
    "code": "def change_root(new_root, pathname):\n    if os.name == 'posix':\n        if not os.path.isabs(pathname):\n            return os.path.join(new_root, pathname)\n        else:\n            return os.path.join(new_root, pathname[1:])\n    elif os.name == 'nt':\n        drive, path = os.path.splitdrive(pathname)\n        if path[0] == '\\\\':\n            path = path[1:]\n        return os.path.join(new_root, path)\n    raise DistutilsPlatformError(f\"nothing known about platform '{os.name}'\")",
    "label": true
  },
  {
    "code": "def format_full_version(info: 'sys._version_info') -> str:\n    version = '{0.major}.{0.minor}.{0.micro}'.format(info)\n    kind = info.releaselevel\n    if kind != 'final':\n        version += kind[0] + str(info.serial)\n    return version",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        interp = 'pp3'\n    elif interp_name == 'cp':\n        interp = 'cp' + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)",
    "label": true
  },
  {
    "code": "def check_list_path_option(options: Values) -> None:\n    if options.path and (options.user or options.local):\n        raise CommandError(\"Cannot combine '--path' with '--user' or '--local'\")",
    "label": true
  },
  {
    "code": "def update_dist_caches(dist_path, fix_zipimporter_caches):\n    normalized_path = normalize_path(dist_path)\n    _uncache(normalized_path, sys.path_importer_cache)\n    if fix_zipimporter_caches:\n        _replace_zip_directory_cache_data(normalized_path)\n    else:\n        _remove_and_clear_zip_directory_cache_data(normalized_path)",
    "label": true
  },
  {
    "code": "def before_log(logger: 'logging.Logger', log_level: int) -> typing.Callable[['RetryCallState'], None]:\n\n    def log_it(retry_state: 'RetryCallState') -> None:\n        if retry_state.fn is None:\n            fn_name = '<unknown>'\n        else:\n            fn_name = _utils.get_callback_name(retry_state.fn)\n        logger.log(log_level, f\"Starting call to '{fn_name}', this is the {_utils.to_ordinal(retry_state.attempt_number)} time calling it.\")\n    return log_it",
    "label": true
  },
  {
    "code": "def pkginfo_to_metadata(egg_info_path: str, pkginfo_path: str) -> Message:\n    with open(pkginfo_path, encoding='utf-8') as headers:\n        pkg_info = Parser().parse(headers)\n    pkg_info.replace_header('Metadata-Version', '2.1')\n    del pkg_info['Provides-Extra']\n    del pkg_info['Requires-Dist']\n    requires_path = os.path.join(egg_info_path, 'requires.txt')\n    if os.path.exists(requires_path):\n        with open(requires_path, encoding='utf-8') as requires_file:\n            requires = requires_file.read()\n        parsed_requirements = sorted(split_sections(requires), key=lambda x: x[0] or '')\n        for extra, reqs in parsed_requirements:\n            for key, value in generate_requirements({extra: reqs}):\n                if (key, value) not in pkg_info.items():\n                    pkg_info[key] = value\n    description = pkg_info['Description']\n    if description:\n        description_lines = pkg_info['Description'].splitlines()\n        dedented_description = '\\n'.join((description_lines[0].lstrip(), textwrap.dedent('\\n'.join(description_lines[1:])), '\\n'))\n        pkg_info.set_payload(dedented_description)\n        del pkg_info['Description']\n    return pkg_info",
    "label": true
  },
  {
    "code": "def _imp(*args, **kwds):\n    before = set(sys.modules.keys())\n    mod = __import__(*args, **kwds)\n    after = set(sys.modules.keys()).difference(before)\n    for m in after:\n        memorise(sys.modules[m])\n    return mod",
    "label": true
  },
  {
    "code": "def get_unpatched(item):\n    lookup = get_unpatched_class if isinstance(item, type) else get_unpatched_function if isinstance(item, types.FunctionType) else lambda item: None\n    return lookup(item)",
    "label": true
  },
  {
    "code": "def make_setuptools_bdist_wheel_args(setup_py_path: str, global_options: Sequence[str], build_options: Sequence[str], destination_dir: str) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, global_options=global_options, unbuffered_output=True)\n    args += ['bdist_wheel', '-d', destination_dir]\n    args += build_options\n    return args",
    "label": true
  },
  {
    "code": "def load_module_from_name(dotted_name: str) -> types.ModuleType:\n    try:\n        return sys.modules[dotted_name]\n    except KeyError:\n        pass\n    with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(io.StringIO()) as stdout:\n        module = importlib.import_module(dotted_name)\n    stderr_value = stderr.getvalue()\n    if stderr_value:\n        logger.error('Captured stderr while importing %s:\\n%s', dotted_name, stderr_value)\n    stdout_value = stdout.getvalue()\n    if stdout_value:\n        logger.info('Captured stdout while importing %s:\\n%s', dotted_name, stdout_value)\n    return module",
    "label": true
  },
  {
    "code": "def install(console: Optional['Console']=None, overflow: 'OverflowMethod'='ignore', crop: bool=False, indent_guides: bool=False, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> None:\n    from pip._vendor.rich import get_console\n    console = console or get_console()\n    assert console is not None\n\n    def display_hook(value: Any) -> None:\n        \"\"\"Replacement sys.displayhook which prettifies objects with Rich.\"\"\"\n        if value is not None:\n            assert console is not None\n            builtins._ = None\n            console.print(value if _safe_isinstance(value, RichRenderable) else Pretty(value, overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all), crop=crop)\n            builtins._ = value\n    if 'get_ipython' in globals():\n        ip = get_ipython()\n        from IPython.core.formatters import BaseFormatter\n\n        class RichFormatter(BaseFormatter):\n            pprint: bool = True\n\n            def __call__(self, value: Any) -> Any:\n                if self.pprint:\n                    return _ipy_display_hook(value, console=get_console(), overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all)\n                else:\n                    return repr(value)\n        rich_formatter = RichFormatter()\n        ip.display_formatter.formatters['text/plain'] = rich_formatter\n    else:\n        sys.displayhook = display_hook",
    "label": true
  },
  {
    "code": "def split_provision(value):\n    global _provision_rx\n    if _provision_rx is None:\n        _provision_rx = re.compile('([a-zA-Z_]\\\\w*(?:\\\\.[a-zA-Z_]\\\\w*)*)(?:\\\\s*\\\\(\\\\s*([^)\\\\s]+)\\\\s*\\\\))?$', re.ASCII)\n    value = value.strip()\n    m = _provision_rx.match(value)\n    if not m:\n        raise ValueError('illegal provides specification: %r' % value)\n    ver = m.group(2) or None\n    if ver:\n        with version.suppress_known_deprecation():\n            ver = version.StrictVersion(ver)\n    return (m.group(1), ver)",
    "label": true
  },
  {
    "code": "def test_lambdify():\n    try:\n        from sympy import symbols, lambdify\n    except ImportError:\n        return\n    settings['recurse'] = True\n    x = symbols('x')\n    y = x ** 2\n    f = lambdify([x], y)\n    z = min\n    d = globals()\n    globalvars(f, recurse=True, builtin=True)\n    assert z is min\n    assert d is globals()",
    "label": true
  },
  {
    "code": "def _select_scheme(ob, name):\n    scheme = _inject_headers(name, _load_scheme(_resolve_scheme(name)))\n    vars(ob).update(_remove_set(ob, _scheme_attrs(scheme)))",
    "label": true
  },
  {
    "code": "def store_mark(obj, mark: Mark) -> None:\n    assert isinstance(mark, Mark), mark\n    obj.pytestmark = [*get_unpacked_marks(obj, consider_mro=False), mark]",
    "label": true
  },
  {
    "code": "def pytest_collection_modifyitems(items: List[nodes.Item], config: Config) -> None:\n    deselect_prefixes = tuple(config.getoption('deselect') or [])\n    if not deselect_prefixes:\n        return\n    remaining = []\n    deselected = []\n    for colitem in items:\n        if colitem.nodeid.startswith(deselect_prefixes):\n            deselected.append(colitem)\n        else:\n            remaining.append(colitem)\n    if deselected:\n        config.hook.pytest_deselected(items=deselected)\n        items[:] = remaining",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def request(method, url, **kwargs):\n    with sessions.Session() as session:\n        return session.request(method=method, url=url, **kwargs)",
    "label": true
  },
  {
    "code": "def make_analysator(f):\n\n    def text_analyse(text):\n        try:\n            rv = f(text)\n        except Exception:\n            return 0.0\n        if not rv:\n            return 0.0\n        try:\n            return min(1.0, max(0.0, float(rv)))\n        except (ValueError, TypeError):\n            return 0.0\n    text_analyse.__doc__ = f.__doc__\n    return staticmethod(text_analyse)",
    "label": true
  },
  {
    "code": "def new_month(customer_list: list[Customer], month: int, year: int) -> None:\n    for cust in customer_list:\n        cust.new_month(month, year)",
    "label": true
  },
  {
    "code": "def wrap_object_attribute(module, name, factory, args=(), kwargs={}):\n    path, attribute = name.rsplit('.', 1)\n    parent = resolve_path(module, path)[2]\n    wrapper = AttributeWrapper(attribute, factory, args, kwargs)\n    apply_patch(parent, attribute, wrapper)\n    return wrapper",
    "label": true
  },
  {
    "code": "def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:\n    if not tokenizer.check('SEMICOLON'):\n        tokenizer.raise_syntax_error(f'Expected end or semicolon (after {after})', span_start=span_start)\n    tokenizer.read()\n    marker = _parse_marker(tokenizer)\n    tokenizer.consume('WS')\n    return marker",
    "label": true
  },
  {
    "code": "def invalid_marker(text):\n    try:\n        evaluate_marker(text)\n    except SyntaxError as e:\n        e.filename = None\n        e.lineno = None\n        return e\n    return False",
    "label": true
  },
  {
    "code": "def _as_bool(value: str) -> bool:\n    try:\n        return _STR_BOOLEAN_MAPPING[value.lower()]\n    except KeyError:\n        raise ValueError(f'invalid truth value {value}')",
    "label": true
  },
  {
    "code": "def _pairwise(iterable):\n    a, b = tee(iterable)\n    next(b, None)\n    yield from zip(a, b)",
    "label": true
  },
  {
    "code": "def test_dtype():\n    try:\n        import numpy as np\n        dti = np.dtype('int')\n        assert np.dtype == dill.copy(np.dtype)\n        assert dti == dill.copy(dti)\n    except ImportError:\n        pass",
    "label": true
  },
  {
    "code": "def _column_type(strings, has_invisible=True, numparse=True):\n    types = [_type(s, has_invisible, numparse) for s in strings]\n    return reduce(_more_generic, types, bool)",
    "label": true
  },
  {
    "code": "def _get_editable_info(dist: BaseDistribution) -> _EditableInfo:\n    editable_project_location = dist.editable_project_location\n    assert editable_project_location\n    location = os.path.normcase(os.path.abspath(editable_project_location))\n    from pip._internal.vcs import RemoteNotFoundError, RemoteNotValidError, vcs\n    vcs_backend = vcs.get_backend_for_dir(location)\n    if vcs_backend is None:\n        display = _format_as_name_version(dist)\n        logger.debug('No VCS found for editable requirement \"%s\" in: %r', display, location)\n        return _EditableInfo(requirement=location, comments=[f'# Editable install with no version control ({display})'])\n    vcs_name = type(vcs_backend).__name__\n    try:\n        req = vcs_backend.get_src_requirement(location, dist.raw_name)\n    except RemoteNotFoundError:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(requirement=location, comments=[f'# Editable {vcs_name} install with no remote ({display})'])\n    except RemoteNotValidError as ex:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(requirement=location, comments=[f'# Editable {vcs_name} install ({display}) with either a deleted local remote or invalid URI:', f\"# '{ex.url}'\"])\n    except BadCommand:\n        logger.warning('cannot determine version of editable source in %s (%s command not found in path)', location, vcs_backend.name)\n        return _EditableInfo(requirement=location, comments=[])\n    except InstallationError as exc:\n        logger.warning('Error when trying to get requirement for VCS system %s', exc)\n    else:\n        return _EditableInfo(requirement=req, comments=[])\n    logger.warning('Could not determine repository location of %s', location)\n    return _EditableInfo(requirement=location, comments=['## !! Could not determine repository location'])",
    "label": true
  },
  {
    "code": "def cpython_tags(python_version: Optional[PythonVersion]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    interpreter = f'cp{_version_nodot(python_version[:2])}'\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    for explicit_abi in ('abi3', 'none'):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    if _abi3_applies(python_version):\n        yield from (Tag(interpreter, 'abi3', platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, 'none', platform_) for platform_ in platforms)\n    if _abi3_applies(python_version):\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = 'cp{version}'.format(version=_version_nodot((python_version[0], minor_version)))\n                yield Tag(interpreter, 'abi3', platform_)",
    "label": true
  },
  {
    "code": "def description_of(lines: Iterable[bytes], name: str='stdin', minimal: bool=False, should_rename_legacy: bool=False) -> Optional[str]:\n    u = UniversalDetector(should_rename_legacy=should_rename_legacy)\n    for line in lines:\n        line = bytearray(line)\n        u.feed(line)\n        if u.done:\n            break\n    u.close()\n    result = u.result\n    if minimal:\n        return result['encoding']\n    if result['encoding']:\n        return f\"{name}: {result['encoding']} with confidence {result['confidence']}\"\n    return f'{name}: no result'",
    "label": true
  },
  {
    "code": "def _check_class_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_pascal_case(name):\n        error_msgs.append(f'Class name \"{name}\" should be in PascalCase format. Class names should have the first letter of each word capitalized with no separation between each word. A single leading underscore can be used to denote a private class.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def get_module(module, paths, info):\n    spec = find_spec(module, paths)\n    if not spec:\n        raise ImportError(\"Can't find %s\" % module)\n    return module_from_spec(spec)",
    "label": true
  },
  {
    "code": "def _deque_mock():\n    base_deque_class = '\\n    class deque(object):\\n        maxlen = 0\\n        def __init__(self, iterable=None, maxlen=None):\\n            self.iterable = iterable or []\\n        def append(self, x): pass\\n        def appendleft(self, x): pass\\n        def clear(self): pass\\n        def count(self, x): return 0\\n        def extend(self, iterable): pass\\n        def extendleft(self, iterable): pass\\n        def pop(self): return self.iterable[0]\\n        def popleft(self): return self.iterable[0]\\n        def remove(self, value): pass\\n        def reverse(self): return reversed(self.iterable)\\n        def rotate(self, n=1): return self\\n        def __iter__(self): return self\\n        def __reversed__(self): return self.iterable[::-1]\\n        def __getitem__(self, index): return self.iterable[index]\\n        def __setitem__(self, index, value): pass\\n        def __delitem__(self, index): pass\\n        def __bool__(self): return bool(self.iterable)\\n        def __nonzero__(self): return bool(self.iterable)\\n        def __contains__(self, o): return o in self.iterable\\n        def __len__(self): return len(self.iterable)\\n        def __copy__(self): return deque(self.iterable)\\n        def copy(self): return deque(self.iterable)\\n        def index(self, x, start=0, end=0): return 0\\n        def insert(self, i, x): pass\\n        def __add__(self, other): pass\\n        def __iadd__(self, other): pass\\n        def __mul__(self, other): pass\\n        def __imul__(self, other): pass\\n        def __rmul__(self, other): pass'\n    if PY39_PLUS:\n        base_deque_class += '\\n        @classmethod\\n        def __class_getitem__(self, item): return cls'\n    return base_deque_class",
    "label": true
  },
  {
    "code": "def _patch_distribution_metadata():\n    from . import _core_metadata\n    'Patch write_pkg_file and read_pkg_file for higher metadata standards'\n    for attr in ('write_pkg_info', 'write_pkg_file', 'read_pkg_file', 'get_metadata_version'):\n        new_val = getattr(_core_metadata, attr)\n        setattr(distutils.dist.DistributionMetadata, attr, new_val)",
    "label": true
  },
  {
    "code": "def build_frequency_dict(text: bytes) -> dict[int, int]:\n    out = {}\n    for byte in text:\n        if byte not in out:\n            out[byte] = 0\n        out[byte] += 1\n    return out",
    "label": true
  },
  {
    "code": "def _parse_multi_options(options, split_token=','):\n    if options:\n        return [o.strip() for o in options.split(split_token) if o.strip()]\n    else:\n        return options",
    "label": true
  },
  {
    "code": "def escape(s: t.Any) -> Markup:\n    if hasattr(s, '__html__'):\n        return Markup(s.__html__())\n    return Markup(str(s).replace('&', '&amp;').replace('>', '&gt;').replace('<', '&lt;').replace(\"'\", '&#39;').replace('\"', '&#34;'))",
    "label": true
  },
  {
    "code": "def escape(markup: str, _escape: _EscapeSubMethod=re.compile('(\\\\\\\\*)(\\\\[[a-z#/@][^[]*?])').sub) -> str:\n\n    def escape_backslashes(match: Match[str]) -> str:\n        \"\"\"Called by re.sub replace matches.\"\"\"\n        backslashes, text = match.groups()\n        return f'{backslashes}{backslashes}\\\\{text}'\n    markup = _escape(escape_backslashes, markup)\n    return markup",
    "label": true
  },
  {
    "code": "def _get_first_non_fixture_func(obj: object, names: Iterable[str]) -> Optional[object]:\n    for name in names:\n        meth: Optional[object] = getattr(obj, name, None)\n        if meth is not None and fixtures.getfixturemarker(meth) is None:\n            return meth\n    return None",
    "label": true
  },
  {
    "code": "def _parse_content_type_header(header):\n    tokens = header.split(';')\n    content_type, params = (tokens[0].strip(), tokens[1:])\n    params_dict = {}\n    items_to_strip = '\"\\' '\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = (param, True)\n            index_of_equals = param.find('=')\n            if index_of_equals != -1:\n                key = param[:index_of_equals].strip(items_to_strip)\n                value = param[index_of_equals + 1:].strip(items_to_strip)\n            params_dict[key.lower()] = value\n    return (content_type, params_dict)",
    "label": true
  },
  {
    "code": "def valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('debugconfig')\n    group.addoption('--setuponly', '--setup-only', action='store_true', help='Only setup fixtures, do not execute tests')\n    group.addoption('--setupshow', '--setup-show', action='store_true', help='Show setup of fixtures while executing tests')",
    "label": true
  },
  {
    "code": "def make_dist(name, version, **kwargs):\n    summary = kwargs.pop('summary', 'Placeholder for summary')\n    md = Metadata(**kwargs)\n    md.name = name\n    md.version = version\n    md.summary = summary or 'Placeholder for summary'\n    return Distribution(md)",
    "label": true
  },
  {
    "code": "def show_actual_vendor_versions(vendor_txt_versions: Dict[str, str]) -> None:\n    for module_name, expected_version in vendor_txt_versions.items():\n        extra_message = ''\n        actual_version = get_vendor_version_from_module(module_name)\n        if not actual_version:\n            extra_message = ' (Unable to locate actual module version, using vendor.txt specified version)'\n            actual_version = expected_version\n        elif parse_version(actual_version) != parse_version(expected_version):\n            extra_message = ' (CONFLICT: vendor.txt suggests version should be {})'.format(expected_version)\n        logger.info('%s==%s%s', module_name, actual_version, extra_message)",
    "label": true
  },
  {
    "code": "def set_config(**kwargs: Any) -> Callable[[Callable[..., None]], Callable[..., None]]:\n\n    def _wrapper(fun: Callable[..., None]) -> Callable[..., None]:\n\n        @functools.wraps(fun)\n        def _forward(self: CheckerTestCase, *args: Any, **test_function_kwargs: Any) -> None:\n            \"\"\"Set option via argparse.\"\"\"\n            for key, value in kwargs.items():\n                self.linter.set_option(key, value)\n            self.checker.open()\n            fun(self, *args, **test_function_kwargs)\n        return _forward\n    return _wrapper",
    "label": true
  },
  {
    "code": "def _coerce_parse_result(results: Union[ParseResults, List[Any]]) -> List[Any]:\n    if isinstance(results, ParseResults):\n        return [_coerce_parse_result(i) for i in results]\n    else:\n        return results",
    "label": true
  },
  {
    "code": "def parse_hex_char(src: str, pos: Pos, hex_len: int) -> tuple[Pos, str]:\n    hex_str = src[pos:pos + hex_len]\n    if len(hex_str) != hex_len or not HEXDIGIT_CHARS.issuperset(hex_str):\n        raise suffixed_err(src, pos, 'Invalid hex value')\n    pos += hex_len\n    hex_int = int(hex_str, 16)\n    if not is_unicode_scalar_value(hex_int):\n        raise suffixed_err(src, pos, 'Escaped character is not a Unicode scalar value')\n    return (pos, chr(hex_int))",
    "label": true
  },
  {
    "code": "def set_cell_size(text: str, total: int) -> str:\n    if _is_single_cell_widths(text):\n        size = len(text)\n        if size < total:\n            return text + ' ' * (total - size)\n        return text[:total]\n    if total <= 0:\n        return ''\n    cell_size = cell_len(text)\n    if cell_size == total:\n        return text\n    if cell_size < total:\n        return text + ' ' * (total - cell_size)\n    start = 0\n    end = len(text)\n    while True:\n        pos = (start + end) // 2\n        before = text[:pos + 1]\n        before_len = cell_len(before)\n        if before_len == total + 1 and cell_len(before[-1]) == 2:\n            return before[:-1] + ' '\n        if before_len == total:\n            return before\n        if before_len > total:\n            end = pos\n        else:\n            start = pos",
    "label": true
  },
  {
    "code": "def wrap_pytest_function_for_tracing(pyfuncitem):\n    _pdb = pytestPDB._init_pdb('runcall')\n    testfunction = pyfuncitem.obj\n\n    @functools.wraps(testfunction)\n    def wrapper(*args, **kwargs):\n        func = functools.partial(testfunction, *args, **kwargs)\n        _pdb.runcall(func)\n    pyfuncitem.obj = wrapper",
    "label": true
  },
  {
    "code": "def size(s: Stack) -> int:\n    side_stack = Stack()\n    count = 0\n    while not s.is_empty():\n        side_stack.push(s.pop())\n        count += 1\n    while not side_stack.is_empty():\n        s.push(side_stack.pop())\n    return count",
    "label": true
  },
  {
    "code": "def run_pyta(filename: str, config_file: str) -> None:\n    import json\n    error_message = '\\nCould not run PythonTA correctly.\\nPlease make sure you have run the setup.py provided on Quercus: that should install PythonTA for you.\\nPlease attend office hours if you require assistance in running PythonTA.'\n    try:\n        import python_ta\n        with open(config_file) as cf:\n            config_dict = json.loads(cf.read())\n            config_dict['output-format'] = 'python_ta.reporters.PlainReporter'\n        python_ta.check_all(filename, config=config_dict)\n    except:\n        pass\n    else:\n        return\n    print(error_message)",
    "label": true
  },
  {
    "code": "def get_best_invocation_for_this_python() -> str:\n    exe = sys.executable\n    exe_name = os.path.basename(exe)\n    found_executable = shutil.which(exe_name)\n    if found_executable and os.path.samefile(found_executable, exe):\n        return exe_name\n    return exe",
    "label": true
  },
  {
    "code": "def pytest_runtest_teardown(item: Item, nextitem: Optional[Item]) -> None:\n    _update_current_test_var(item, 'teardown')\n    item.session._setupstate.teardown_exact(nextitem)\n    _update_current_test_var(item, None)",
    "label": true
  },
  {
    "code": "def is_classdef_type(node: nodes.ClassDef) -> bool:\n    if node.name == 'type':\n        return True\n    return any((isinstance(b, nodes.Name) and b.name == 'type' for b in node.bases))",
    "label": true
  },
  {
    "code": "def yield_fixture(fixture_function=None, *args, scope='function', params=None, autouse=False, ids=None, name=None):\n    warnings.warn(YIELD_FIXTURE, stacklevel=2)\n    return fixture(fixture_function, *args, scope=scope, params=params, autouse=autouse, ids=ids, name=name)",
    "label": true
  },
  {
    "code": "def infer_type_sub(node, context: InferenceContext | None=None):\n    node_scope, _ = node.scope().lookup('type')\n    if not isinstance(node_scope, nodes.Module) or node_scope.qname() != 'builtins':\n        raise UseInferenceDefault()\n    class_src = '\\n    class type:\\n        def __class_getitem__(cls, key):\\n            return cls\\n     '\n    node = extract_node(class_src)\n    return node.infer(context=context)",
    "label": true
  },
  {
    "code": "def dict_to_sequence(d):\n    if hasattr(d, 'items'):\n        d = d.items()\n    return d",
    "label": true
  },
  {
    "code": "def _looks_like_path(name: str) -> bool:\n    if os.path.sep in name:\n        return True\n    if os.path.altsep is not None and os.path.altsep in name:\n        return True\n    if name.startswith('.'):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def cell_len(text: str, _cell_len: Callable[[str], int]=cached_cell_len) -> int:\n    if len(text) < 512:\n        return _cell_len(text)\n    _get_size = get_character_cell_size\n    total_size = sum((_get_size(character) for character in text))\n    return total_size",
    "label": true
  },
  {
    "code": "def parse_multiline_str(src: str, pos: Pos, *, literal: bool) -> tuple[Pos, str]:\n    pos += 3\n    if src.startswith('\\n', pos):\n        pos += 1\n    if literal:\n        delim = \"'\"\n        end_pos = skip_until(src, pos, \"'''\", error_on=ILLEGAL_MULTILINE_LITERAL_STR_CHARS, error_on_eof=True)\n        result = src[pos:end_pos]\n        pos = end_pos + 3\n    else:\n        delim = '\"'\n        pos, result = parse_basic_str(src, pos, multiline=True)\n    if not src.startswith(delim, pos):\n        return (pos, result)\n    pos += 1\n    if not src.startswith(delim, pos):\n        return (pos, result + delim)\n    pos += 1\n    return (pos, result + delim * 2)",
    "label": true
  },
  {
    "code": "def _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n    try:\n        spec = Specifier(''.join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs)\n    oper: Optional[Operator] = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(f'Undefined {op!r} on {lhs!r} and {rhs!r}.')\n    return oper(lhs, rhs)",
    "label": true
  },
  {
    "code": "def _check_regexp_csv(value: list[str] | tuple[str] | str) -> Iterable[str]:\n    if isinstance(value, (list, tuple)):\n        yield from value\n    else:\n        regexps: deque[deque[str] | None] = deque([None])\n        open_braces = False\n        for char in value:\n            if char == '{':\n                open_braces = True\n            elif char == '}' and open_braces:\n                open_braces = False\n            if char == ',' and (not open_braces):\n                regexps.append(None)\n            elif regexps[-1] is None:\n                regexps.pop()\n                regexps.append(deque([char]))\n            else:\n                regexps[-1].append(char)\n        yield from (''.join(regexp).strip() for regexp in regexps if regexp is not None)",
    "label": true
  },
  {
    "code": "def class_instance_as_index(node: SuccessfulInferenceResult) -> nodes.Const | None:\n    context = InferenceContext()\n    try:\n        for inferred in node.igetattr('__index__', context=context):\n            if not isinstance(inferred, bases.BoundMethod):\n                continue\n            context.boundnode = node\n            context.callcontext = CallContext(args=[], callee=inferred)\n            for result in inferred.infer_call_result(node, context=context):\n                if isinstance(result, nodes.Const) and isinstance(result.value, int):\n                    return result\n    except InferenceError:\n        pass\n    return None",
    "label": true
  },
  {
    "code": "def get_module_part(dotted_name: str, context_file: str | None=None) -> str:\n    if dotted_name.startswith('os.path'):\n        return 'os.path'\n    parts = dotted_name.split('.')\n    if context_file is not None:\n        if parts[0] in BUILTIN_MODULES:\n            if len(parts) > 2:\n                raise ImportError(dotted_name)\n            return parts[0]\n    path: list[str] | None = None\n    starti = 0\n    if parts[0] == '':\n        assert context_file is not None, 'explicit relative import, but no context_file?'\n        path = []\n        starti = 1\n    while starti < len(parts) and parts[starti] == '':\n        starti += 1\n        assert context_file is not None, 'explicit relative import, but no context_file?'\n        context_file = os.path.dirname(context_file)\n    for i in range(starti, len(parts)):\n        try:\n            file_from_modpath(parts[starti:i + 1], path=path, context_file=context_file)\n        except ImportError:\n            if i < max(1, len(parts) - 2):\n                raise\n            return '.'.join(parts[:i])\n    return dotted_name",
    "label": true
  },
  {
    "code": "def retry(*r_args, **r_kwargs):\n\n    def decorate(func):\n\n        @functools.wraps(func)\n        def wrapper(*f_args, **f_kwargs):\n            bound = functools.partial(func, *f_args, **f_kwargs)\n            return retry_call(bound, *r_args, **r_kwargs)\n        return wrapper\n    return decorate",
    "label": true
  },
  {
    "code": "def _is_trailing_comma(tokens: list[tokenize.TokenInfo], index: int) -> bool:\n    token = tokens[index]\n    if token.exact_type != tokenize.COMMA:\n        return False\n    left_tokens = list(itertools.islice(tokens, index + 1, None))\n    more_tokens_on_line = False\n    for remaining_token in left_tokens:\n        if remaining_token.start[0] == token.start[0]:\n            more_tokens_on_line = True\n            if remaining_token.type not in (tokenize.NEWLINE, tokenize.COMMENT):\n                return False\n    if not more_tokens_on_line:\n        return False\n\n    def get_curline_index_start() -> int:\n        \"\"\"Get the index denoting the start of the current line.\"\"\"\n        for subindex, token in enumerate(reversed(tokens[:index])):\n            if token.type == tokenize.NEWLINE:\n                return index - subindex\n        return 0\n    curline_start = get_curline_index_start()\n    expected_tokens = {'return', 'yield'}\n    return any(('=' in prevtoken.string or prevtoken.string in expected_tokens for prevtoken in tokens[curline_start:index]))",
    "label": true
  },
  {
    "code": "def _binary_operators_from_module(module: types.ModuleType) -> dict[type[ast.operator], str]:\n    binary_operators = {module.Add: '+', module.BitAnd: '&', module.BitOr: '|', module.BitXor: '^', module.Div: '/', module.FloorDiv: '//', module.MatMult: '@', module.Mod: '%', module.Mult: '*', module.Pow: '**', module.Sub: '-', module.LShift: '<<', module.RShift: '>>'}\n    return binary_operators",
    "label": true
  },
  {
    "code": "def install_req_from_editable(editable_req: str, comes_from: Optional[Union[InstallRequirement, str]]=None, *, use_pep517: Optional[bool]=None, isolated: bool=False, global_options: Optional[List[str]]=None, hash_options: Optional[Dict[str, List[str]]]=None, constraint: bool=False, user_supplied: bool=False, permit_editable_wheels: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    parts = parse_req_from_editable(editable_req)\n    return InstallRequirement(parts.requirement, comes_from=comes_from, user_supplied=user_supplied, editable=True, permit_editable_wheels=permit_editable_wheels, link=parts.link, constraint=constraint, use_pep517=use_pep517, isolated=isolated, global_options=global_options, hash_options=hash_options, config_settings=config_settings, extras=parts.extras)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e272(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def deinit():\n    if orig_stdout is not None:\n        sys.stdout = orig_stdout\n    if orig_stderr is not None:\n        sys.stderr = orig_stderr",
    "label": true
  },
  {
    "code": "def find_bridge_by_id(bridges: list[list], bridge_id: int) -> list:\n    for bridge in bridges:\n        if bridge[COLUMN_ID] == bridge_id:\n            return bridge\n    return []",
    "label": true
  },
  {
    "code": "def get_all_styles():\n    yield from STYLE_MAP\n    for name, _ in find_plugin_styles():\n        yield name",
    "label": true
  },
  {
    "code": "def _combining_class(cp: int) -> int:\n    v = unicodedata.combining(chr(cp))\n    if v == 0:\n        if not unicodedata.name(chr(cp)):\n            raise ValueError('Unknown character in unicodedata')\n    return v",
    "label": true
  },
  {
    "code": "def _infer_binary_operation(left: InferenceResult, right: InferenceResult, binary_opnode: nodes.AugAssign | nodes.BinOp, context: InferenceContext, flow_factory: GetFlowFactory) -> Generator[InferenceResult | util.BadBinaryOperationMessage, None, None]:\n    context, reverse_context = _get_binop_contexts(context, left, right)\n    left_type = helpers.object_type(left)\n    right_type = helpers.object_type(right)\n    methods = flow_factory(left, left_type, binary_opnode, right, right_type, context, reverse_context)\n    for method in methods:\n        try:\n            results = list(method())\n        except AttributeError:\n            continue\n        except AttributeInferenceError:\n            continue\n        except InferenceError:\n            yield util.Uninferable\n            return\n        else:\n            if any((isinstance(result, util.UninferableBase) for result in results)):\n                yield util.Uninferable\n                return\n            if all(map(_is_not_implemented, results)):\n                continue\n            not_implemented = sum((1 for result in results if _is_not_implemented(result)))\n            if not_implemented and not_implemented != len(results):\n                yield util.Uninferable\n                return\n            yield from results\n            return\n    yield util.BadBinaryOperationMessage(left_type, binary_opnode.op, right_type)",
    "label": true
  },
  {
    "code": "def method_caller(method_name, *args, **kwargs):\n\n    def call_method(target):\n        func = getattr(target, method_name)\n        return func(*args, **kwargs)\n    return call_method",
    "label": true
  },
  {
    "code": "def make_str(value: t.Any) -> str:\n    if isinstance(value, bytes):\n        try:\n            return value.decode(sys.getfilesystemencoding())\n        except UnicodeError:\n            return value.decode('utf-8', 'replace')\n    return str(value)",
    "label": true
  },
  {
    "code": "def _should_use_importlib_metadata() -> bool:\n    with contextlib.suppress(KeyError, ValueError):\n        return bool(strtobool(os.environ['_PIP_USE_IMPORTLIB_METADATA']))\n    if sys.version_info < (3, 11):\n        return False\n    import importlib.metadata\n    return bool(getattr(importlib.metadata, '_PIP_USE_IMPORTLIB_METADATA', True))",
    "label": true
  },
  {
    "code": "def error_of_type(handler: nodes.ExceptHandler, error_type: str | type[Exception] | tuple[str | type[Exception], ...]) -> bool:\n\n    def stringify_error(error: str | type[Exception]) -> str:\n        if not isinstance(error, str):\n            return error.__name__\n        return error\n    if not isinstance(error_type, tuple):\n        error_type = (error_type,)\n    expected_errors = {stringify_error(error) for error in error_type}\n    if not handler.type:\n        return False\n    return handler.catch(expected_errors)",
    "label": true
  },
  {
    "code": "def sort_code_string(code: str, extension: Optional[str]=None, config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, disregard_skip: bool=False, show_diff: Union[bool, TextIO]=False, **config_kwargs: Any) -> str:\n    input_stream = StringIO(code)\n    output_stream = StringIO()\n    config = _config(path=file_path, config=config, **config_kwargs)\n    sort_stream(input_stream, output_stream, extension=extension, config=config, file_path=file_path, disregard_skip=disregard_skip, show_diff=show_diff)\n    output_stream.seek(0)\n    return output_stream.read()",
    "label": true
  },
  {
    "code": "def compute_tagline(tags: list[str]) -> str:\n    impls = sorted({tag.split('-')[0] for tag in tags})\n    abivers = sorted({tag.split('-')[1] for tag in tags})\n    platforms = sorted({tag.split('-')[2] for tag in tags})\n    return '-'.join(['.'.join(impls), '.'.join(abivers), '.'.join(platforms)])",
    "label": true
  },
  {
    "code": "def parse_rgb_hex(hex_color: str) -> ColorTriplet:\n    assert len(hex_color) == 6, 'must be 6 characters'\n    color = ColorTriplet(int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16))\n    return color",
    "label": true
  },
  {
    "code": "def create_urllib3_context(ssl_version=None, cert_reqs=None, options=None, ciphers=None):\n    if not ssl_version or ssl_version == PROTOCOL_TLS:\n        ssl_version = PROTOCOL_TLS_CLIENT\n    context = SSLContext(ssl_version)\n    context.set_ciphers(ciphers or DEFAULT_CIPHERS)\n    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs\n    if options is None:\n        options = 0\n        options |= OP_NO_SSLv2\n        options |= OP_NO_SSLv3\n        options |= OP_NO_COMPRESSION\n        options |= OP_NO_TICKET\n    context.options |= options\n    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(context, 'post_handshake_auth', None) is not None:\n        context.post_handshake_auth = True\n\n    def disable_check_hostname():\n        if getattr(context, 'check_hostname', None) is not None:\n            context.check_hostname = False\n    if cert_reqs == ssl.CERT_REQUIRED:\n        context.verify_mode = cert_reqs\n        disable_check_hostname()\n    else:\n        disable_check_hostname()\n        context.verify_mode = cert_reqs\n    if hasattr(context, 'keylog_filename'):\n        sslkeylogfile = os.environ.get('SSLKEYLOGFILE')\n        if sslkeylogfile:\n            context.keylog_filename = sslkeylogfile\n    return context",
    "label": true
  },
  {
    "code": "def _version_from_file(lines):\n\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None",
    "label": true
  },
  {
    "code": "def _override_check_invalid_name_in_main():\n    old_visit_assignname = NameChecker.visit_assignname\n\n    def patched_visit_assignname(self, node):\n        if _is_in_main(node):\n            self._check_name('variable', node.name, node)\n        else:\n            old_visit_assignname(self, node)\n    NameChecker.visit_assignname = patched_visit_assignname",
    "label": true
  },
  {
    "code": "def console_main() -> int:\n    try:\n        code = main()\n        sys.stdout.flush()\n        return code\n    except BrokenPipeError:\n        devnull = os.open(os.devnull, os.O_WRONLY)\n        os.dup2(devnull, sys.stdout.fileno())\n        return 1",
    "label": true
  },
  {
    "code": "def iter_decode(input, fallback_encoding, errors='replace'):\n    decoder = IncrementalDecoder(fallback_encoding, errors)\n    generator = _iter_decode_generator(input, decoder)\n    encoding = next(generator)\n    return (generator, encoding)",
    "label": true
  },
  {
    "code": "def join_lines(lines_enum: ReqFileLines) -> ReqFileLines:\n    primary_line_number = None\n    new_line: List[str] = []\n    for line_number, line in lines_enum:\n        if not line.endswith('\\\\') or COMMENT_RE.match(line):\n            if COMMENT_RE.match(line):\n                line = ' ' + line\n            if new_line:\n                new_line.append(line)\n                assert primary_line_number is not None\n                yield (primary_line_number, ''.join(new_line))\n                new_line = []\n            else:\n                yield (line_number, line)\n        else:\n            if not new_line:\n                primary_line_number = line_number\n            new_line.append(line.strip('\\\\'))\n    if new_line:\n        assert primary_line_number is not None\n        yield (primary_line_number, ''.join(new_line))",
    "label": true
  },
  {
    "code": "def truncate_if_required(explanation: List[str], item: Item, max_length: Optional[int]=None) -> List[str]:\n    if _should_truncate_item(item):\n        return _truncate_explanation(explanation)\n    return explanation",
    "label": true
  },
  {
    "code": "def _subprocess_transform():\n    communicate = (bytes('string', 'ascii'), bytes('string', 'ascii'))\n    communicate_signature = 'def communicate(self, input=None, timeout=None)'\n    args = '        self, args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None,\\n        preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None,\\n        universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True,\\n        start_new_session=False, pass_fds=(), *, encoding=None, errors=None, text=None'\n    if PY39_PLUS:\n        args += ', user=None, group=None, extra_groups=None, umask=-1'\n    if PY310_PLUS:\n        args += ', pipesize=-1'\n    if PY311_PLUS:\n        args += ', process_group=None'\n    init = f'\\n        def __init__({args}):\\n            pass'\n    wait_signature = 'def wait(self, timeout=None)'\n    ctx_manager = '\\n        def __enter__(self): return self\\n        def __exit__(self, *args): pass\\n    '\n    py3_args = 'args = []'\n    check_output_signature = '\\n    check_output(\\n        args, *,\\n        stdin=None,\\n        stderr=None,\\n        shell=False,\\n        cwd=None,\\n        encoding=None,\\n        errors=None,\\n        universal_newlines=False,\\n        timeout=None,\\n        env=None,\\n        text=None,\\n        restore_signals=True,\\n        preexec_fn=None,\\n        pass_fds=(),\\n        input=None,\\n        bufsize=0,\\n        executable=None,\\n        close_fds=False,\\n        startupinfo=None,\\n        creationflags=0,\\n        start_new_session=False\\n    ):\\n    '.strip()\n    code = textwrap.dedent(f'\\n    def {check_output_signature}\\n        if universal_newlines:\\n            return \"\"\\n        return b\"\"\\n\\n    class Popen(object):\\n        returncode = pid = 0\\n        stdin = stdout = stderr = file()\\n        {py3_args}\\n\\n        {communicate_signature}:\\n            return {communicate!r}\\n        {wait_signature}:\\n            return self.returncode\\n        def poll(self):\\n            return self.returncode\\n        def send_signal(self, signal):\\n            pass\\n        def terminate(self):\\n            pass\\n        def kill(self):\\n            pass\\n        {ctx_manager}\\n       ')\n    if PY39_PLUS:\n        code += '\\n    @classmethod\\n    def __class_getitem__(cls, item):\\n        pass\\n        '\n    init_lines = textwrap.dedent(init).splitlines()\n    indented_init = '\\n'.join((' ' * 4 + line for line in init_lines))\n    code += indented_init\n    return parse(code)",
    "label": true
  },
  {
    "code": "def infer_typing_attr(node: Subscript, ctx: context.InferenceContext | None=None) -> Iterator[ClassDef]:\n    try:\n        value = next(node.value.infer())\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not value.qname().startswith('typing.') or value.qname() in TYPING_ALIAS:\n        raise UseInferenceDefault\n    if isinstance(value, ClassDef) and value.qname() in {'typing.Generic', 'typing.Annotated', 'typing_extensions.Annotated'}:\n        func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n        value.locals['__class_getitem__'] = [func_to_add]\n        if isinstance(node.parent, ClassDef) and node in node.parent.bases and getattr(node.parent, '__cache', None):\n            cache = node.parent.__cache\n            if cache.get(node.parent.slots) is not None:\n                del cache[node.parent.slots]\n        return iter([value])\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(value.qname().split('.')[-1]))\n    return node.infer(context=ctx)",
    "label": true
  },
  {
    "code": "def any_specified_encoding(sequence: bytes, search_zone: int=8192) -> Optional[str]:\n    if not isinstance(sequence, bytes):\n        raise TypeError\n    seq_len: int = len(sequence)\n    results: List[str] = findall(RE_POSSIBLE_ENCODING_INDICATION, sequence[:min(seq_len, search_zone)].decode('ascii', errors='ignore'))\n    if len(results) == 0:\n        return None\n    for specified_encoding in results:\n        specified_encoding = specified_encoding.lower().replace('-', '_')\n        encoding_alias: str\n        encoding_iana: str\n        for encoding_alias, encoding_iana in aliases.items():\n            if encoding_alias == specified_encoding:\n                return encoding_iana\n            if encoding_iana == specified_encoding:\n                return encoding_iana\n    return None",
    "label": true
  },
  {
    "code": "def _unpack_zipfile_obj(zipfile_obj, extract_dir, progress_filter=default_filter):\n    for info in zipfile_obj.infolist():\n        name = info.filename\n        if name.startswith('/') or '..' in name.split('/'):\n            continue\n        target = os.path.join(extract_dir, *name.split('/'))\n        target = progress_filter(name, target)\n        if not target:\n            continue\n        if name.endswith('/'):\n            ensure_directory(target)\n        else:\n            ensure_directory(target)\n            data = zipfile_obj.read(info.filename)\n            with open(target, 'wb') as f:\n                f.write(data)\n        unix_attributes = info.external_attr >> 16\n        if unix_attributes:\n            os.chmod(target, unix_attributes)",
    "label": true
  },
  {
    "code": "def _set_default(type_param, default):\n    if isinstance(default, (tuple, list)):\n        type_param.__default__ = tuple((typing._type_check(d, 'Default must be a type') for d in default))\n    elif default != _marker:\n        if isinstance(type_param, ParamSpec) and default is ...:\n            type_param.__default__ = default\n        else:\n            type_param.__default__ = typing._type_check(default, 'Default must be a type')\n    else:\n        type_param.__default__ = None",
    "label": true
  },
  {
    "code": "def parse_key(src: str, pos: Pos) -> Tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: Optional[str] = src[pos]\n        except IndexError:\n            char = None\n        if char != '.':\n            return (pos, key)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def sample(iterable, k, weights=None):\n    if k == 0:\n        return []\n    iterable = iter(iterable)\n    if weights is None:\n        return _sample_unweighted(iterable, k)\n    else:\n        weights = iter(weights)\n        return _sample_weighted(iterable, k, weights)",
    "label": true
  },
  {
    "code": "def get_node_last_lineno(node: nodes.NodeNG) -> int:\n    if getattr(node, 'finalbody', False):\n        return get_node_last_lineno(node.finalbody[-1])\n    if getattr(node, 'orelse', False):\n        return get_node_last_lineno(node.orelse[-1])\n    if getattr(node, 'handlers', False):\n        return get_node_last_lineno(node.handlers[-1])\n    if getattr(node, 'body', False):\n        return get_node_last_lineno(node.body[-1])\n    return node.lineno",
    "label": true
  },
  {
    "code": "def get_bridge_condition(bridges: list[list], bridge_id: int) -> float:\n    bridge = find_bridge_by_id(bridges, bridge_id)\n    if bridge == []:\n        return MISSING_BCI\n    for i in range(len(bridge[COLUMN_BCI][INDEX_BCI_YEARS])):\n        if bridge[COLUMN_BCI][INDEX_BCI_SCORES][i] != MISSING_BCI:\n            return bridge[COLUMN_BCI][INDEX_BCI_SCORES][i]\n    return MISSING_BCI",
    "label": true
  },
  {
    "code": "def _parse_extras(tokenizer: Tokenizer) -> List[str]:\n    if not tokenizer.check('LEFT_BRACKET', peek=True):\n        return []\n    with tokenizer.enclosing_tokens('LEFT_BRACKET', 'RIGHT_BRACKET', around='extras'):\n        tokenizer.consume('WS')\n        extras = _parse_extras_list(tokenizer)\n        tokenizer.consume('WS')\n    return extras",
    "label": true
  },
  {
    "code": "def time(raw: str) -> Time:\n    value = parse_rfc3339(raw)\n    if not isinstance(value, _datetime.time):\n        raise ValueError('time() only accepts time strings.')\n    return item(value)",
    "label": true
  },
  {
    "code": "def get_exception_handlers(node: nodes.NodeNG, exception: type[Exception] | str=Exception) -> list[nodes.ExceptHandler] | None:\n    context = find_try_except_wrapper_node(node)\n    if isinstance(context, nodes.TryExcept):\n        return [handler for handler in context.handlers if error_of_type(handler, exception)]\n    return []",
    "label": true
  },
  {
    "code": "def parse_annotations(node: nodes.NodeNG, class_tvars: Optional[List[type]]=None) -> List[Tuple[type, str]]:\n    if isinstance(node, nodes.FunctionDef):\n        arg_types = []\n        no_class_tvars = class_tvars is None\n        is_methodcall = isinstance(node.parent, nodes.ClassDef)\n        if no_class_tvars and is_methodcall:\n            self_type = _node_to_type(node.parent.name)\n        elif no_class_tvars or not is_methodcall:\n            self_type = None\n        elif node.parent.name in _BUILTIN_TO_TYPING:\n            self_type = eval(_BUILTIN_TO_TYPING[node.parent.name])[tuple((_node_to_type(tv) for tv in class_tvars))]\n        else:\n            self_type = _node_to_type(node.parent.name)\n        for arg, annotation in zip(node.args.args, node.args.annotations):\n            if getattr(arg, 'name', None) == 'self' and annotation is None:\n                arg_types.append(self_type)\n            else:\n                arg_types.append(_ann_node_to_type(annotation).getValue())\n        alternatives = []\n        for num_optional in range(len(node.args.defaults) + 1):\n            alternatives.append(arg_types[:len(arg_types) - num_optional])\n        rtype = _ann_node_to_type(node.returns).getValue()\n        callables = [(create_Callable(arg_types, rtype, class_tvars), node.type) for arg_types in alternatives]\n        return callables\n    elif isinstance(node, nodes.AssignName) and isinstance(node.parent, nodes.AnnAssign):\n        return [_ann_node_to_type(node.parent.annotation).getValue(), 'attribute']",
    "label": true
  },
  {
    "code": "def _has_default_namedtuple_repr(obj: object) -> bool:\n    obj_file = None\n    try:\n        obj_file = inspect.getfile(obj.__repr__)\n    except (OSError, TypeError):\n        pass\n    default_repr_file = inspect.getfile(_dummy_namedtuple.__repr__)\n    return obj_file == default_repr_file",
    "label": true
  },
  {
    "code": "def _maybe_adjust_parameters(cls):\n    tvars = []\n    if '__orig_bases__' in cls.__dict__:\n        tvars = _collect_type_vars(cls.__orig_bases__)\n        gvars = None\n        for base in cls.__orig_bases__:\n            if isinstance(base, typing._GenericAlias) and base.__origin__ in (typing.Generic, Protocol):\n                the_base = base.__origin__.__name__\n                if gvars is not None:\n                    raise TypeError('Cannot inherit from Generic[...] and/or Protocol[...] multiple types.')\n                gvars = base.__parameters__\n        if gvars is None:\n            gvars = tvars\n        else:\n            tvarset = set(tvars)\n            gvarset = set(gvars)\n            if not tvarset <= gvarset:\n                s_vars = ', '.join((str(t) for t in tvars if t not in gvarset))\n                s_args = ', '.join((str(g) for g in gvars))\n                raise TypeError(f'Some type variables ({s_vars}) are not listed in {the_base}[{s_args}]')\n            tvars = gvars\n    cls.__parameters__ = tuple(tvars)",
    "label": true
  },
  {
    "code": "def is_python(text, filename='<string>'):\n    try:\n        compile(text, filename, 'exec')\n    except (SyntaxError, TypeError):\n        return False\n    else:\n        return True",
    "label": true
  },
  {
    "code": "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    tokenizer.consume('WS')\n    name_token = tokenizer.expect('IDENTIFIER', expected='package name at the start of dependency specifier')\n    name = name_token.text\n    tokenizer.consume('WS')\n    extras = _parse_extras(tokenizer)\n    tokenizer.consume('WS')\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n    tokenizer.expect('END', expected='end of dependency specifier')\n    return ParsedRequirement(name, url, extras, specifier, marker)",
    "label": true
  },
  {
    "code": "def ssl_wrap_socket(sock: socket.socket, keyfile: str | None=None, certfile: str | None=None, cert_reqs: int | None=None, ca_certs: str | None=None, server_hostname: str | None=None, ssl_version: int | None=None, ciphers: str | None=None, ssl_context: ssl.SSLContext | None=None, ca_cert_dir: str | None=None, key_password: str | None=None, ca_cert_data: None | str | bytes=None, tls_in_tls: bool=False) -> ssl.SSLSocket | SSLTransportType:\n    context = ssl_context\n    if context is None:\n        context = create_urllib3_context(ssl_version, cert_reqs, ciphers=ciphers)\n    if ca_certs or ca_cert_dir or ca_cert_data:\n        try:\n            context.load_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)\n        except OSError as e:\n            raise SSLError(e) from e\n    elif ssl_context is None and hasattr(context, 'load_default_certs'):\n        context.load_default_certs()\n    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):\n        raise SSLError('Client private key is encrypted, password is required')\n    if certfile:\n        if key_password is None:\n            context.load_cert_chain(certfile, keyfile)\n        else:\n            context.load_cert_chain(certfile, keyfile, key_password)\n    try:\n        context.set_alpn_protocols(ALPN_PROTOCOLS)\n    except NotImplementedError:\n        pass\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n    return ssl_sock",
    "label": true
  },
  {
    "code": "def get_config_var(name):\n    if name == 'SO':\n        import warnings\n        warnings.warn('SO is deprecated, use EXT_SUFFIX', DeprecationWarning, 2)\n    return get_config_vars().get(name)",
    "label": true
  },
  {
    "code": "def apply_filters(stream, filters, lexer=None):\n\n    def _apply(filter_, stream):\n        yield from filter_.filter(lexer, stream)\n    for filter_ in filters:\n        stream = _apply(filter_, stream)\n    return stream",
    "label": true
  },
  {
    "code": "def register_check(check, codes=None):\n\n    def _add_check(check, kind, codes, args):\n        if check in _checks[kind]:\n            _checks[kind][check][0].extend(codes or [])\n        else:\n            _checks[kind][check] = (codes or [''], args)\n    if inspect.isfunction(check):\n        args = _get_parameters(check)\n        if args and args[0] in ('physical_line', 'logical_line'):\n            if codes is None:\n                codes = ERRORCODE_REGEX.findall(check.__doc__ or '')\n            _add_check(check, args[0], codes, args)\n    elif inspect.isclass(check):\n        if _get_parameters(check.__init__)[:2] == ['self', 'tree']:\n            _add_check(check, 'tree', codes, None)\n    return check",
    "label": true
  },
  {
    "code": "def directory_size(path: str) -> Union[int, float]:\n    size = 0.0\n    for root, _dirs, files in os.walk(path):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            size += file_size(file_path)\n    return size",
    "label": true
  },
  {
    "code": "def _get_binop_flow(left: InferenceResult, left_type: InferenceResult | None, binary_opnode: nodes.AugAssign | nodes.BinOp, right: InferenceResult, right_type: InferenceResult | None, context: InferenceContext, reverse_context: InferenceContext) -> list[functools.partial[Generator[InferenceResult, None, None]]]:\n    op = binary_opnode.op\n    if _same_type(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_subtype(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_supertype(left_type, right_type):\n        methods = [_bin_op(right, binary_opnode, op, left, reverse_context, reverse=True), _bin_op(left, binary_opnode, op, right, context)]\n    else:\n        methods = [_bin_op(left, binary_opnode, op, right, context), _bin_op(right, binary_opnode, op, left, reverse_context, reverse=True)]\n    if PY310_PLUS and op == '|' and (isinstance(left, (bases.UnionType, nodes.ClassDef)) or (isinstance(left, nodes.Const) and left.value is None)) and (isinstance(right, (bases.UnionType, nodes.ClassDef)) or (isinstance(right, nodes.Const) and right.value is None)):\n        methods.extend([functools.partial(_bin_op_or_union_type, left, right)])\n    return methods",
    "label": true
  },
  {
    "code": "def open_source_file(filename: str) -> tuple[TextIOWrapper, str, str]:\n    with open(filename, 'rb') as byte_stream:\n        encoding = detect_encoding(byte_stream.readline)[0]\n    stream = open(filename, newline=None, encoding=encoding)\n    data = stream.read()\n    return (stream, encoding, data)",
    "label": true
  },
  {
    "code": "def show_formats():\n    from ..fancy_getopt import FancyGetopt\n    formats = []\n    for format in bdist.format_commands:\n        formats.append(('formats=' + format, None, bdist.format_commands[format][1]))\n    pretty_printer = FancyGetopt(formats)\n    pretty_printer.print_help('List of available distribution formats:')",
    "label": true
  },
  {
    "code": "def create_command(name: str, **kwargs: Any) -> Command:\n    module_path, class_name, summary = commands_dict[name]\n    module = importlib.import_module(module_path)\n    command_class = getattr(module, class_name)\n    command = command_class(name=name, summary=summary, **kwargs)\n    return command",
    "label": true
  },
  {
    "code": "def _visible_exprs(exprs: Iterable[pyparsing.ParserElement]):\n    non_diagramming_exprs = (pyparsing.ParseElementEnhance, pyparsing.PositionToken, pyparsing.And._ErrorStop)\n    return [e for e in exprs if not (e.customName or e.resultsName or isinstance(e, non_diagramming_exprs))]",
    "label": true
  },
  {
    "code": "def check_nfc(label: str) -> None:\n    if unicodedata.normalize('NFC', label) != label:\n        raise IDNAError('Label must be in Normalization Form C')",
    "label": true
  },
  {
    "code": "def single_line(val):\n    if '\\n' in val:\n        msg = 'newlines are not allowed in `summary` and will break in the future'\n        SetuptoolsDeprecationWarning.emit('Invalid config.', msg)\n        val = val.strip().split('\\n')[0]\n    return val",
    "label": true
  },
  {
    "code": "def get_supported(version: Optional[str]=None, platforms: Optional[List[str]]=None, impl: Optional[str]=None, abis: Optional[List[str]]=None) -> List[Tag]:\n    supported: List[Tag] = []\n    python_version: Optional[PythonVersion] = None\n    if version is not None:\n        python_version = _get_python_version(version)\n    interpreter = _get_custom_interpreter(impl, version)\n    platforms = _expand_allowed_platforms(platforms)\n    is_cpython = (impl or interpreter_name()) == 'cp'\n    if is_cpython:\n        supported.extend(cpython_tags(python_version=python_version, abis=abis, platforms=platforms))\n    else:\n        supported.extend(generic_tags(interpreter=interpreter, abis=abis, platforms=platforms))\n    supported.extend(compatible_tags(python_version=python_version, interpreter=interpreter, platforms=platforms))\n    return supported",
    "label": true
  },
  {
    "code": "def pager(generator: t.Iterable[str], color: t.Optional[bool]=None) -> None:\n    stdout = _default_text_stdout()\n    if stdout is None:\n        stdout = StringIO()\n    if not isatty(sys.stdin) or not isatty(stdout):\n        return _nullpager(stdout, generator, color)\n    pager_cmd = (os.environ.get('PAGER', None) or '').strip()\n    if pager_cmd:\n        if WIN:\n            return _tempfilepager(generator, pager_cmd, color)\n        return _pipepager(generator, pager_cmd, color)\n    if os.environ.get('TERM') in ('dumb', 'emacs'):\n        return _nullpager(stdout, generator, color)\n    if WIN or sys.platform.startswith('os2'):\n        return _tempfilepager(generator, 'more <', color)\n    if hasattr(os, 'system') and os.system('(less) 2>/dev/null') == 0:\n        return _pipepager(generator, 'less', color)\n    import tempfile\n    fd, filename = tempfile.mkstemp()\n    os.close(fd)\n    try:\n        if hasattr(os, 'system') and os.system(f'more \"{filename}\"') == 0:\n            return _pipepager(generator, 'more', color)\n        return _nullpager(stdout, generator, color)\n    finally:\n        os.unlink(filename)",
    "label": true
  },
  {
    "code": "def write_exports(exports, stream):\n    if sys.version_info[0] >= 3:\n        stream = codecs.getwriter('utf-8')(stream)\n    cp = configparser.ConfigParser()\n    for k, v in exports.items():\n        cp.add_section(k)\n        for entry in v.values():\n            if entry.suffix is None:\n                s = entry.prefix\n            else:\n                s = '%s:%s' % (entry.prefix, entry.suffix)\n            if entry.flags:\n                s = '%s [%s]' % (s, ', '.join(entry.flags))\n            cp.set(k, entry.name, s)\n    cp.write(stream)",
    "label": true
  },
  {
    "code": "def _bypass_ensure_directory(path):\n    if not WRITE_SUPPORT:\n        raise IOError('\"os.mkdir\" not supported on this platform.')\n    dirname, filename = split(path)\n    if dirname and filename and (not isdir(dirname)):\n        _bypass_ensure_directory(dirname)\n        try:\n            mkdir(dirname, 493)\n        except FileExistsError:\n            pass",
    "label": true
  },
  {
    "code": "def do_striptags(value: 't.Union[str, HasHTML]') -> str:\n    if hasattr(value, '__html__'):\n        value = t.cast('HasHTML', value).__html__()\n    return Markup(str(value)).striptags()",
    "label": true
  },
  {
    "code": "def is_connection_dropped(conn):\n    sock = getattr(conn, 'sock', False)\n    if sock is False:\n        return False\n    if sock is None:\n        return True\n    try:\n        return wait_for_read(sock, timeout=0.0)\n    except NoWayToWaitForSocketError:\n        return False",
    "label": true
  },
  {
    "code": "def render_trailing_whitespace(msg, _node, source_lines=None):\n    line = msg.line\n    start_index, end_index = (len(source_lines[line - 1].rstrip()), len(source_lines[line - 1]))\n    yield from render_context(line - 1, line, source_lines)\n    yield (line, slice(start_index, end_index), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 2, source_lines)",
    "label": true
  },
  {
    "code": "def preprocess(content: str) -> ReqFileLines:\n    lines_enum: ReqFileLines = enumerate(content.splitlines(), start=1)\n    lines_enum = join_lines(lines_enum)\n    lines_enum = ignore_comments(lines_enum)\n    lines_enum = expand_env_variables(lines_enum)\n    return lines_enum",
    "label": true
  },
  {
    "code": "def _check_download_dir(link: Link, download_dir: str, hashes: Optional[Hashes], warn_on_hash_mismatch: bool=True) -> Optional[str]:\n    download_path = os.path.join(download_dir, link.filename)\n    if not os.path.exists(download_path):\n        return None\n    logger.info('File was already downloaded %s', download_path)\n    if hashes:\n        try:\n            hashes.check_against_path(download_path)\n        except HashMismatch:\n            if warn_on_hash_mismatch:\n                logger.warning('Previously-downloaded file %s has bad hash. Re-downloading.', download_path)\n            os.unlink(download_path)\n            return None\n    return download_path",
    "label": true
  },
  {
    "code": "def find_package_path(name: str, package_dir: Mapping[str, str], root_dir: _Path) -> str:\n    parts = name.split('.')\n    for i in range(len(parts), 0, -1):\n        partial_name = '.'.join(parts[:i])\n        if partial_name in package_dir:\n            parent = package_dir[partial_name]\n            return os.path.join(root_dir, parent, *parts[i:])\n    parent = package_dir.get('') or ''\n    return os.path.join(root_dir, *parent.split('/'), *parts)",
    "label": true
  },
  {
    "code": "def _coerce_parse_result(results: Union[ParseResults, List[Any]]) -> List[Any]:\n    if isinstance(results, ParseResults):\n        return [_coerce_parse_result(i) for i in results]\n    else:\n        return results",
    "label": true
  },
  {
    "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = (path, netloc)\n    if auth:\n        netloc = '@'.join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:\n        path = ''\n    return urlunparse((scheme, netloc, path, '', query, fragment))",
    "label": true
  },
  {
    "code": "def handle_requirement_line(line: ParsedLine, options: Optional[optparse.Values]=None) -> ParsedRequirement:\n    line_comes_from = '{} {} (line {})'.format('-c' if line.constraint else '-r', line.filename, line.lineno)\n    assert line.is_requirement\n    if line.is_editable:\n        return ParsedRequirement(requirement=line.requirement, is_editable=line.is_editable, comes_from=line_comes_from, constraint=line.constraint)\n    else:\n        req_options = {}\n        for dest in SUPPORTED_OPTIONS_REQ_DEST:\n            if dest in line.opts.__dict__ and line.opts.__dict__[dest]:\n                req_options[dest] = line.opts.__dict__[dest]\n        line_source = f'line {line.lineno} of {line.filename}'\n        return ParsedRequirement(requirement=line.requirement, is_editable=line.is_editable, comes_from=line_comes_from, constraint=line.constraint, options=req_options, line_source=line_source)",
    "label": true
  },
  {
    "code": "def _lookup_in_mro(node, name) -> list:\n    attrs = node.locals.get(name, [])\n    nodes = itertools.chain.from_iterable((ancestor.locals.get(name, []) for ancestor in node.ancestors(recurs=True)))\n    values = list(itertools.chain(attrs, nodes))\n    if not values:\n        raise AttributeInferenceError(attribute=name, target=node)\n    return values",
    "label": true
  },
  {
    "code": "def random_combination(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    indices = sorted(sample(range(n), r))\n    return tuple((pool[i] for i in indices))",
    "label": true
  },
  {
    "code": "def fake_traceback(exc_value: BaseException, tb: t.Optional[TracebackType], filename: str, lineno: int) -> TracebackType:\n    if tb is not None:\n        locals = get_template_locals(tb.tb_frame.f_locals)\n        locals.pop('__jinja_exception__', None)\n    else:\n        locals = {}\n    globals = {'__name__': filename, '__file__': filename, '__jinja_exception__': exc_value}\n    code: CodeType = compile('\\n' * (lineno - 1) + 'raise __jinja_exception__', filename, 'exec')\n    location = 'template'\n    if tb is not None:\n        function = tb.tb_frame.f_code.co_name\n        if function == 'root':\n            location = 'top-level template code'\n        elif function.startswith('block_'):\n            location = f'block {function[6:]!r}'\n    if sys.version_info >= (3, 8):\n        code = code.replace(co_name=location)\n    else:\n        code = CodeType(code.co_argcount, code.co_kwonlyargcount, code.co_nlocals, code.co_stacksize, code.co_flags, code.co_code, code.co_consts, code.co_names, code.co_varnames, code.co_filename, location, code.co_firstlineno, code.co_lnotab, code.co_freevars, code.co_cellvars)\n    try:\n        exec(code, globals, locals)\n    except BaseException:\n        return sys.exc_info()[2].tb_next",
    "label": true
  },
  {
    "code": "def render_scope(scope: 'Mapping[str, Any]', *, title: Optional[TextType]=None, sort_keys: bool=True, indent_guides: bool=False, max_length: Optional[int]=None, max_string: Optional[int]=None) -> 'ConsoleRenderable':\n    highlighter = ReprHighlighter()\n    items_table = Table.grid(padding=(0, 1), expand=False)\n    items_table.add_column(justify='right')\n\n    def sort_items(item: Tuple[str, Any]) -> Tuple[bool, str]:\n        \"\"\"Sort special variables first, then alphabetically.\"\"\"\n        key, _ = item\n        return (not key.startswith('__'), key.lower())\n    items = sorted(scope.items(), key=sort_items) if sort_keys else scope.items()\n    for key, value in items:\n        key_text = Text.assemble((key, 'scope.key.special' if key.startswith('__') else 'scope.key'), (' =', 'scope.equals'))\n        items_table.add_row(key_text, Pretty(value, highlighter=highlighter, indent_guides=indent_guides, max_length=max_length, max_string=max_string))\n    return Panel.fit(items_table, title=title, border_style='scope.border', padding=(0, 1))",
    "label": true
  },
  {
    "code": "def get_lexer(environment: 'Environment') -> 'Lexer':\n    key = (environment.block_start_string, environment.block_end_string, environment.variable_start_string, environment.variable_end_string, environment.comment_start_string, environment.comment_end_string, environment.line_statement_prefix, environment.line_comment_prefix, environment.trim_blocks, environment.lstrip_blocks, environment.newline_sequence, environment.keep_trailing_newline)\n    lexer = _lexer_cache.get(key)\n    if lexer is None:\n        _lexer_cache[key] = lexer = Lexer(environment)\n    return lexer",
    "label": true
  },
  {
    "code": "def pytest_fixture_post_finalizer(fixturedef: FixtureDef[object]) -> None:\n    if fixturedef.cached_result is not None:\n        config = fixturedef._fixturemanager.config\n        if config.option.setupshow:\n            _show_fixture_action(fixturedef, 'TEARDOWN')\n            if hasattr(fixturedef, 'cached_param'):\n                del fixturedef.cached_param",
    "label": true
  },
  {
    "code": "def fib(n):\n    assert n >= 0\n    if n <= 1:\n        return n\n    else:\n        return fib(n - 1) + fib(n - 2)",
    "label": true
  },
  {
    "code": "def _idna_encode(name):\n    if name and any((ord(x) >= 128 for x in name)):\n        try:\n            from pip._vendor import idna\n        except ImportError:\n            six.raise_from(LocationParseError(\"Unable to parse URL without the 'idna' module\"), None)\n        try:\n            return idna.encode(name.lower(), strict=True, std3_rules=True)\n        except idna.IDNAError:\n            six.raise_from(LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name), None)\n    return name.lower().encode('ascii')",
    "label": true
  },
  {
    "code": "def parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> Tuple[Pos, list]:\n    pos += 1\n    array: list = []\n    pos = skip_comments_and_array_ws(src, pos)\n    if src.startswith(']', pos):\n        return (pos + 1, array)\n    while True:\n        pos, val = parse_value(src, pos, parse_float)\n        array.append(val)\n        pos = skip_comments_and_array_ws(src, pos)\n        c = src[pos:pos + 1]\n        if c == ']':\n            return (pos + 1, array)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed array')\n        pos += 1\n        pos = skip_comments_and_array_ws(src, pos)\n        if src.startswith(']', pos):\n            return (pos + 1, array)",
    "label": true
  },
  {
    "code": "def test_iter_encode():\n    assert b''.join(iter_encode([], 'latin1')) == b''\n    assert b''.join(iter_encode([''], 'latin1')) == b''\n    assert b''.join(iter_encode(['\u00e9'], 'latin1')) == b'\\xe9'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'latin1')) == b'\\xe9'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16')) == b'\\xe9\\x00'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16le')) == b'\\xe9\\x00'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16be')) == b'\\x00\\xe9'\n    assert b''.join(iter_encode(['', 'h\\uf7e9', '', 'llo'], 'x-user-defined')) == b'h\\xe9llo'",
    "label": true
  },
  {
    "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def normalize_path(path: Any) -> str:\n    str_path = str(path)\n    parent, file_name = os.path.split(str_path)\n    if parent:\n        raise ValueError(f'{path!r} must be only a file name')\n    return file_name",
    "label": true
  },
  {
    "code": "def _handle_ns(packageName, path_item):\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n    try:\n        spec = importer.find_spec(packageName)\n    except AttributeError:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            loader = importer.find_module(packageName)\n    else:\n        loader = spec.loader if spec else None\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = types.ModuleType(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module, '__path__'):\n        raise TypeError('Not a package:', packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        importlib.import_module(packageName)\n        _rebuild_mod_path(path, packageName, module)\n    return subpath",
    "label": true
  },
  {
    "code": "def _version2fieldlist(version):\n    if version == '1.0':\n        return _241_FIELDS\n    elif version == '1.1':\n        return _314_FIELDS\n    elif version == '1.2':\n        return _345_FIELDS\n    elif version in ('1.3', '2.1'):\n        return _345_FIELDS + tuple((f for f in _566_FIELDS if f not in _345_FIELDS))\n    elif version == '2.0':\n        raise ValueError('Metadata 2.0 is withdrawn and not supported')\n    elif version == '2.2':\n        return _643_FIELDS\n    raise MetadataUnrecognizedVersionError(version)",
    "label": true
  },
  {
    "code": "def build_class(name: str, basenames: Iterable[str]=(), doc: str | None=None) -> nodes.ClassDef:\n    node = nodes.ClassDef(name)\n    node.postinit(bases=[nodes.Name(name=base, parent=node) for base in basenames], body=[], decorators=None, doc_node=nodes.Const(value=doc) if doc else None)\n    return node",
    "label": true
  },
  {
    "code": "def _pathlib_compat(path):\n    try:\n        return path.__fspath__()\n    except AttributeError:\n        return str(path)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e211(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def get_session():\n    adapter = CacheControlAdapter(DictCache(), cache_etags=True, serializer=None, heuristic=None)\n    sess = requests.Session()\n    sess.mount('http://', adapter)\n    sess.mount('https://', adapter)\n    sess.cache_controller = adapter.controller\n    return sess",
    "label": true
  },
  {
    "code": "def split_when(iterable, pred, maxsplit=-1):\n    if maxsplit == 0:\n        yield list(iterable)\n        return\n    it = iter(iterable)\n    try:\n        cur_item = next(it)\n    except StopIteration:\n        return\n    buf = [cur_item]\n    for next_item in it:\n        if pred(cur_item, next_item):\n            yield buf\n            if maxsplit == 1:\n                yield ([next_item] + list(it))\n                return\n            buf = []\n            maxsplit -= 1\n        buf.append(next_item)\n        cur_item = next_item\n    yield buf",
    "label": true
  },
  {
    "code": "def print_results(hits: List['TransformedHit'], name_column_width: Optional[int]=None, terminal_width: Optional[int]=None) -> None:\n    if not hits:\n        return\n    if name_column_width is None:\n        name_column_width = max([len(hit['name']) + len(highest_version(hit.get('versions', ['-']))) for hit in hits]) + 4\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary'] or ''\n        latest = highest_version(hit.get('versions', ['-']))\n        if terminal_width is not None:\n            target_width = terminal_width - name_column_width - 5\n            if target_width > 10:\n                summary_lines = textwrap.wrap(summary, target_width)\n                summary = ('\\n' + ' ' * (name_column_width + 3)).join(summary_lines)\n        name_latest = f'{name} ({latest})'\n        line = f'{name_latest:{name_column_width}} - {summary}'\n        try:\n            write_output(line)\n            print_dist_installation_info(name, latest)\n        except UnicodeEncodeError:\n            pass",
    "label": true
  },
  {
    "code": "def url_to_path(url: str) -> str:\n    assert url.startswith('file:'), f'You can only turn file: urls into filenames (not {url!r})'\n    _, netloc, path, _, _ = urllib.parse.urlsplit(url)\n    if not netloc or netloc == 'localhost':\n        netloc = ''\n    elif WINDOWS:\n        netloc = '\\\\\\\\' + netloc\n    else:\n        raise ValueError(f'non-local file URIs are not supported on this platform: {url!r}')\n    path = urllib.request.url2pathname(netloc + path)\n    if WINDOWS and (not netloc) and (len(path) >= 3) and (path[0] == '/') and (path[1] in string.ascii_letters) and (path[2:4] in (':', ':/')):\n        path = path[1:]\n    return path",
    "label": true
  },
  {
    "code": "def calculate_average_condition(bridge: list, start: int, stop: int) -> float:\n    if bridge == []:\n        return MISSING_BCI\n    years = bridge[COLUMN_BCI][INDEX_BCI_YEARS]\n    scores = bridge[COLUMN_BCI][INDEX_BCI_SCORES]\n    index_of_years_in_range = []\n    for i in range(len(years)):\n        if start <= int(years[i]) < stop:\n            index_of_years_in_range.append(i)\n    bci_scores = []\n    for i in index_of_years_in_range:\n        if years[i] != MISSING_BCI:\n            bci_scores.append(scores[i])\n    if index_of_years_in_range == [] or bci_scores == []:\n        return 0.0\n    return sum(bci_scores) / len(bci_scores)",
    "label": true
  },
  {
    "code": "def calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n    lon1, lat1, lon2, lat2 = (math.radians(lon1), math.radians(lat1), math.radians(lon2), math.radians(lat2))\n    lon_diff = lon2 - lon1\n    lat_diff = lat2 - lat1\n    a = math.sin(lat_diff / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(lon_diff / 2) ** 2\n    c = 2 * math.asin(math.sqrt(a))\n    return round(c * EARTH_RADIUS, 3)",
    "label": true
  },
  {
    "code": "def ignore_comments(lines_enum: ReqFileLines) -> ReqFileLines:\n    for line_number, line in lines_enum:\n        line = COMMENT_RE.sub('', line)\n        line = line.strip()\n        if line:\n            yield (line_number, line)",
    "label": true
  },
  {
    "code": "def insert_missing_modules(modules: Dict[str, ModuleType], module_name: str) -> None:\n    module_parts = module_name.split('.')\n    child_module: Union[ModuleType, None] = None\n    module: Union[ModuleType, None] = None\n    child_name: str = ''\n    while module_name:\n        if module_name not in modules:\n            try:\n                if not sys.meta_path:\n                    raise ModuleNotFoundError\n                module = importlib.import_module(module_name)\n            except ModuleNotFoundError:\n                module = ModuleType(module_name, doc=\"Empty module created by pytest's importmode=importlib.\")\n        else:\n            module = modules[module_name]\n        if child_module:\n            if not hasattr(module, child_name):\n                setattr(module, child_name, child_module)\n                modules[module_name] = module\n        child_module, child_name = (module, module_name.rpartition('.')[-1])\n        module_parts.pop(-1)\n        module_name = '.'.join(module_parts)",
    "label": true
  },
  {
    "code": "def trace_parse_action(f: ParseAction) -> ParseAction:\n    f = _trim_arity(f)\n\n    def z(*paArgs):\n        thisFunc = f.__name__\n        s, l, t = paArgs[-3:]\n        if len(paArgs) > 3:\n            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc\n        sys.stderr.write(f'>>entering {thisFunc}(line: {line(l, s)!r}, {l}, {t!r})\\n')\n        try:\n            ret = f(*paArgs)\n        except Exception as exc:\n            sys.stderr.write(f'<<leaving {thisFunc} (exception: {exc})\\n')\n            raise\n        sys.stderr.write(f'<<leaving {thisFunc} (ret: {ret!r})\\n')\n        return ret\n    z.__name__ = f.__name__\n    return z",
    "label": true
  },
  {
    "code": "def load_plugins() -> None:\n    for ep in entry_points(group='typeguard.checker_lookup'):\n        try:\n            plugin = ep.load()\n        except Exception as exc:\n            warnings.warn(f'Failed to load plugin {ep.name!r}: {qualified_name(exc)}: {exc}', stacklevel=2)\n            continue\n        if not callable(plugin):\n            warnings.warn(f'Plugin {ep} returned a non-callable object: {plugin!r}', stacklevel=2)\n            continue\n        checker_lookup_functions.insert(0, plugin)",
    "label": true
  },
  {
    "code": "def get_lexer_for_mimetype(_mime, **options):\n    for modname, name, _, _, mimetypes in LEXERS.values():\n        if _mime in mimetypes:\n            if name not in _lexer_cache:\n                _load_lexers(modname)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _mime in cls.mimetypes:\n            return cls(**options)\n    raise ClassNotFound('no lexer for mimetype %r found' % _mime)",
    "label": true
  },
  {
    "code": "def test_method_with_internal_import_should_work():\n    import re\n    back_fn = dill.loads(dill.dumps(get_fun_with_internal_import()))\n    import inspect\n    if hasattr(inspect, 'getclosurevars'):\n        vars = inspect.getclosurevars(back_fn)\n        assert vars.globals == {}\n        assert vars.nonlocals == {}\n    assert back_fn() == re.compile('$')\n    assert '__builtins__' in back_fn.__globals__",
    "label": true
  },
  {
    "code": "def parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:\n    if multiline:\n        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape_multiline\n    else:\n        error_on = ILLEGAL_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape\n    result = ''\n    start_pos = pos\n    while True:\n        try:\n            char = src[pos]\n        except IndexError:\n            raise suffixed_err(src, pos, 'Unterminated string') from None\n        if char == '\"':\n            if not multiline:\n                return (pos + 1, result + src[start_pos:pos])\n            if src.startswith('\"\"\"', pos):\n                return (pos + 3, result + src[start_pos:pos])\n            pos += 1\n            continue\n        if char == '\\\\':\n            result += src[start_pos:pos]\n            pos, parsed_escape = parse_escapes(src, pos)\n            result += parsed_escape\n            start_pos = pos\n            continue\n        if char in error_on:\n            raise suffixed_err(src, pos, f'Illegal character {char!r}')\n        pos += 1",
    "label": true
  },
  {
    "code": "def _replace_multiple(value, needles_and_replacements):\n\n    def replacer(match):\n        return needles_and_replacements[match.group(0)]\n    pattern = re.compile('|'.join([re.escape(needle) for needle in needles_and_replacements.keys()]))\n    result = pattern.sub(replacer, value)\n    return result",
    "label": true
  },
  {
    "code": "def _handle_python_version(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    version_info, error_msg = _convert_python_version(value)\n    if error_msg is not None:\n        msg = 'invalid --python-version value: {!r}: {}'.format(value, error_msg)\n        raise_option_error(parser, option=option, msg=msg)\n    parser.values.python_version = version_info",
    "label": true
  },
  {
    "code": "def get_lexer_for_mimetype(_mime, **options):\n    for modname, name, _, _, mimetypes in LEXERS.values():\n        if _mime in mimetypes:\n            if name not in _lexer_cache:\n                _load_lexers(modname)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _mime in cls.mimetypes:\n            return cls(**options)\n    raise ClassNotFound('no lexer for mimetype %r found' % _mime)",
    "label": true
  },
  {
    "code": "def _display_value(value: Any, max_length: int=_DEFAULT_MAX_VALUE_LENGTH) -> str:\n    s = repr(value)\n    if not DEBUG_CONTRACTS and len(s) > max_length:\n        i = (max_length - 3) // 2\n        return s[:i] + '...' + s[-i:]\n    else:\n        return s",
    "label": true
  },
  {
    "code": "def build_source(location: str, *, candidates_from_page: CandidatesFromPage, page_validator: PageValidator, expand_dir: bool, cache_link_parsing: bool) -> Tuple[Optional[str], Optional[LinkSource]]:\n    path: Optional[str] = None\n    url: Optional[str] = None\n    if os.path.exists(location):\n        url = path_to_url(location)\n        path = location\n    elif location.startswith('file:'):\n        url = location\n        path = url_to_path(location)\n    elif is_url(location):\n        url = location\n    if url is None:\n        msg = \"Location '%s' is ignored: it is either a non-existing path or lacks a specific scheme.\"\n        logger.warning(msg, location)\n        return (None, None)\n    if path is None:\n        source: LinkSource = _RemoteFileSource(candidates_from_page=candidates_from_page, page_validator=page_validator, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    if os.path.isdir(path):\n        if expand_dir:\n            source = _FlatDirectorySource(candidates_from_page=candidates_from_page, path=path)\n        else:\n            source = _IndexDirectorySource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    elif os.path.isfile(path):\n        source = _LocalFileSource(candidates_from_page=candidates_from_page, link=Link(url, cache_link_parsing=cache_link_parsing))\n        return (url, source)\n    logger.warning(\"Location '%s' is ignored: it is neither a file nor a directory.\", location)\n    return (url, None)",
    "label": true
  },
  {
    "code": "def h(x):\n\n    def g(x):\n        return x\n    return g(x) - x",
    "label": true
  },
  {
    "code": "def _match_vcs_scheme(url: str) -> Optional[str]:\n    for scheme in vcs.schemes:\n        if url.lower().startswith(scheme) and url[len(scheme)] in '+:':\n            return scheme\n    return None",
    "label": true
  },
  {
    "code": "def service_request_transform(node):\n    code = '\\n    def __getattr__(self, attr):\\n        return 0\\n    '\n    func_getattr = extract_node(code)\n    node.locals['__getattr__'] = [func_getattr]\n    return node",
    "label": true
  },
  {
    "code": "def _normalize_extra_values(results: Any) -> Any:\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == 'extra':\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == 'extra':\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = (lhs, op, rhs)\n    return results",
    "label": true
  },
  {
    "code": "def _proxy_helper(obj):\n    _repr = repr(obj)\n    try:\n        _str = str(obj)\n    except ReferenceError:\n        return id(None)\n    if _str == _repr:\n        return id(obj)\n    try:\n        address = int(_str.rstrip('>').split(' at ')[-1], base=16)\n    except ValueError:\n        if not IS_PYPY:\n            address = int(_repr.rstrip('>').split(' at ')[-1], base=16)\n        else:\n            objects = iter(gc.get_objects())\n            for _obj in objects:\n                if repr(_obj) == _str:\n                    return id(_obj)\n            msg = \"Cannot reference object for proxy at '%s'\" % id(obj)\n            raise ReferenceError(msg)\n    return address",
    "label": true
  },
  {
    "code": "def _enable_all_extensions(run: Run, value: str | None) -> None:\n    assert value is None\n    for filename in Path(extensions.__file__).parent.iterdir():\n        if filename.suffix == '.py' and (not filename.stem.startswith('_')):\n            extension_name = f'pylint.extensions.{filename.stem}'\n            if extension_name not in run._plugins:\n                run._plugins.append(extension_name)",
    "label": true
  },
  {
    "code": "def check_typevar(value: Any, origin_type: TypeVar, args: tuple[Any, ...], memo: TypeCheckMemo, *, subclass_check: bool=False) -> None:\n    if origin_type.__bound__ is not None:\n        annotation = Type[origin_type.__bound__] if subclass_check else origin_type.__bound__\n        check_type_internal(value, annotation, memo)\n    elif origin_type.__constraints__:\n        for constraint in origin_type.__constraints__:\n            annotation = Type[constraint] if subclass_check else constraint\n            try:\n                check_type_internal(value, annotation, memo)\n            except TypeCheckError:\n                pass\n            else:\n                break\n        else:\n            formatted_constraints = ', '.join((get_type_name(constraint) for constraint in origin_type.__constraints__))\n            raise TypeCheckError(f'does not match any of the constraints ({formatted_constraints})')",
    "label": true
  },
  {
    "code": "def dumpIO_source(object, **kwds):\n    from .source import importable, getname\n    from io import BytesIO as StringIO\n    alias = kwds.pop('alias', '')\n    name = str(alias) or getname(object)\n    name = '\\n#NAME: %s\\n' % name\n    file = StringIO()\n    file.write(b(''.join([importable(object, alias=alias), name])))\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def make_player(generic_name: str) -> Player:\n    name = input(f'Enter a name for {generic_name}: ')\n    type = input('What type would you like this player to be?\\n(1 - Random Player, 2 - User Player, 3 - Strategic Player)\\n')\n    while not type.isnumeric() or not 1 <= int(type) <= 3:\n        print('Oops, looks like you entered something invalid.')\n        type = input('What type would you like this player to be?\\n(1 - Random Player, 2 - User Player, 3 - Strategic Player)')\n    type = int(type)\n    if type == 1:\n        return RandomPlayer(name)\n    elif type == 2:\n        return UserPlayer(name)\n    elif type == 3:\n        return StrategicPlayer(name)",
    "label": true
  },
  {
    "code": "def wrap_stream(stream, convert, strip, autoreset, wrap):\n    if wrap:\n        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)\n        if wrapper.should_wrap():\n            stream = wrapper.stream\n    return stream",
    "label": true
  },
  {
    "code": "def _http_transform():\n    code = textwrap.dedent('\\n    from enum import IntEnum\\n    from collections import namedtuple\\n    _HTTPStatus = namedtuple(\\'_HTTPStatus\\', \\'value phrase description\\')\\n\\n    class HTTPStatus(IntEnum):\\n\\n        @property\\n        def phrase(self):\\n            return \"\"\\n        @property\\n        def value(self):\\n            return 0\\n        @property\\n        def description(self):\\n            return \"\"\\n\\n        # informational\\n        CONTINUE = _HTTPStatus(100, \\'Continue\\', \\'Request received, please continue\\')\\n        SWITCHING_PROTOCOLS = _HTTPStatus(101, \\'Switching Protocols\\',\\n                \\'Switching to new protocol; obey Upgrade header\\')\\n        PROCESSING = _HTTPStatus(102, \\'Processing\\', \\'\\')\\n        OK = _HTTPStatus(200, \\'OK\\', \\'Request fulfilled, document follows\\')\\n        CREATED = _HTTPStatus(201, \\'Created\\', \\'Document created, URL follows\\')\\n        ACCEPTED = _HTTPStatus(202, \\'Accepted\\',\\n            \\'Request accepted, processing continues off-line\\')\\n        NON_AUTHORITATIVE_INFORMATION = _HTTPStatus(203,\\n            \\'Non-Authoritative Information\\', \\'Request fulfilled from cache\\')\\n        NO_CONTENT = _HTTPStatus(204, \\'No Content\\', \\'Request fulfilled, nothing follows\\')\\n        RESET_CONTENT =_HTTPStatus(205, \\'Reset Content\\', \\'Clear input form for further input\\')\\n        PARTIAL_CONTENT = _HTTPStatus(206, \\'Partial Content\\', \\'Partial content follows\\')\\n        MULTI_STATUS = _HTTPStatus(207, \\'Multi-Status\\', \\'\\')\\n        ALREADY_REPORTED = _HTTPStatus(208, \\'Already Reported\\', \\'\\')\\n        IM_USED = _HTTPStatus(226, \\'IM Used\\', \\'\\')\\n        MULTIPLE_CHOICES = _HTTPStatus(300, \\'Multiple Choices\\',\\n            \\'Object has several resources -- see URI list\\')\\n        MOVED_PERMANENTLY = _HTTPStatus(301, \\'Moved Permanently\\',\\n            \\'Object moved permanently -- see URI list\\')\\n        FOUND = _HTTPStatus(302, \\'Found\\', \\'Object moved temporarily -- see URI list\\')\\n        SEE_OTHER = _HTTPStatus(303, \\'See Other\\', \\'Object moved -- see Method and URL list\\')\\n        NOT_MODIFIED = _HTTPStatus(304, \\'Not Modified\\',\\n            \\'Document has not changed since given time\\')\\n        USE_PROXY = _HTTPStatus(305, \\'Use Proxy\\',\\n            \\'You must use proxy specified in Location to access this resource\\')\\n        TEMPORARY_REDIRECT = _HTTPStatus(307, \\'Temporary Redirect\\',\\n            \\'Object moved temporarily -- see URI list\\')\\n        PERMANENT_REDIRECT = _HTTPStatus(308, \\'Permanent Redirect\\',\\n            \\'Object moved permanently -- see URI list\\')\\n        BAD_REQUEST = _HTTPStatus(400, \\'Bad Request\\',\\n            \\'Bad request syntax or unsupported method\\')\\n        UNAUTHORIZED = _HTTPStatus(401, \\'Unauthorized\\',\\n            \\'No permission -- see authorization schemes\\')\\n        PAYMENT_REQUIRED = _HTTPStatus(402, \\'Payment Required\\',\\n            \\'No payment -- see charging schemes\\')\\n        FORBIDDEN = _HTTPStatus(403, \\'Forbidden\\',\\n            \\'Request forbidden -- authorization will not help\\')\\n        NOT_FOUND = _HTTPStatus(404, \\'Not Found\\',\\n            \\'Nothing matches the given URI\\')\\n        METHOD_NOT_ALLOWED = _HTTPStatus(405, \\'Method Not Allowed\\',\\n            \\'Specified method is invalid for this resource\\')\\n        NOT_ACCEPTABLE = _HTTPStatus(406, \\'Not Acceptable\\',\\n            \\'URI not available in preferred format\\')\\n        PROXY_AUTHENTICATION_REQUIRED = _HTTPStatus(407,\\n            \\'Proxy Authentication Required\\',\\n            \\'You must authenticate with this proxy before proceeding\\')\\n        REQUEST_TIMEOUT = _HTTPStatus(408, \\'Request Timeout\\',\\n            \\'Request timed out; try again later\\')\\n        CONFLICT = _HTTPStatus(409, \\'Conflict\\', \\'Request conflict\\')\\n        GONE = _HTTPStatus(410, \\'Gone\\',\\n            \\'URI no longer exists and has been permanently removed\\')\\n        LENGTH_REQUIRED = _HTTPStatus(411, \\'Length Required\\',\\n            \\'Client must specify Content-Length\\')\\n        PRECONDITION_FAILED = _HTTPStatus(412, \\'Precondition Failed\\',\\n            \\'Precondition in headers is false\\')\\n        REQUEST_ENTITY_TOO_LARGE = _HTTPStatus(413, \\'Request Entity Too Large\\',\\n            \\'Entity is too large\\')\\n        REQUEST_URI_TOO_LONG = _HTTPStatus(414, \\'Request-URI Too Long\\',\\n            \\'URI is too long\\')\\n        UNSUPPORTED_MEDIA_TYPE = _HTTPStatus(415, \\'Unsupported Media Type\\',\\n            \\'Entity body in unsupported format\\')\\n        REQUESTED_RANGE_NOT_SATISFIABLE = _HTTPStatus(416,\\n            \\'Requested Range Not Satisfiable\\',\\n            \\'Cannot satisfy request range\\')\\n        EXPECTATION_FAILED = _HTTPStatus(417, \\'Expectation Failed\\',\\n            \\'Expect condition could not be satisfied\\')\\n        MISDIRECTED_REQUEST = _HTTPStatus(421, \\'Misdirected Request\\',\\n            \\'Server is not able to produce a response\\')\\n        UNPROCESSABLE_ENTITY = _HTTPStatus(422, \\'Unprocessable Entity\\')\\n        LOCKED = _HTTPStatus(423, \\'Locked\\')\\n        FAILED_DEPENDENCY = _HTTPStatus(424, \\'Failed Dependency\\')\\n        UPGRADE_REQUIRED = _HTTPStatus(426, \\'Upgrade Required\\')\\n        PRECONDITION_REQUIRED = _HTTPStatus(428, \\'Precondition Required\\',\\n            \\'The origin server requires the request to be conditional\\')\\n        TOO_MANY_REQUESTS = _HTTPStatus(429, \\'Too Many Requests\\',\\n            \\'The user has sent too many requests in \\'\\n            \\'a given amount of time (\"rate limiting\")\\')\\n        REQUEST_HEADER_FIELDS_TOO_LARGE = _HTTPStatus(431,\\n            \\'Request Header Fields Too Large\\',\\n            \\'The server is unwilling to process the request because its header \\'\\n            \\'fields are too large\\')\\n        UNAVAILABLE_FOR_LEGAL_REASONS = _HTTPStatus(451,\\n            \\'Unavailable For Legal Reasons\\',\\n            \\'The server is denying access to the \\'\\n            \\'resource as a consequence of a legal demand\\')\\n        INTERNAL_SERVER_ERROR = _HTTPStatus(500, \\'Internal Server Error\\',\\n            \\'Server got itself in trouble\\')\\n        NOT_IMPLEMENTED = _HTTPStatus(501, \\'Not Implemented\\',\\n            \\'Server does not support this operation\\')\\n        BAD_GATEWAY = _HTTPStatus(502, \\'Bad Gateway\\',\\n            \\'Invalid responses from another server/proxy\\')\\n        SERVICE_UNAVAILABLE = _HTTPStatus(503, \\'Service Unavailable\\',\\n            \\'The server cannot process the request due to a high load\\')\\n        GATEWAY_TIMEOUT = _HTTPStatus(504, \\'Gateway Timeout\\',\\n            \\'The gateway server did not receive a timely response\\')\\n        HTTP_VERSION_NOT_SUPPORTED = _HTTPStatus(505, \\'HTTP Version Not Supported\\',\\n            \\'Cannot fulfill request\\')\\n        VARIANT_ALSO_NEGOTIATES = _HTTPStatus(506, \\'Variant Also Negotiates\\')\\n        INSUFFICIENT_STORAGE = _HTTPStatus(507, \\'Insufficient Storage\\')\\n        LOOP_DETECTED = _HTTPStatus(508, \\'Loop Detected\\')\\n        NOT_EXTENDED = _HTTPStatus(510, \\'Not Extended\\')\\n        NETWORK_AUTHENTICATION_REQUIRED = _HTTPStatus(511,\\n            \\'Network Authentication Required\\',\\n            \\'The client needs to authenticate to gain network access\\')\\n    ')\n    return AstroidBuilder(AstroidManager()).string_build(code)",
    "label": true
  },
  {
    "code": "def _get_line_with_reprcrash_message(config: Config, rep: BaseReport, tw: TerminalWriter, word_markup: Dict[str, bool]) -> str:\n    verbose_word = rep._get_verbose_word(config)\n    word = tw.markup(verbose_word, **word_markup)\n    node = _get_node_id_with_markup(tw, config, rep)\n    line = f'{word} {node}'\n    line_width = wcswidth(line)\n    try:\n        msg = rep.longrepr.reprcrash.message\n    except AttributeError:\n        pass\n    else:\n        if not running_on_ci():\n            available_width = tw.fullwidth - line_width\n            msg = _format_trimmed(' - {}', msg, available_width)\n        else:\n            msg = f' - {msg}'\n        if msg is not None:\n            line += msg\n    return line",
    "label": true
  },
  {
    "code": "def test_pickled_inner():\n    add5 = adder(x)\n    pinner = pickle.dumps(add5)\n    p5add = pickle.loads(pinner)\n    assert p5add(y) == x + y",
    "label": true
  },
  {
    "code": "def test_x_user_defined():\n    encoded = b'2,\\x0c\\x0b\\x1aO\\xd9#\\xcb\\x0f\\xc9\\xbbt\\xcf\\xa8\\xca'\n    decoded = '2,\\x0c\\x0b\\x1aO\\uf7d9#\\uf7cb\\x0f\\uf7c9\\uf7bbt\\uf7cf\\uf7a8\\uf7ca'\n    encoded = b'aa'\n    decoded = 'aa'\n    assert decode(encoded, 'x-user-defined') == (decoded, lookup('x-user-defined'))\n    assert encode(decoded, 'x-user-defined') == encoded",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def _infer_decorator_callchain(node):\n    if not isinstance(node, FunctionDef):\n        return None\n    if not node.parent:\n        return None\n    try:\n        result = next(node.infer_call_result(node.parent), None)\n    except InferenceError:\n        return None\n    if isinstance(result, bases.Instance):\n        result = result._proxied\n    if isinstance(result, ClassDef):\n        if result.is_subtype_of('builtins.classmethod'):\n            return 'classmethod'\n        if result.is_subtype_of('builtins.staticmethod'):\n            return 'staticmethod'\n    if isinstance(result, FunctionDef):\n        if not result.decorators:\n            return None\n        for decorator in result.decorators.nodes:\n            if isinstance(decorator, node_classes.Name):\n                if decorator.name in BUILTIN_DESCRIPTORS:\n                    return decorator.name\n            if isinstance(decorator, node_classes.Attribute) and isinstance(decorator.expr, node_classes.Name) and (decorator.expr.name == 'builtins') and (decorator.attrname in BUILTIN_DESCRIPTORS):\n                return decorator.attrname\n    return None",
    "label": true
  },
  {
    "code": "def _get_python_inc_posix_prefix(prefix):\n    implementation = 'pypy' if IS_PYPY else 'python'\n    python_dir = implementation + get_python_version() + build_flags\n    return os.path.join(prefix, 'include', python_dir)",
    "label": true
  },
  {
    "code": "def guess_lexer(_text, **options):\n    if not isinstance(_text, str):\n        inencoding = options.get('inencoding', options.get('encoding'))\n        if inencoding:\n            _text = _text.decode(inencoding or 'utf8')\n        else:\n            _text, _ = guess_decode(_text)\n    ft = get_filetype_from_buffer(_text)\n    if ft is not None:\n        try:\n            return get_lexer_by_name(ft, **options)\n        except ClassNotFound:\n            pass\n    best_lexer = [0.0, None]\n    for lexer in _iter_lexerclasses():\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        if rv > best_lexer[0]:\n            best_lexer[:] = (rv, lexer)\n    if not best_lexer[0] or best_lexer[1] is None:\n        raise ClassNotFound('no lexer matching the text found')\n    return best_lexer[1](**options)",
    "label": true
  },
  {
    "code": "def _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n    if len(lines) < 2 or lines[0][:4] != 'musl':\n        return None\n    m = re.match('Version (\\\\d+)\\\\.(\\\\d+)', lines[1])\n    if not m:\n        return None\n    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))",
    "label": true
  },
  {
    "code": "def _parse_version_many(tokenizer: Tokenizer) -> str:\n    parsed_specifiers = ''\n    while tokenizer.check('SPECIFIER'):\n        span_start = tokenizer.position\n        parsed_specifiers += tokenizer.read().text\n        if tokenizer.check('VERSION_PREFIX_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('.* suffix can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position + 1)\n        if tokenizer.check('VERSION_LOCAL_LABEL_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('Local version label can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position)\n        tokenizer.consume('WS')\n        if not tokenizer.check('COMMA'):\n            break\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def set_file_position(body, pos):\n    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except (IOError, OSError):\n            pos = _FAILEDTELL\n    return pos",
    "label": true
  },
  {
    "code": "def check_byteslike(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, (bytearray, bytes, memoryview)):\n        raise TypeCheckError('is not bytes-like')",
    "label": true
  },
  {
    "code": "def _forgiving_version(version):\n    version = version.replace(' ', '.')\n    match = _PEP440_FALLBACK.search(version)\n    if match:\n        safe = match['safe']\n        rest = version[len(safe):]\n    else:\n        safe = '0'\n        rest = version\n    local = f'sanitized.{_safe_segment(rest)}'.strip('.')\n    return f'{safe}.dev0+{local}'",
    "label": true
  },
  {
    "code": "def tags_f(args):\n    from .tags import tags\n    names = (tags(wheel, args.python_tag, args.abi_tag, args.platform_tag, args.build, args.remove) for wheel in args.wheel)\n    for name in names:\n        print(name)",
    "label": true
  },
  {
    "code": "def _check_class_type_annotations(klass: type, instance: Any) -> None:\n    klass_mod = _get_module(klass)\n    cls_annotations = typing.get_type_hints(klass, localns=klass_mod.__dict__)\n    for attr, annotation in cls_annotations.items():\n        value = getattr(instance, attr)\n        try:\n            _debug(f'Checking type of attribute {attr} for {klass.__qualname__} instance')\n            check_type(value, annotation, collection_check_strategy=CollectionCheckStrategy.ALL_ITEMS)\n        except TypeCheckError:\n            raise AssertionError(f'{_display_value(value)} did not match type annotation for attribute {attr}: {_display_annotation(annotation)}')",
    "label": true
  },
  {
    "code": "def get_parser_module(type_comments: bool=True) -> ParserModule:\n    parser_module = ast\n    if type_comments and _ast_py3:\n        parser_module = _ast_py3\n    unary_op_classes = _unary_operators_from_module(parser_module)\n    cmp_op_classes = _compare_operators_from_module(parser_module)\n    bool_op_classes = _bool_operators_from_module(parser_module)\n    bin_op_classes = _binary_operators_from_module(parser_module)\n    context_classes = _contexts_from_module(parser_module)\n    return ParserModule(parser_module, unary_op_classes, cmp_op_classes, bool_op_classes, bin_op_classes, context_classes)",
    "label": true
  },
  {
    "code": "def _zip_equal_generator(iterables):\n    for combo in zip_longest(*iterables, fillvalue=_marker):\n        for val in combo:\n            if val is _marker:\n                raise UnequalIterablesError()\n        yield combo",
    "label": true
  },
  {
    "code": "def create_dict_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot declare {key} twice')\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.get_or_create_nest(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']', pos):\n        raise suffixed_err(src, pos, \"Expected ']' at the end of a table declaration\")\n    return (pos + 1, key)",
    "label": true
  },
  {
    "code": "def derive_importpath(import_path: str, raising: bool) -> Tuple[str, object]:\n    if not isinstance(import_path, str) or '.' not in import_path:\n        raise TypeError(f'must be absolute import path string, not {import_path!r}')\n    module, attr = import_path.rsplit('.', 1)\n    target = resolve(module)\n    if raising:\n        annotated_getattr(target, attr, ann=module)\n    return (attr, target)",
    "label": true
  },
  {
    "code": "def _is_present_dir(path: Traversable) -> bool:\n    with contextlib.suppress(FileNotFoundError):\n        return path.is_dir()\n    return False",
    "label": true
  },
  {
    "code": "def enum(*sequential: Any, **named: Any) -> Type[Any]:\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    reverse = {value: key for key, value in enums.items()}\n    enums['reverse_mapping'] = reverse\n    return type('Enum', (), enums)",
    "label": true
  },
  {
    "code": "def windowed_complete(iterable, n):\n    if n < 0:\n        raise ValueError('n must be >= 0')\n    seq = tuple(iterable)\n    size = len(seq)\n    if n > size:\n        raise ValueError('n must be <= len(seq)')\n    for i in range(size - n + 1):\n        beginning = seq[:i]\n        middle = seq[i:i + n]\n        end = seq[i + n:]\n        yield (beginning, middle, end)",
    "label": true
  },
  {
    "code": "def distinct_combinations(iterable, r):\n    if r < 0:\n        raise ValueError('r must be non-negative')\n    elif r == 0:\n        yield ()\n        return\n    pool = tuple(iterable)\n    generators = [unique_everseen(enumerate(pool), key=itemgetter(1))]\n    current_combo = [None] * r\n    level = 0\n    while generators:\n        try:\n            cur_idx, p = next(generators[-1])\n        except StopIteration:\n            generators.pop()\n            level -= 1\n            continue\n        current_combo[level] = p\n        if level + 1 == r:\n            yield tuple(current_combo)\n        else:\n            generators.append(unique_everseen(enumerate(pool[cur_idx + 1:], cur_idx + 1), key=itemgetter(1)))\n            level += 1",
    "label": true
  },
  {
    "code": "def match_to_number(match: 're.Match', parse_float: 'ParseFloat') -> Any:\n    if match.group('floatpart'):\n        return parse_float(match.group())\n    return int(match.group(), 0)",
    "label": true
  },
  {
    "code": "def _rx_indent(level):\n    tab_width = 8\n    if tab_width == 2:\n        space_repeat = '+'\n    else:\n        space_repeat = '{1,%d}' % (tab_width - 1)\n    if level == 1:\n        level_repeat = ''\n    else:\n        level_repeat = '{%s}' % level\n    return '(?:\\\\t| %s\\\\t| {%s})%s.*\\\\n' % (space_repeat, tab_width, level_repeat)",
    "label": true
  },
  {
    "code": "def wtf(x, y, z):\n\n    def zzz():\n        return x\n\n    def yyy():\n        return y\n\n    def xxx():\n        return z\n    return (zzz, yyy)",
    "label": true
  },
  {
    "code": "def _is_builtin_module(module):\n    if not hasattr(module, '__file__'):\n        return True\n    if module.__file__ is None:\n        return False\n    names = ['base_prefix', 'base_exec_prefix', 'exec_prefix', 'prefix', 'real_prefix']\n    rp = os.path.realpath\n    return any((module.__file__.startswith(getattr(sys, name)) or rp(module.__file__).startswith(rp(getattr(sys, name))) for name in names if hasattr(sys, name))) or module.__file__.endswith(EXTENSION_SUFFIXES) or 'site-packages' in module.__file__",
    "label": true
  },
  {
    "code": "def get_process_umask():\n    result = os.umask(18)\n    os.umask(result)\n    return result",
    "label": true
  },
  {
    "code": "def try_cleanup(path: Path, consider_lock_dead_if_created_before: float) -> None:\n    if ensure_deletable(path, consider_lock_dead_if_created_before):\n        maybe_delete_a_numbered_dir(path)",
    "label": true
  },
  {
    "code": "def evaluate_marker(text, extra=None):\n    try:\n        marker = packaging.markers.Marker(text)\n        return marker.evaluate()\n    except packaging.markers.InvalidMarker as e:\n        raise SyntaxError(e) from e",
    "label": true
  },
  {
    "code": "def parse_name_and_version(p):\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException(\"Ill-formed name/version string: '%s'\" % p)\n    d = m.groupdict()\n    return (d['name'].strip().lower(), d['ver'])",
    "label": true
  },
  {
    "code": "def reset_all():\n    if AnsiToWin32 is not None:\n        AnsiToWin32(orig_stdout).reset_all()",
    "label": true
  },
  {
    "code": "def _generate_tree_from_node(node: ReadNode) -> HuffmanTree:\n    if node.l_type == 1:\n        left_tree = HuffmanTree(None)\n        left_tree.number = node.l_data\n    else:\n        left_tree = HuffmanTree(node.l_data)\n    if node.r_type == 1:\n        right_tree = HuffmanTree(None)\n        right_tree.number = node.r_data\n    else:\n        right_tree = HuffmanTree(node.r_data)\n    return HuffmanTree(None, left_tree, right_tree)",
    "label": true
  },
  {
    "code": "def check_file(filename: Union[str, Path], show_diff: Union[bool, TextIO]=False, config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, disregard_skip: bool=True, extension: Optional[str]=None, **config_kwargs: Any) -> bool:\n    file_config: Config = config\n    if 'config_trie' in config_kwargs:\n        config_trie = config_kwargs.pop('config_trie', None)\n        if config_trie:\n            config_info = config_trie.search(filename)\n            if config.verbose:\n                print(f'{config_info[0]} used for file {filename}')\n            file_config = Config(**config_info[1])\n    with io.File.read(filename) as source_file:\n        return check_stream(source_file.stream, show_diff=show_diff, extension=extension, config=file_config, file_path=file_path or source_file.path, disregard_skip=disregard_skip, **config_kwargs)",
    "label": true
  },
  {
    "code": "def guess_lexer(_text, **options):\n    if not isinstance(_text, str):\n        inencoding = options.get('inencoding', options.get('encoding'))\n        if inencoding:\n            _text = _text.decode(inencoding or 'utf8')\n        else:\n            _text, _ = guess_decode(_text)\n    ft = get_filetype_from_buffer(_text)\n    if ft is not None:\n        try:\n            return get_lexer_by_name(ft, **options)\n        except ClassNotFound:\n            pass\n    best_lexer = [0.0, None]\n    for lexer in _iter_lexerclasses():\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        if rv > best_lexer[0]:\n            best_lexer[:] = (rv, lexer)\n    if not best_lexer[0] or best_lexer[1] is None:\n        raise ClassNotFound('no lexer matching the text found')\n    return best_lexer[1](**options)",
    "label": true
  },
  {
    "code": "def _makeTags(tagStr, xml, suppress_LT=Suppress('<'), suppress_GT=Suppress('>')):\n    if isinstance(tagStr, str_type):\n        resname = tagStr\n        tagStr = Keyword(tagStr, caseless=not xml)\n    else:\n        resname = tagStr.name\n    tagAttrName = Word(alphas, alphanums + '_-:')\n    if xml:\n        tagAttrValue = dbl_quoted_string.copy().set_parse_action(remove_quotes)\n        openTag = suppress_LT + tagStr('tag') + Dict(ZeroOrMore(Group(tagAttrName + Suppress('=') + tagAttrValue))) + Opt('/', default=[False])('empty').set_parse_action(lambda s, l, t: t[0] == '/') + suppress_GT\n    else:\n        tagAttrValue = quoted_string.copy().set_parse_action(remove_quotes) | Word(printables, exclude_chars='>')\n        openTag = suppress_LT + tagStr('tag') + Dict(ZeroOrMore(Group(tagAttrName.set_parse_action(lambda t: t[0].lower()) + Opt(Suppress('=') + tagAttrValue)))) + Opt('/', default=[False])('empty').set_parse_action(lambda s, l, t: t[0] == '/') + suppress_GT\n    closeTag = Combine(Literal('</') + tagStr + '>', adjacent=False)\n    openTag.set_name('<%s>' % resname)\n    openTag.add_parse_action(lambda t: t.__setitem__('start' + ''.join(resname.replace(':', ' ').title().split()), t.copy()))\n    closeTag = closeTag('end' + ''.join(resname.replace(':', ' ').title().split())).set_name('</%s>' % resname)\n    openTag.tag = resname\n    closeTag.tag = resname\n    openTag.tag_body = SkipTo(closeTag())\n    return (openTag, closeTag)",
    "label": true
  },
  {
    "code": "def _raise_for_invalid_entrypoint(specification: str) -> None:\n    entry = get_export_entry(specification)\n    if entry is not None and entry.suffix is None:\n        raise MissingCallableSuffix(str(entry))",
    "label": true
  },
  {
    "code": "def _convert_extras_requirements(extras_require: Dict[str, Dict[str, Requirement]]) -> Mapping[str, _Ordered[Requirement]]:\n    output: Mapping[str, _Ordered[Requirement]] = defaultdict(dict)\n    for section, v in extras_require.items():\n        output[section]\n        for r in v.values():\n            output[section + _suffix_for(r)].setdefault(r)\n    return output",
    "label": true
  },
  {
    "code": "def _dev_pkgs() -> AbstractSet[str]:\n    pkgs = {'pip'}\n    if _should_suppress_build_backends():\n        pkgs |= {'setuptools', 'distribute', 'wheel'}\n    return pkgs",
    "label": true
  },
  {
    "code": "def is_archive_file(name: str) -> bool:\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def load_cdll(name: str, macos10_16_path: str) -> CDLL:\n    try:\n        path: str | None\n        if version_info >= (10, 16):\n            path = macos10_16_path\n        else:\n            path = find_library(name)\n        if not path:\n            raise OSError\n        return CDLL(path, use_errno=True)\n    except OSError:\n        raise ImportError(f'The library {name} failed to load') from None",
    "label": true
  },
  {
    "code": "def path_to_url(path: str) -> str:\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib.parse.urljoin('file:', urllib.request.pathname2url(path))\n    return url",
    "label": true
  },
  {
    "code": "def _is_linux_i686() -> bool:\n    elf_header = _get_elf_header()\n    if elf_header is None:\n        return False\n    result = elf_header.e_ident_class == elf_header.ELFCLASS32\n    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB\n    result &= elf_header.e_machine == elf_header.EM_386\n    return result",
    "label": true
  },
  {
    "code": "def _get_env(environment: Dict[str, str], name: str) -> str:\n    value: Union[str, Undefined] = environment.get(name, _undefined)\n    if isinstance(value, Undefined):\n        raise UndefinedEnvironmentName(f'{name!r} does not exist in evaluation environment.')\n    return value",
    "label": true
  },
  {
    "code": "def add_ext_suffix_39(vars):\n    import _imp\n    ext_suffix = _imp.extension_suffixes()[0]\n    vars.update(EXT_SUFFIX=ext_suffix, SO=ext_suffix)",
    "label": true
  },
  {
    "code": "def all_unique(iterable, key=None):\n    seenset = set()\n    seenset_add = seenset.add\n    seenlist = []\n    seenlist_add = seenlist.append\n    for element in map(key, iterable) if key else iterable:\n        try:\n            if element in seenset:\n                return False\n            seenset_add(element)\n        except TypeError:\n            if element in seenlist:\n                return False\n            seenlist_add(element)\n    return True",
    "label": true
  },
  {
    "code": "def is_hashable(node: nodes.NodeNG) -> bool:\n    try:\n        for inferred in node.infer():\n            if isinstance(inferred, (nodes.ClassDef, util.UninferableBase)):\n                return True\n            if not hasattr(inferred, 'igetattr'):\n                return True\n            hash_fn = next(inferred.igetattr('__hash__'))\n            if hash_fn.parent is inferred:\n                return True\n            if getattr(hash_fn, 'value', True) is not None:\n                return True\n        return False\n    except astroid.InferenceError:\n        return True",
    "label": true
  },
  {
    "code": "def get_extended_length_path_str(path: str) -> str:\n    long_path_prefix = '\\\\\\\\?\\\\'\n    unc_long_path_prefix = '\\\\\\\\?\\\\UNC\\\\'\n    if path.startswith((long_path_prefix, unc_long_path_prefix)):\n        return path\n    if path.startswith('\\\\\\\\'):\n        return unc_long_path_prefix + path[2:]\n    return long_path_prefix + path",
    "label": true
  },
  {
    "code": "def transform_pyqt_signal(node: nodes.FunctionDef) -> None:\n    module = parse('\\n    _UNSET = object()\\n\\n    class pyqtSignal(object):\\n        def connect(self, slot, type=None, no_receiver_check=False):\\n            pass\\n        def disconnect(self, slot=_UNSET):\\n            pass\\n        def emit(self, *args):\\n            pass\\n    ')\n    signal_cls: nodes.ClassDef = module['pyqtSignal']\n    node.instance_attrs['emit'] = [signal_cls['emit']]\n    node.instance_attrs['disconnect'] = [signal_cls['disconnect']]\n    node.instance_attrs['connect'] = [signal_cls['connect']]",
    "label": true
  },
  {
    "code": "def resolve_path(module, name):\n    if isinstance(module, string_types):\n        __import__(module)\n        module = sys.modules[module]\n    parent = module\n    path = name.split('.')\n    attribute = path[0]\n\n    def lookup_attribute(parent, attribute):\n        if inspect.isclass(parent):\n            for cls in inspect.getmro(parent):\n                if attribute in vars(cls):\n                    return vars(cls)[attribute]\n            else:\n                return getattr(parent, attribute)\n        else:\n            return getattr(parent, attribute)\n    original = lookup_attribute(parent, attribute)\n    for attribute in path[1:]:\n        parent = original\n        original = lookup_attribute(parent, attribute)\n    return (parent, attribute, original)",
    "label": true
  },
  {
    "code": "def is_registered_in_singledispatchmethod_function(node: nodes.FunctionDef) -> bool:\n    singledispatchmethod_qnames = ('functools.singledispatchmethod', 'singledispatch.singledispatchmethod')\n    decorators = node.decorators.nodes if node.decorators else []\n    for decorator in decorators:\n        func_def = find_inferred_fn_from_register(decorator)\n        if func_def:\n            return decorated_with(func_def, singledispatchmethod_qnames)\n    return False",
    "label": true
  },
  {
    "code": "def get_path_uid(path: str) -> int:\n    if hasattr(os, 'O_NOFOLLOW'):\n        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)\n        file_uid = os.fstat(fd).st_uid\n        os.close(fd)\n    elif not os.path.islink(path):\n        file_uid = os.stat(path).st_uid\n    else:\n        raise OSError(f'{path} is a symlink; Will not return uid for symlinks')\n    return file_uid",
    "label": true
  },
  {
    "code": "def _combining_class(cp: int) -> int:\n    v = unicodedata.combining(chr(cp))\n    if v == 0:\n        if not unicodedata.name(chr(cp)):\n            raise ValueError('Unknown character in unicodedata')\n    return v",
    "label": true
  },
  {
    "code": "def _load_items_from_file(keychain: SecKeychainRef, path: str) -> tuple[list[CFTypeRef], list[CFTypeRef]]:\n    certificates = []\n    identities = []\n    result_array = None\n    with open(path, 'rb') as f:\n        raw_filedata = f.read()\n    try:\n        filedata = CoreFoundation.CFDataCreate(CoreFoundation.kCFAllocatorDefault, raw_filedata, len(raw_filedata))\n        result_array = CoreFoundation.CFArrayRef()\n        result = Security.SecItemImport(filedata, None, None, None, 0, None, keychain, ctypes.byref(result_array))\n        _assert_no_error(result)\n        result_count = CoreFoundation.CFArrayGetCount(result_array)\n        for index in range(result_count):\n            item = CoreFoundation.CFArrayGetValueAtIndex(result_array, index)\n            item = ctypes.cast(item, CoreFoundation.CFTypeRef)\n            if _is_cert(item):\n                CoreFoundation.CFRetain(item)\n                certificates.append(item)\n            elif _is_identity(item):\n                CoreFoundation.CFRetain(item)\n                identities.append(item)\n    finally:\n        if result_array:\n            CoreFoundation.CFRelease(result_array)\n        CoreFoundation.CFRelease(filedata)\n    return (identities, certificates)",
    "label": true
  },
  {
    "code": "def _get_text_stdout(buffer_stream: t.BinaryIO) -> t.TextIO:\n    text_stream = _NonClosingTextIOWrapper(io.BufferedWriter(_WindowsConsoleWriter(STDOUT_HANDLE)), 'utf-16-le', 'strict', line_buffering=True)\n    return t.cast(t.TextIO, ConsoleStream(text_stream, buffer_stream))",
    "label": true
  },
  {
    "code": "def merge_family(left, right) -> None:\n    result = {}\n    for kl, vl in left.items():\n        for kr, vr in right.items():\n            if not isinstance(vl, list):\n                raise TypeError(type(vl))\n            result[kl] = vl + vr\n    left.update(result)",
    "label": true
  },
  {
    "code": "def _find_parametrized_scope(argnames: Sequence[str], arg2fixturedefs: Mapping[str, Sequence[fixtures.FixtureDef[object]]], indirect: Union[bool, Sequence[str]]) -> Scope:\n    if isinstance(indirect, Sequence):\n        all_arguments_are_fixtures = len(indirect) == len(argnames)\n    else:\n        all_arguments_are_fixtures = bool(indirect)\n    if all_arguments_are_fixtures:\n        fixturedefs = arg2fixturedefs or {}\n        used_scopes = [fixturedef[0]._scope for name, fixturedef in fixturedefs.items() if name in argnames]\n        return min(used_scopes, default=Scope.Function)\n    return Scope.Function",
    "label": true
  },
  {
    "code": "def _is_inside_context_manager(node: nodes.Call) -> bool:\n    frame = node.frame(future=True)\n    if not isinstance(frame, (nodes.FunctionDef, astroid.BoundMethod, astroid.UnboundMethod)):\n        return False\n    return frame.name == '__enter__' or utils.decorated_with(frame, 'contextlib.contextmanager')",
    "label": true
  },
  {
    "code": "def is_python_source(filename: str | None) -> bool:\n    if not filename:\n        return False\n    return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS",
    "label": true
  },
  {
    "code": "def parse_credentials(netloc):\n    username = password = None\n    if '@' in netloc:\n        prefix, netloc = netloc.rsplit('@', 1)\n        if ':' not in prefix:\n            username = prefix\n        else:\n            username, password = prefix.split(':', 1)\n    if username:\n        username = unquote(username)\n    if password:\n        password = unquote(password)\n    return (username, password, netloc)",
    "label": true
  },
  {
    "code": "def _find_packages(dist: Distribution) -> Iterator[str]:\n    yield from iter(dist.packages or [])\n    py_modules = dist.py_modules or []\n    nested_modules = [mod for mod in py_modules if '.' in mod]\n    if dist.ext_package:\n        yield dist.ext_package\n    else:\n        ext_modules = dist.ext_modules or []\n        nested_modules += [x.name for x in ext_modules if '.' in x.name]\n    for module in nested_modules:\n        package, _, _ = module.rpartition('.')\n        yield package",
    "label": true
  },
  {
    "code": "def _get_ansi_code(msg_style: MessageStyle) -> str:\n    ansi_code = [ANSI_STYLES[effect] for effect in msg_style.style]\n    if msg_style.color:\n        if msg_style.color.isdigit():\n            ansi_code.extend(['38', '5'])\n            ansi_code.append(msg_style.color)\n        else:\n            ansi_code.append(ANSI_COLORS[msg_style.color])\n    if ansi_code:\n        return ANSI_PREFIX + ';'.join(ansi_code) + ANSI_END\n    return ''",
    "label": true
  },
  {
    "code": "def table_lines_from_stats(stats: LinterStats, old_stats: LinterStats | None, stat_type: Literal['duplicated_lines', 'message_types']) -> list[str]:\n    lines: list[str] = []\n    if stat_type == 'duplicated_lines':\n        new: list[tuple[str, int | float]] = [('nb_duplicated_lines', stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', stats.duplicated_lines['percent_duplicated_lines'])]\n        if old_stats:\n            old: list[tuple[str, str | int | float]] = [('nb_duplicated_lines', old_stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', old_stats.duplicated_lines['percent_duplicated_lines'])]\n        else:\n            old = [('nb_duplicated_lines', 'NC'), ('percent_duplicated_lines', 'NC')]\n    elif stat_type == 'message_types':\n        new = [('convention', stats.convention), ('refactor', stats.refactor), ('warning', stats.warning), ('error', stats.error)]\n        if old_stats:\n            old = [('convention', old_stats.convention), ('refactor', old_stats.refactor), ('warning', old_stats.warning), ('error', old_stats.error)]\n        else:\n            old = [('convention', 'NC'), ('refactor', 'NC'), ('warning', 'NC'), ('error', 'NC')]\n    for index, value in enumerate(new):\n        new_value = value[1]\n        old_value = old[index][1]\n        diff_str = diff_string(old_value, new_value) if isinstance(old_value, float) else old_value\n        new_str = f'{new_value:.3f}' if isinstance(new_value, float) else str(new_value)\n        old_str = f'{old_value:.3f}' if isinstance(old_value, float) else str(old_value)\n        lines.extend((value[0].replace('_', ' '), new_str, old_str, diff_str))\n    return lines",
    "label": true
  },
  {
    "code": "def ignore_case(value: V) -> V:\n    if isinstance(value, str):\n        return t.cast(V, value.lower())\n    return value",
    "label": true
  },
  {
    "code": "def parse_netloc(netloc: str) -> Tuple[Optional[str], Optional[int]]:\n    url = build_url_from_netloc(netloc)\n    parsed = urllib.parse.urlparse(url)\n    return (parsed.hostname, parsed.port)",
    "label": true
  },
  {
    "code": "def _get_text_stderr(buffer_stream: t.BinaryIO) -> t.TextIO:\n    text_stream = _NonClosingTextIOWrapper(io.BufferedWriter(_WindowsConsoleWriter(STDERR_HANDLE)), 'utf-16-le', 'strict', line_buffering=True)\n    return t.cast(t.TextIO, ConsoleStream(text_stream, buffer_stream))",
    "label": true
  },
  {
    "code": "def _parse(markup: str) -> Iterable[Tuple[int, Optional[str], Optional[Tag]]]:\n    position = 0\n    _divmod = divmod\n    _Tag = Tag\n    for match in RE_TAGS.finditer(markup):\n        full_text, escapes, tag_text = match.groups()\n        start, end = match.span()\n        if start > position:\n            yield (start, markup[position:start], None)\n        if escapes:\n            backslashes, escaped = _divmod(len(escapes), 2)\n            if backslashes:\n                yield (start, '\\\\' * backslashes, None)\n                start += backslashes * 2\n            if escaped:\n                yield (start, full_text[len(escapes):], None)\n                position = end\n                continue\n        text, equals, parameters = tag_text.partition('=')\n        yield (start, None, _Tag(text, parameters if equals else None))\n        position = end\n    if position < len(markup):\n        yield (position, markup[position:], None)",
    "label": true
  },
  {
    "code": "def construct_package_dir(packages: List[str], package_path: _Path) -> Dict[str, str]:\n    parent_pkgs = remove_nested_packages(packages)\n    prefix = Path(package_path).parts\n    return {pkg: '/'.join([*prefix, *pkg.split('.')]) for pkg in parent_pkgs}",
    "label": true
  },
  {
    "code": "def _environment_config_check(environment: 'Environment') -> 'Environment':\n    assert issubclass(environment.undefined, Undefined), \"'undefined' must be a subclass of 'jinja2.Undefined'.\"\n    assert environment.block_start_string != environment.variable_start_string != environment.comment_start_string, 'block, variable and comment start strings must be different.'\n    assert environment.newline_sequence in {'\\r', '\\r\\n', '\\n'}, \"'newline_sequence' must be one of '\\\\n', '\\\\r\\\\n', or '\\\\r'.\"\n    return environment",
    "label": true
  },
  {
    "code": "def run_commands(dist):\n    try:\n        dist.run_commands()\n    except KeyboardInterrupt:\n        raise SystemExit('interrupted')\n    except OSError as exc:\n        if DEBUG:\n            sys.stderr.write('error: {}\\n'.format(exc))\n            raise\n        else:\n            raise SystemExit('error: {}'.format(exc))\n    except (DistutilsError, CCompilerError) as msg:\n        if DEBUG:\n            raise\n        else:\n            raise SystemExit('error: ' + str(msg))\n    return dist",
    "label": true
  },
  {
    "code": "def show_tags(options: Values) -> None:\n    tag_limit = 10\n    target_python = make_target_python(options)\n    tags = target_python.get_tags()\n    formatted_target = target_python.format_given()\n    suffix = ''\n    if formatted_target:\n        suffix = f' (target: {formatted_target})'\n    msg = 'Compatible tags: {}{}'.format(len(tags), suffix)\n    logger.info(msg)\n    if options.verbose < 1 and len(tags) > tag_limit:\n        tags_limited = True\n        tags = tags[:tag_limit]\n    else:\n        tags_limited = False\n    with indent_log():\n        for tag in tags:\n            logger.info(str(tag))\n        if tags_limited:\n            msg = '...\\n[First {tag_limit} tags shown. Pass --verbose to show all.]'.format(tag_limit=tag_limit)\n            logger.info(msg)",
    "label": true
  },
  {
    "code": "def _handle_merge_hash(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    if not parser.values.hashes:\n        parser.values.hashes = {}\n    try:\n        algo, digest = value.split(':', 1)\n    except ValueError:\n        parser.error('Arguments to {} must be a hash name followed by a value, like --hash=sha256:abcde...'.format(opt_str))\n    if algo not in STRONG_HASHES:\n        parser.error('Allowed hash algorithms for {} are {}.'.format(opt_str, ', '.join(STRONG_HASHES)))\n    parser.values.hashes.setdefault(algo, []).append(digest)",
    "label": true
  },
  {
    "code": "def report_raw_stats(sect: Section, stats: LinterStats, old_stats: LinterStats | None) -> None:\n    total_lines = stats.code_type_count['total']\n    sect.insert(0, Paragraph([Text(f'{total_lines} lines have been analyzed\\n')]))\n    lines = ['type', 'number', '%', 'previous', 'difference']\n    for node_type in ('code', 'docstring', 'comment', 'empty'):\n        node_type = cast(Literal['code', 'docstring', 'comment', 'empty'], node_type)\n        total = stats.code_type_count[node_type]\n        percent = float(total * 100) / total_lines if total_lines else None\n        old = old_stats.code_type_count[node_type] if old_stats else None\n        diff_str = diff_string(old, total) if old else None\n        lines += [node_type, str(total), f'{percent:.2f}' if percent is not None else 'NC', str(old) if old else 'NC', diff_str if diff_str else 'NC']\n    sect.append(Table(children=lines, cols=5, rheaders=1))",
    "label": true
  },
  {
    "code": "def create_command(name: str, **kwargs: Any) -> Command:\n    module_path, class_name, summary = commands_dict[name]\n    module = importlib.import_module(module_path)\n    command_class = getattr(module, class_name)\n    command = command_class(name=name, summary=summary, **kwargs)\n    return command",
    "label": true
  },
  {
    "code": "def get_log_level_for_setting(config: Config, *setting_names: str) -> Optional[int]:\n    for setting_name in setting_names:\n        log_level = config.getoption(setting_name)\n        if log_level is None:\n            log_level = config.getini(setting_name)\n        if log_level:\n            break\n    else:\n        return None\n    if isinstance(log_level, str):\n        log_level = log_level.upper()\n    try:\n        return int(getattr(logging, log_level, log_level))\n    except ValueError as e:\n        raise UsageError(\"'{}' is not recognized as a logging level name for '{}'. Please consider passing the logging level num instead.\".format(log_level, setting_name)) from e",
    "label": true
  },
  {
    "code": "def do_items(value: t.Union[t.Mapping[K, V], Undefined]) -> t.Iterator[t.Tuple[K, V]]:\n    if isinstance(value, Undefined):\n        return\n    if not isinstance(value, abc.Mapping):\n        raise TypeError('Can only get item pairs from a mapping.')\n    yield from value.items()",
    "label": true
  },
  {
    "code": "def ensure_no_io(modulename: str) -> None:\n    test_module = sys.modules[modulename]\n    setattr(test_module, 'input', _mock_disallow('input'))\n    setattr(test_module, 'print', _mock_disallow('print'))",
    "label": true
  },
  {
    "code": "def _flat_list(nested_list):\n    ret = []\n    for item in nested_list:\n        if isinstance(item, list):\n            for subitem in item:\n                ret.append(subitem)\n        else:\n            ret.append(item)\n    return ret",
    "label": true
  },
  {
    "code": "def is_attribute_typed_annotation(node: nodes.ClassDef | astroid.Instance, attr_name: str) -> bool:\n    attribute = node.locals.get(attr_name, [None])[0]\n    if attribute and isinstance(attribute, nodes.AssignName) and isinstance(attribute.parent, nodes.AnnAssign):\n        return True\n    for base in node.bases:\n        inferred = safe_infer(base)\n        if inferred and isinstance(inferred, nodes.ClassDef) and is_attribute_typed_annotation(inferred, attr_name):\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def decode(s: Union[str, bytes, bytearray], strict: bool=False, uts46: bool=False, std3_rules: bool=False) -> str:\n    try:\n        if isinstance(s, (bytes, bytearray)):\n            s = s.decode('ascii')\n    except UnicodeDecodeError:\n        raise IDNAError('Invalid ASCII in A-label')\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split('.')\n    if not labels or labels == ['']:\n        raise IDNAError('Empty domain')\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError('Empty label')\n    if trailing_dot:\n        result.append('')\n    return '.'.join(result)",
    "label": true
  },
  {
    "code": "def apply(dist: 'Distribution', config: dict, filename: _Path) -> 'Distribution':\n    if not config:\n        return dist\n    root_dir = os.path.dirname(filename) or '.'\n    _apply_project_table(dist, config, root_dir)\n    _apply_tool_table(dist, config, filename)\n    current_directory = os.getcwd()\n    os.chdir(root_dir)\n    try:\n        dist._finalize_requires()\n        dist._finalize_license_files()\n    finally:\n        os.chdir(current_directory)\n    return dist",
    "label": true
  },
  {
    "code": "def bygroups(*args):\n\n    def callback(lexer, match, ctx=None):\n        for i, action in enumerate(args):\n            if action is None:\n                continue\n            elif type(action) is _TokenType:\n                data = match.group(i + 1)\n                if data:\n                    yield (match.start(i + 1), action, data)\n            else:\n                data = match.group(i + 1)\n                if data is not None:\n                    if ctx:\n                        ctx.pos = match.start(i + 1)\n                    for item in action(lexer, _PseudoMatch(match.start(i + 1), data), ctx):\n                        if item:\n                            yield item\n        if ctx:\n            ctx.pos = match.end()\n    return callback",
    "label": true
  },
  {
    "code": "def _remove_path_dot_segments(path):\n    segments = path.split('/')\n    output = []\n    for segment in segments:\n        if segment == '.':\n            continue\n        elif segment != '..':\n            output.append(segment)\n        elif output:\n            output.pop()\n    if path.startswith('/') and (not output or output[0]):\n        output.insert(0, '')\n    if path.endswith(('/.', '/..')):\n        output.append('')\n    return '/'.join(output)",
    "label": true
  },
  {
    "code": "def test_decode():\n    assert decode(b'\\x80', 'latin1') == ('\u20ac', lookup('latin1'))\n    assert decode(b'\\x80', lookup('latin1')) == ('\u20ac', lookup('latin1'))\n    assert decode(b'\\xc3\\xa9', 'utf8') == ('\u00e9', lookup('utf8'))\n    assert decode(b'\\xc3\\xa9', UTF8) == ('\u00e9', lookup('utf8'))\n    assert decode(b'\\xc3\\xa9', 'ascii') == ('\u00c3\u00a9', lookup('ascii'))\n    assert decode(b'\\xef\\xbb\\xbf\\xc3\\xa9', 'ascii') == ('\u00e9', lookup('utf8'))\n    assert decode(b'\\xfe\\xff\\x00\\xe9', 'ascii') == ('\u00e9', lookup('utf-16be'))\n    assert decode(b'\\xff\\xfe\\xe9\\x00', 'ascii') == ('\u00e9', lookup('utf-16le'))\n    assert decode(b'\\xfe\\xff\\xe9\\x00', 'ascii') == ('\\ue900', lookup('utf-16be'))\n    assert decode(b'\\xff\\xfe\\x00\\xe9', 'ascii') == ('\\ue900', lookup('utf-16le'))\n    assert decode(b'\\x00\\xe9', 'UTF-16BE') == ('\u00e9', lookup('utf-16be'))\n    assert decode(b'\\xe9\\x00', 'UTF-16LE') == ('\u00e9', lookup('utf-16le'))\n    assert decode(b'\\xe9\\x00', 'UTF-16') == ('\u00e9', lookup('utf-16le'))\n    assert decode(b'\\xe9\\x00', 'UTF-16BE') == ('\\ue900', lookup('utf-16be'))\n    assert decode(b'\\x00\\xe9', 'UTF-16LE') == ('\\ue900', lookup('utf-16le'))\n    assert decode(b'\\x00\\xe9', 'UTF-16') == ('\\ue900', lookup('utf-16le'))",
    "label": true
  },
  {
    "code": "def skip_until(src: str, pos: Pos, expect: str, *, error_on: FrozenSet[str], error_on_eof: bool) -> Pos:\n    try:\n        new_pos = src.index(expect, pos)\n    except ValueError:\n        new_pos = len(src)\n        if error_on_eof:\n            raise suffixed_err(src, new_pos, f'Expected \"{expect!r}\"')\n    if not error_on.isdisjoint(src[pos:new_pos]):\n        while src[pos] not in error_on:\n            pos += 1\n        raise suffixed_err(src, pos, f'Found invalid character \"{src[pos]!r}\"')\n    return new_pos",
    "label": true
  },
  {
    "code": "def _check_generate_dataclass_init(node: nodes.ClassDef) -> bool:\n    if '__init__' in node.locals:\n        return False\n    found = None\n    for decorator_attribute in node.decorators.nodes:\n        if not isinstance(decorator_attribute, nodes.Call):\n            continue\n        if _looks_like_dataclass_decorator(decorator_attribute):\n            found = decorator_attribute\n    if found is None:\n        return True\n    return not any((keyword.arg == 'init' and (not keyword.value.bool_value()) for keyword in found.keywords))",
    "label": true
  },
  {
    "code": "def update_counts(s, counts):\n    for char in s:\n        if char in counts:\n            counts[char] += 1",
    "label": true
  },
  {
    "code": "def is_valid_flyer(ticket: str) -> bool:\n    if len(ticket) == 17:\n        return True\n    elif len(ticket) == 21:\n        calculation = int(ticket[FLYER]) + int(ticket[FLYER + 1])\n        calculation2 = calculation + int(ticket[FLYER + 2])\n        return calculation2 % 10 == int(ticket[FLYER + 3])\n    else:\n        return False",
    "label": true
  },
  {
    "code": "def circular_shifts(iterable):\n    lst = list(iterable)\n    return take(len(lst), windowed(cycle(lst), len(lst)))",
    "label": true
  },
  {
    "code": "def pip_self_version_check(session: PipSession, options: optparse.Values) -> None:\n    installed_dist = get_default_environment().get_distribution('pip')\n    if not installed_dist:\n        return\n    try:\n        upgrade_prompt = _self_version_check_logic(state=SelfCheckState(cache_dir=options.cache_dir), current_time=datetime.datetime.utcnow(), local_version=installed_dist.version, get_remote_version=functools.partial(_get_current_remote_pip_version, session, options))\n        if upgrade_prompt is not None:\n            logger.warning('[present-rich] %s', upgrade_prompt)\n    except Exception:\n        logger.warning('There was an error checking the latest version of pip.')\n        logger.debug('See below for error', exc_info=True)",
    "label": true
  },
  {
    "code": "def get_file_paths(rel_path: AnyStr) -> Generator[AnyStr, None, None]:\n    if not os.path.isdir(rel_path):\n        yield rel_path\n    else:\n        for root, _, files in os.walk(rel_path):\n            for filename in (f for f in files if f.endswith('.py')):\n                yield os.path.join(root, filename)",
    "label": true
  },
  {
    "code": "def _infer_copy_method(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[nodes.NodeNG]:\n    assert isinstance(node.func, nodes.Attribute)\n    inferred_orig, inferred_copy = itertools.tee(node.func.expr.infer(context=context))\n    if all((isinstance(inferred_node, (nodes.Dict, nodes.List, nodes.Set, objects.FrozenSet)) for inferred_node in inferred_orig)):\n        return inferred_copy\n    raise UseInferenceDefault()",
    "label": true
  },
  {
    "code": "def uts46_remap(domain: str, std3_rules: bool=True, transitional: bool=False) -> str:\n    from .uts46data import uts46data\n    output = ''\n    for pos, char in enumerate(domain):\n        code_point = ord(char)\n        try:\n            uts46row = uts46data[code_point if code_point < 256 else bisect.bisect_left(uts46data, (code_point, 'Z')) - 1]\n            status = uts46row[1]\n            replacement = None\n            if len(uts46row) == 3:\n                replacement = uts46row[2]\n            if status == 'V' or (status == 'D' and (not transitional)) or (status == '3' and (not std3_rules) and (replacement is None)):\n                output += char\n            elif replacement is not None and (status == 'M' or (status == '3' and (not std3_rules)) or (status == 'D' and transitional)):\n                output += replacement\n            elif status != 'I':\n                raise IndexError()\n        except IndexError:\n            raise InvalidCodepoint('Codepoint {} not allowed at position {} in {}'.format(_unot(code_point), pos + 1, repr(domain)))\n    return unicodedata.normalize('NFC', output)",
    "label": true
  },
  {
    "code": "def _load_messages_config(path: str, default_path: str) -> dict:\n    merge_into = toml.load(default_path)\n    if Path(default_path).resolve() == Path(path).resolve():\n        return merge_into\n    try:\n        merge_from = toml.load(path)\n    except FileNotFoundError:\n        print(f'[WARNING] Could not find messages config file at {str(Path(path).resolve())}. Using default messages config file at {str(Path(default_path).resolve())}.')\n        return merge_into\n    for category in merge_from:\n        if category not in merge_into:\n            merge_into[category] = {}\n        for checker in merge_from[category]:\n            if checker not in merge_into[category]:\n                merge_into[category][checker] = {}\n            for error_code in merge_from[category][checker]:\n                merge_into[category][checker][error_code] = merge_from[category][checker][error_code]\n    return merge_into",
    "label": true
  },
  {
    "code": "def _lookup_style(style):\n    if isinstance(style, str):\n        return get_style_by_name(style)\n    return style",
    "label": true
  },
  {
    "code": "def foo(x):\n\n    def bar(y):\n        return squared(x) + y\n    return bar",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def entry_points(text: str, text_source='entry-points') -> Dict[str, dict]:\n    parser = ConfigParser(default_section=None, delimiters=('=',))\n    parser.optionxform = str\n    parser.read_string(text, text_source)\n    groups = {k: dict(v.items()) for k, v in parser.items()}\n    groups.pop(parser.default_section, None)\n    return groups",
    "label": true
  },
  {
    "code": "def is_object_one_of_types(obj: object, fully_qualified_types_names: Collection[str]) -> bool:\n    for type_name in get_object_types_mro_as_strings(obj):\n        if type_name in fully_qualified_types_names:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _idna_encode(name):\n    if name and any((ord(x) >= 128 for x in name)):\n        try:\n            from pip._vendor import idna\n        except ImportError:\n            six.raise_from(LocationParseError(\"Unable to parse URL without the 'idna' module\"), None)\n        try:\n            return idna.encode(name.lower(), strict=True, std3_rules=True)\n        except idna.IDNAError:\n            six.raise_from(LocationParseError(u\"Name '%s' is not a valid IDNA label\" % name), None)\n    return name.lower().encode('ascii')",
    "label": true
  },
  {
    "code": "def token_map(func, *args) -> ParseAction:\n\n    def pa(s, l, t):\n        return [func(tokn, *args) for tokn in t]\n    func_name = getattr(func, '__name__', getattr(func, '__class__').__name__)\n    pa.__name__ = func_name\n    return pa",
    "label": true
  },
  {
    "code": "def requires_to_requires_dist(requirement: Requirement) -> str:\n    if getattr(requirement, 'url', None):\n        return ' @ ' + requirement.url\n    requires_dist = []\n    for spec in requirement.specifier:\n        requires_dist.append(spec.operator + spec.version)\n    if requires_dist:\n        return ' ' + ','.join(sorted(requires_dist))\n    else:\n        return ''",
    "label": true
  },
  {
    "code": "def find_filter_class(filtername):\n    if filtername in FILTERS:\n        return FILTERS[filtername]\n    for name, cls in find_plugin_filters():\n        if name == filtername:\n            return cls\n    return None",
    "label": true
  },
  {
    "code": "def measure_table(rows: t.Iterable[t.Tuple[str, str]]) -> t.Tuple[int, ...]:\n    widths: t.Dict[int, int] = {}\n    for row in rows:\n        for idx, col in enumerate(row):\n            widths[idx] = max(widths.get(idx, 0), term_len(col))\n    return tuple((y for x, y in sorted(widths.items())))",
    "label": true
  },
  {
    "code": "def load_session(filename=None, main=None, **kwds):\n    warnings.warn('load_session() has been renamed load_module().', PendingDeprecationWarning)\n    load_module(filename, module=main, **kwds)",
    "label": true
  },
  {
    "code": "def wheel_version(wheel_data: Message) -> Tuple[int, ...]:\n    version_text = wheel_data['Wheel-Version']\n    if version_text is None:\n        raise UnsupportedWheel('WHEEL is missing Wheel-Version')\n    version = version_text.strip()\n    try:\n        return tuple(map(int, version.split('.')))\n    except ValueError:\n        raise UnsupportedWheel(f'invalid Wheel-Version: {version!r}')",
    "label": true
  },
  {
    "code": "def _setitems(dest, source):\n    for k, v in source.items():\n        dest[k] = v",
    "label": true
  },
  {
    "code": "def check(obj, *args, **kwds):\n    verbose = kwds.pop('verbose', False)\n    python = kwds.pop('python', None)\n    if python is None:\n        import sys\n        python = sys.executable\n    isinstance(python, str)\n    import subprocess\n    fail = True\n    try:\n        _obj = dumps(obj, *args, **kwds)\n        fail = False\n    finally:\n        if fail and verbose:\n            print('DUMP FAILED')\n    msg = '%s -c import dill; print(dill.loads(%s))' % (python, repr(_obj))\n    msg = 'SUCCESS' if not subprocess.call(msg.split(None, 2)) else 'LOAD FAILED'\n    if verbose:\n        print(msg)\n    return",
    "label": true
  },
  {
    "code": "def nth_product(index, *args):\n    pools = list(map(tuple, reversed(args)))\n    ns = list(map(len, pools))\n    c = reduce(mul, ns)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    result = []\n    for pool, n in zip(pools, ns):\n        result.append(pool[index % n])\n        index //= n\n    return tuple(reversed(result))",
    "label": true
  },
  {
    "code": "def get_build_version():\n    prefix = 'MSC v.'\n    i = sys.version.find(prefix)\n    if i == -1:\n        return 6\n    i = i + len(prefix)\n    s, rest = sys.version[i:].split(' ', 1)\n    majorVersion = int(s[:-2]) - 6\n    if majorVersion >= 13:\n        majorVersion += 1\n    minorVersion = int(s[2:3]) / 10.0\n    if majorVersion == 6:\n        minorVersion = 0\n    if majorVersion >= 6:\n        return majorVersion + minorVersion\n    return None",
    "label": true
  },
  {
    "code": "def read_keys(base, key):\n    try:\n        handle = RegOpenKeyEx(base, key)\n    except RegError:\n        return None\n    L = []\n    i = 0\n    while True:\n        try:\n            k = RegEnumKey(handle, i)\n        except RegError:\n            break\n        L.append(k)\n        i += 1\n    return L",
    "label": true
  },
  {
    "code": "def find_formatter_class(alias):\n    for module_name, name, aliases, _, _ in FORMATTERS.values():\n        if alias in aliases:\n            if name not in _formatter_cache:\n                _load_formatters(module_name)\n            return _formatter_cache[name]\n    for _, cls in find_plugin_formatters():\n        if alias in cls.aliases:\n            return cls",
    "label": true
  },
  {
    "code": "def _is_cert(item):\n    expected = Security.SecCertificateGetTypeID()\n    return CoreFoundation.CFGetTypeID(item) == expected",
    "label": true
  },
  {
    "code": "def _init():\n    for code, titles in _codes.items():\n        for title in titles:\n            setattr(codes, title, code)\n            if not title.startswith(('\\\\', '/')):\n                setattr(codes, title.upper(), code)\n\n    def doc(code):\n        names = ', '.join((f'``{n}``' for n in _codes[code]))\n        return '* %d: %s' % (code, names)\n    global __doc__\n    __doc__ = __doc__ + '\\n' + '\\n'.join((doc(code) for code in sorted(_codes))) if __doc__ is not None else None",
    "label": true
  },
  {
    "code": "def finder(package):\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result",
    "label": true
  },
  {
    "code": "def ircformat(color, text):\n    if len(color) < 1:\n        return text\n    add = sub = ''\n    if '_' in color:\n        add += '\\x1d'\n        sub = '\\x1d' + sub\n        color = color.strip('_')\n    if '*' in color:\n        add += '\\x02'\n        sub = '\\x02' + sub\n        color = color.strip('*')\n    if len(color) > 0:\n        add += '\\x03' + str(IRC_COLOR_MAP[color]).zfill(2)\n        sub = '\\x03' + sub\n    return add + text + sub\n    return '<' + add + '>' + text + '</' + sub + '>'",
    "label": true
  },
  {
    "code": "def interpreter_name() -> str:\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name",
    "label": true
  },
  {
    "code": "def _read(filename):\n    if (2, 5) < sys.version_info < (3, 0):\n        with open(filename, 'rU') as f:\n            return f.read()\n    elif (3, 0) <= sys.version_info < (4, 0):\n        'Read the source code.'\n        try:\n            with open(filename, 'rb') as f:\n                encoding, _ = tokenize.detect_encoding(f.readline)\n        except (LookupError, SyntaxError, UnicodeError):\n            with open(filename, encoding='latin-1') as f:\n                return f.read()\n        with open(filename, 'r', encoding=encoding) as f:\n            return f.read()",
    "label": true
  },
  {
    "code": "def _find_vc2015():\n    try:\n        key = winreg.OpenKeyEx(winreg.HKEY_LOCAL_MACHINE, 'Software\\\\Microsoft\\\\VisualStudio\\\\SxS\\\\VC7', access=winreg.KEY_READ | winreg.KEY_WOW64_32KEY)\n    except OSError:\n        log.debug('Visual C++ is not registered')\n        return (None, None)\n    best_version = 0\n    best_dir = None\n    with key:\n        for i in count():\n            try:\n                v, vc_dir, vt = winreg.EnumValue(key, i)\n            except OSError:\n                break\n            if v and vt == winreg.REG_SZ and os.path.isdir(vc_dir):\n                try:\n                    version = int(float(v))\n                except (ValueError, TypeError):\n                    continue\n                if version >= 14 and version > best_version:\n                    best_version, best_dir = (version, vc_dir)\n    return (best_version, best_dir)",
    "label": true
  },
  {
    "code": "def all_equal(iterable):\n    g = groupby(iterable)\n    return next(g, True) and (not next(g, False))",
    "label": true
  },
  {
    "code": "def parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, list]:\n    pos += 1\n    array: list = []\n    pos = skip_comments_and_array_ws(src, pos)\n    if src.startswith(']', pos):\n        return (pos + 1, array)\n    while True:\n        pos, val = parse_value(src, pos, parse_float)\n        array.append(val)\n        pos = skip_comments_and_array_ws(src, pos)\n        c = src[pos:pos + 1]\n        if c == ']':\n            return (pos + 1, array)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed array')\n        pos += 1\n        pos = skip_comments_and_array_ws(src, pos)\n        if src.startswith(']', pos):\n            return (pos + 1, array)",
    "label": true
  },
  {
    "code": "def reorder_items(items: Sequence[nodes.Item]) -> List[nodes.Item]:\n    argkeys_cache: Dict[Scope, Dict[nodes.Item, Dict[_Key, None]]] = {}\n    items_by_argkey: Dict[Scope, Dict[_Key, Deque[nodes.Item]]] = {}\n    for scope in HIGH_SCOPES:\n        d: Dict[nodes.Item, Dict[_Key, None]] = {}\n        argkeys_cache[scope] = d\n        item_d: Dict[_Key, Deque[nodes.Item]] = defaultdict(deque)\n        items_by_argkey[scope] = item_d\n        for item in items:\n            keys = dict.fromkeys(get_parametrized_fixture_keys(item, scope), None)\n            if keys:\n                d[item] = keys\n                for key in keys:\n                    item_d[key].append(item)\n    items_dict = dict.fromkeys(items, None)\n    return list(reorder_items_atscope(items_dict, argkeys_cache, items_by_argkey, Scope.Session))",
    "label": true
  },
  {
    "code": "def get_with_lines(lines: list[str], num_whitespace: int) -> str:\n    endpoint = len(lines)\n    for i in range(len(lines)):\n        if lines[i].strip() != '' and (not lines[i][num_whitespace].isspace()):\n            endpoint = i\n            break\n    return '\\n'.join(lines[:endpoint])",
    "label": true
  },
  {
    "code": "def with_cleanup(func: Any) -> Any:\n\n    def configure_tempdir_registry(registry: TempDirectoryTypeRegistry) -> None:\n        for t in KEEPABLE_TEMPDIR_TYPES:\n            registry.set_delete(t, False)\n\n    def wrapper(self: RequirementCommand, options: Values, args: List[Any]) -> Optional[int]:\n        assert self.tempdir_registry is not None\n        if options.no_clean:\n            configure_tempdir_registry(self.tempdir_registry)\n        try:\n            return func(self, options, args)\n        except PreviousBuildDirError:\n            configure_tempdir_registry(self.tempdir_registry)\n            raise\n    return wrapper",
    "label": true
  },
  {
    "code": "def iter_fields(fields):\n    if isinstance(fields, dict):\n        return ((k, v) for k, v in six.iteritems(fields))\n    return ((k, v) for k, v in fields)",
    "label": true
  },
  {
    "code": "def test_code_object():\n    import warnings\n    from dill._dill import ALL_CODE_PARAMS, CODE_PARAMS, CODE_VERSION, _create_code\n    code = function_c.__code__\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    LNOTAB = getattr(code, 'co_lnotab', b'')\n    if warnings.filters:\n        del warnings.filters[0]\n    fields = {f: getattr(code, 'co_' + f) for f in CODE_PARAMS}\n    fields.setdefault('posonlyargcount', 0)\n    fields.setdefault('lnotab', LNOTAB)\n    fields.setdefault('linetable', b'')\n    fields.setdefault('qualname', fields['name'])\n    fields.setdefault('exceptiontable', b'')\n    fields.setdefault('endlinetable', None)\n    fields.setdefault('columntable', None)\n    for version, _, params in ALL_CODE_PARAMS:\n        args = tuple((fields[p] for p in params.split()))\n        try:\n            _create_code(*args)\n            if version >= (3, 10):\n                _create_code(fields['lnotab'], *args)\n        except Exception as error:\n            raise Exception('failed to construct code object with format version {}'.format(version)) from error",
    "label": true
  },
  {
    "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)",
    "label": true
  },
  {
    "code": "def _macos_vers(_cache=[]):\n    if not _cache:\n        version = platform.mac_ver()[0]\n        if version == '':\n            plist = '/System/Library/CoreServices/SystemVersion.plist'\n            if os.path.exists(plist):\n                if hasattr(plistlib, 'readPlist'):\n                    plist_content = plistlib.readPlist(plist)\n                    if 'ProductVersion' in plist_content:\n                        version = plist_content['ProductVersion']\n        _cache.append(version.split('.'))\n    return _cache[0]",
    "label": true
  },
  {
    "code": "def run_setup(setup_script, args):\n    setup_dir = os.path.abspath(os.path.dirname(setup_script))\n    with setup_context(setup_dir):\n        try:\n            sys.argv[:] = [setup_script] + list(args)\n            sys.path.insert(0, setup_dir)\n            working_set.__init__()\n            working_set.callbacks.append(lambda dist: dist.activate())\n            with DirectorySandbox(setup_dir):\n                ns = dict(__file__=setup_script, __name__='__main__')\n                _execfile(setup_script, ns)\n        except SystemExit as v:\n            if v.args and v.args[0]:\n                raise",
    "label": true
  },
  {
    "code": "def _normalize_name(name: str) -> str:\n    name = name.lower().replace('_', '-')\n    if name.startswith('--'):\n        name = name[2:]\n    return name",
    "label": true
  },
  {
    "code": "def _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n            if isinstance(lhs, Variable):\n                environment_key = lhs.value\n                lhs_value = environment[environment_key]\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                environment_key = rhs.value\n                rhs_value = environment[environment_key]\n            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in ['and', 'or']\n            if marker == 'or':\n                groups.append([])\n    return any((all(item) for item in groups))",
    "label": true
  },
  {
    "code": "def _byte_to_str_length(codec: str) -> int:\n    if codec.startswith('utf-32'):\n        return 4\n    if codec.startswith('utf-16'):\n        return 2\n    return 1",
    "label": true
  },
  {
    "code": "def hide_setuptools():\n    _distutils_hack = sys.modules.get('_distutils_hack', None)\n    if _distutils_hack is not None:\n        _distutils_hack._remove_shim()\n    modules = filter(_needs_hiding, sys.modules)\n    _clear_modules(modules)",
    "label": true
  },
  {
    "code": "def _write_pyc_fp(fp: IO[bytes], source_stat: os.stat_result, co: types.CodeType) -> None:\n    fp.write(importlib.util.MAGIC_NUMBER)\n    flags = b'\\x00\\x00\\x00\\x00'\n    fp.write(flags)\n    mtime = int(source_stat.st_mtime) & 4294967295\n    size = source_stat.st_size & 4294967295\n    fp.write(struct.pack('<LL', mtime, size))\n    fp.write(marshal.dumps(co))",
    "label": true
  },
  {
    "code": "def enquote_executable(executable):\n    if ' ' in executable:\n        if executable.startswith('/usr/bin/env '):\n            env, _executable = executable.split(' ', 1)\n            if ' ' in _executable and (not _executable.startswith('\"')):\n                executable = '%s \"%s\"' % (env, _executable)\n        elif not executable.startswith('\"'):\n            executable = '\"%s\"' % executable\n    return executable",
    "label": true
  },
  {
    "code": "def wrap(s):\n    paragraphs = s.splitlines()\n    wrapped = ('\\n'.join(textwrap.wrap(para)) for para in paragraphs)\n    return '\\n\\n'.join(wrapped)",
    "label": true
  },
  {
    "code": "def parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:\n    if multiline:\n        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape_multiline\n    else:\n        error_on = ILLEGAL_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape\n    result = ''\n    start_pos = pos\n    while True:\n        try:\n            char = src[pos]\n        except IndexError:\n            raise suffixed_err(src, pos, 'Unterminated string') from None\n        if char == '\"':\n            if not multiline:\n                return (pos + 1, result + src[start_pos:pos])\n            if src.startswith('\"\"\"', pos):\n                return (pos + 3, result + src[start_pos:pos])\n            pos += 1\n            continue\n        if char == '\\\\':\n            result += src[start_pos:pos]\n            pos, parsed_escape = parse_escapes(src, pos)\n            result += parsed_escape\n            start_pos = pos\n            continue\n        if char in error_on:\n            raise suffixed_err(src, pos, f'Illegal character {char!r}')\n        pos += 1",
    "label": true
  },
  {
    "code": "def get_http_url(link: Link, download: Downloader, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> File:\n    temp_dir = TempDirectory(kind='unpack', globally_managed=True)\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n        content_type = None\n    else:\n        from_path, content_type = download(link, temp_dir.path)\n        if hashes:\n            hashes.check_against_path(from_path)\n    return File(from_path, content_type)",
    "label": true
  },
  {
    "code": "def clean_bci_data(bci_years: list[str], start_year: int, bci_scores: list) -> None:\n    for i in range(len(bci_scores)):\n        bci_years += [str(start_year - i)]\n        if bci_scores[i] == '':\n            bci_scores[i] = MISSING_BCI\n        else:\n            bci_scores[i] = float(bci_scores[i])",
    "label": true
  },
  {
    "code": "def _get_custom_platforms(arch: str) -> List[str]:\n    arch_prefix, arch_sep, arch_suffix = arch.partition('_')\n    if arch.startswith('macosx'):\n        arches = _mac_platforms(arch)\n    elif arch_prefix in ['manylinux2014', 'manylinux2010']:\n        arches = _custom_manylinux_platforms(arch)\n    else:\n        arches = [arch]\n    return arches",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def parse_basic_str_escape(src: str, pos: Pos, *, multiline: bool=False) -> tuple[Pos, str]:\n    escape_id = src[pos:pos + 2]\n    pos += 2\n    if multiline and escape_id in {'\\\\ ', '\\\\\\t', '\\\\\\n'}:\n        if escape_id != '\\\\\\n':\n            pos = skip_chars(src, pos, TOML_WS)\n            try:\n                char = src[pos]\n            except IndexError:\n                return (pos, '')\n            if char != '\\n':\n                raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\")\n            pos += 1\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        return (pos, '')\n    if escape_id == '\\\\u':\n        return parse_hex_char(src, pos, 4)\n    if escape_id == '\\\\U':\n        return parse_hex_char(src, pos, 8)\n    try:\n        return (pos, BASIC_STR_ESCAPE_REPLACEMENTS[escape_id])\n    except KeyError:\n        raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\") from None",
    "label": true
  },
  {
    "code": "def _identify_module(file, main=None):\n    from pickletools import genops\n    UNICODE = {'UNICODE', 'BINUNICODE', 'SHORT_BINUNICODE'}\n    found_import = False\n    try:\n        for opcode, arg, pos in genops(file.peek(256)):\n            if not found_import:\n                if opcode.name in ('GLOBAL', 'SHORT_BINUNICODE') and arg.endswith('_import_module'):\n                    found_import = True\n            elif opcode.name in UNICODE:\n                return arg\n        else:\n            raise UnpicklingError('reached STOP without finding main module')\n    except (NotImplementedError, ValueError) as error:\n        if isinstance(error, NotImplementedError) and main is not None:\n            return None\n        raise UnpicklingError('unable to identify main module') from error",
    "label": true
  },
  {
    "code": "def str_eval(token: str) -> str:\n    if token[0:2].lower() in {'fr', 'rf'}:\n        token = token[2:]\n    elif token[0].lower() in {'r', 'u', 'f'}:\n        token = token[1:]\n    if token[0:3] in {'\"\"\"', \"'''\"}:\n        return token[3:-3]\n    return token[1:-1]",
    "label": true
  },
  {
    "code": "def always_reversible(iterable):\n    try:\n        return reversed(iterable)\n    except TypeError:\n        return reversed(list(iterable))",
    "label": true
  },
  {
    "code": "def _dunder_dict(instance, attributes):\n    obj = node_classes.Dict(parent=instance)\n    keys = [node_classes.Const(value=value, parent=obj) for value in list(attributes.keys())]\n    values = [elem[-1] for elem in attributes.values()]\n    obj.postinit(list(zip(keys, values)))\n    return obj",
    "label": true
  },
  {
    "code": "def _bypass_ensure_directory(path):\n    if not WRITE_SUPPORT:\n        raise IOError('\"os.mkdir\" not supported on this platform.')\n    dirname, filename = split(path)\n    if dirname and filename and (not isdir(dirname)):\n        _bypass_ensure_directory(dirname)\n        try:\n            mkdir(dirname, 493)\n        except FileExistsError:\n            pass",
    "label": true
  },
  {
    "code": "def _match_link(link: Link, candidate: 'Candidate') -> bool:\n    if candidate.source_link:\n        return links_equivalent(link, candidate.source_link)\n    return False",
    "label": true
  },
  {
    "code": "def _dnsname_to_stdlib(name):\n\n    def idna_encode(name):\n        \"\"\"\n        Borrowed wholesale from the Python Cryptography Project. It turns out\n        that we can't just safely call `idna.encode`: it can explode for\n        wildcard names. This avoids that problem.\n        \"\"\"\n        from pip._vendor import idna\n        try:\n            for prefix in [u'*.', u'.']:\n                if name.startswith(prefix):\n                    name = name[len(prefix):]\n                    return prefix.encode('ascii') + idna.encode(name)\n            return idna.encode(name)\n        except idna.core.IDNAError:\n            return None\n    if ':' in name:\n        return name\n    name = idna_encode(name)\n    if name is None:\n        return None\n    elif sys.version_info >= (3, 0):\n        name = name.decode('utf-8')\n    return name",
    "label": true
  },
  {
    "code": "def _has_parent_of_type(node: nodes.Call, node_type: nodes.Keyword | nodes.Starred, statement: nodes.Statement) -> bool:\n    parent = node.parent\n    while not isinstance(parent, node_type) and statement.parent_of(parent):\n        parent = parent.parent\n    return isinstance(parent, node_type)",
    "label": true
  },
  {
    "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def python_2_unicode_compatible(klass):\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied to %s because it doesn't define __str__().\" % klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass",
    "label": true
  },
  {
    "code": "def invoke(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f",
    "label": true
  },
  {
    "code": "def _signature_from_arguments(arguments: nodes.Arguments) -> _ParameterSignature:\n    kwarg = arguments.kwarg\n    vararg = arguments.vararg\n    args = [arg.name for arg in chain(arguments.posonlyargs, arguments.args) if arg.name != 'self']\n    kwonlyargs = [arg.name for arg in arguments.kwonlyargs]\n    return _ParameterSignature(args, kwonlyargs, vararg, kwarg)",
    "label": true
  },
  {
    "code": "def _python_requires(dist: 'Distribution', val: dict, _root_dir):\n    from setuptools.extern.packaging.specifiers import SpecifierSet\n    _set_config(dist, 'python_requires', SpecifierSet(val))",
    "label": true
  },
  {
    "code": "def _ipaddress_match(ipname, host_ip):\n    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())\n    return ip == host_ip",
    "label": true
  },
  {
    "code": "def set_partitions(iterable, k=None):\n    L = list(iterable)\n    n = len(L)\n    if k is not None:\n        if k < 1:\n            raise ValueError(\"Can't partition in a negative or zero number of groups\")\n        elif k > n:\n            return\n\n    def set_partitions_helper(L, k):\n        n = len(L)\n        if k == 1:\n            yield [L]\n        elif n == k:\n            yield [[s] for s in L]\n        else:\n            e, *M = L\n            for p in set_partitions_helper(M, k - 1):\n                yield [[e], *p]\n            for p in set_partitions_helper(M, k):\n                for i in range(len(p)):\n                    yield (p[:i] + [[e] + p[i]] + p[i + 1:])\n    if k is None:\n        for k in range(1, n + 1):\n            yield from set_partitions_helper(L, k)\n    else:\n        yield from set_partitions_helper(L, k)",
    "label": true
  },
  {
    "code": "def _get_checkers_infos(linter: PyLinter) -> dict[str, dict[str, Any]]:\n    by_checker: dict[str, dict[str, Any]] = {}\n    for checker in linter.get_checkers():\n        name = checker.name\n        if name != MAIN_CHECKER_NAME:\n            try:\n                by_checker[name]['checker'] = checker\n                with warnings.catch_warnings():\n                    warnings.filterwarnings('ignore', category=DeprecationWarning)\n                    by_checker[name]['options'] += checker.options_and_values()\n                by_checker[name]['msgs'].update(checker.msgs)\n                by_checker[name]['reports'] += checker.reports\n            except KeyError:\n                with warnings.catch_warnings():\n                    warnings.filterwarnings('ignore', category=DeprecationWarning)\n                    by_checker[name] = {'checker': checker, 'options': list(checker.options_and_values()), 'msgs': dict(checker.msgs), 'reports': list(checker.reports)}\n    return by_checker",
    "label": true
  },
  {
    "code": "def function_to_method(n, klass):\n    if isinstance(n, FunctionDef):\n        if n.type == 'classmethod':\n            return bases.BoundMethod(n, klass)\n        if n.type == 'property':\n            return n\n        if n.type != 'staticmethod':\n            return bases.UnboundMethod(n)\n    return n",
    "label": true
  },
  {
    "code": "def _apply(dist: 'Distribution', filepath: _Path, other_files: Iterable[_Path]=(), ignore_option_errors: bool=False) -> Tuple['ConfigHandler', ...]:\n    from setuptools.dist import _Distribution\n    filepath = os.path.abspath(filepath)\n    if not os.path.isfile(filepath):\n        raise FileError(f'Configuration file {filepath} does not exist.')\n    current_directory = os.getcwd()\n    os.chdir(os.path.dirname(filepath))\n    filenames = [*other_files, filepath]\n    try:\n        _Distribution.parse_config_files(dist, filenames=filenames)\n        handlers = parse_configuration(dist, dist.command_options, ignore_option_errors=ignore_option_errors)\n        dist._finalize_license_files()\n    finally:\n        os.chdir(current_directory)\n    return handlers",
    "label": true
  },
  {
    "code": "def split_format_field_names(format_string: str) -> tuple[str, Iterable[tuple[bool, str]]]:\n    try:\n        return _string.formatter_field_name_split(format_string)\n    except ValueError as e:\n        raise IncompleteFormatString() from e",
    "label": true
  },
  {
    "code": "def baditems(obj, exact=False, safe=False):\n    if not hasattr(obj, '__iter__'):\n        return [j for j in (badobjects(obj, 0, exact, safe),) if j is not None]\n    obj = obj.values() if getattr(obj, 'values', None) else obj\n    _obj = []\n    [_obj.append(badobjects(i, 0, exact, safe)) for i in obj if i not in _obj]\n    return [j for j in _obj if j is not None]",
    "label": true
  },
  {
    "code": "def _get_raise_exc(node: nodes.Raise) -> str:\n    exceptions = node.nodes_of_class(nodes.Name)\n    try:\n        return f'{nodes.Raise.__name__} {next(exceptions).name}'\n    except StopIteration:\n        return nodes.Raise.__name__",
    "label": true
  },
  {
    "code": "def dist_factory(path_item, entry, only):\n    lower = entry.lower()\n    is_egg_info = lower.endswith('.egg-info')\n    is_dist_info = lower.endswith('.dist-info') and os.path.isdir(os.path.join(path_item, entry))\n    is_meta = is_egg_info or is_dist_info\n    return distributions_from_metadata if is_meta else find_distributions if not only and _is_egg_path(entry) else resolve_egg_link if not only and lower.endswith('.egg-link') else NoDists()",
    "label": true
  },
  {
    "code": "def _self_version_check_logic(*, state: SelfCheckState, current_time: datetime.datetime, local_version: DistributionVersion, get_remote_version: Callable[[], Optional[str]]) -> Optional[UpgradePrompt]:\n    remote_version_str = state.get(current_time)\n    if remote_version_str is None:\n        remote_version_str = get_remote_version()\n        if remote_version_str is None:\n            logger.debug('No remote pip version found')\n            return None\n        state.set(remote_version_str, current_time)\n    remote_version = parse_version(remote_version_str)\n    logger.debug('Remote version of pip: %s', remote_version)\n    logger.debug('Local version of pip:  %s', local_version)\n    pip_installed_by_pip = was_installed_by_pip('pip')\n    logger.debug('Was pip installed by pip? %s', pip_installed_by_pip)\n    if not pip_installed_by_pip:\n        return None\n    local_version_is_older = local_version < remote_version and local_version.base_version != remote_version.base_version\n    if local_version_is_older:\n        return UpgradePrompt(old=str(local_version), new=remote_version_str)\n    return None",
    "label": true
  },
  {
    "code": "def _get_report_choice(key: str) -> int:\n    import doctest\n    return {DOCTEST_REPORT_CHOICE_UDIFF: doctest.REPORT_UDIFF, DOCTEST_REPORT_CHOICE_CDIFF: doctest.REPORT_CDIFF, DOCTEST_REPORT_CHOICE_NDIFF: doctest.REPORT_NDIFF, DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE: doctest.REPORT_ONLY_FIRST_FAILURE, DOCTEST_REPORT_CHOICE_NONE: 0}[key]",
    "label": true
  },
  {
    "code": "def create_main_parser() -> ConfigOptionParser:\n    parser = ConfigOptionParser(usage='\\n%prog <command> [options]', add_help_option=False, formatter=UpdatingDefaultsHelpFormatter(), name='global', prog=get_prog())\n    parser.disable_interspersed_args()\n    parser.version = get_pip_version()\n    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)\n    parser.add_option_group(gen_opts)\n    parser.main = True\n    description = [''] + [f'{name:27} {command_info.summary}' for name, command_info in commands_dict.items()]\n    parser.description = '\\n'.join(description)\n    return parser",
    "label": true
  },
  {
    "code": "def _ensure_quoted_url(url: str) -> str:\n    result = urllib.parse.urlparse(url)\n    is_local_path = not result.netloc\n    path = _clean_url_path(result.path, is_local_path=is_local_path)\n    return urllib.parse.urlunparse(result._replace(path=path))",
    "label": true
  },
  {
    "code": "def value_chain(*args):\n    for value in args:\n        if isinstance(value, (str, bytes)):\n            yield value\n            continue\n        try:\n            yield from value\n        except TypeError:\n            yield value",
    "label": true
  },
  {
    "code": "def locatedExpr(expr: ParserElement) -> ParserElement:\n    locator = Empty().set_parse_action(lambda ss, ll, tt: ll)\n    return Group(locator('locn_start') + expr('value') + locator.copy().leaveWhitespace()('locn_end'))",
    "label": true
  },
  {
    "code": "def _get_node_id_with_markup(tw: TerminalWriter, config: Config, rep: BaseReport):\n    nodeid = config.cwd_relative_nodeid(rep.nodeid)\n    path, *parts = nodeid.split('::')\n    if parts:\n        parts_markup = tw.markup('::'.join(parts), bold=True)\n        return path + '::' + parts_markup\n    else:\n        return path",
    "label": true
  },
  {
    "code": "def get_exe_prefixes(exe_filename):\n    prefixes = [('PURELIB/', ''), ('PLATLIB/pywin32_system32', ''), ('PLATLIB/', ''), ('SCRIPTS/', 'EGG-INFO/scripts/'), ('DATA/lib/site-packages', '')]\n    z = zipfile.ZipFile(exe_filename)\n    try:\n        for info in z.infolist():\n            name = info.filename\n            parts = name.split('/')\n            if len(parts) == 3 and parts[2] == 'PKG-INFO':\n                if parts[1].endswith('.egg-info'):\n                    prefixes.insert(0, ('/'.join(parts[:2]), 'EGG-INFO/'))\n                    break\n            if len(parts) != 2 or not name.endswith('.pth'):\n                continue\n            if name.endswith('-nspkg.pth'):\n                continue\n            if parts[0].upper() in ('PURELIB', 'PLATLIB'):\n                contents = z.read(name).decode()\n                for pth in yield_lines(contents):\n                    pth = pth.strip().replace('\\\\', '/')\n                    if not pth.startswith('import'):\n                        prefixes.append(('%s/%s/' % (parts[0], pth), ''))\n    finally:\n        z.close()\n    prefixes = [(x.lower(), y) for x, y in prefixes]\n    prefixes.sort()\n    prefixes.reverse()\n    return prefixes",
    "label": true
  },
  {
    "code": "def remove_nested_packages(packages: List[str]) -> List[str]:\n    pkgs = sorted(packages, key=len)\n    top_level = pkgs[:]\n    size = len(pkgs)\n    for i, name in enumerate(reversed(pkgs)):\n        if any((name.startswith(f'{other}.') for other in top_level)):\n            top_level.pop(size - i - 1)\n    return top_level",
    "label": true
  },
  {
    "code": "def _create_weakproxy(obj, callable=False, *args):\n    from weakref import proxy\n    if obj is None:\n        if callable:\n            return proxy(lambda x: x, *args)\n        from collections import UserDict\n        return proxy(UserDict(), *args)\n    return proxy(obj, *args)",
    "label": true
  },
  {
    "code": "def open_file(filename: str, mode: str='r', encoding: t.Optional[str]=None, errors: t.Optional[str]='strict', lazy: bool=False, atomic: bool=False) -> t.IO[t.Any]:\n    if lazy:\n        return t.cast(t.IO[t.Any], LazyFile(filename, mode, encoding, errors, atomic=atomic))\n    f, should_close = open_stream(filename, mode, encoding, errors, atomic=atomic)\n    if not should_close:\n        f = t.cast(t.IO[t.Any], KeepOpenFile(f))\n    return f",
    "label": true
  },
  {
    "code": "def resolve_ssl_version(candidate: None | int | str) -> int:\n    if candidate is None:\n        return PROTOCOL_TLS\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'PROTOCOL_' + candidate)\n        return typing.cast(int, res)\n    return candidate",
    "label": true
  },
  {
    "code": "def _create_cfstring_array(lst):\n    cf_arr = None\n    try:\n        cf_arr = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        if not cf_arr:\n            raise MemoryError('Unable to allocate memory!')\n        for item in lst:\n            cf_str = _cfstr(item)\n            if not cf_str:\n                raise MemoryError('Unable to allocate memory!')\n            try:\n                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)\n            finally:\n                CoreFoundation.CFRelease(cf_str)\n    except BaseException as e:\n        if cf_arr:\n            CoreFoundation.CFRelease(cf_arr)\n        raise ssl.SSLError('Unable to allocate array: %s' % (e,))\n    return cf_arr",
    "label": true
  },
  {
    "code": "def _dump_time(v):\n    utcoffset = v.utcoffset()\n    if utcoffset is None:\n        return v.isoformat()\n    return v.isoformat()[:-6]",
    "label": true
  },
  {
    "code": "def infer_map(self: nodes.Dict, context: InferenceContext | None=None) -> Iterator[nodes.Dict]:\n    if not any((isinstance(k, nodes.DictUnpack) for k, _ in self.items)):\n        yield self\n    else:\n        items = _infer_map(self, context)\n        new_seq = type(self)(self.lineno, self.col_offset, self.parent)\n        new_seq.postinit(list(items.items()))\n        yield new_seq",
    "label": true
  },
  {
    "code": "def resolve_class(qualified_class_name: str, package_dir: Optional[Mapping[str, str]]=None, root_dir: Optional[_Path]=None) -> Callable:\n    root_dir = root_dir or os.getcwd()\n    idx = qualified_class_name.rfind('.')\n    class_name = qualified_class_name[idx + 1:]\n    pkg_name = qualified_class_name[:idx]\n    _parent_path, path, module_name = _find_module(pkg_name, package_dir, root_dir)\n    module = _load_spec(_find_spec(module_name, path), module_name)\n    return getattr(module, class_name)",
    "label": true
  },
  {
    "code": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {'implementation_name': implementation_name, 'implementation_version': iver, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'python_full_version': platform.python_version(), 'platform_python_implementation': platform.python_implementation(), 'python_version': '.'.join(platform.python_version_tuple()[:2]), 'sys_platform': sys.platform}",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable: Iterable[_T], key: Optional[Callable[[_T], _U]]=None) -> Iterator[_T]:\n    seen: Set[Union[_T, _U]] = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def _get_simple_response(url: str, session: PipSession) -> Response:\n    if is_archive_file(Link(url).filename):\n        _ensure_api_response(url, session=session)\n    logger.debug('Getting page %s', redact_auth_from_url(url))\n    resp = session.get(url, headers={'Accept': ', '.join(['application/vnd.pypi.simple.v1+json', 'application/vnd.pypi.simple.v1+html; q=0.1', 'text/html; q=0.01']), 'Cache-Control': 'max-age=0'})\n    raise_for_status(resp)\n    _ensure_api_header(resp)\n    logger.debug('Fetched page %s as %s', redact_auth_from_url(url), resp.headers.get('Content-Type', 'Unknown'))\n    return resp",
    "label": true
  },
  {
    "code": "def with_metaclass(meta, *bases):\n\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})",
    "label": true
  },
  {
    "code": "def iglob(path_glob):\n    if _CHECK_RECURSIVE_GLOB.search(path_glob):\n        msg = 'invalid glob %r: recursive glob \"**\" must be used alone'\n        raise ValueError(msg % path_glob)\n    if _CHECK_MISMATCH_SET.search(path_glob):\n        msg = \"invalid glob %r: mismatching set marker '{' or '}'\"\n        raise ValueError(msg % path_glob)\n    return _iglob(path_glob)",
    "label": true
  },
  {
    "code": "def _simulate_installation_of(to_install: List[InstallRequirement], package_set: PackageSet) -> Set[NormalizedName]:\n    installed = set()\n    for inst_req in to_install:\n        abstract_dist = make_distribution_for_install_requirement(inst_req)\n        dist = abstract_dist.get_metadata_distribution()\n        name = dist.canonical_name\n        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))\n        installed.add(name)\n    return installed",
    "label": true
  },
  {
    "code": "def is_compatible(wheel, tags=None):\n    if not isinstance(wheel, Wheel):\n        wheel = Wheel(wheel)\n    result = False\n    if tags is None:\n        tags = COMPATIBLE_TAGS\n    for ver, abi, arch in tags:\n        if ver in wheel.pyver and abi in wheel.abi and (arch in wheel.arch):\n            result = True\n            break\n    return result",
    "label": true
  },
  {
    "code": "def get_all_distribution_names(url=None):\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()",
    "label": true
  },
  {
    "code": "def _compare_approx(full_object: object, message_data: Sequence[Tuple[str, str, str]], number_of_elements: int, different_ids: Sequence[object], max_abs_diff: float, max_rel_diff: float) -> List[str]:\n    message_list = list(message_data)\n    message_list.insert(0, ('Index', 'Obtained', 'Expected'))\n    max_sizes = [0, 0, 0]\n    for index, obtained, expected in message_list:\n        max_sizes[0] = max(max_sizes[0], len(index))\n        max_sizes[1] = max(max_sizes[1], len(obtained))\n        max_sizes[2] = max(max_sizes[2], len(expected))\n    explanation = [f'comparison failed. Mismatched elements: {len(different_ids)} / {number_of_elements}:', f'Max absolute difference: {max_abs_diff}', f'Max relative difference: {max_rel_diff}'] + [f'{indexes:<{max_sizes[0]}} | {obtained:<{max_sizes[1]}} | {expected:<{max_sizes[2]}}' for indexes, obtained, expected in message_list]\n    return explanation",
    "label": true
  },
  {
    "code": "def _generic_io_transform(node, name, cls):\n    io_module = AstroidManager().ast_from_module_name('_io')\n    attribute_object = io_module[cls]\n    instance = attribute_object.instantiate_class()\n    node.locals[name] = [instance]",
    "label": true
  },
  {
    "code": "def _is_metaclass(klass, seen=None) -> bool:\n    if klass.name == 'type':\n        return True\n    if seen is None:\n        seen = set()\n    for base in klass.bases:\n        try:\n            for baseobj in base.infer():\n                baseobj_name = baseobj.qname()\n                if baseobj_name in seen:\n                    continue\n                seen.add(baseobj_name)\n                if isinstance(baseobj, bases.Instance):\n                    return False\n                if baseobj is klass:\n                    continue\n                if not isinstance(baseobj, ClassDef):\n                    continue\n                if baseobj._type == 'metaclass':\n                    return True\n                if _is_metaclass(baseobj, seen):\n                    return True\n        except InferenceError:\n            continue\n    return False",
    "label": true
  },
  {
    "code": "def get_lexer_by_name(_alias, **options):\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls(**options)\n    raise ClassNotFound('no lexer for alias %r found' % _alias)",
    "label": true
  },
  {
    "code": "def build_wheel(wheel_directory, config_settings, metadata_directory=None):\n    prebuilt_whl = _find_already_built_wheel(metadata_directory)\n    if prebuilt_whl:\n        shutil.copy2(prebuilt_whl, wheel_directory)\n        return os.path.basename(prebuilt_whl)\n    return _build_backend().build_wheel(wheel_directory, config_settings, metadata_directory)",
    "label": true
  },
  {
    "code": "def splitext(path: str) -> Tuple[str, str]:\n    base, ext = posixpath.splitext(path)\n    if base.lower().endswith('.tar'):\n        ext = base[-4:] + ext\n        base = base[:-4]\n    return (base, ext)",
    "label": true
  },
  {
    "code": "def _get_enchant_dict_help(inner_enchant_dicts: list[tuple[Any, enchant.ProviderDesc]], pyenchant_available: bool) -> str:\n    if inner_enchant_dicts:\n        dict_as_str = [f'{d[0]} ({d[1].name})' for d in inner_enchant_dicts]\n        enchant_help = f\"Available dictionaries: {', '.join(dict_as_str)}\"\n    else:\n        enchant_help = 'No available dictionaries : You need to install '\n        if not pyenchant_available:\n            enchant_help += 'both the python package and '\n        enchant_help += 'the system dependency for enchant to work.'\n    return f'Spelling dictionary name. {enchant_help}.'",
    "label": true
  },
  {
    "code": "def setup(**attrs):\n    logging.configure()\n    _install_setup_requires(attrs)\n    return distutils.core.setup(**attrs)",
    "label": true
  },
  {
    "code": "def _correct_article(noun: str) -> str:\n    if noun.lower()[0] in 'aeiou':\n        return 'an ' + noun\n    else:\n        return 'a ' + noun",
    "label": true
  },
  {
    "code": "def _dnsname_match(dn, hostname, max_wildcards=1):\n    pats = []\n    if not dn:\n        return False\n    parts = dn.split('.')\n    leftmost = parts[0]\n    remainder = parts[1:]\n    wildcards = leftmost.count('*')\n    if wildcards > max_wildcards:\n        raise CertificateError('too many wildcards in certificate DNS name: ' + repr(dn))\n    if not wildcards:\n        return dn.lower() == hostname.lower()\n    if leftmost == '*':\n        pats.append('[^.]+')\n    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n        pats.append(re.escape(leftmost))\n    else:\n        pats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n    for frag in remainder:\n        pats.append(re.escape(frag))\n    pat = re.compile('\\\\A' + '\\\\.'.join(pats) + '\\\\Z', re.IGNORECASE)\n    return pat.match(hostname)",
    "label": true
  },
  {
    "code": "def wrap_stream(stream, convert, strip, autoreset, wrap):\n    if wrap:\n        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)\n        if wrapper.should_wrap():\n            stream = wrapper.stream\n    return stream",
    "label": true
  },
  {
    "code": "def consume(iterator, n=None):\n    if n is None:\n        deque(iterator, maxlen=0)\n    else:\n        next(islice(iterator, n, n), None)",
    "label": true
  },
  {
    "code": "def reinit():\n    if wrapped_stdout is not None:\n        sys.stdout = wrapped_stdout\n    if wrapped_stderr is not None:\n        sys.stderr = wrapped_stderr",
    "label": true
  },
  {
    "code": "def _infer_caller():\n\n    def is_this_file(frame_info):\n        return frame_info.filename == __file__\n\n    def is_wrapper(frame_info):\n        return frame_info.function == 'wrapper'\n    not_this_file = itertools.filterfalse(is_this_file, inspect.stack())\n    callers = itertools.filterfalse(is_wrapper, not_this_file)\n    return next(callers).frame",
    "label": true
  },
  {
    "code": "def _find_adapter(registry, ob):\n    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))\n    for t in types:\n        if t in registry:\n            return registry[t]",
    "label": true
  },
  {
    "code": "def _is_has_never_check_common_name_reliable(openssl_version: str, openssl_version_number: int, implementation_name: str, version_info: _TYPE_VERSION_INFO, pypy_version_info: _TYPE_VERSION_INFO | None) -> bool:\n    is_openssl = openssl_version.startswith('OpenSSL ')\n    is_openssl_issue_14579_fixed = openssl_version_number >= 269488335\n    return is_openssl and (is_openssl_issue_14579_fixed or _is_bpo_43522_fixed(implementation_name, version_info, pypy_version_info))",
    "label": true
  },
  {
    "code": "def circular_shifts(iterable):\n    lst = list(iterable)\n    return take(len(lst), windowed(cycle(lst), len(lst)))",
    "label": true
  },
  {
    "code": "def ensure_binary(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def install_req_from_editable(editable_req: str, comes_from: Optional[Union[InstallRequirement, str]]=None, *, use_pep517: Optional[bool]=None, isolated: bool=False, global_options: Optional[List[str]]=None, hash_options: Optional[Dict[str, List[str]]]=None, constraint: bool=False, user_supplied: bool=False, permit_editable_wheels: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    parts = parse_req_from_editable(editable_req)\n    return InstallRequirement(parts.requirement, comes_from=comes_from, user_supplied=user_supplied, editable=True, permit_editable_wheels=permit_editable_wheels, link=parts.link, constraint=constraint, use_pep517=use_pep517, isolated=isolated, global_options=global_options, hash_options=hash_options, config_settings=config_settings, extras=parts.extras)",
    "label": true
  },
  {
    "code": "def CacheControl(sess, cache=None, cache_etags=True, serializer=None, heuristic=None, controller_class=None, adapter_class=None, cacheable_methods=None):\n    cache = DictCache() if cache is None else cache\n    adapter_class = adapter_class or CacheControlAdapter\n    adapter = adapter_class(cache, cache_etags=cache_etags, serializer=serializer, heuristic=heuristic, controller_class=controller_class, cacheable_methods=cacheable_methods)\n    sess.mount('http://', adapter)\n    sess.mount('https://', adapter)\n    return sess",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e221(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _generic_abi() -> List[str]:\n    ext_suffix = _get_config_var('EXT_SUFFIX', warn=True)\n    if not isinstance(ext_suffix, str) or ext_suffix[0] != '.':\n        raise SystemError(\"invalid sysconfig.get_config_var('EXT_SUFFIX')\")\n    parts = ext_suffix.split('.')\n    if len(parts) < 3:\n        return _cpython_abis(sys.version_info[:2])\n    soabi = parts[1]\n    if soabi.startswith('cpython'):\n        abi = 'cp' + soabi.split('-')[1]\n    elif soabi.startswith('cp'):\n        abi = soabi.split('-')[0]\n    elif soabi.startswith('pypy'):\n        abi = '-'.join(soabi.split('-')[:2])\n    elif soabi.startswith('graalpy'):\n        abi = '-'.join(soabi.split('-')[:3])\n    elif soabi:\n        abi = soabi\n    else:\n        return []\n    return [_normalize_string(abi)]",
    "label": true
  },
  {
    "code": "def iter_entry_points(group_name):\n    try:\n        from importlib.metadata import entry_points\n    except ImportError:\n        try:\n            from importlib_metadata import entry_points\n        except ImportError:\n            try:\n                from pip._vendor.pkg_resources import iter_entry_points\n            except (ImportError, OSError):\n                return []\n            else:\n                return iter_entry_points(group_name)\n    groups = entry_points()\n    if hasattr(groups, 'select'):\n        return groups.select(group=group_name)\n    else:\n        return groups.get(group_name, [])",
    "label": true
  },
  {
    "code": "def _container_getitem(instance, elts, index, context: InferenceContext | None=None):\n    try:\n        if isinstance(index, Slice):\n            index_slice = _infer_slice(index, context=context)\n            new_cls = instance.__class__()\n            new_cls.elts = elts[index_slice]\n            new_cls.parent = instance.parent\n            return new_cls\n        if isinstance(index, Const):\n            return elts[index.value]\n    except ValueError as exc:\n        raise AstroidValueError(message='Slice {index!r} cannot index container', node=instance, index=index, context=context) from exc\n    except IndexError as exc:\n        raise AstroidIndexError(message='Index {index!s} out of range', node=instance, index=index, context=context) from exc\n    except TypeError as exc:\n        raise AstroidTypeError(message='Type error {error!r}', node=instance, index=index, context=context) from exc\n    raise AstroidTypeError(f'Could not use {index} as subscript index')",
    "label": true
  },
  {
    "code": "def _to_str(size: int, suffixes: Iterable[str], base: int, *, precision: Optional[int]=1, separator: Optional[str]=' ') -> str:\n    if size == 1:\n        return '1 byte'\n    elif size < base:\n        return '{:,} bytes'.format(size)\n    for i, suffix in enumerate(suffixes, 2):\n        unit = base ** i\n        if size < unit:\n            break\n    return '{:,.{precision}f}{separator}{}'.format(base * size / unit, suffix, precision=precision, separator=separator)",
    "label": true
  },
  {
    "code": "def today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n    return today > given",
    "label": true
  },
  {
    "code": "def inspect_format_method(callable: t.Callable) -> t.Optional[str]:\n    if not isinstance(callable, (types.MethodType, types.BuiltinMethodType)) or callable.__name__ not in ('format', 'format_map'):\n        return None\n    obj = callable.__self__\n    if isinstance(obj, str):\n        return obj\n    return None",
    "label": true
  },
  {
    "code": "def check_test_suite(dist, attr, value):\n    if not isinstance(value, str):\n        raise DistutilsSetupError('test_suite must be a string')",
    "label": true
  },
  {
    "code": "def guess_content_type(filename, default='application/octet-stream'):\n    if filename:\n        return mimetypes.guess_type(filename)[0] or default\n    return default",
    "label": true
  },
  {
    "code": "def _get_http_response_filename(resp: Response, link: Link) -> str:\n    filename = link.filename\n    content_disposition = resp.headers.get('content-disposition')\n    if content_disposition:\n        filename = parse_content_disposition(content_disposition, filename)\n    ext: Optional[str] = splitext(filename)[1]\n    if not ext:\n        ext = mimetypes.guess_extension(resp.headers.get('content-type', ''))\n        if ext:\n            filename += ext\n    if not ext and link.url != resp.url:\n        ext = os.path.splitext(resp.url)[1]\n        if ext:\n            filename += ext\n    return filename",
    "label": true
  },
  {
    "code": "def find_undeclared(nodes: t.Iterable[nodes.Node], names: t.Iterable[str]) -> t.Set[str]:\n    visitor = UndeclaredNameVisitor(names)\n    try:\n        for node in nodes:\n            visitor.visit(node)\n    except VisitorExit:\n        pass\n    return visitor.undeclared",
    "label": true
  },
  {
    "code": "def split_leading_dir(path: str) -> List[str]:\n    path = path.lstrip('/').lstrip('\\\\')\n    if '/' in path and ('\\\\' in path and path.find('/') < path.find('\\\\') or '\\\\' not in path):\n        return path.split('/', 1)\n    elif '\\\\' in path:\n        return path.split('\\\\', 1)\n    else:\n        return [path, '']",
    "label": true
  },
  {
    "code": "def validate(eps: metadata.EntryPoints):\n    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))\n    return eps",
    "label": true
  },
  {
    "code": "def is_sorted(iterable, key=None, reverse=False):\n    compare = lt if reverse else gt\n    it = iterable if key is None else map(key, iterable)\n    return not any(starmap(compare, pairwise(it)))",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def protect_pip_from_modification_on_windows(modifying_pip: bool) -> None:\n    pip_names = ['pip', f'pip{sys.version_info.major}', f'pip{sys.version_info.major}.{sys.version_info.minor}']\n    should_show_use_python_msg = modifying_pip and WINDOWS and (os.path.basename(sys.argv[0]) in pip_names)\n    if should_show_use_python_msg:\n        new_command = [sys.executable, '-m', 'pip'] + sys.argv[1:]\n        raise CommandError('To modify pip, please run the following command:\\n{}'.format(' '.join(new_command)))",
    "label": true
  },
  {
    "code": "def _normalize_cached(filename, _cache={}):\n    try:\n        return _cache[filename]\n    except KeyError:\n        _cache[filename] = result = normalize_path(filename)\n        return result",
    "label": true
  },
  {
    "code": "def ensure_directory(path):\n    dirname = os.path.dirname(path)\n    os.makedirs(dirname, exist_ok=True)",
    "label": true
  },
  {
    "code": "def test_iterable(value: t.Any) -> bool:\n    try:\n        iter(value)\n    except TypeError:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _astroid_wrapper(func: Callable[[str], nodes.Module], modname: str) -> nodes.Module | None:\n    print(f'parsing {modname}...')\n    try:\n        return func(modname)\n    except astroid.exceptions.AstroidBuildingException as exc:\n        print(exc)\n    except Exception:\n        traceback.print_exc()\n    return None",
    "label": true
  },
  {
    "code": "def _is_broken_pipe_error(exc_class: Type[BaseException], exc: BaseException) -> bool:\n    if exc_class is BrokenPipeError:\n        return True\n    if not WINDOWS:\n        return False\n    return isinstance(exc, OSError) and exc.errno in (errno.EINVAL, errno.EPIPE)",
    "label": true
  },
  {
    "code": "def python_entrypoint_name(value: str) -> bool:\n    if not ENTRYPOINT_REGEX.match(value):\n        return False\n    if not RECOMMEDED_ENTRYPOINT_REGEX.match(value):\n        msg = f'Entry point `{value}` does not follow recommended pattern: '\n        msg += RECOMMEDED_ENTRYPOINT_PATTERN\n        _logger.warning(msg)\n    return True",
    "label": true
  },
  {
    "code": "def _glob_paths_csv_transformer(value: str) -> Sequence[str]:\n    paths: list[str] = []\n    for path in _csv_transformer(value):\n        paths.extend(glob(_path_transformer(path), recursive=True))\n    return paths",
    "label": true
  },
  {
    "code": "def unraisable_exception_runtest_hook() -> Generator[None, None, None]:\n    with catch_unraisable_exception() as cm:\n        yield\n        if cm.unraisable:\n            if cm.unraisable.err_msg is not None:\n                err_msg = cm.unraisable.err_msg\n            else:\n                err_msg = 'Exception ignored in'\n            msg = f'{err_msg}: {cm.unraisable.object!r}\\n\\n'\n            msg += ''.join(traceback.format_exception(cm.unraisable.exc_type, cm.unraisable.exc_value, cm.unraisable.exc_traceback))\n            warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))",
    "label": true
  },
  {
    "code": "def direct_url_from_link(link: Link, source_dir: Optional[str]=None, link_is_in_wheel_cache: bool=False) -> DirectUrl:\n    if link.is_vcs:\n        vcs_backend = vcs.get_backend_for_scheme(link.scheme)\n        assert vcs_backend\n        url, requested_revision, _ = vcs_backend.get_url_rev_and_auth(link.url_without_fragment)\n        if link_is_in_wheel_cache:\n            assert requested_revision\n            commit_id = requested_revision\n        else:\n            assert source_dir\n            commit_id = vcs_backend.get_revision(source_dir)\n        return DirectUrl(url=url, info=VcsInfo(vcs=vcs_backend.name, commit_id=commit_id, requested_revision=requested_revision), subdirectory=link.subdirectory_fragment)\n    elif link.is_existing_dir():\n        return DirectUrl(url=link.url_without_fragment, info=DirInfo(), subdirectory=link.subdirectory_fragment)\n    else:\n        hash = None\n        hash_name = link.hash_name\n        if hash_name:\n            hash = f'{hash_name}={link.hash}'\n        return DirectUrl(url=link.url_without_fragment, info=ArchiveInfo(hash=hash), subdirectory=link.subdirectory_fragment)",
    "label": true
  },
  {
    "code": "def main() -> None:\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n    parser = argparse.ArgumentParser(description='OS distro info tool')\n    parser.add_argument('--json', '-j', help='Output in machine readable format', action='store_true')\n    parser.add_argument('--root-dir', '-r', type=str, dest='root_dir', help='Path to the root filesystem directory (defaults to /)')\n    args = parser.parse_args()\n    if args.root_dir:\n        dist = LinuxDistribution(include_lsb=False, include_uname=False, include_oslevel=False, root_dir=args.root_dir)\n    else:\n        dist = _distro\n    if args.json:\n        logger.info(json.dumps(dist.info(), indent=4, sort_keys=True))\n    else:\n        logger.info('Name: %s', dist.name(pretty=True))\n        distribution_version = dist.version(pretty=True)\n        logger.info('Version: %s', distribution_version)\n        distribution_codename = dist.codename()\n        logger.info('Codename: %s', distribution_codename)",
    "label": true
  },
  {
    "code": "def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:\n    retval = _parse_marker(tokenizer)\n    tokenizer.expect('END', expected='end of marker expression')\n    return retval",
    "label": true
  },
  {
    "code": "def isascii(s):\n    try:\n        s.encode('ascii')\n        return True\n    except UnicodeError:\n        return False",
    "label": true
  },
  {
    "code": "def remove_prefix(text, prefix):\n    null, prefix, rest = text.rpartition(prefix)\n    return rest",
    "label": true
  },
  {
    "code": "def expand_env_variables(lines_enum: ReqFileLines) -> ReqFileLines:\n    for line_number, line in lines_enum:\n        for env_var, var_name in ENV_VAR_RE.findall(line):\n            value = os.getenv(var_name)\n            if not value:\n                continue\n            line = line.replace(env_var, value)\n        yield (line_number, line)",
    "label": true
  },
  {
    "code": "def _is_compatible(name: str, arch: str, version: _GLibCVersion) -> bool:\n    sys_glibc = _get_glibc_version()\n    if sys_glibc < version:\n        return False\n    try:\n        import _manylinux\n    except ImportError:\n        return True\n    if hasattr(_manylinux, 'manylinux_compatible'):\n        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n        if result is not None:\n            return bool(result)\n        return True\n    if version == _GLibCVersion(2, 5):\n        if hasattr(_manylinux, 'manylinux1_compatible'):\n            return bool(_manylinux.manylinux1_compatible)\n    if version == _GLibCVersion(2, 12):\n        if hasattr(_manylinux, 'manylinux2010_compatible'):\n            return bool(_manylinux.manylinux2010_compatible)\n    if version == _GLibCVersion(2, 17):\n        if hasattr(_manylinux, 'manylinux2014_compatible'):\n            return bool(_manylinux.manylinux2014_compatible)\n    return True",
    "label": true
  },
  {
    "code": "def _looks_like_signal(node: nodes.FunctionDef, signal_name: str='pyqtSignal') -> bool:\n    klasses = node.instance_attrs.get('__class__', [])\n    if node.qname().partition('.')[0] in {'PySide2', 'PySide6'}:\n        return any((cls.qname() == 'Signal' for cls in klasses))\n    if klasses:\n        try:\n            return klasses[0].name == signal_name\n        except AttributeError:\n            pass\n    return False",
    "label": true
  },
  {
    "code": "def divide_line(text: str, width: int, fold: bool=True) -> List[int]:\n    divides: List[int] = []\n    append = divides.append\n    line_position = 0\n    _cell_len = cell_len\n    for start, _end, word in words(text):\n        word_length = _cell_len(word.rstrip())\n        if line_position + word_length > width:\n            if word_length > width:\n                if fold:\n                    chopped_words = chop_cells(word, max_size=width, position=0)\n                    for last, line in loop_last(chopped_words):\n                        if start:\n                            append(start)\n                        if last:\n                            line_position = _cell_len(line)\n                        else:\n                            start += len(line)\n                else:\n                    if start:\n                        append(start)\n                    line_position = _cell_len(word)\n            elif line_position and start:\n                append(start)\n                line_position = _cell_len(word)\n        else:\n            line_position += _cell_len(word)\n    return divides",
    "label": true
  },
  {
    "code": "def _get_python_inc_from_config(plat_specific, spec_prefix):\n    if spec_prefix is None:\n        return get_config_var('CONF' * plat_specific + 'INCLUDEPY')",
    "label": true
  },
  {
    "code": "def lint(filename: str, options: Sequence[str]=()) -> int:\n    full_path = os.path.abspath(filename)\n    parent_path = os.path.dirname(full_path)\n    child_path = os.path.basename(full_path)\n    while parent_path != '/' and os.path.exists(os.path.join(parent_path, '__init__.py')):\n        child_path = os.path.join(os.path.basename(parent_path), child_path)\n        parent_path = os.path.dirname(parent_path)\n    run_cmd = 'import sys; from pylint.lint import Run; Run(sys.argv[1:])'\n    cmd = [sys.executable, '-c', run_cmd] + ['--msg-template', '{path}:{line}: {category} ({msg_id}, {symbol}, {obj}) {msg}', '-r', 'n', child_path] + list(options)\n    with Popen(cmd, stdout=PIPE, cwd=parent_path, env=_get_env(), universal_newlines=True) as process:\n        for line in process.stdout:\n            if line.startswith('No config file found'):\n                continue\n            parts = line.split(':')\n            if parts and parts[0] == child_path:\n                line = ':'.join([filename] + parts[1:])\n            print(line, end=' ')\n        process.wait()\n        return process.returncode",
    "label": true
  },
  {
    "code": "def _print_help(what, name):\n    try:\n        if what == 'lexer':\n            cls = get_lexer_by_name(name)\n            print('Help on the %s lexer:' % cls.name)\n            print(dedent(cls.__doc__))\n        elif what == 'formatter':\n            cls = find_formatter_class(name)\n            print('Help on the %s formatter:' % cls.name)\n            print(dedent(cls.__doc__))\n        elif what == 'filter':\n            cls = find_filter_class(name)\n            print('Help on the %s filter:' % name)\n            print(dedent(cls.__doc__))\n        return 0\n    except (AttributeError, ValueError):\n        print('%s not found!' % what, file=sys.stderr)\n        return 1",
    "label": true
  },
  {
    "code": "def _is_from_future_import(stmt: nodes.ImportFrom, name: str) -> bool | None:\n    try:\n        module = stmt.do_import_module(stmt.modname)\n    except astroid.AstroidBuildingException:\n        return None\n    for local_node in module.locals.get(name, []):\n        if isinstance(local_node, nodes.ImportFrom) and local_node.modname == FUTURE:\n            return True\n    return None",
    "label": true
  },
  {
    "code": "def consumer(func):\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        gen = func(*args, **kwargs)\n        next(gen)\n        return gen\n    return wrapper",
    "label": true
  },
  {
    "code": "def transient_function_wrapper(module, name):\n\n    def _decorator(wrapper):\n\n        def _wrapper(wrapped, instance, args, kwargs):\n            target_wrapped = args[0]\n            if instance is None:\n                target_wrapper = wrapper\n            elif inspect.isclass(instance):\n                target_wrapper = wrapper.__get__(None, instance)\n            else:\n                target_wrapper = wrapper.__get__(instance, type(instance))\n\n            def _execute(wrapped, instance, args, kwargs):\n                parent, attribute, original = resolve_path(module, name)\n                replacement = FunctionWrapper(original, target_wrapper)\n                setattr(parent, attribute, replacement)\n                try:\n                    return wrapped(*args, **kwargs)\n                finally:\n                    setattr(parent, attribute, original)\n            return FunctionWrapper(target_wrapped, _execute)\n        return FunctionWrapper(wrapper, _wrapper)\n    return _decorator",
    "label": true
  },
  {
    "code": "def _have_working_poll():\n    try:\n        poll_obj = select.poll()\n        _retry_on_intr(poll_obj.poll, 0)\n    except (AttributeError, OSError):\n        return False\n    else:\n        return True",
    "label": true
  },
  {
    "code": "def _get_poly_vars(t: type) -> Set[str]:\n    if isinstance(t, TypeVar) and t.__name__ in _TYPESHED_TVARS:\n        return set([t.__name__])\n    elif isinstance(t, _GenericAlias) and t.__args__:\n        pvars = set()\n        for arg in t.__args__:\n            pvars.update(_get_poly_vars(arg))\n        return pvars\n    return set()",
    "label": true
  },
  {
    "code": "def _afterpoint(string):\n    if _isnumber(string) or _isnumber_with_thousands_separator(string):\n        if _isint(string):\n            return -1\n        else:\n            pos = string.rfind('.')\n            pos = string.lower().rfind('e') if pos < 0 else pos\n            if pos >= 0:\n                return len(string) - pos - 1\n            else:\n                return -1\n    else:\n        return -1",
    "label": true
  },
  {
    "code": "def cpython_tags(python_version: Optional[PythonVersion]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    interpreter = f'cp{_version_nodot(python_version[:2])}'\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    for explicit_abi in ('abi3', 'none'):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    if _abi3_applies(python_version):\n        yield from (Tag(interpreter, 'abi3', platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, 'none', platform_) for platform_ in platforms)\n    if _abi3_applies(python_version):\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = 'cp{version}'.format(version=_version_nodot((python_version[0], minor_version)))\n                yield Tag(interpreter, 'abi3', platform_)",
    "label": true
  },
  {
    "code": "def _handle_runs(char_list):\n    buf = []\n    for c in char_list:\n        if len(c) == 1:\n            if buf and buf[-1][1] == chr(ord(c) - 1):\n                buf[-1] = (buf[-1][0], c)\n            else:\n                buf.append((c, c))\n        else:\n            buf.append((c, c))\n    for a, b in buf:\n        if a == b:\n            yield a\n        else:\n            yield ('%s-%s' % (a, b))",
    "label": true
  },
  {
    "code": "def is_archive_file(name: str) -> bool:\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def run_script(dist_spec, script_name):\n    ns = sys._getframe(1).f_globals\n    name = ns['__name__']\n    ns.clear()\n    ns['__name__'] = name\n    require(dist_spec)[0].run_script(script_name, ns)",
    "label": true
  },
  {
    "code": "def match_to_localtime(match: re.Match) -> time:\n    hour_str, minute_str, sec_str, micros_str = match.groups()\n    micros = int(micros_str.ljust(6, '0')) if micros_str else 0\n    return time(int(hour_str), int(minute_str), int(sec_str), micros)",
    "label": true
  },
  {
    "code": "def pass_eval_context(f: F) -> F:\n    f.jinja_pass_arg = _PassArg.eval_context\n    return f",
    "label": true
  },
  {
    "code": "def git_hook(strict: bool=False, modify: bool=False, lazy: bool=False, settings_file: str='', directories: Optional[List[str]]=None) -> int:\n    diff_cmd = ['git', 'diff-index', '--cached', '--name-only', '--diff-filter=ACMRTUXB', 'HEAD']\n    if lazy:\n        diff_cmd.remove('--cached')\n    if directories:\n        diff_cmd.extend(directories)\n    files_modified = get_lines(diff_cmd)\n    if not files_modified:\n        return 0\n    errors = 0\n    config = Config(settings_file=settings_file, settings_path=os.path.dirname(os.path.abspath(files_modified[0])))\n    for filename in files_modified:\n        if filename.endswith('.py'):\n            staged_cmd = ['git', 'show', f':{filename}']\n            staged_contents = get_output(staged_cmd)\n            try:\n                if not api.check_code_string(staged_contents, file_path=Path(filename), config=config):\n                    errors += 1\n                    if modify:\n                        api.sort_file(filename, config=config)\n            except exceptions.FileSkipped:\n                pass\n    return errors if strict else 0",
    "label": true
  },
  {
    "code": "def _should_enforce():\n    enforce = os.getenv('SETUPTOOLS_ENFORCE_DEPRECATION', 'false').lower()\n    return enforce in ('true', 'on', 'ok', '1')",
    "label": true
  },
  {
    "code": "def _get_if_statement_ancestor(node: nodes.NodeNG) -> nodes.If | None:\n    for parent in node.node_ancestors():\n        if isinstance(parent, nodes.If):\n            return parent\n    return None",
    "label": true
  },
  {
    "code": "def parse_netloc(netloc: str) -> Tuple[Optional[str], Optional[int]]:\n    url = build_url_from_netloc(netloc)\n    parsed = urllib.parse.urlparse(url)\n    return (parsed.hostname, parsed.port)",
    "label": true
  },
  {
    "code": "def markup_join(seq: t.Iterable[t.Any]) -> str:\n    buf = []\n    iterator = map(soft_str, seq)\n    for arg in iterator:\n        buf.append(arg)\n        if hasattr(arg, '__html__'):\n            return Markup('').join(chain(buf, iterator))\n    return concat(buf)",
    "label": true
  },
  {
    "code": "def ask_password(message: str) -> str:\n    _check_no_input(message)\n    return getpass.getpass(message)",
    "label": true
  },
  {
    "code": "def _safe_isinstance(obj: object, class_or_tuple: Union[type, Tuple[type, ...]]) -> bool:\n    try:\n        return isinstance(obj, class_or_tuple)\n    except Exception:\n        return False",
    "label": true
  },
  {
    "code": "def set_threshold(level):\n    orig = _global_log.level\n    _global_log.setLevel(level)\n    return orig",
    "label": true
  },
  {
    "code": "def _fix_abiflags(parts: Tuple[str]) -> Generator[str, None, None]:\n    ldversion = sysconfig.get_config_var('LDVERSION')\n    abiflags = getattr(sys, 'abiflags', None)\n    if not ldversion or not abiflags or (not ldversion.endswith(abiflags)):\n        yield from parts\n        return\n    for part in parts:\n        if part.endswith(ldversion):\n            part = part[:0 - len(abiflags)]\n        yield part",
    "label": true
  },
  {
    "code": "def release_gone():\n    itop, mp, src = (id_to_obj.pop, memo.pop, getrefcount)\n    [(itop(id_), mp(id_)) for id_, obj in list(id_to_obj.items()) if src(obj) < 4]",
    "label": true
  },
  {
    "code": "def test_recursive_function():\n    global fib\n    fib2 = copy(fib, recurse=True)\n    fib3 = copy(fib)\n    fib4 = fib\n    del fib\n    assert fib2(5) == 5\n    for _fib in (fib3, fib4):\n        try:\n            _fib(5)\n        except Exception:\n            pass\n        else:\n            raise AssertionError(\"Function fib shouldn't have been found\")\n    fib = fib4",
    "label": true
  },
  {
    "code": "def _generic_abi() -> List[str]:\n    ext_suffix = _get_config_var('EXT_SUFFIX', warn=True)\n    if not isinstance(ext_suffix, str) or ext_suffix[0] != '.':\n        raise SystemError(\"invalid sysconfig.get_config_var('EXT_SUFFIX')\")\n    parts = ext_suffix.split('.')\n    if len(parts) < 3:\n        return _cpython_abis(sys.version_info[:2])\n    soabi = parts[1]\n    if soabi.startswith('cpython'):\n        abi = 'cp' + soabi.split('-')[1]\n    elif soabi.startswith('cp'):\n        abi = soabi.split('-')[0]\n    elif soabi.startswith('pypy'):\n        abi = '-'.join(soabi.split('-')[:2])\n    elif soabi.startswith('graalpy'):\n        abi = '-'.join(soabi.split('-')[:3])\n    elif soabi:\n        abi = soabi\n    else:\n        return []\n    return [_normalize_string(abi)]",
    "label": true
  },
  {
    "code": "def _get_name_and_version(name, version, for_filename=False):\n    if for_filename:\n        name = _FILESAFE.sub('-', name)\n        version = _FILESAFE.sub('-', version.replace(' ', '.'))\n    return '%s-%s' % (name, version)",
    "label": true
  },
  {
    "code": "def check_variable_assignment(value: object, varname: str, annotation: Any, memo: TypeCheckMemo) -> Any:\n    if _suppression.type_checks_suppressed:\n        return value\n    try:\n        check_type_internal(value, annotation, memo)\n    except TypeCheckError as exc:\n        qualname = qualified_name(value, add_class_prefix=True)\n        exc.append_path_element(f'value assigned to {varname} ({qualname})')\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n    return value",
    "label": true
  },
  {
    "code": "def _rst_escape_first_column(rows, headers):\n\n    def escape_empty(val):\n        if isinstance(val, (str, bytes)) and (not val.strip()):\n            return '..'\n        else:\n            return val\n    new_headers = list(headers)\n    new_rows = []\n    if headers:\n        new_headers[0] = escape_empty(headers[0])\n    for row in rows:\n        new_row = list(row)\n        if new_row:\n            new_row[0] = escape_empty(row[0])\n        new_rows.append(new_row)\n    return (new_rows, new_headers)",
    "label": true
  },
  {
    "code": "def extract_from_urllib3() -> None:\n    util.SSLContext = orig_util_SSLContext\n    util.ssl_.SSLContext = orig_util_SSLContext\n    util.IS_PYOPENSSL = False\n    util.ssl_.IS_PYOPENSSL = False",
    "label": true
  },
  {
    "code": "def _safe_isinstance(obj: object, class_or_tuple: Union[type, Tuple[type, ...]]) -> bool:\n    try:\n        return isinstance(obj, class_or_tuple)\n    except Exception:\n        return False",
    "label": true
  },
  {
    "code": "def find_eggs_in_zip(importer, path_item, only=False):\n    if importer.archive.endswith('.whl'):\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        return\n    for subitem in metadata.resource_listdir(''):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith(('.dist-info', '.egg-info')):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('terminal reporting', 'Reporting', after='general')\n    group._addoption('-v', '--verbose', action='count', default=0, dest='verbose', help='Increase verbosity')\n    group._addoption('--no-header', action='store_true', default=False, dest='no_header', help='Disable header')\n    group._addoption('--no-summary', action='store_true', default=False, dest='no_summary', help='Disable summary')\n    group._addoption('-q', '--quiet', action=MoreQuietAction, default=0, dest='verbose', help='Decrease verbosity')\n    group._addoption('--verbosity', dest='verbose', type=int, default=0, help='Set verbosity. Default: 0.')\n    group._addoption('-r', action='store', dest='reportchars', default=_REPORTCHARS_DEFAULT, metavar='chars', help=\"Show extra test summary info as specified by chars: (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll except passed (p/P), or (A)ll. (w)arnings are enabled by default (see --disable-warnings), 'N' can be used to reset the list. (default: 'fE').\")\n    group._addoption('--disable-warnings', '--disable-pytest-warnings', default=False, dest='disable_warnings', action='store_true', help='Disable warnings summary')\n    group._addoption('-l', '--showlocals', action='store_true', dest='showlocals', default=False, help='Show locals in tracebacks (disabled by default)')\n    group._addoption('--no-showlocals', action='store_false', dest='showlocals', help='Hide locals in tracebacks (negate --showlocals passed through addopts)')\n    group._addoption('--tb', metavar='style', action='store', dest='tbstyle', default='auto', choices=['auto', 'long', 'short', 'no', 'line', 'native'], help='Traceback print mode (auto/long/short/line/native/no)')\n    group._addoption('--show-capture', action='store', dest='showcapture', choices=['no', 'stdout', 'stderr', 'log', 'all'], default='all', help='Controls how captured stdout/stderr/log is shown on failed tests. Default: all.')\n    group._addoption('--fulltrace', '--full-trace', action='store_true', default=False, help=\"Don't cut any tracebacks (default is to cut)\")\n    group._addoption('--color', metavar='color', action='store', dest='color', default='auto', choices=['yes', 'no', 'auto'], help='Color terminal output (yes/no/auto)')\n    group._addoption('--code-highlight', default='yes', choices=['yes', 'no'], help='Whether code should be highlighted (only if --color is also enabled). Default: yes.')\n    parser.addini('console_output_style', help='Console output: \"classic\", or with additional progress information (\"progress\" (percentage) | \"count\" | \"progress-even-when-capture-no\" (forces progress even when capture=no)', default='progress')",
    "label": true
  },
  {
    "code": "def get_win_folder_via_ctypes(csidl_name: str) -> str:\n    csidl_const = {'CSIDL_APPDATA': 26, 'CSIDL_COMMON_APPDATA': 35, 'CSIDL_LOCAL_APPDATA': 28, 'CSIDL_PERSONAL': 5, 'CSIDL_MYPICTURES': 39, 'CSIDL_MYVIDEO': 14, 'CSIDL_MYMUSIC': 13, 'CSIDL_DOWNLOADS': 40, 'CSIDL_DESKTOPDIRECTORY': 16}.get(csidl_name)\n    if csidl_const is None:\n        msg = f'Unknown CSIDL name: {csidl_name}'\n        raise ValueError(msg)\n    buf = ctypes.create_unicode_buffer(1024)\n    windll = getattr(ctypes, 'windll')\n    windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n    if any((ord(c) > 255 for c in buf)):\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n    if csidl_name == 'CSIDL_DOWNLOADS':\n        return os.path.join(buf.value, 'Downloads')\n    return buf.value",
    "label": true
  },
  {
    "code": "def check_nfc(label: str) -> None:\n    if unicodedata.normalize('NFC', label) != label:\n        raise IDNAError('Label must be in Normalization Form C')",
    "label": true
  },
  {
    "code": "def check_type(value: object, expected_type: Any, *, forward_ref_policy: ForwardRefPolicy=TypeCheckConfiguration().forward_ref_policy, typecheck_fail_callback: TypeCheckFailCallback | None=TypeCheckConfiguration().typecheck_fail_callback, collection_check_strategy: CollectionCheckStrategy=TypeCheckConfiguration().collection_check_strategy) -> Any:\n    if type(expected_type) is tuple:\n        expected_type = Union[expected_type]\n    config = TypeCheckConfiguration(forward_ref_policy=forward_ref_policy, typecheck_fail_callback=typecheck_fail_callback, collection_check_strategy=collection_check_strategy)\n    if _suppression.type_checks_suppressed or expected_type is Any:\n        return value\n    frame = sys._getframe(1)\n    memo = TypeCheckMemo(frame.f_globals, frame.f_locals, config=config)\n    try:\n        check_type_internal(value, expected_type, memo)\n    except TypeCheckError as exc:\n        exc.append_path_element(qualified_name(value, add_class_prefix=True))\n        if config.typecheck_fail_callback:\n            config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n    return value",
    "label": true
  },
  {
    "code": "def is_module_name_part_of_extension_package_whitelist(module_name: str, package_whitelist: set[str]) -> bool:\n    parts = module_name.split('.')\n    return any(('.'.join(parts[:x]) in package_whitelist for x in range(1, len(parts) + 1)))",
    "label": true
  },
  {
    "code": "def _is_binary_writer(stream: t.IO[t.Any], default: bool=False) -> bool:\n    try:\n        stream.write(b'')\n    except Exception:\n        try:\n            stream.write('')\n            return False\n        except Exception:\n            pass\n        return default\n    return True",
    "label": true
  },
  {
    "code": "def compress_bytes(text: bytes, codes: dict[int, str]) -> bytes:\n    bits = ''.join([codes[byte] for byte in text])\n    return bytes([int(bits[a:a + 8].ljust(8, '0'), 2) for a in range(0, len(bits), 8)])",
    "label": true
  },
  {
    "code": "def set_threshold(level):\n    logging.root.setLevel(level * 10)\n    return set_threshold.unpatched(level)",
    "label": true
  },
  {
    "code": "def parse(line: str) -> Tuple[str, str]:\n    comment_start = line.find('#')\n    if comment_start != -1:\n        return (line[:comment_start], line[comment_start + 1:].strip())\n    return (line, '')",
    "label": true
  },
  {
    "code": "def fixup_namespace_packages(path_item, parent=None):\n    _imp.acquire_lock()\n    try:\n        for package in _namespace_packages.get(parent, ()):\n            subpath = _handle_ns(package, path_item)\n            if subpath:\n                fixup_namespace_packages(subpath, package)\n    finally:\n        _imp.release_lock()",
    "label": true
  },
  {
    "code": "def resolve_cert_reqs(candidate: None | int | str) -> VerifyMode:\n    if candidate is None:\n        return CERT_REQUIRED\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'CERT_' + candidate)\n        return res\n    return candidate",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e305(msg, _node, source_lines=None):\n    line = msg.line - 1\n    if 'found 0' in msg.msg:\n        yield from render_context(line - 1, line + 1, source_lines)\n        yield from ((None, slice(None, None), LineType.ERROR, NEW_BLANK_LINE_MESSAGE) for _ in range(0, 2))\n    else:\n        line -= 1\n        yield from render_context(line - 1, line + 1, source_lines)\n        yield from render_blank_line(line)\n        yield (None, slice(None, None), LineType.ERROR, NEW_BLANK_LINE_MESSAGE)\n    yield from render_context(msg.line, msg.line + 2, source_lines)",
    "label": true
  },
  {
    "code": "def pytest_report_from_serializable(data: Dict[str, Any]) -> Optional[Union[CollectReport, TestReport]]:\n    if '$report_type' in data:\n        if data['$report_type'] == 'TestReport':\n            return TestReport._from_json(data)\n        elif data['$report_type'] == 'CollectReport':\n            return CollectReport._from_json(data)\n        assert False, 'Unknown report_type unserialize data: {}'.format(data['$report_type'])\n    return None",
    "label": true
  },
  {
    "code": "def _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n            if isinstance(lhs, Variable):\n                environment_key = lhs.value\n                lhs_value = environment[environment_key]\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                environment_key = rhs.value\n                rhs_value = environment[environment_key]\n            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in ['and', 'or']\n            if marker == 'or':\n                groups.append([])\n    return any((all(item) for item in groups))",
    "label": true
  },
  {
    "code": "def compress_for_output_listing(paths: Iterable[str]) -> Tuple[Set[str], Set[str]]:\n    will_remove = set(paths)\n    will_skip = set()\n    folders = set()\n    files = set()\n    for path in will_remove:\n        if path.endswith('.pyc'):\n            continue\n        if path.endswith('__init__.py') or '.dist-info' in path:\n            folders.add(os.path.dirname(path))\n        files.add(path)\n    _normcased_files = set(map(os.path.normcase, files))\n    folders = compact(folders)\n    for folder in folders:\n        for dirpath, _, dirfiles in os.walk(folder):\n            for fname in dirfiles:\n                if fname.endswith('.pyc'):\n                    continue\n                file_ = os.path.join(dirpath, fname)\n                if os.path.isfile(file_) and os.path.normcase(file_) not in _normcased_files:\n                    will_skip.add(file_)\n    will_remove = files | {os.path.join(folder, '*') for folder in folders}\n    return (will_remove, will_skip)",
    "label": true
  },
  {
    "code": "def subst_vars(s, local_vars):\n    check_environ()\n    lookup = dict(os.environ)\n    lookup.update(((name, str(value)) for name, value in local_vars.items()))\n    try:\n        return _subst_compat(s).format_map(lookup)\n    except KeyError as var:\n        raise ValueError(f'invalid variable {var}')",
    "label": true
  },
  {
    "code": "def _create_function(fcode, fglobals, fname=None, fdefaults=None, fclosure=None, fdict=None, fkwdefaults=None):\n    func = FunctionType(fcode, fglobals or dict(), fname, fdefaults, fclosure)\n    if fdict is not None:\n        func.__dict__.update(fdict)\n    if fkwdefaults is not None:\n        func.__kwdefaults__ = fkwdefaults\n    if '__builtins__' not in func.__globals__:\n        func.__globals__['__builtins__'] = globals()['__builtins__']\n    return func",
    "label": true
  },
  {
    "code": "def _handle_no_binary(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(value, existing.no_binary, existing.only_binary)",
    "label": true
  },
  {
    "code": "def _transform_url(url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]) -> Tuple[str, NetlocTuple]:\n    purl = urllib.parse.urlsplit(url)\n    netloc_tuple = transform_netloc(purl.netloc)\n    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)\n    surl = urllib.parse.urlunsplit(url_pieces)\n    return (surl, cast('NetlocTuple', netloc_tuple))",
    "label": true
  },
  {
    "code": "def webify(color):\n    if color.startswith('calc') or color.startswith('var'):\n        return color\n    else:\n        return '#' + color",
    "label": true
  },
  {
    "code": "def _extract_single_node(code: str, module_name: str='') -> nodes.NodeNG:\n    ret = extract_node(code, module_name)\n    if isinstance(ret, list):\n        return ret[0]\n    return ret",
    "label": true
  },
  {
    "code": "def _iglob(path_glob):\n    rich_path_glob = RICH_GLOB.split(path_glob, 1)\n    if len(rich_path_glob) > 1:\n        assert len(rich_path_glob) == 3, rich_path_glob\n        prefix, set, suffix = rich_path_glob\n        for item in set.split(','):\n            for path in _iglob(''.join((prefix, item, suffix))):\n                yield path\n    elif '**' not in path_glob:\n        for item in std_iglob(path_glob):\n            yield item\n    else:\n        prefix, radical = path_glob.split('**', 1)\n        if prefix == '':\n            prefix = '.'\n        if radical == '':\n            radical = '*'\n        else:\n            radical = radical.lstrip('/')\n            radical = radical.lstrip('\\\\')\n        for path, dir, files in os.walk(prefix):\n            path = os.path.normpath(path)\n            for fn in _iglob(os.path.join(path, radical)):\n                yield fn",
    "label": true
  },
  {
    "code": "def _yield_default_files() -> Iterator[Path]:\n    for config_name in CONFIG_NAMES:\n        try:\n            if config_name.is_file():\n                if config_name.suffix == '.toml' and (not _toml_has_config(config_name)):\n                    continue\n                if config_name.suffix == '.cfg' and (not _cfg_has_config(config_name)):\n                    continue\n                yield config_name.resolve()\n        except OSError:\n            pass",
    "label": true
  },
  {
    "code": "def find_try_except_wrapper_node(node: nodes.NodeNG) -> nodes.ExceptHandler | nodes.TryExcept | None:\n    current = node\n    ignores = (nodes.ExceptHandler, nodes.TryExcept)\n    while current and (not isinstance(current.parent, ignores)):\n        current = current.parent\n    if current and isinstance(current.parent, ignores):\n        return current.parent\n    return None",
    "label": true
  },
  {
    "code": "def is_func_decorator(node: nodes.NodeNG) -> bool:\n    for parent in node.node_ancestors():\n        if isinstance(parent, nodes.Decorators):\n            return True\n        if parent.is_statement or isinstance(parent, (nodes.Lambda, nodes.ComprehensionScope, nodes.ListComp)):\n            break\n    return False",
    "label": true
  },
  {
    "code": "def test_class_instances():\n    assert dill.pickles(o)\n    assert dill.pickles(oc)\n    assert dill.pickles(n)\n    assert dill.pickles(nc)\n    assert dill.pickles(m)",
    "label": true
  },
  {
    "code": "def format_filename(filename: 't.Union[str, bytes, os.PathLike[str], os.PathLike[bytes]]', shorten: bool=False) -> str:\n    if shorten:\n        filename = os.path.basename(filename)\n    else:\n        filename = os.fspath(filename)\n    if isinstance(filename, bytes):\n        filename = filename.decode(sys.getfilesystemencoding(), 'replace')\n    else:\n        filename = filename.encode('utf-8', 'surrogateescape').decode('utf-8', 'replace')\n    return filename",
    "label": true
  },
  {
    "code": "def get_win_folder_from_registry(csidl_name: str) -> str:\n    shell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': 'Common AppData', 'CSIDL_LOCAL_APPDATA': 'Local AppData', 'CSIDL_PERSONAL': 'Personal', 'CSIDL_DOWNLOADS': '{374DE290-123F-4565-9164-39C4925E467B}', 'CSIDL_MYPICTURES': 'My Pictures', 'CSIDL_MYVIDEO': 'My Video', 'CSIDL_MYMUSIC': 'My Music'}.get(csidl_name)\n    if shell_folder_name is None:\n        msg = f'Unknown CSIDL name: {csidl_name}'\n        raise ValueError(msg)\n    if sys.platform != 'win32':\n        raise NotImplementedError\n    import winreg\n    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, 'Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Explorer\\\\Shell Folders')\n    directory, _ = winreg.QueryValueEx(key, shell_folder_name)\n    return str(directory)",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def build_wheel_legacy(name: str, setup_py_path: str, source_dir: str, global_options: List[str], build_options: List[str], tempd: str) -> Optional[str]:\n    wheel_args = make_setuptools_bdist_wheel_args(setup_py_path, global_options=global_options, build_options=build_options, destination_dir=tempd)\n    spin_message = f'Building wheel for {name} (setup.py)'\n    with open_spinner(spin_message) as spinner:\n        logger.debug('Destination directory: %s', tempd)\n        try:\n            output = call_subprocess(wheel_args, command_desc='python setup.py bdist_wheel', cwd=source_dir, spinner=spinner)\n        except Exception:\n            spinner.finish('error')\n            logger.error('Failed building wheel for %s', name)\n            return None\n        names = os.listdir(tempd)\n        wheel_path = get_legacy_build_wheel_path(names=names, temp_dir=tempd, name=name, command_args=wheel_args, command_output=output)\n        return wheel_path",
    "label": true
  },
  {
    "code": "def _cfg_has_config(path: Path | str) -> bool:\n    parser = configparser.ConfigParser()\n    try:\n        parser.read(path, encoding='utf-8')\n    except configparser.Error:\n        return False\n    return any((section.startswith('pylint.') for section in parser.sections()))",
    "label": true
  },
  {
    "code": "def distributions_from_metadata(path):\n    root = os.path.dirname(path)\n    if os.path.isdir(path):\n        if len(os.listdir(path)) == 0:\n            return\n        metadata = PathMetadata(root, path)\n    else:\n        metadata = FileMetadata(path)\n    entry = os.path.basename(path)\n    yield Distribution.from_location(root, entry, metadata, precedence=DEVELOP_DIST)",
    "label": true
  },
  {
    "code": "def _find_virtual_namespaces(pkg_roots: Dict[str, str]) -> Iterator[str]:\n    for pkg in pkg_roots:\n        if '.' not in pkg:\n            continue\n        parts = pkg.split('.')\n        for i in range(len(parts) - 1, 0, -1):\n            partial_name = '.'.join(parts[:i])\n            path = Path(find_package_path(partial_name, pkg_roots, ''))\n            if not path.exists() or partial_name not in pkg_roots:\n                yield partial_name",
    "label": true
  },
  {
    "code": "def safe_identifier(name: str) -> str:\n    safe = re.sub('\\\\W|^(?=\\\\d)', '_', name)\n    assert safe.isidentifier()\n    return safe",
    "label": true
  },
  {
    "code": "def test_contract_start_dates() -> None:\n    customers = create_customers(test_dict)\n    for c in customers:\n        for pl in c._phone_lines:\n            assert pl.contract.start == datetime.date(year=2017, month=12, day=25)\n            if hasattr(pl.contract, 'end'):\n                assert pl.contract.end == datetime.date(year=2019, month=6, day=25)",
    "label": true
  },
  {
    "code": "def _dist_info_files(whl_zip):\n    res = []\n    for path in whl_zip.namelist():\n        m = re.match('[^/\\\\\\\\]+-[^/\\\\\\\\]+\\\\.dist-info/', path)\n        if m:\n            res.append(path)\n    if res:\n        return res\n    raise Exception('No .dist-info folder found in wheel')",
    "label": true
  },
  {
    "code": "def invoke(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f",
    "label": true
  },
  {
    "code": "def _match_link(link: Link, candidate: 'Candidate') -> bool:\n    if candidate.source_link:\n        return links_equivalent(link, candidate.source_link)\n    return False",
    "label": true
  },
  {
    "code": "def unzip(iterable):\n    head, iterable = spy(iter(iterable))\n    if not head:\n        return ()\n    head = head[0]\n    iterables = tee(iterable, len(head))\n\n    def itemgetter(i):\n\n        def getter(obj):\n            try:\n                return obj[i]\n            except IndexError:\n                raise StopIteration\n        return getter\n    return tuple((map(itemgetter(i), it) for i, it in enumerate(iterables)))",
    "label": true
  },
  {
    "code": "def dist_from_wheel_url(name: str, url: str, session: PipSession) -> BaseDistribution:\n    with LazyZipOverHTTP(url, session) as zf:\n        wheel = MemoryWheel(zf.name, zf)\n        return get_wheel_distribution(wheel, canonicalize_name(name))",
    "label": true
  },
  {
    "code": "def load_lexer_from_file(filename, lexername='CustomLexer', **options):\n    try:\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        if lexername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' % (lexername, filename))\n        lexer_class = custom_namespace[lexername]\n        return lexer_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom lexer: %s' % err)",
    "label": true
  },
  {
    "code": "def map_reduce(iterable, keyfunc, valuefunc=None, reducefunc=None):\n    valuefunc = (lambda x: x) if valuefunc is None else valuefunc\n    ret = defaultdict(list)\n    for item in iterable:\n        key = keyfunc(item)\n        value = valuefunc(item)\n        ret[key].append(value)\n    if reducefunc is not None:\n        for key, value_list in ret.items():\n            ret[key] = reducefunc(value_list)\n    ret.default_factory = None\n    return ret",
    "label": true
  },
  {
    "code": "def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:\n    if not tokenizer.check('SEMICOLON'):\n        tokenizer.raise_syntax_error(f'Expected end or semicolon (after {after})', span_start=span_start)\n    tokenizer.read()\n    marker = _parse_marker(tokenizer)\n    tokenizer.consume('WS')\n    return marker",
    "label": true
  },
  {
    "code": "def infer_typing_cast(node: Call, ctx: context.InferenceContext | None=None) -> Iterator[NodeNG]:\n    if not isinstance(node.func, (Name, Attribute)):\n        raise UseInferenceDefault\n    try:\n        func = next(node.func.infer(context=ctx))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(func, FunctionDef) or func.qname() != 'typing.cast' or len(node.args) != 2:\n        raise UseInferenceDefault\n    return node.args[1].infer(context=ctx)",
    "label": true
  },
  {
    "code": "def _yn_validator(opt: str, _: str, value: Any) -> bool:\n    if isinstance(value, int):\n        return bool(value)\n    if isinstance(value, str):\n        value = value.lower()\n    if value in {'y', 'yes', 'true'}:\n        return True\n    if value in {'n', 'no', 'false'}:\n        return False\n    msg = 'option %s: invalid yn value %r, should be in (y, yes, true, n, no, false)'\n    raise optparse.OptionValueError(msg % (opt, value))",
    "label": true
  },
  {
    "code": "def shebang_matches(text, regex):\n    index = text.find('\\n')\n    if index >= 0:\n        first_line = text[:index].lower()\n    else:\n        first_line = text.lower()\n    if first_line.startswith('#!'):\n        try:\n            found = [x for x in split_path_re.split(first_line[2:].strip()) if x and (not x.startswith('-'))][-1]\n        except IndexError:\n            return False\n        regex = re.compile('^%s(\\\\.(exe|cmd|bat|bin))?$' % regex, re.IGNORECASE)\n        if regex.search(found) is not None:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _start_of_option(ctx: Context, value: str) -> bool:\n    if not value:\n        return False\n    c = value[0]\n    return c in ctx._opt_prefixes",
    "label": true
  },
  {
    "code": "def _populate_context_lookup(call: nodes.Call, context: InferenceContext | None):\n    context_lookup: dict[InferenceResult, InferenceContext] = {}\n    if context is None:\n        return context_lookup\n    for arg in call.args:\n        if isinstance(arg, nodes.Starred):\n            context_lookup[arg.value] = context\n        else:\n            context_lookup[arg] = context\n    keywords = call.keywords if call.keywords is not None else []\n    for keyword in keywords:\n        context_lookup[keyword.value] = context\n    return context_lookup",
    "label": true
  },
  {
    "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)",
    "label": true
  },
  {
    "code": "def mute_string(text):\n    start = text.index(text[-1]) + 1\n    end = len(text) - 1\n    if text[-3:] in ('\"\"\"', \"'''\"):\n        start += 2\n        end -= 2\n    return text[:start] + 'x' * (end - start) + text[end:]",
    "label": true
  },
  {
    "code": "def badobjects(obj, depth=0, exact=False, safe=False):\n    from dill import pickles\n    if not depth:\n        if pickles(obj, exact, safe):\n            return None\n        return obj\n    return dict(((attr, badobjects(getattr(obj, attr), depth - 1, exact, safe)) for attr in dir(obj) if not pickles(getattr(obj, attr), exact, safe)))",
    "label": true
  },
  {
    "code": "def open(file: Union[str, 'PathLike[str]', bytes], mode: Union[Literal['rb'], Literal['rt'], Literal['r']]='r', buffering: int=-1, encoding: Optional[str]=None, errors: Optional[str]=None, newline: Optional[str]=None, *, total: Optional[int]=None, description: str='Reading...', auto_refresh: bool=True, console: Optional[Console]=None, transient: bool=False, get_time: Optional[Callable[[], float]]=None, refresh_per_second: float=10, style: StyleType='bar.back', complete_style: StyleType='bar.complete', finished_style: StyleType='bar.finished', pulse_style: StyleType='bar.pulse', disable: bool=False) -> Union[ContextManager[BinaryIO], ContextManager[TextIO]]:\n    columns: List['ProgressColumn'] = [TextColumn('[progress.description]{task.description}')] if description else []\n    columns.extend((BarColumn(style=style, complete_style=complete_style, finished_style=finished_style, pulse_style=pulse_style), DownloadColumn(), TimeRemainingColumn()))\n    progress = Progress(*columns, auto_refresh=auto_refresh, console=console, transient=transient, get_time=get_time, refresh_per_second=refresh_per_second or 10, disable=disable)\n    reader = progress.open(file, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, total=total, description=description)\n    return _ReadContext(progress, reader)",
    "label": true
  },
  {
    "code": "def _cmpkey(epoch: int, release: Tuple[int, ...], pre: Optional[Tuple[str, int]], post: Optional[Tuple[str, int]], dev: Optional[Tuple[str, int]], local: Optional[Tuple[SubLocalType]]) -> CmpKey:\n    _release = tuple(reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release)))))\n    if pre is None and post is None and (dev is not None):\n        _pre: PrePostDevType = NegativeInfinity\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n    if post is None:\n        _post: PrePostDevType = NegativeInfinity\n    else:\n        _post = post\n    if dev is None:\n        _dev: PrePostDevType = Infinity\n    else:\n        _dev = dev\n    if local is None:\n        _local: LocalType = NegativeInfinity\n    else:\n        _local = tuple(((i, '') if isinstance(i, int) else (NegativeInfinity, i) for i in local))\n    return (epoch, _release, _pre, _post, _dev, _local)",
    "label": true
  },
  {
    "code": "def install_editable(*, global_options: Sequence[str], prefix: Optional[str], home: Optional[str], use_user_site: bool, name: str, setup_py_path: str, isolated: bool, build_env: BuildEnvironment, unpacked_source_directory: str) -> None:\n    logger.info('Running setup.py develop for %s', name)\n    args = make_setuptools_develop_args(setup_py_path, global_options=global_options, no_user_config=isolated, prefix=prefix, home=home, use_user_site=use_user_site)\n    with indent_log():\n        with build_env:\n            call_subprocess(args, command_desc='python setup.py develop', cwd=unpacked_source_directory)",
    "label": true
  },
  {
    "code": "def open_if_exists(filename: str, mode: str='rb') -> t.Optional[t.IO]:\n    if not os.path.isfile(filename):\n        return None\n    return open(filename, mode)",
    "label": true
  },
  {
    "code": "def run_setup(script_name, script_args=None, stop_after='run'):\n    if stop_after not in ('init', 'config', 'commandline', 'run'):\n        raise ValueError(\"invalid value for 'stop_after': {!r}\".format(stop_after))\n    global _setup_stop_after, _setup_distribution\n    _setup_stop_after = stop_after\n    save_argv = sys.argv.copy()\n    g = {'__file__': script_name, '__name__': '__main__'}\n    try:\n        try:\n            sys.argv[0] = script_name\n            if script_args is not None:\n                sys.argv[1:] = script_args\n            with tokenize.open(script_name) as f:\n                code = f.read().replace('\\\\r\\\\n', '\\\\n')\n                exec(code, g)\n        finally:\n            sys.argv = save_argv\n            _setup_stop_after = None\n    except SystemExit:\n        pass\n    if _setup_distribution is None:\n        raise RuntimeError(\"'distutils.core.setup()' was never called -- perhaps '%s' is not a Distutils setup script?\" % script_name)\n    return _setup_distribution",
    "label": true
  },
  {
    "code": "def _handle_get_simple_fail(link: Link, reason: Union[str, Exception], meth: Optional[Callable[..., None]]=None) -> None:\n    if meth is None:\n        meth = logger.debug\n    meth('Could not fetch URL %s: %s - skipping', link, reason)",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def doc(msg_id: str) -> None:\n    msg_url = HELP_URL + '#' + msg_id.lower()\n    print('Opening {} in a browser.'.format(msg_url))\n    webbrowser.open(msg_url)",
    "label": true
  },
  {
    "code": "def iter_except(func, exception, first=None):\n    try:\n        if first is not None:\n            yield first()\n        while 1:\n            yield func()\n    except exception:\n        pass",
    "label": true
  },
  {
    "code": "def get_cookie_header(jar, request):\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get('Cookie')",
    "label": true
  },
  {
    "code": "def _generic_abi() -> List[str]:\n    ext_suffix = _get_config_var('EXT_SUFFIX', warn=True)\n    if not isinstance(ext_suffix, str) or ext_suffix[0] != '.':\n        raise SystemError(\"invalid sysconfig.get_config_var('EXT_SUFFIX')\")\n    parts = ext_suffix.split('.')\n    if len(parts) < 3:\n        return _cpython_abis(sys.version_info[:2])\n    soabi = parts[1]\n    if soabi.startswith('cpython'):\n        abi = 'cp' + soabi.split('-')[1]\n    elif soabi.startswith('cp'):\n        abi = soabi.split('-')[0]\n    elif soabi.startswith('pypy'):\n        abi = '-'.join(soabi.split('-')[:2])\n    elif soabi.startswith('graalpy'):\n        abi = '-'.join(soabi.split('-')[:3])\n    elif soabi:\n        abi = soabi\n    else:\n        return []\n    return [_normalize_string(abi)]",
    "label": true
  },
  {
    "code": "def easy_class_getitem_inference(node, context: InferenceContext | None=None):\n    func_to_add = extract_node(CLASS_GET_ITEM_TEMPLATE)\n    node.locals['__class_getitem__'] = [func_to_add]",
    "label": true
  },
  {
    "code": "def get_codes(tree: HuffmanTree) -> dict[int, str]:\n    if tree.is_leaf():\n        return {}\n    return _get_codes_helper(tree, '')",
    "label": true
  },
  {
    "code": "def poll_wait_for_socket(sock, read=False, write=False, timeout=None):\n    if not read and (not write):\n        raise RuntimeError('must specify at least one of read=True, write=True')\n    mask = 0\n    if read:\n        mask |= select.POLLIN\n    if write:\n        mask |= select.POLLOUT\n    poll_obj = select.poll()\n    poll_obj.register(sock, mask)\n\n    def do_poll(t):\n        if t is not None:\n            t *= 1000\n        return poll_obj.poll(t)\n    return bool(_retry_on_intr(do_poll, timeout))",
    "label": true
  },
  {
    "code": "def _dev_pkgs() -> AbstractSet[str]:\n    pkgs = {'pip'}\n    if _should_suppress_build_backends():\n        pkgs |= {'setuptools', 'distribute', 'wheel'}\n    return pkgs",
    "label": true
  },
  {
    "code": "def compose(*funcs):\n\n    def compose_two(f1, f2):\n        return lambda *args, **kwargs: f1(f2(*args, **kwargs))\n    return functools.reduce(compose_two, funcs)",
    "label": true
  },
  {
    "code": "def get_all_elements(node: nodes.NodeNG) -> Iterable[nodes.NodeNG]:\n    if isinstance(node, (nodes.Tuple, nodes.List)):\n        for child in node.elts:\n            yield from get_all_elements(child)\n    else:\n        yield node",
    "label": true
  },
  {
    "code": "def ratio_resolve(total: int, edges: Sequence[Edge]) -> List[int]:\n    sizes = [edge.size or None for edge in edges]\n    _Fraction = Fraction\n    while None in sizes:\n        flexible_edges = [(index, edge) for index, (size, edge) in enumerate(zip(sizes, edges)) if size is None]\n        remaining = total - sum((size or 0 for size in sizes))\n        if remaining <= 0:\n            return [edge.minimum_size or 1 if size is None else size for size, edge in zip(sizes, edges)]\n        portion = _Fraction(remaining, sum((edge.ratio or 1 for _, edge in flexible_edges)))\n        for index, edge in flexible_edges:\n            if portion * edge.ratio <= edge.minimum_size:\n                sizes[index] = edge.minimum_size\n                break\n        else:\n            remainder = _Fraction(0)\n            for index, edge in flexible_edges:\n                size, remainder = divmod(portion * edge.ratio + remainder, 1)\n                sizes[index] = size\n            break\n    return cast(List[int], sizes)",
    "label": true
  },
  {
    "code": "def format(tokens, formatter, outfile=None):\n    try:\n        if not outfile:\n            realoutfile = getattr(formatter, 'encoding', None) and BytesIO() or StringIO()\n            formatter.format(tokens, realoutfile)\n            return realoutfile.getvalue()\n        else:\n            formatter.format(tokens, outfile)\n    except TypeError:\n        from pip._vendor.pygments.formatter import Formatter\n        if isinstance(formatter, type) and issubclass(formatter, Formatter):\n            raise TypeError('format() argument must be a formatter instance, not a class')\n        raise",
    "label": true
  },
  {
    "code": "def get_current_context(silent: bool=False) -> t.Optional['Context']:\n    try:\n        return t.cast('Context', _local.stack[-1])\n    except (AttributeError, IndexError) as e:\n        if not silent:\n            raise RuntimeError('There is no active click context.') from e\n    return None",
    "label": true
  },
  {
    "code": "def _getcommand():\n    var = (('a', 'a'), ('ab', 'ab'), ('abc', 'abclear'), ('abo', 'aboveleft'), ('al', 'all'), ('ar', 'ar'), ('ar', 'args'), ('arga', 'argadd'), ('argd', 'argdelete'), ('argdo', 'argdo'), ('arge', 'argedit'), ('argg', 'argglobal'), ('argl', 'arglocal'), ('argu', 'argument'), ('as', 'ascii'), ('au', 'au'), ('b', 'buffer'), ('bN', 'bNext'), ('ba', 'ball'), ('bad', 'badd'), ('bd', 'bdelete'), ('bel', 'belowright'), ('bf', 'bfirst'), ('bl', 'blast'), ('bm', 'bmodified'), ('bn', 'bnext'), ('bo', 'botright'), ('bp', 'bprevious'), ('br', 'br'), ('br', 'brewind'), ('brea', 'break'), ('breaka', 'breakadd'), ('breakd', 'breakdel'), ('breakl', 'breaklist'), ('bro', 'browse'), ('bu', 'bu'), ('buf', 'buf'), ('bufdo', 'bufdo'), ('buffers', 'buffers'), ('bun', 'bunload'), ('bw', 'bwipeout'), ('c', 'c'), ('c', 'change'), ('cN', 'cN'), ('cN', 'cNext'), ('cNf', 'cNf'), ('cNf', 'cNfile'), ('cabc', 'cabclear'), ('cad', 'cad'), ('cad', 'caddexpr'), ('caddb', 'caddbuffer'), ('caddf', 'caddfile'), ('cal', 'call'), ('cat', 'catch'), ('cb', 'cbuffer'), ('cc', 'cc'), ('ccl', 'cclose'), ('cd', 'cd'), ('ce', 'center'), ('cex', 'cexpr'), ('cf', 'cfile'), ('cfir', 'cfirst'), ('cg', 'cgetfile'), ('cgetb', 'cgetbuffer'), ('cgete', 'cgetexpr'), ('changes', 'changes'), ('chd', 'chdir'), ('che', 'checkpath'), ('checkt', 'checktime'), ('cl', 'cl'), ('cl', 'clist'), ('cla', 'clast'), ('clo', 'close'), ('cmapc', 'cmapclear'), ('cn', 'cn'), ('cn', 'cnext'), ('cnew', 'cnewer'), ('cnf', 'cnf'), ('cnf', 'cnfile'), ('co', 'copy'), ('col', 'colder'), ('colo', 'colorscheme'), ('com', 'com'), ('comc', 'comclear'), ('comp', 'compiler'), ('con', 'con'), ('con', 'continue'), ('conf', 'confirm'), ('cope', 'copen'), ('cp', 'cprevious'), ('cpf', 'cpfile'), ('cq', 'cquit'), ('cr', 'crewind'), ('cs', 'cs'), ('cscope', 'cscope'), ('cstag', 'cstag'), ('cuna', 'cunabbrev'), ('cw', 'cwindow'), ('d', 'd'), ('d', 'delete'), ('de', 'de'), ('debug', 'debug'), ('debugg', 'debuggreedy'), ('del', 'del'), ('delc', 'delcommand'), ('delel', 'delel'), ('delep', 'delep'), ('deletel', 'deletel'), ('deletep', 'deletep'), ('deletl', 'deletl'), ('deletp', 'deletp'), ('delf', 'delf'), ('delf', 'delfunction'), ('dell', 'dell'), ('delm', 'delmarks'), ('delp', 'delp'), ('dep', 'dep'), ('di', 'di'), ('di', 'display'), ('diffg', 'diffget'), ('diffo', 'diffoff'), ('diffp', 'diffpatch'), ('diffpu', 'diffput'), ('diffs', 'diffsplit'), ('difft', 'diffthis'), ('diffu', 'diffupdate'), ('dig', 'dig'), ('dig', 'digraphs'), ('dir', 'dir'), ('dj', 'djump'), ('dl', 'dl'), ('dli', 'dlist'), ('do', 'do'), ('doau', 'doau'), ('dp', 'dp'), ('dr', 'drop'), ('ds', 'dsearch'), ('dsp', 'dsplit'), ('e', 'e'), ('e', 'edit'), ('ea', 'ea'), ('earlier', 'earlier'), ('ec', 'ec'), ('echoe', 'echoerr'), ('echom', 'echomsg'), ('echon', 'echon'), ('el', 'else'), ('elsei', 'elseif'), ('em', 'emenu'), ('en', 'en'), ('en', 'endif'), ('endf', 'endf'), ('endf', 'endfunction'), ('endfo', 'endfor'), ('endfun', 'endfun'), ('endt', 'endtry'), ('endw', 'endwhile'), ('ene', 'enew'), ('ex', 'ex'), ('exi', 'exit'), ('exu', 'exusage'), ('f', 'f'), ('f', 'file'), ('files', 'files'), ('filet', 'filet'), ('filetype', 'filetype'), ('fin', 'fin'), ('fin', 'find'), ('fina', 'finally'), ('fini', 'finish'), ('fir', 'first'), ('fix', 'fixdel'), ('fo', 'fold'), ('foldc', 'foldclose'), ('foldd', 'folddoopen'), ('folddoc', 'folddoclosed'), ('foldo', 'foldopen'), ('for', 'for'), ('fu', 'fu'), ('fu', 'function'), ('fun', 'fun'), ('g', 'g'), ('go', 'goto'), ('gr', 'grep'), ('grepa', 'grepadd'), ('gui', 'gui'), ('gvim', 'gvim'), ('h', 'h'), ('h', 'help'), ('ha', 'hardcopy'), ('helpf', 'helpfind'), ('helpg', 'helpgrep'), ('helpt', 'helptags'), ('hi', 'hi'), ('hid', 'hide'), ('his', 'history'), ('i', 'i'), ('ia', 'ia'), ('iabc', 'iabclear'), ('if', 'if'), ('ij', 'ijump'), ('il', 'ilist'), ('imapc', 'imapclear'), ('in', 'in'), ('intro', 'intro'), ('is', 'isearch'), ('isp', 'isplit'), ('iuna', 'iunabbrev'), ('j', 'join'), ('ju', 'jumps'), ('k', 'k'), ('kee', 'keepmarks'), ('keepa', 'keepa'), ('keepalt', 'keepalt'), ('keepj', 'keepjumps'), ('keepp', 'keeppatterns'), ('l', 'l'), ('l', 'list'), ('lN', 'lN'), ('lN', 'lNext'), ('lNf', 'lNf'), ('lNf', 'lNfile'), ('la', 'la'), ('la', 'last'), ('lad', 'lad'), ('lad', 'laddexpr'), ('laddb', 'laddbuffer'), ('laddf', 'laddfile'), ('lan', 'lan'), ('lan', 'language'), ('lat', 'lat'), ('later', 'later'), ('lb', 'lbuffer'), ('lc', 'lcd'), ('lch', 'lchdir'), ('lcl', 'lclose'), ('lcs', 'lcs'), ('lcscope', 'lcscope'), ('le', 'left'), ('lefta', 'leftabove'), ('lex', 'lexpr'), ('lf', 'lfile'), ('lfir', 'lfirst'), ('lg', 'lgetfile'), ('lgetb', 'lgetbuffer'), ('lgete', 'lgetexpr'), ('lgr', 'lgrep'), ('lgrepa', 'lgrepadd'), ('lh', 'lhelpgrep'), ('ll', 'll'), ('lla', 'llast'), ('lli', 'llist'), ('lmak', 'lmake'), ('lmapc', 'lmapclear'), ('lne', 'lne'), ('lne', 'lnext'), ('lnew', 'lnewer'), ('lnf', 'lnf'), ('lnf', 'lnfile'), ('lo', 'lo'), ('lo', 'loadview'), ('loadk', 'loadk'), ('loadkeymap', 'loadkeymap'), ('loc', 'lockmarks'), ('lockv', 'lockvar'), ('lol', 'lolder'), ('lop', 'lopen'), ('lp', 'lprevious'), ('lpf', 'lpfile'), ('lr', 'lrewind'), ('ls', 'ls'), ('lt', 'ltag'), ('lua', 'lua'), ('luado', 'luado'), ('luafile', 'luafile'), ('lv', 'lvimgrep'), ('lvimgrepa', 'lvimgrepadd'), ('lw', 'lwindow'), ('m', 'move'), ('ma', 'ma'), ('ma', 'mark'), ('mak', 'make'), ('marks', 'marks'), ('mat', 'match'), ('menut', 'menut'), ('menut', 'menutranslate'), ('mes', 'mes'), ('messages', 'messages'), ('mk', 'mk'), ('mk', 'mkexrc'), ('mks', 'mksession'), ('mksp', 'mkspell'), ('mkv', 'mkv'), ('mkv', 'mkvimrc'), ('mkvie', 'mkview'), ('mo', 'mo'), ('mod', 'mode'), ('mz', 'mz'), ('mz', 'mzscheme'), ('mzf', 'mzfile'), ('n', 'n'), ('n', 'next'), ('nb', 'nbkey'), ('nbc', 'nbclose'), ('nbs', 'nbstart'), ('ne', 'ne'), ('new', 'new'), ('nmapc', 'nmapclear'), ('noa', 'noa'), ('noautocmd', 'noautocmd'), ('noh', 'nohlsearch'), ('nu', 'number'), ('o', 'o'), ('o', 'open'), ('ol', 'oldfiles'), ('omapc', 'omapclear'), ('on', 'only'), ('opt', 'options'), ('ownsyntax', 'ownsyntax'), ('p', 'p'), ('p', 'print'), ('pc', 'pclose'), ('pe', 'pe'), ('pe', 'perl'), ('ped', 'pedit'), ('perld', 'perldo'), ('po', 'pop'), ('popu', 'popu'), ('popu', 'popup'), ('pp', 'ppop'), ('pr', 'pr'), ('pre', 'preserve'), ('prev', 'previous'), ('pro', 'pro'), ('prof', 'profile'), ('profd', 'profdel'), ('promptf', 'promptfind'), ('promptr', 'promptrepl'), ('ps', 'psearch'), ('ptN', 'ptN'), ('ptN', 'ptNext'), ('pta', 'ptag'), ('ptf', 'ptfirst'), ('ptj', 'ptjump'), ('ptl', 'ptlast'), ('ptn', 'ptn'), ('ptn', 'ptnext'), ('ptp', 'ptprevious'), ('ptr', 'ptrewind'), ('pts', 'ptselect'), ('pu', 'put'), ('pw', 'pwd'), ('py', 'py'), ('py', 'python'), ('py3', 'py3'), ('py3', 'py3'), ('py3do', 'py3do'), ('pydo', 'pydo'), ('pyf', 'pyfile'), ('python3', 'python3'), ('q', 'q'), ('q', 'quit'), ('qa', 'qall'), ('quita', 'quitall'), ('r', 'r'), ('r', 'read'), ('re', 're'), ('rec', 'recover'), ('red', 'red'), ('red', 'redo'), ('redi', 'redir'), ('redr', 'redraw'), ('redraws', 'redrawstatus'), ('reg', 'registers'), ('res', 'resize'), ('ret', 'retab'), ('retu', 'return'), ('rew', 'rewind'), ('ri', 'right'), ('rightb', 'rightbelow'), ('ru', 'ru'), ('ru', 'runtime'), ('rub', 'ruby'), ('rubyd', 'rubydo'), ('rubyf', 'rubyfile'), ('rundo', 'rundo'), ('rv', 'rviminfo'), ('sN', 'sNext'), ('sa', 'sargument'), ('sal', 'sall'), ('san', 'sandbox'), ('sav', 'saveas'), ('sb', 'sbuffer'), ('sbN', 'sbNext'), ('sba', 'sball'), ('sbf', 'sbfirst'), ('sbl', 'sblast'), ('sbm', 'sbmodified'), ('sbn', 'sbnext'), ('sbp', 'sbprevious'), ('sbr', 'sbrewind'), ('scrip', 'scrip'), ('scrip', 'scriptnames'), ('scripte', 'scriptencoding'), ('scs', 'scs'), ('scscope', 'scscope'), ('se', 'set'), ('setf', 'setfiletype'), ('setg', 'setglobal'), ('setl', 'setlocal'), ('sf', 'sfind'), ('sfir', 'sfirst'), ('sh', 'shell'), ('si', 'si'), ('sig', 'sig'), ('sign', 'sign'), ('sil', 'silent'), ('sim', 'simalt'), ('sl', 'sl'), ('sl', 'sleep'), ('sla', 'slast'), ('sm', 'smagic'), ('sm', 'smap'), ('sme', 'sme'), ('smenu', 'smenu'), ('sn', 'snext'), ('sni', 'sniff'), ('sno', 'snomagic'), ('snoreme', 'snoreme'), ('snoremenu', 'snoremenu'), ('so', 'so'), ('so', 'source'), ('sor', 'sort'), ('sp', 'split'), ('spe', 'spe'), ('spe', 'spellgood'), ('spelld', 'spelldump'), ('spelli', 'spellinfo'), ('spellr', 'spellrepall'), ('spellu', 'spellundo'), ('spellw', 'spellwrong'), ('spr', 'sprevious'), ('sre', 'srewind'), ('st', 'st'), ('st', 'stop'), ('sta', 'stag'), ('star', 'star'), ('star', 'startinsert'), ('start', 'start'), ('startg', 'startgreplace'), ('startr', 'startreplace'), ('stj', 'stjump'), ('stopi', 'stopinsert'), ('sts', 'stselect'), ('sun', 'sunhide'), ('sunme', 'sunme'), ('sunmenu', 'sunmenu'), ('sus', 'suspend'), ('sv', 'sview'), ('sw', 'swapname'), ('sy', 'sy'), ('syn', 'syn'), ('sync', 'sync'), ('syncbind', 'syncbind'), ('syntime', 'syntime'), ('t', 't'), ('tN', 'tN'), ('tN', 'tNext'), ('ta', 'ta'), ('ta', 'tag'), ('tab', 'tab'), ('tabN', 'tabN'), ('tabN', 'tabNext'), ('tabc', 'tabclose'), ('tabd', 'tabdo'), ('tabe', 'tabedit'), ('tabf', 'tabfind'), ('tabfir', 'tabfirst'), ('tabl', 'tablast'), ('tabm', 'tabmove'), ('tabn', 'tabnext'), ('tabnew', 'tabnew'), ('tabo', 'tabonly'), ('tabp', 'tabprevious'), ('tabr', 'tabrewind'), ('tabs', 'tabs'), ('tags', 'tags'), ('tc', 'tcl'), ('tcld', 'tcldo'), ('tclf', 'tclfile'), ('te', 'tearoff'), ('tf', 'tfirst'), ('th', 'throw'), ('tj', 'tjump'), ('tl', 'tlast'), ('tm', 'tm'), ('tm', 'tmenu'), ('tn', 'tn'), ('tn', 'tnext'), ('to', 'topleft'), ('tp', 'tprevious'), ('tr', 'tr'), ('tr', 'trewind'), ('try', 'try'), ('ts', 'tselect'), ('tu', 'tu'), ('tu', 'tunmenu'), ('u', 'u'), ('u', 'undo'), ('un', 'un'), ('una', 'unabbreviate'), ('undoj', 'undojoin'), ('undol', 'undolist'), ('unh', 'unhide'), ('unl', 'unl'), ('unlo', 'unlockvar'), ('uns', 'unsilent'), ('up', 'update'), ('v', 'v'), ('ve', 've'), ('ve', 'version'), ('verb', 'verbose'), ('vert', 'vertical'), ('vi', 'vi'), ('vi', 'visual'), ('vie', 'view'), ('vim', 'vimgrep'), ('vimgrepa', 'vimgrepadd'), ('viu', 'viusage'), ('vmapc', 'vmapclear'), ('vne', 'vnew'), ('vs', 'vsplit'), ('w', 'w'), ('w', 'write'), ('wN', 'wNext'), ('wa', 'wall'), ('wh', 'while'), ('win', 'win'), ('win', 'winsize'), ('winc', 'wincmd'), ('windo', 'windo'), ('winp', 'winpos'), ('wn', 'wnext'), ('wp', 'wprevious'), ('wq', 'wq'), ('wqa', 'wqall'), ('ws', 'wsverb'), ('wundo', 'wundo'), ('wv', 'wviminfo'), ('x', 'x'), ('x', 'xit'), ('xa', 'xall'), ('xmapc', 'xmapclear'), ('xme', 'xme'), ('xmenu', 'xmenu'), ('xnoreme', 'xnoreme'), ('xnoremenu', 'xnoremenu'), ('xunme', 'xunme'), ('xunmenu', 'xunmenu'), ('xwininfo', 'xwininfo'), ('y', 'yank'))\n    return var",
    "label": true
  },
  {
    "code": "def load(__fp: BinaryIO, *, parse_float: ParseFloat=float) -> dict[str, Any]:\n    b = __fp.read()\n    try:\n        s = b.decode()\n    except AttributeError:\n        raise TypeError(\"File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`\") from None\n    return loads(s, parse_float=parse_float)",
    "label": true
  },
  {
    "code": "def urldefragauth(url):\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    if not netloc:\n        netloc, path = (path, netloc)\n    netloc = netloc.rsplit('@', 1)[-1]\n    return urlunparse((scheme, netloc, path, params, query, ''))",
    "label": true
  },
  {
    "code": "def make_dist(name, version, **kwargs):\n    summary = kwargs.pop('summary', 'Placeholder for summary')\n    md = Metadata(**kwargs)\n    md.name = name\n    md.version = version\n    md.summary = summary or 'Placeholder for summary'\n    return Distribution(md)",
    "label": true
  },
  {
    "code": "def infer_len(node, context: InferenceContext | None=None):\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault('TypeError: len() must take no keyword arguments')\n    if len(call.positional_arguments) != 1:\n        raise UseInferenceDefault('TypeError: len() must take exactly one argument ({len}) given'.format(len=len(call.positional_arguments)))\n    [argument_node] = call.positional_arguments\n    try:\n        return nodes.Const(helpers.object_len(argument_node, context=context))\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc",
    "label": true
  },
  {
    "code": "def pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:\n    iterable = iter(iterable)\n    return zip_longest(iterable, iterable)",
    "label": true
  },
  {
    "code": "def _glibc_version_string_confstr() -> Optional[str]:\n    try:\n        version_string: str = getattr(os, 'confstr')('CS_GNU_LIBC_VERSION')\n        assert version_string is not None\n        _, version = version_string.rsplit()\n    except (AssertionError, AttributeError, OSError, ValueError):\n        return None\n    return version",
    "label": true
  },
  {
    "code": "def newer_pairwise(sources, targets):\n    if len(sources) != len(targets):\n        raise ValueError(\"'sources' and 'targets' must be same length\")\n    n_sources = []\n    n_targets = []\n    for i in range(len(sources)):\n        if newer(sources[i], targets[i]):\n            n_sources.append(sources[i])\n            n_targets.append(targets[i])\n    return (n_sources, n_targets)",
    "label": true
  },
  {
    "code": "def _is_compatible(arch: str, version: _GLibCVersion) -> bool:\n    sys_glibc = _get_glibc_version()\n    if sys_glibc < version:\n        return False\n    try:\n        import _manylinux\n    except ImportError:\n        return True\n    if hasattr(_manylinux, 'manylinux_compatible'):\n        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n        if result is not None:\n            return bool(result)\n        return True\n    if version == _GLibCVersion(2, 5):\n        if hasattr(_manylinux, 'manylinux1_compatible'):\n            return bool(_manylinux.manylinux1_compatible)\n    if version == _GLibCVersion(2, 12):\n        if hasattr(_manylinux, 'manylinux2010_compatible'):\n            return bool(_manylinux.manylinux2010_compatible)\n    if version == _GLibCVersion(2, 17):\n        if hasattr(_manylinux, 'manylinux2014_compatible'):\n            return bool(_manylinux.manylinux2014_compatible)\n    return True",
    "label": true
  },
  {
    "code": "def _render_segments(segments: Iterable[Segment]) -> str:\n\n    def escape(text: str) -> str:\n        \"\"\"Escape html.\"\"\"\n        return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n    fragments: List[str] = []\n    append_fragment = fragments.append\n    theme = DEFAULT_TERMINAL_THEME\n    for text, style, control in Segment.simplify(segments):\n        if control:\n            continue\n        text = escape(text)\n        if style:\n            rule = style.get_html_style(theme)\n            text = f'<span style=\"{rule}\">{text}</span>' if rule else text\n            if style.link:\n                text = f'<a href=\"{style.link}\" target=\"_blank\">{text}</a>'\n        append_fragment(text)\n    code = ''.join(fragments)\n    html = JUPYTER_HTML_FORMAT.format(code=code)\n    return html",
    "label": true
  },
  {
    "code": "def unpack_f(args):\n    from .unpack import unpack\n    unpack(args.wheelfile, args.dest)",
    "label": true
  },
  {
    "code": "def _infer_instance_from_annotation(node: nodes.NodeNG, ctx: context.InferenceContext | None=None) -> Iterator[UninferableBase | bases.Instance]:\n    klass = None\n    try:\n        klass = next(node.infer(context=ctx))\n    except (InferenceError, StopIteration):\n        yield Uninferable\n    if not isinstance(klass, nodes.ClassDef):\n        yield Uninferable\n    elif klass.root().name in {'typing', '_collections_abc', ''}:\n        if klass.name in _INFERABLE_TYPING_TYPES:\n            yield klass.instantiate_class()\n        else:\n            yield Uninferable\n    else:\n        yield klass.instantiate_class()",
    "label": true
  },
  {
    "code": "def has_leading_dir(paths: Iterable[str]) -> bool:\n    common_prefix = None\n    for path in paths:\n        prefix, rest = split_leading_dir(path)\n        if not prefix:\n            return False\n        elif common_prefix is None:\n            common_prefix = prefix\n        elif prefix != common_prefix:\n            return False\n    return True",
    "label": true
  },
  {
    "code": "def _find_binary_writer(stream: t.IO[t.Any]) -> t.Optional[t.BinaryIO]:\n    if _is_binary_writer(stream, False):\n        return t.cast(t.BinaryIO, stream)\n    buf = getattr(stream, 'buffer', None)\n    if buf is not None and _is_binary_writer(buf, True):\n        return t.cast(t.BinaryIO, buf)\n    return None",
    "label": true
  },
  {
    "code": "def is_ancestor_name(frame: nodes.ClassDef, node: nodes.NodeNG) -> bool:\n    if not isinstance(frame, nodes.ClassDef):\n        return False\n    return any((node in base.nodes_of_class(nodes.Name) for base in frame.bases))",
    "label": true
  },
  {
    "code": "def is_valid_seat(ticket: str) -> bool:\n    seat = int(ticket[SEAT:SEAT + 2])\n    return 30 >= seat >= 1 and ticket[SEAT + 2] in 'ABCDEF'",
    "label": true
  },
  {
    "code": "def _is_allowed_assignment(statement) -> bool:\n    if not isinstance(statement, nodes.Assign):\n        return False\n    names = []\n    for target in statement.targets:\n        names.extend((node.name for node in target.nodes_of_class(nodes.AssignName, nodes.Name)))\n    return all((re.match(UpperCaseStyle.CONST_NAME_RGX, name) or re.match(DEFAULT_PATTERNS['typealias'], name) for name in names))",
    "label": true
  },
  {
    "code": "def _split_aix(cmd):\n    pivot = os.path.basename(cmd[0]) == 'ld_so_aix'\n    return (cmd[:pivot], cmd[pivot:])",
    "label": true
  },
  {
    "code": "def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:\n    segments = []\n    try:\n        segments.append(cb.__qualname__)\n    except AttributeError:\n        try:\n            segments.append(cb.__name__)\n        except AttributeError:\n            pass\n    if not segments:\n        return repr(cb)\n    else:\n        try:\n            if cb.__module__:\n                segments.insert(0, cb.__module__)\n        except AttributeError:\n            pass\n        return '.'.join(segments)",
    "label": true
  },
  {
    "code": "def _flattened_scope_names(iterator: Iterator[nodes.Global | nodes.Nonlocal]) -> set[str]:\n    values = (set(stmt.names) for stmt in iterator)\n    return set(itertools.chain.from_iterable(values))",
    "label": true
  },
  {
    "code": "def _spec_from_modpath(modpath: list[str], path: Sequence[str] | None=None, context: str | None=None) -> spec.ModuleSpec:\n    assert modpath\n    location = None\n    if context is not None:\n        try:\n            found_spec = spec.find_spec(modpath, [context])\n            location = found_spec.location\n        except ImportError:\n            found_spec = spec.find_spec(modpath, path)\n            location = found_spec.location\n    else:\n        found_spec = spec.find_spec(modpath, path)\n    if found_spec.type == spec.ModuleType.PY_COMPILED:\n        try:\n            assert found_spec.location is not None\n            location = get_source_file(found_spec.location)\n            return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n        except NoSourceFile:\n            return found_spec._replace(location=location)\n    elif found_spec.type == spec.ModuleType.C_BUILTIN:\n        return found_spec._replace(location=None)\n    elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:\n        assert found_spec.location is not None\n        location = _has_init(found_spec.location)\n        return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n    return found_spec",
    "label": true
  },
  {
    "code": "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    tokenizer.consume('WS')\n    name_token = tokenizer.expect('IDENTIFIER', expected='package name at the start of dependency specifier')\n    name = name_token.text\n    tokenizer.consume('WS')\n    extras = _parse_extras(tokenizer)\n    tokenizer.consume('WS')\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n    tokenizer.expect('END', expected='end of dependency specifier')\n    return ParsedRequirement(name, url, extras, specifier, marker)",
    "label": true
  },
  {
    "code": "def deprecated(func):\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        warnings.warn(f'{func.__name__} is deprecated. Use files() instead. Refer to https://importlib-resources.readthedocs.io/en/latest/using.html#migrating-from-legacy for migration advice.', DeprecationWarning, stacklevel=2)\n        return func(*args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def should_do_markup(file: TextIO) -> bool:\n    if os.environ.get('PY_COLORS') == '1':\n        return True\n    if os.environ.get('PY_COLORS') == '0':\n        return False\n    if 'NO_COLOR' in os.environ:\n        return False\n    if 'FORCE_COLOR' in os.environ:\n        return True\n    return hasattr(file, 'isatty') and file.isatty() and (os.environ.get('TERM') != 'dumb')",
    "label": true
  },
  {
    "code": "def _build_one_inside_env(req: InstallRequirement, output_dir: str, build_options: List[str], global_options: List[str], editable: bool) -> Optional[str]:\n    with TempDirectory(kind='wheel') as temp_dir:\n        assert req.name\n        if req.use_pep517:\n            assert req.metadata_directory\n            assert req.pep517_backend\n            if global_options:\n                logger.warning('Ignoring --global-option when building %s using PEP 517', req.name)\n            if build_options:\n                logger.warning('Ignoring --build-option when building %s using PEP 517', req.name)\n            if editable:\n                wheel_path = build_wheel_editable(name=req.name, backend=req.pep517_backend, metadata_directory=req.metadata_directory, tempd=temp_dir.path)\n            else:\n                wheel_path = build_wheel_pep517(name=req.name, backend=req.pep517_backend, metadata_directory=req.metadata_directory, tempd=temp_dir.path)\n        else:\n            wheel_path = build_wheel_legacy(name=req.name, setup_py_path=req.setup_py_path, source_dir=req.unpacked_source_directory, global_options=global_options, build_options=build_options, tempd=temp_dir.path)\n        if wheel_path is not None:\n            wheel_name = os.path.basename(wheel_path)\n            dest_path = os.path.join(output_dir, wheel_name)\n            try:\n                wheel_hash, length = hash_file(wheel_path)\n                shutil.move(wheel_path, dest_path)\n                logger.info('Created wheel for %s: filename=%s size=%d sha256=%s', req.name, wheel_name, length, wheel_hash.hexdigest())\n                logger.info('Stored in directory: %s', output_dir)\n                return dest_path\n            except Exception as e:\n                logger.warning('Building wheel for %s failed: %s', req.name, e)\n        if not req.use_pep517:\n            _clean_one_legacy(req, global_options)\n        return None",
    "label": true
  },
  {
    "code": "def _create_lock(locked, *args):\n    from threading import Lock\n    lock = Lock()\n    if locked:\n        if not lock.acquire(False):\n            raise UnpicklingError('Cannot acquire lock')\n    return lock",
    "label": true
  },
  {
    "code": "def _is_linux_i686() -> bool:\n    elf_header = _get_elf_header()\n    if elf_header is None:\n        return False\n    result = elf_header.e_ident_class == elf_header.ELFCLASS32\n    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB\n    result &= elf_header.e_machine == elf_header.EM_386\n    return result",
    "label": true
  },
  {
    "code": "def _asciidoc_row(is_header, *args):\n\n    def make_header_line(is_header, colwidths, colaligns):\n        alignment = {'left': '<', 'right': '>', 'center': '^', 'decimal': '>'}\n        asciidoc_alignments = zip(colwidths, [alignment[colalign] for colalign in colaligns])\n        asciidoc_column_specifiers = ['{:d}{}'.format(width, align) for width, align in asciidoc_alignments]\n        header_list = ['cols=\"' + ','.join(asciidoc_column_specifiers) + '\"']\n        options_list = []\n        if is_header:\n            options_list.append('header')\n        if options_list:\n            header_list += ['options=\"' + ','.join(options_list) + '\"']\n        return '[{}]\\n|===='.format(','.join(header_list))\n    if len(args) == 2:\n        return make_header_line(False, *args)\n    elif len(args) == 3:\n        cell_values, colwidths, colaligns = args\n        data_line = '|' + '|'.join(cell_values)\n        if is_header:\n            return make_header_line(True, colwidths, colaligns) + '\\n' + data_line\n        else:\n            return data_line\n    else:\n        raise ValueError(' _asciidoc_row() requires two (colwidths, colaligns) ' + 'or three (cell_values, colwidths, colaligns) arguments) ')",
    "label": true
  },
  {
    "code": "def check_externally_managed() -> None:\n    if running_under_virtualenv():\n        return\n    marker = os.path.join(sysconfig.get_path('stdlib'), 'EXTERNALLY-MANAGED')\n    if not os.path.isfile(marker):\n        return\n    raise ExternallyManagedEnvironment.from_config(marker)",
    "label": true
  },
  {
    "code": "def spy(iterable, n=1):\n    it = iter(iterable)\n    head = take(n, it)\n    return (head.copy(), chain(head, it))",
    "label": true
  },
  {
    "code": "def pytest_cmdline_main(config: Config) -> Optional[Union[int, ExitCode]]:\n    if config.option.version > 0:\n        showversion(config)\n        return 0\n    elif config.option.help:\n        config._do_configure()\n        showhelp(config)\n        config._ensure_unconfigure()\n        return 0\n    return None",
    "label": true
  },
  {
    "code": "def search_packages_info(query: List[str]) -> Generator[_PackageInfo, None, None]:\n    env = get_default_environment()\n    installed = {dist.canonical_name: dist for dist in env.iter_all_distributions()}\n    query_names = [canonicalize_name(name) for name in query]\n    missing = sorted([name for name, pkg in zip(query, query_names) if pkg not in installed])\n    if missing:\n        logger.warning('Package(s) not found: %s', ', '.join(missing))\n\n    def _get_requiring_packages(current_dist: BaseDistribution) -> Iterator[str]:\n        return (dist.metadata['Name'] or 'UNKNOWN' for dist in installed.values() if current_dist.canonical_name in {canonicalize_name(d.name) for d in dist.iter_dependencies()})\n    for query_name in query_names:\n        try:\n            dist = installed[query_name]\n        except KeyError:\n            continue\n        requires = sorted((req.name for req in dist.iter_dependencies()), key=str.lower)\n        required_by = sorted(_get_requiring_packages(dist), key=str.lower)\n        try:\n            entry_points_text = dist.read_text('entry_points.txt')\n            entry_points = entry_points_text.splitlines(keepends=False)\n        except FileNotFoundError:\n            entry_points = []\n        files_iter = dist.iter_declared_entries()\n        if files_iter is None:\n            files: Optional[List[str]] = None\n        else:\n            files = sorted(files_iter)\n        metadata = dist.metadata\n        yield _PackageInfo(name=dist.raw_name, version=str(dist.version), location=dist.location or '', editable_project_location=dist.editable_project_location, requires=requires, required_by=required_by, installer=dist.installer, metadata_version=dist.metadata_version or '', classifiers=metadata.get_all('Classifier', []), summary=metadata.get('Summary', ''), homepage=metadata.get('Home-page', ''), project_urls=metadata.get_all('Project-URL', []), author=metadata.get('Author', ''), author_email=metadata.get('Author-email', ''), license=metadata.get('License', ''), entry_points=entry_points, files=files)",
    "label": true
  },
  {
    "code": "def load_launcher_manifest(name):\n    manifest = pkg_resources.resource_string(__name__, 'launcher manifest.xml')\n    return manifest.decode('utf-8') % vars()",
    "label": true
  },
  {
    "code": "def strtobool(val: str) -> int:\n    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(f'invalid truth value {val!r}')",
    "label": true
  },
  {
    "code": "def warning_record_to_str(warning_message: warnings.WarningMessage) -> str:\n    warn_msg = warning_message.message\n    msg = warnings.formatwarning(str(warn_msg), warning_message.category, warning_message.filename, warning_message.lineno, warning_message.line)\n    if warning_message.source is not None:\n        try:\n            import tracemalloc\n        except ImportError:\n            pass\n        else:\n            tb = tracemalloc.get_object_traceback(warning_message.source)\n            if tb is not None:\n                formatted_tb = '\\n'.join(tb.format())\n                msg += f'\\nObject allocated at:\\n{formatted_tb}'\n            else:\n                url = 'https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings'\n                msg += 'Enable tracemalloc to get traceback where the object was allocated.\\n'\n                msg += f'See {url} for more info.'\n    return msg",
    "label": true
  },
  {
    "code": "def getchar(echo: bool=False) -> str:\n    global _getchar\n    if _getchar is None:\n        from ._termui_impl import getchar as f\n        _getchar = f\n    return _getchar(echo)",
    "label": true
  },
  {
    "code": "def cpython_tags(python_version: Optional[PythonVersion]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    interpreter = f'cp{_version_nodot(python_version[:2])}'\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    for explicit_abi in ('abi3', 'none'):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    if _abi3_applies(python_version):\n        yield from (Tag(interpreter, 'abi3', platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, 'none', platform_) for platform_ in platforms)\n    if _abi3_applies(python_version):\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = 'cp{version}'.format(version=_version_nodot((python_version[0], minor_version)))\n                yield Tag(interpreter, 'abi3', platform_)",
    "label": true
  },
  {
    "code": "def is_ipaddress(hostname: str | bytes) -> bool:\n    if isinstance(hostname, bytes):\n        hostname = hostname.decode('ascii')\n    return bool(_IPV4_RE.match(hostname) or _BRACELESS_IPV6_ADDRZ_RE.match(hostname))",
    "label": true
  },
  {
    "code": "def _prefix_with_indent(s: Union[Text, str], console: Console, *, prefix: str, indent: str) -> Text:\n    if isinstance(s, Text):\n        text = s\n    else:\n        text = console.render_str(s)\n    return console.render_str(prefix, overflow='ignore') + console.render_str(f'\\n{indent}', overflow='ignore').join(text.split(allow_blank=True))",
    "label": true
  },
  {
    "code": "def allowed_gai_family():\n    family = socket.AF_INET\n    if HAS_IPV6:\n        family = socket.AF_UNSPEC\n    return family",
    "label": true
  },
  {
    "code": "def check_io(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if origin_type is TextIO or (origin_type is IO and args == (str,)):\n        if not isinstance(value, TextIOBase):\n            raise TypeCheckError('is not a text based I/O object')\n    elif origin_type is BinaryIO or (origin_type is IO and args == (bytes,)):\n        if not isinstance(value, (RawIOBase, BufferedIOBase)):\n            raise TypeCheckError('is not a binary I/O object')\n    elif not isinstance(value, IOBase):\n        raise TypeCheckError('is not an I/O object')",
    "label": true
  },
  {
    "code": "def _read_list_from_msg(msg: Message, field: str) -> Optional[List[str]]:\n    values = msg.get_all(field, None)\n    if values == []:\n        return None\n    return values",
    "label": true
  },
  {
    "code": "def unquote_header_value(value, is_filename=False):\n    if value and value[0] == value[-1] == '\"':\n        value = value[1:-1]\n        if not is_filename or value[:2] != '\\\\\\\\':\n            return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n    return value",
    "label": true
  },
  {
    "code": "def _iter_decode_generator(input, decoder):\n    decode = decoder.decode\n    input = iter(input)\n    for chunck in input:\n        output = decode(chunck)\n        if output:\n            assert decoder.encoding is not None\n            yield decoder.encoding\n            yield output\n            break\n    else:\n        output = decode(b'', final=True)\n        assert decoder.encoding is not None\n        yield decoder.encoding\n        if output:\n            yield output\n        return\n    for chunck in input:\n        output = decode(chunck)\n        if output:\n            yield output\n    output = decode(b'', final=True)\n    if output:\n        yield output",
    "label": true
  },
  {
    "code": "def get_all_formatters():\n    for info in FORMATTERS.values():\n        if info[1] not in _formatter_cache:\n            _load_formatters(info[0])\n        yield _formatter_cache[info[1]]\n    for _, formatter in find_plugin_formatters():\n        yield formatter",
    "label": true
  },
  {
    "code": "def get_all_formatters():\n    for info in FORMATTERS.values():\n        if info[1] not in _formatter_cache:\n            _load_formatters(info[0])\n        yield _formatter_cache[info[1]]\n    for _, formatter in find_plugin_formatters():\n        yield formatter",
    "label": true
  },
  {
    "code": "def scheme(name):\n    if not enabled() or not name.endswith('_prefix'):\n        return name\n    return 'osx_framework_library'",
    "label": true
  },
  {
    "code": "def referrednested(func, recurse=True):\n    import gc\n    funcs = set()\n    for co in nestedcode(func, recurse):\n        for obj in gc.get_referrers(co):\n            _ = getattr(obj, '__func__', None)\n            if getattr(_, '__code__', None) is co:\n                funcs.add(obj)\n            elif getattr(obj, '__code__', None) is co:\n                funcs.add(obj)\n            elif getattr(obj, 'f_code', None) is co:\n                funcs.add(obj)\n            elif hasattr(obj, 'co_code') and obj is co:\n                funcs.add(obj)\n    return list(funcs)",
    "label": true
  },
  {
    "code": "def prepend_scheme_if_needed(url, new_scheme):\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = (path, netloc)\n    if auth:\n        netloc = '@'.join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:\n        path = ''\n    return urlunparse((scheme, netloc, path, '', query, fragment))",
    "label": true
  },
  {
    "code": "def _looks_like_namespace(node) -> bool:\n    func = node.func\n    if isinstance(func, nodes.Attribute):\n        return func.attrname == 'Namespace' and isinstance(func.expr, nodes.Name) and (func.expr.name == 'argparse')\n    return False",
    "label": true
  },
  {
    "code": "def _get_mro(cls):\n    if platform.python_implementation() == 'Jython':\n        return (cls,) + cls.__bases__\n    return inspect.getmro(cls)",
    "label": true
  },
  {
    "code": "def discover_post_import_hooks(group):\n    try:\n        import pkg_resources\n    except ImportError:\n        return\n    for entrypoint in pkg_resources.iter_entry_points(group=group):\n        callback = _create_import_hook_from_entrypoint(entrypoint)\n        register_post_import_hook(callback, entrypoint.name)",
    "label": true
  },
  {
    "code": "def num_mock_patch_args(function) -> int:\n    patchings = getattr(function, 'patchings', None)\n    if not patchings:\n        return 0\n    mock_sentinel = getattr(sys.modules.get('mock'), 'DEFAULT', object())\n    ut_mock_sentinel = getattr(sys.modules.get('unittest.mock'), 'DEFAULT', object())\n    return len([p for p in patchings if not p.attribute_name and (p.new is mock_sentinel or p.new is ut_mock_sentinel)])",
    "label": true
  },
  {
    "code": "def add_metaclass(metaclass):\n\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper",
    "label": true
  },
  {
    "code": "def _import_module(name):\n    __import__(name)\n    return sys.modules[name]",
    "label": true
  },
  {
    "code": "def _get_flag_lookup() -> Dict[str, int]:\n    import doctest\n    return dict(DONT_ACCEPT_TRUE_FOR_1=doctest.DONT_ACCEPT_TRUE_FOR_1, DONT_ACCEPT_BLANKLINE=doctest.DONT_ACCEPT_BLANKLINE, NORMALIZE_WHITESPACE=doctest.NORMALIZE_WHITESPACE, ELLIPSIS=doctest.ELLIPSIS, IGNORE_EXCEPTION_DETAIL=doctest.IGNORE_EXCEPTION_DETAIL, COMPARISON_FLAGS=doctest.COMPARISON_FLAGS, ALLOW_UNICODE=_get_allow_unicode_flag(), ALLOW_BYTES=_get_allow_bytes_flag(), NUMBER=_get_number_flag())",
    "label": true
  },
  {
    "code": "def _align_column(strings, alignment, minwidth=0, has_invisible=True, enable_widechars=False, is_multiline=False):\n    strings, padfn = _align_column_choose_padfn(strings, alignment, has_invisible)\n    width_fn = _align_column_choose_width_fn(has_invisible, enable_widechars, is_multiline)\n    s_widths = list(map(width_fn, strings))\n    maxwidth = max(max(_flat_list(s_widths)), minwidth)\n    if is_multiline:\n        if not enable_widechars and (not has_invisible):\n            padded_strings = ['\\n'.join([padfn(maxwidth, s) for s in ms.splitlines()]) for ms in strings]\n        else:\n            s_lens = [[len(s) for s in re.split('[\\r\\n]', ms)] for ms in strings]\n            visible_widths = [[maxwidth - (w - l) for w, l in zip(mw, ml)] for mw, ml in zip(s_widths, s_lens)]\n            padded_strings = ['\\n'.join([padfn(w, s) for s, w in zip(ms.splitlines() or ms, mw)]) for ms, mw in zip(strings, visible_widths)]\n    elif not enable_widechars and (not has_invisible):\n        padded_strings = [padfn(maxwidth, s) for s in strings]\n    else:\n        s_lens = list(map(len, strings))\n        visible_widths = [maxwidth - (w - l) for w, l in zip(s_widths, s_lens)]\n        padded_strings = [padfn(w, s) for s, w in zip(strings, visible_widths)]\n    return padded_strings",
    "label": true
  },
  {
    "code": "def _encode_target(target):\n    path, query = TARGET_RE.match(target).groups()\n    target = _encode_invalid_chars(path, PATH_CHARS)\n    query = _encode_invalid_chars(query, QUERY_CHARS)\n    if query is not None:\n        target += '?' + query\n    return target",
    "label": true
  },
  {
    "code": "def make_decorator(wrapping_func, result_index=0):\n\n    def decorator(*wrapping_args, **wrapping_kwargs):\n\n        def outer_wrapper(f):\n\n            def inner_wrapper(*args, **kwargs):\n                result = f(*args, **kwargs)\n                wrapping_args_ = list(wrapping_args)\n                wrapping_args_.insert(result_index, result)\n                return wrapping_func(*wrapping_args_, **wrapping_kwargs)\n            return inner_wrapper\n        return outer_wrapper\n    return decorator",
    "label": true
  },
  {
    "code": "def double_stack(s: Stack) -> Stack:\n    aiden_is_gay = Stack()\n    while not s.is_empty():\n        aiden_is_gay.push(s.pop())\n    while not aiden_is_gay.is_empty():\n        gay = aiden_is_gay.pop()\n        s.push(gay)\n        s.push(gay)",
    "label": true
  },
  {
    "code": "def install_warning_logger() -> None:\n    warnings.simplefilter('default', PipDeprecationWarning, append=True)\n    global _original_showwarning\n    if _original_showwarning is None:\n        _original_showwarning = warnings.showwarning\n        warnings.showwarning = _showwarning",
    "label": true
  },
  {
    "code": "def zip_offset(*iterables, offsets, longest=False, fillvalue=None):\n    if len(iterables) != len(offsets):\n        raise ValueError(\"Number of iterables and offsets didn't match\")\n    staggered = []\n    for it, n in zip(iterables, offsets):\n        if n < 0:\n            staggered.append(chain(repeat(fillvalue, -n), it))\n        elif n > 0:\n            staggered.append(islice(it, n, None))\n        else:\n            staggered.append(it)\n    if longest:\n        return zip_longest(*staggered, fillvalue=fillvalue)\n    return zip(*staggered)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('general')\n    group.addoption('--lf', '--last-failed', action='store_true', dest='lf', help='Rerun only the tests that failed at the last run (or all if none failed)')\n    group.addoption('--ff', '--failed-first', action='store_true', dest='failedfirst', help='Run all tests, but run the last failures first. This may re-order tests and thus lead to repeated fixture setup/teardown.')\n    group.addoption('--nf', '--new-first', action='store_true', dest='newfirst', help='Run tests from new files first, then the rest of the tests sorted by file mtime')\n    group.addoption('--cache-show', action='append', nargs='?', dest='cacheshow', help=\"Show cache contents, don't perform collection or tests. Optional argument: glob (default: '*').\")\n    group.addoption('--cache-clear', action='store_true', dest='cacheclear', help='Remove all cache contents at start of test run')\n    cache_dir_default = '.pytest_cache'\n    if 'TOX_ENV_DIR' in os.environ:\n        cache_dir_default = os.path.join(os.environ['TOX_ENV_DIR'], cache_dir_default)\n    parser.addini('cache_dir', default=cache_dir_default, help='Cache directory path')\n    group.addoption('--lfnf', '--last-failed-no-failures', action='store', dest='last_failed_no_failures', choices=('all', 'none'), default='all', help='With ``--lf``, determines whether to execute tests when there are no previously (known) failures or when no cached ``lastfailed`` data was found. ``all`` (the default) runs the full test suite again. ``none`` just emits a message about no known failures and exits successfully.')",
    "label": true
  },
  {
    "code": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    tags = set()\n    interpreters, abis, platforms = tag.split('-')\n    for interpreter in interpreters.split('.'):\n        for abi in abis.split('.'):\n            for platform_ in platforms.split('.'):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)",
    "label": true
  },
  {
    "code": "def _get_ttype_name(ttype):\n    fname = STANDARD_TYPES.get(ttype)\n    if fname:\n        return fname\n    aname = ''\n    while fname is None:\n        aname = ttype[-1] + aname\n        ttype = ttype.parent\n        fname = STANDARD_TYPES.get(ttype)\n    return fname + aname",
    "label": true
  },
  {
    "code": "def copymode(src, dest):\n    import shutil\n    shutil.copymode(src, dest)",
    "label": true
  },
  {
    "code": "def split_after(iterable, pred, maxsplit=-1):\n    if maxsplit == 0:\n        yield list(iterable)\n        return\n    buf = []\n    it = iter(iterable)\n    for item in it:\n        buf.append(item)\n        if pred(item) and buf:\n            yield buf\n            if maxsplit == 1:\n                yield list(it)\n                return\n            buf = []\n            maxsplit -= 1\n    if buf:\n        yield buf",
    "label": true
  },
  {
    "code": "def _normalize_name(name: str) -> str:\n    name = name.lower().replace('_', '-')\n    if name.startswith('--'):\n        name = name[2:]\n    return name",
    "label": true
  },
  {
    "code": "def get_formatter_by_name(_alias, **options):\n    cls = find_formatter_class(_alias)\n    if cls is None:\n        raise ClassNotFound('no formatter found for name %r' % _alias)\n    return cls(**options)",
    "label": true
  },
  {
    "code": "def parse_uri(uri):\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])",
    "label": true
  },
  {
    "code": "def map_route(bridges: list[list], lat: float, lon: float, max_bridges: int, radius: int) -> list[int]:\n    visited_bridges = []\n    for dummy_i in range(max_bridges):\n        near_bridges = find_bridges_in_radius(bridges, lat, lon, radius, visited_bridges)\n        if not near_bridges:\n            continue\n        visited_bridges.append(find_worst_bci(bridges, near_bridges))\n        bridge = find_bridge_by_id(bridges, visited_bridges[-1])\n        lat = bridge[COLUMN_LAT]\n        lon = bridge[COLUMN_LON]\n    return visited_bridges",
    "label": true
  },
  {
    "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if '=' not in item:\n            result[item] = None\n            continue\n        name, value = item.split('=', 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result",
    "label": true
  },
  {
    "code": "def parse_wheel_filename(filename: str) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n    if not filename.endswith('.whl'):\n        raise InvalidWheelFilename(f\"Invalid wheel filename (extension must be '.whl'): {filename}\")\n    filename = filename[:-4]\n    dashes = filename.count('-')\n    if dashes not in (4, 5):\n        raise InvalidWheelFilename(f'Invalid wheel filename (wrong number of parts): {filename}')\n    parts = filename.split('-', dashes - 2)\n    name_part = parts[0]\n    if '__' in name_part or re.match('^[\\\\w\\\\d._]*$', name_part, re.UNICODE) is None:\n        raise InvalidWheelFilename(f'Invalid project name: {filename}')\n    name = canonicalize_name(name_part)\n    version = Version(parts[1])\n    if dashes == 5:\n        build_part = parts[2]\n        build_match = _build_tag_regex.match(build_part)\n        if build_match is None:\n            raise InvalidWheelFilename(f\"Invalid build number: {build_part} in '{filename}'\")\n        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))\n    else:\n        build = ()\n    tags = parse_tag(parts[-1])\n    return (name, version, build, tags)",
    "label": true
  },
  {
    "code": "def guess_filename(obj):\n    name = getattr(obj, 'name', None)\n    if name and isinstance(name, basestring) and (name[0] != '<') and (name[-1] != '>'):\n        return os.path.basename(name)",
    "label": true
  },
  {
    "code": "def _write_provides_extra(file, processed_extras, safe, unsafe):\n    previous = processed_extras.get(safe)\n    if previous == unsafe:\n        SetuptoolsDeprecationWarning.emit('Ambiguity during \"extra\" normalization for dependencies.', f'\\n            {previous!r} and {unsafe!r} normalize to the same value:\\n\\n                {safe!r}\\n\\n            In future versions, setuptools might halt the build process.\\n            ', see_url='https://peps.python.org/pep-0685/')\n    else:\n        processed_extras[safe] = unsafe\n        file.write(f'Provides-Extra: {safe}\\n')",
    "label": true
  },
  {
    "code": "def _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n    if len(lines) < 2 or lines[0][:4] != 'musl':\n        return None\n    m = re.match('Version (\\\\d+)\\\\.(\\\\d+)', lines[1])\n    if not m:\n        return None\n    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))",
    "label": true
  },
  {
    "code": "def unpack_tarfile(filename, extract_dir, progress_filter=default_filter):\n    try:\n        tarobj = tarfile.open(filename)\n    except tarfile.TarError as e:\n        raise UnrecognizedFormat('%s is not a compressed or uncompressed tar file' % (filename,)) from e\n    for member, final_dst in _iter_open_tar(tarobj, extract_dir, progress_filter):\n        try:\n            tarobj._extract_member(member, final_dst)\n        except tarfile.ExtractError:\n            pass\n    return True",
    "label": true
  },
  {
    "code": "def parse_literal_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1\n    start_pos = pos\n    pos = skip_until(src, pos, \"'\", error_on=ILLEGAL_LITERAL_STR_CHARS, error_on_eof=True)\n    return (pos + 1, src[start_pos:pos])",
    "label": true
  },
  {
    "code": "def add_to_line(comments: Optional[List[str]], original_string: str='', removed: bool=False, comment_prefix: str='') -> str:\n    if removed:\n        return parse(original_string)[0]\n    if not comments:\n        return original_string\n    unique_comments: List[str] = []\n    for comment in comments:\n        if comment not in unique_comments:\n            unique_comments.append(comment)\n    return f\"{parse(original_string)[0]}{comment_prefix} {'; '.join(unique_comments)}\"",
    "label": true
  },
  {
    "code": "def webify(color):\n    if color.startswith('calc') or color.startswith('var'):\n        return color\n    else:\n        return '#' + color",
    "label": true
  },
  {
    "code": "def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:\n    tokenizer.consume('WS')\n    marker_var_left = _parse_marker_var(tokenizer)\n    tokenizer.consume('WS')\n    marker_op = _parse_marker_op(tokenizer)\n    tokenizer.consume('WS')\n    marker_var_right = _parse_marker_var(tokenizer)\n    tokenizer.consume('WS')\n    return (marker_var_left, marker_op, marker_var_right)",
    "label": true
  },
  {
    "code": "def is_ipaddress(hostname):\n    if not six.PY2 and isinstance(hostname, bytes):\n        hostname = hostname.decode('ascii')\n    return bool(IPV4_RE.match(hostname) or BRACELESS_IPV6_ADDRZ_RE.match(hostname))",
    "label": true
  },
  {
    "code": "def platform_tags(arch: str) -> Iterator[str]:\n    sys_musl = _get_musl_version(sys.executable)\n    if sys_musl is None:\n        return\n    for minor in range(sys_musl.minor, -1, -1):\n        yield f'musllinux_{sys_musl.major}_{minor}_{arch}'",
    "label": true
  },
  {
    "code": "def _add_dunder_class(func, member) -> None:\n    python_cls = member.__class__\n    cls_name = getattr(python_cls, '__name__', None)\n    if not cls_name:\n        return\n    cls_bases = [ancestor.__name__ for ancestor in python_cls.__bases__]\n    ast_klass = build_class(cls_name, cls_bases, python_cls.__doc__)\n    func.instance_attrs['__class__'] = [ast_klass]",
    "label": true
  },
  {
    "code": "def is_iterable(value: nodes.NodeNG, check_async: bool=False) -> bool:\n    if check_async:\n        protocol_check = _supports_async_iteration_protocol\n    else:\n        protocol_check = _supports_iteration_protocol\n    return _supports_protocol(value, protocol_check)",
    "label": true
  },
  {
    "code": "def fix_help_options(options):\n    new_options = []\n    for help_tuple in options:\n        new_options.append(help_tuple[0:3])\n    return new_options",
    "label": true
  },
  {
    "code": "def check_package_set(package_set: PackageSet, should_ignore: Optional[Callable[[str], bool]]=None) -> CheckResult:\n    warn_legacy_versions_and_specifiers(package_set)\n    missing = {}\n    conflicting = {}\n    for package_name, package_detail in package_set.items():\n        missing_deps: Set[Missing] = set()\n        conflicting_deps: Set[Conflicting] = set()\n        if should_ignore and should_ignore(package_name):\n            continue\n        for req in package_detail.dependencies:\n            name = canonicalize_name(req.name)\n            if name not in package_set:\n                missed = True\n                if req.marker is not None:\n                    missed = req.marker.evaluate({'extra': ''})\n                if missed:\n                    missing_deps.add((name, req))\n                continue\n            version = package_set[name].version\n            if not req.specifier.contains(version, prereleases=True):\n                conflicting_deps.add((name, version, req))\n        if missing_deps:\n            missing[package_name] = sorted(missing_deps, key=str)\n        if conflicting_deps:\n            conflicting[package_name] = sorted(conflicting_deps, key=str)\n    return (missing, conflicting)",
    "label": true
  },
  {
    "code": "def _indentation(lexer, match, ctx):\n    indentation = match.group(0)\n    yield (match.start(), Whitespace, indentation)\n    ctx.last_indentation = indentation\n    ctx.pos = match.end()\n    if hasattr(ctx, 'block_state') and ctx.block_state and indentation.startswith(ctx.block_indentation) and (indentation != ctx.block_indentation):\n        ctx.stack.append(ctx.block_state)\n    else:\n        ctx.block_state = None\n        ctx.block_indentation = None\n        ctx.stack.append('content')",
    "label": true
  },
  {
    "code": "def _check_record_param_type(param: str, v: str) -> None:\n    __tracebackhide__ = True\n    if not isinstance(v, str):\n        msg = '{param} parameter needs to be a string, but {g} given'\n        raise TypeError(msg.format(param=param, g=type(v).__name__))",
    "label": true
  },
  {
    "code": "def retry(*dargs: t.Any, **dkw: t.Any) -> t.Any:\n    if len(dargs) == 1 and callable(dargs[0]):\n        return retry()(dargs[0])\n    else:\n\n        def wrap(f: WrappedFn) -> WrappedFn:\n            if isinstance(f, retry_base):\n                warnings.warn(f'Got retry_base instance ({f.__class__.__name__}) as callable argument, this will probably hang indefinitely (did you mean retry={f.__class__.__name__}(...)?)')\n            r: 'BaseRetrying'\n            if iscoroutinefunction(f):\n                r = AsyncRetrying(*dargs, **dkw)\n            elif tornado and hasattr(tornado.gen, 'is_coroutine_function') and tornado.gen.is_coroutine_function(f):\n                r = TornadoRetrying(*dargs, **dkw)\n            else:\n                r = Retrying(*dargs, **dkw)\n            return r.wraps(f)\n        return wrap",
    "label": true
  },
  {
    "code": "def _warn_incompatibility_with_xunit2(request: FixtureRequest, fixture_name: str) -> None:\n    from _pytest.warning_types import PytestWarning\n    xml = request.config.stash.get(xml_key, None)\n    if xml is not None and xml.family not in ('xunit1', 'legacy'):\n        request.node.warn(PytestWarning(\"{fixture_name} is incompatible with junit_family '{family}' (use 'legacy' or 'xunit1')\".format(fixture_name=fixture_name, family=xml.family)))",
    "label": true
  },
  {
    "code": "def has_magic(s):\n    if isinstance(s, bytes):\n        match = magic_check_bytes.search(s)\n    else:\n        match = magic_check.search(s)\n    return match is not None",
    "label": true
  },
  {
    "code": "def _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n    if len(lines) < 2 or lines[0][:4] != 'musl':\n        return None\n    m = re.match('Version (\\\\d+)\\\\.(\\\\d+)', lines[1])\n    if not m:\n        return None\n    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))",
    "label": true
  },
  {
    "code": "def is_sys_guard(node: nodes.If) -> bool:\n    if isinstance(node.test, nodes.Compare):\n        value = node.test.left\n        if isinstance(value, nodes.Subscript):\n            value = value.value\n        if isinstance(value, nodes.Attribute) and value.as_string() == 'sys.version_info':\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def is_within_directory(directory: str, target: str) -> bool:\n    abs_directory = os.path.abspath(directory)\n    abs_target = os.path.abspath(target)\n    prefix = os.path.commonprefix([abs_directory, abs_target])\n    return prefix == abs_directory",
    "label": true
  },
  {
    "code": "def default_context():\n\n    def format_full_version(info):\n        version = '%s.%s.%s' % (info.major, info.minor, info.micro)\n        kind = info.releaselevel\n        if kind != 'final':\n            version += kind[0] + str(info.serial)\n        return version\n    if hasattr(sys, 'implementation'):\n        implementation_version = format_full_version(sys.implementation.version)\n        implementation_name = sys.implementation.name\n    else:\n        implementation_version = '0'\n        implementation_name = ''\n    ppv = platform.python_version()\n    m = _DIGITS.match(ppv)\n    pv = m.group(0)\n    result = {'implementation_name': implementation_name, 'implementation_version': implementation_version, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_python_implementation': platform.python_implementation(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'platform_in_venv': str(in_venv()), 'python_full_version': ppv, 'python_version': pv, 'sys_platform': sys.platform}\n    return result",
    "label": true
  },
  {
    "code": "def get_int_opt(options, optname, default=None):\n    string = options.get(optname, default)\n    try:\n        return int(string)\n    except TypeError:\n        raise OptionError('Invalid type %r for option %s; you must give an integer value' % (string, optname))\n    except ValueError:\n        raise OptionError('Invalid value %r for option %s; you must give an integer value' % (string, optname))",
    "label": true
  },
  {
    "code": "def _yn_transformer(value: str) -> bool:\n    value = value.lower()\n    if value in YES_VALUES:\n        return True\n    if value in NO_VALUES:\n        return False\n    raise argparse.ArgumentTypeError(None, f\"Invalid yn value '{value}', should be in {(*YES_VALUES, *NO_VALUES)}\")",
    "label": true
  },
  {
    "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def remove_prefix(text, prefix):\n    null, prefix, rest = text.rpartition(prefix)\n    return rest",
    "label": true
  },
  {
    "code": "def _worker_check_single_file(file_item: FileItem) -> tuple[int, str | None, str, str | None, list[Message], LinterStats, int, defaultdict[str, list[Any]]]:\n    if not _worker_linter:\n        raise RuntimeError('Worker linter not yet initialised')\n    _worker_linter.open()\n    _worker_linter.check_single_file_item(file_item)\n    mapreduce_data = defaultdict(list)\n    for checker in _worker_linter.get_checkers():\n        data = checker.get_map_data()\n        if data is not None:\n            mapreduce_data[checker.name].append(data)\n    msgs = _worker_linter.reporter.messages\n    assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)\n    _worker_linter.reporter.reset()\n    if _worker_linter.current_name is None:\n        warnings.warn('In pylint 3.0 the current_name attribute of the linter object should be a string. If unknown it should be initialized as an empty string.', DeprecationWarning)\n    return (id(multiprocessing.current_process()), _worker_linter.current_name, file_item.filepath, _worker_linter.file_state.base_name, msgs, _worker_linter.stats, _worker_linter.msg_status, mapreduce_data)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('terminal reporting')\n    group.addoption('--junitxml', '--junit-xml', action='store', dest='xmlpath', metavar='path', type=functools.partial(filename_arg, optname='--junitxml'), default=None, help='Create junit-xml style report file at given path')\n    group.addoption('--junitprefix', '--junit-prefix', action='store', metavar='str', default=None, help='Prepend prefix to classnames in junit-xml output')\n    parser.addini('junit_suite_name', 'Test suite name for JUnit report', default='pytest')\n    parser.addini('junit_logging', 'Write captured log messages to JUnit report: one of no|log|system-out|system-err|out-err|all', default='no')\n    parser.addini('junit_log_passing_tests', 'Capture log information for passing tests to JUnit report: ', type='bool', default=True)\n    parser.addini('junit_duration_report', 'Duration time to report: one of total|call', default='total')\n    parser.addini('junit_family', 'Emit XML for schema: one of legacy|xunit1|xunit2', default='xunit2')",
    "label": true
  },
  {
    "code": "def _is_setup_py(path: Path) -> bool:\n    if path.name != 'setup.py':\n        return False\n    contents = path.read_bytes()\n    return b'setuptools' in contents or b'distutils' in contents",
    "label": true
  },
  {
    "code": "def _validate_dependencies_met() -> None:\n    from cryptography.x509.extensions import Extensions\n    if getattr(Extensions, 'get_extension_for_class', None) is None:\n        raise ImportError(\"'cryptography' module missing required functionality.  Try upgrading to v1.3.4 or newer.\")\n    from OpenSSL.crypto import X509\n    x509 = X509()\n    if getattr(x509, '_x509', None) is None:\n        raise ImportError(\"'pyOpenSSL' module missing required functionality. Try upgrading to v0.14 or newer.\")",
    "label": true
  },
  {
    "code": "def _cert_array_from_pem(pem_bundle):\n    pem_bundle = pem_bundle.replace(b'\\r\\n', b'\\n')\n    der_certs = [base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)]\n    if not der_certs:\n        raise ssl.SSLError('No root certificates specified')\n    cert_array = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n    if not cert_array:\n        raise ssl.SSLError('Unable to allocate memory!')\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError('Unable to allocate memory!')\n            cert = Security.SecCertificateCreateWithData(CoreFoundation.kCFAllocatorDefault, certdata)\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError('Unable to build cert object!')\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        CoreFoundation.CFRelease(cert_array)\n        raise\n    return cert_array",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('general')\n    group._addoption('--pdb', dest='usepdb', action='store_true', help='Start the interactive Python debugger on errors or KeyboardInterrupt')\n    group._addoption('--pdbcls', dest='usepdb_cls', metavar='modulename:classname', type=_validate_usepdb_cls, help='Specify a custom interactive Python debugger for use with --pdb.For example: --pdbcls=IPython.terminal.debugger:TerminalPdb')\n    group._addoption('--trace', dest='trace', action='store_true', help='Immediately break when running each test')",
    "label": true
  },
  {
    "code": "def pack(o, stream, **kwargs):\n    packer = Packer(**kwargs)\n    stream.write(packer.pack(o))",
    "label": true
  },
  {
    "code": "def _parse_specifier(tokenizer: Tokenizer) -> str:\n    with tokenizer.enclosing_tokens('LEFT_PARENTHESIS', 'RIGHT_PARENTHESIS'):\n        tokenizer.consume('WS')\n        parsed_specifiers = _parse_version_many(tokenizer)\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def _cpython_abis(py_version: PythonVersion, warn: bool=False) -> List[str]:\n    py_version = tuple(py_version)\n    abis = []\n    version = _version_nodot(py_version[:2])\n    debug = pymalloc = ucs4 = ''\n    with_debug = _get_config_var('Py_DEBUG', warn)\n    has_refcount = hasattr(sys, 'gettotalrefcount')\n    has_ext = '_d.pyd' in EXTENSION_SUFFIXES\n    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n        debug = 'd'\n    if py_version < (3, 8):\n        with_pymalloc = _get_config_var('WITH_PYMALLOC', warn)\n        if with_pymalloc or with_pymalloc is None:\n            pymalloc = 'm'\n        if py_version < (3, 3):\n            unicode_size = _get_config_var('Py_UNICODE_SIZE', warn)\n            if unicode_size == 4 or (unicode_size is None and sys.maxunicode == 1114111):\n                ucs4 = 'u'\n    elif debug:\n        abis.append(f'cp{version}')\n    abis.insert(0, 'cp{version}{debug}{pymalloc}{ucs4}'.format(version=version, debug=debug, pymalloc=pymalloc, ucs4=ucs4))\n    return abis",
    "label": true
  },
  {
    "code": "def test_trace_to_file(stream_trace):\n    file = tempfile.NamedTemporaryFile(mode='r')\n    with detect.trace(file.name, mode='w'):\n        dill.dumps(test_obj)\n    file_trace = file.read()\n    file.close()\n    reghex = re.compile('0x[0-9A-Za-z]+')\n    file_trace, stream_trace = (reghex.sub('0x', file_trace), reghex.sub('0x', stream_trace))\n    regdict = re.compile('(dict\\\\.__repr__ of ).*')\n    file_trace, stream_trace = (regdict.sub('\\\\1{}>', file_trace), regdict.sub('\\\\1{}>', stream_trace))\n    assert file_trace == stream_trace",
    "label": true
  },
  {
    "code": "def is_valid_cidr(string_network):\n    if string_network.count('/') == 1:\n        try:\n            mask = int(string_network.split('/')[1])\n        except ValueError:\n            return False\n        if mask < 1 or mask > 32:\n            return False\n        try:\n            socket.inet_aton(string_network.split('/')[0])\n        except OSError:\n            return False\n    else:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _is_within_name_length(node_type: str, name: str) -> str:\n    msg = ''\n    name_length_limit = VAR_NAME_LENGTHS[node_type]\n    if len(name) > name_length_limit:\n        msg = f'{node_type.capitalize()} name \"{name}\" exceeds the limit of {name_length_limit} characters.'\n    return msg",
    "label": true
  },
  {
    "code": "def _some_attrgetter(*items):\n\n    def _acessor(obj):\n        values = (_attrgetter(i)(obj) for i in items)\n        return next((i for i in values if i is not None), None)\n    return _acessor",
    "label": true
  },
  {
    "code": "def find_plugin_formatters():\n    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):\n        yield (entrypoint.name, entrypoint.load())",
    "label": true
  },
  {
    "code": "def pass_context(f: 't.Callable[te.Concatenate[Context, P], R]') -> 't.Callable[P, R]':\n\n    def new_func(*args: 'P.args', **kwargs: 'P.kwargs') -> 'R':\n        return f(get_current_context(), *args, **kwargs)\n    return update_wrapper(new_func, f)",
    "label": true
  },
  {
    "code": "def parse_literal_str(src: str, pos: Pos) -> Tuple[Pos, str]:\n    pos += 1\n    start_pos = pos\n    pos = skip_until(src, pos, \"'\", error_on=ILLEGAL_LITERAL_STR_CHARS, error_on_eof=True)\n    return (pos + 1, src[start_pos:pos])",
    "label": true
  },
  {
    "code": "def pytest_unconfigure() -> None:\n    global RUNNER_CLASS\n    RUNNER_CLASS = None",
    "label": true
  },
  {
    "code": "def diff_string(old: int | float, new: int | float) -> str:\n    diff = abs(old - new)\n    diff_str = f\"{CMPS[cmp(old, new)]}{diff and f'{diff:.2f}' or ''}\"\n    return diff_str",
    "label": true
  },
  {
    "code": "def resolve_ssl_version(candidate):\n    if candidate is None:\n        return PROTOCOL_TLS\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'PROTOCOL_' + candidate)\n        return res\n    return candidate",
    "label": true
  },
  {
    "code": "def terminal_encoding(term):\n    if getattr(term, 'encoding', None):\n        return term.encoding\n    import locale\n    return locale.getpreferredencoding()",
    "label": true
  },
  {
    "code": "def _build_one(req: InstallRequirement, output_dir: str, verify: bool, build_options: List[str], global_options: List[str], editable: bool) -> Optional[str]:\n    artifact = 'editable' if editable else 'wheel'\n    try:\n        ensure_dir(output_dir)\n    except OSError as e:\n        logger.warning('Building %s for %s failed: %s', artifact, req.name, e)\n        return None\n    with req.build_env:\n        wheel_path = _build_one_inside_env(req, output_dir, build_options, global_options, editable)\n    if wheel_path and verify:\n        try:\n            _verify_one(req, wheel_path)\n        except (InvalidWheelFilename, UnsupportedWheel) as e:\n            logger.warning('Built %s for %s is invalid: %s', artifact, req.name, e)\n            return None\n    return wheel_path",
    "label": true
  },
  {
    "code": "def configure():\n    err_handler = logging.StreamHandler()\n    err_handler.setLevel(logging.WARNING)\n    out_handler = logging.StreamHandler(sys.stdout)\n    out_handler.addFilter(_not_warning)\n    handlers = (err_handler, out_handler)\n    logging.basicConfig(format='{message}', style='{', handlers=handlers, level=logging.DEBUG)\n    if inspect.ismodule(distutils.dist.log):\n        monkey.patch_func(set_threshold, distutils.log, 'set_threshold')\n        distutils.dist.log = distutils.log",
    "label": true
  },
  {
    "code": "def build_parser() -> optparse.OptionParser:\n    parser = optparse.OptionParser(add_help_option=False)\n    option_factories = SUPPORTED_OPTIONS + SUPPORTED_OPTIONS_REQ\n    for option_factory in option_factories:\n        option = option_factory()\n        parser.add_option(option)\n\n    def parser_exit(self: Any, msg: str) -> 'NoReturn':\n        raise OptionParsingError(msg)\n    parser.exit = parser_exit\n    return parser",
    "label": true
  },
  {
    "code": "def _parse_filters(f_strs):\n    filters = []\n    if not f_strs:\n        return filters\n    for f_str in f_strs:\n        if ':' in f_str:\n            fname, fopts = f_str.split(':', 1)\n            filters.append((fname, _parse_options([fopts])))\n        else:\n            filters.append((f_str, {}))\n    return filters",
    "label": true
  },
  {
    "code": "def _enter_pdb(node: Node, excinfo: ExceptionInfo[BaseException], rep: BaseReport) -> BaseReport:\n    tw = node.config.pluginmanager.getplugin('terminalreporter')._tw\n    tw.line()\n    showcapture = node.config.option.showcapture\n    for sectionname, content in (('stdout', rep.capstdout), ('stderr', rep.capstderr), ('log', rep.caplog)):\n        if showcapture in (sectionname, 'all') and content:\n            tw.sep('>', 'captured ' + sectionname)\n            if content[-1:] == '\\n':\n                content = content[:-1]\n            tw.line(content)\n    tw.sep('>', 'traceback')\n    rep.toterminal(tw)\n    tw.sep('>', 'entering PDB')\n    tb = _postmortem_traceback(excinfo)\n    rep._pdbshown = True\n    post_mortem(tb)\n    return rep",
    "label": true
  },
  {
    "code": "def _hash_of_file(path: str, algorithm: str) -> str:\n    with open(path, 'rb') as archive:\n        hash = hashlib.new(algorithm)\n        for chunk in read_chunks(archive):\n            hash.update(chunk)\n    return hash.hexdigest()",
    "label": true
  },
  {
    "code": "def groupby_transform(iterable, keyfunc=None, valuefunc=None, reducefunc=None):\n    ret = groupby(iterable, keyfunc)\n    if valuefunc:\n        ret = ((k, map(valuefunc, g)) for k, g in ret)\n    if reducefunc:\n        ret = ((k, reducefunc(g)) for k, g in ret)\n    return ret",
    "label": true
  },
  {
    "code": "def is_response_to_head(response):\n    method = response._method\n    if isinstance(method, int):\n        return method == 3\n    return method.upper() == 'HEAD'",
    "label": true
  },
  {
    "code": "def _is_in_main(node):\n    if not hasattr(node, 'parent'):\n        return False\n    parent = node.parent\n    try:\n        if isinstance(parent, nodes.If) and parent.test.left.name == '__name__' and (parent.test.ops[0][1].value == '__main__'):\n            return True\n        else:\n            return _is_in_main(parent)\n    except (AttributeError, IndexError) as e:\n        return _is_in_main(parent)",
    "label": true
  },
  {
    "code": "def findsource(obj) -> Tuple[Optional[Source], int]:\n    try:\n        sourcelines, lineno = inspect.findsource(obj)\n    except Exception:\n        return (None, -1)\n    source = Source()\n    source.lines = [line.rstrip() for line in sourcelines]\n    return (source, lineno)",
    "label": true
  },
  {
    "code": "def split_into(iterable, sizes):\n    it = iter(iterable)\n    for size in sizes:\n        if size is None:\n            yield list(it)\n            return\n        else:\n            yield list(islice(it, size))",
    "label": true
  },
  {
    "code": "def connection_from_url(url, **kw):\n    scheme, host, port = get_host(url)\n    port = port or port_by_scheme.get(scheme, 80)\n    if scheme == 'https':\n        return HTTPSConnectionPool(host, port=port, **kw)\n    else:\n        return HTTPConnectionPool(host, port=port, **kw)",
    "label": true
  },
  {
    "code": "def _is_jupyter() -> bool:\n    try:\n        get_ipython\n    except NameError:\n        return False\n    ipython = get_ipython()\n    shell = ipython.__class__.__name__\n    if 'google.colab' in str(ipython.__class__) or os.getenv('DATABRICKS_RUNTIME_VERSION') or shell == 'ZMQInteractiveShell':\n        return True\n    elif shell == 'TerminalInteractiveShell':\n        return False\n    else:\n        return False",
    "label": true
  },
  {
    "code": "def b(x):\n    import codecs\n    return codecs.latin_1_encode(x)[0]",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('debugconfig')\n    group.addoption('--version', '-V', action='count', default=0, dest='version', help='Display pytest version and information about plugins. When given twice, also display information about plugins.')\n    group._addoption('-h', '--help', action=HelpAction, dest='help', help='Show help message and configuration info')\n    group._addoption('-p', action='append', dest='plugins', default=[], metavar='name', help='Early-load given plugin module name or entry point (multi-allowed). To avoid loading of plugins, use the `no:` prefix, e.g. `no:doctest`.')\n    group.addoption('--traceconfig', '--trace-config', action='store_true', default=False, help='Trace considerations of conftest.py files')\n    group.addoption('--debug', action='store', nargs='?', const='pytestdebug.log', dest='debug', metavar='DEBUG_FILE_NAME', help=\"Store internal tracing debug information in this log file. This file is opened with 'w' and truncated as a result, care advised. Default: pytestdebug.log.\")\n    group._addoption('-o', '--override-ini', dest='override_ini', action='append', help='Override ini option with \"option=value\" style, e.g. `-o xfail_strict=True -o cache_dir=cache`.')",
    "label": true
  },
  {
    "code": "def _filter_uninferable_nodes(elts: Sequence[InferenceResult], context: InferenceContext) -> Iterator[SuccessfulInferenceResult]:\n    for elt in elts:\n        if isinstance(elt, util.UninferableBase):\n            yield nodes.Unknown()\n        else:\n            for inferred in elt.infer(context):\n                if not isinstance(inferred, util.UninferableBase):\n                    yield inferred\n                else:\n                    yield nodes.Unknown()",
    "label": true
  },
  {
    "code": "def linux_distribution(full_distribution_name: bool=True) -> Tuple[str, str, str]:\n    warnings.warn(\"distro.linux_distribution() is deprecated. It should only be used as a compatibility shim with Python's platform.linux_distribution(). Please use distro.id(), distro.version() and distro.name() instead.\", DeprecationWarning, stacklevel=2)\n    return _distro.linux_distribution(full_distribution_name)",
    "label": true
  },
  {
    "code": "def write_requirements(cmd, basename, filename):\n    dist = cmd.distribution\n    meta = dist.metadata\n    data = io.StringIO()\n    install_requires, extras_require = _prepare(meta._normalized_install_requires, meta._normalized_extras_require)\n    _write_requirements(data, install_requires)\n    for extra in sorted(extras_require):\n        data.write('\\n[{extra}]\\n'.format(**vars()))\n        _write_requirements(data, extras_require[extra])\n    cmd.write_or_delete_file('requirements', filename, data.getvalue())",
    "label": true
  },
  {
    "code": "def docstring_headline(obj):\n    if not obj.__doc__:\n        return ''\n    res = []\n    for line in obj.__doc__.strip().splitlines():\n        if line.strip():\n            res.append(' ' + line.strip())\n        else:\n            break\n    return ''.join(res).lstrip()",
    "label": true
  },
  {
    "code": "def echo_via_pager(text_or_generator: t.Union[t.Iterable[str], t.Callable[[], t.Iterable[str]], str], color: t.Optional[bool]=None) -> None:\n    color = resolve_color_default(color)\n    if inspect.isgeneratorfunction(text_or_generator):\n        i = t.cast(t.Callable[[], t.Iterable[str]], text_or_generator)()\n    elif isinstance(text_or_generator, str):\n        i = [text_or_generator]\n    else:\n        i = iter(t.cast(t.Iterable[str], text_or_generator))\n    text_generator = (el if isinstance(el, str) else str(el) for el in i)\n    from ._termui_impl import pager\n    return pager(itertools.chain(text_generator, '\\n'), color)",
    "label": true
  },
  {
    "code": "def _force_symlink(root: Path, target: Union[str, PurePath], link_to: Union[str, Path]) -> None:\n    current_symlink = root.joinpath(target)\n    try:\n        current_symlink.unlink()\n    except OSError:\n        pass\n    try:\n        current_symlink.symlink_to(link_to)\n    except Exception:\n        pass",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        interp = 'pp3'\n    elif interp_name == 'cp':\n        interp = 'cp' + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)",
    "label": true
  },
  {
    "code": "def expand_env_variables(lines_enum: ReqFileLines) -> ReqFileLines:\n    for line_number, line in lines_enum:\n        for env_var, var_name in ENV_VAR_RE.findall(line):\n            value = os.getenv(var_name)\n            if not value:\n                continue\n            line = line.replace(env_var, value)\n        yield (line_number, line)",
    "label": true
  },
  {
    "code": "def expire_after(delta, date=None):\n    date = date or datetime.utcnow()\n    return date + delta",
    "label": true
  },
  {
    "code": "def substrings_indexes(seq, reverse=False):\n    r = range(1, len(seq) + 1)\n    if reverse:\n        r = reversed(r)\n    return ((seq[i:i + L], i, i + L) for L in r for i in range(len(seq) - L + 1))",
    "label": true
  },
  {
    "code": "def avg_length(tree: HuffmanTree, freq_dict: dict[int, int]) -> float:\n    accumulator = 0\n    total = 0\n    codes = get_codes(tree)\n    for symbol in freq_dict:\n        tmp = freq_dict[symbol]\n        accumulator += tmp * len(codes[symbol])\n        total += tmp\n    if total:\n        return accumulator / total\n    else:\n        return 0",
    "label": true
  },
  {
    "code": "def lookup(label):\n    label = ascii_lower(label.strip('\\t\\n\\x0c\\r '))\n    name = LABELS.get(label)\n    if name is None:\n        return None\n    encoding = CACHE.get(name)\n    if encoding is None:\n        if name == 'x-user-defined':\n            from .x_user_defined import codec_info\n        else:\n            python_name = PYTHON_NAMES.get(name, name)\n            codec_info = codecs.lookup(python_name)\n        encoding = Encoding(name, codec_info)\n        CACHE[name] = encoding\n    return encoding",
    "label": true
  },
  {
    "code": "def has_tls() -> bool:\n    try:\n        import _ssl\n        return True\n    except ImportError:\n        pass\n    from pip._vendor.urllib3.util import IS_PYOPENSSL\n    return IS_PYOPENSSL",
    "label": true
  },
  {
    "code": "def get_file_url(link: Link, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> File:\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link.file_path\n    if hashes:\n        hashes.check_against_path(from_path)\n    return File(from_path, None)",
    "label": true
  },
  {
    "code": "def make_vcs_requirement_url(repo_url: str, rev: str, project_name: str, subdir: Optional[str]=None) -> str:\n    egg_project_name = project_name.replace('-', '_')\n    req = f'{repo_url}@{rev}#egg={egg_project_name}'\n    if subdir:\n        req += f'&subdirectory={subdir}'\n    return req",
    "label": true
  },
  {
    "code": "def numpy_supports_type_hints() -> bool:\n    np_ver = _get_numpy_version()\n    return np_ver and np_ver > NUMPY_VERSION_TYPE_HINTS_SUPPORT",
    "label": true
  },
  {
    "code": "def adder(augend):\n    zero = [0]\n\n    def inner(addend):\n        return addend + augend + zero[0]\n    return inner",
    "label": true
  },
  {
    "code": "def configure():\n    err_handler = logging.StreamHandler()\n    err_handler.setLevel(logging.WARNING)\n    out_handler = logging.StreamHandler(sys.stdout)\n    out_handler.addFilter(_not_warning)\n    handlers = (err_handler, out_handler)\n    logging.basicConfig(format='{message}', style='{', handlers=handlers, level=logging.DEBUG)",
    "label": true
  },
  {
    "code": "def _have_compatible_abi(arch: str) -> bool:\n    if arch == 'armv7l':\n        return _is_linux_armhf()\n    if arch == 'i686':\n        return _is_linux_i686()\n    return arch in {'x86_64', 'aarch64', 'ppc64', 'ppc64le', 's390x'}",
    "label": true
  },
  {
    "code": "def _ensure_newline_before_comment(output: List[str]) -> List[str]:\n    new_output: List[str] = []\n\n    def is_comment(line: Optional[str]) -> bool:\n        return line.startswith('#') if line else False\n    for line, prev_line in zip(output, [None] + output):\n        if is_comment(line) and prev_line != '' and (not is_comment(prev_line)):\n            new_output.append('')\n        new_output.append(line)\n    return new_output",
    "label": true
  },
  {
    "code": "def from_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n    return OrderedDict(value)",
    "label": true
  },
  {
    "code": "def _wipe_internal_state_for_tests():\n    global orig_stdout, orig_stderr\n    orig_stdout = None\n    orig_stderr = None\n    global wrapped_stdout, wrapped_stderr\n    wrapped_stdout = None\n    wrapped_stderr = None\n    global atexit_done\n    atexit_done = False\n    global fixed_windows_console\n    fixed_windows_console = False\n    try:\n        atexit.unregister(reset_all)\n    except AttributeError:\n        pass",
    "label": true
  },
  {
    "code": "def decide_user_install(use_user_site: Optional[bool], prefix_path: Optional[str]=None, target_dir: Optional[str]=None, root_path: Optional[str]=None, isolated_mode: bool=False) -> bool:\n    if use_user_site is not None and (not use_user_site):\n        logger.debug('Non-user install by explicit request')\n        return False\n    if use_user_site:\n        if prefix_path:\n            raise CommandError(\"Can not combine '--user' and '--prefix' as they imply different installation locations\")\n        if virtualenv_no_global():\n            raise InstallationError(\"Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\")\n        logger.debug('User install by explicit request')\n        return True\n    assert use_user_site is None\n    if prefix_path or target_dir:\n        logger.debug('Non-user install due to --prefix or --target option')\n        return False\n    if not site.ENABLE_USER_SITE:\n        logger.debug('Non-user install because user site-packages disabled')\n        return False\n    if site_packages_writable(root=root_path, isolated=isolated_mode):\n        logger.debug('Non-user install because site-packages writeable')\n        return False\n    logger.info('Defaulting to user installation because normal site-packages is not writeable')\n    return True",
    "label": true
  },
  {
    "code": "def wrap_stream(stream, convert, strip, autoreset, wrap):\n    if wrap:\n        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)\n        if wrapper.should_wrap():\n            stream = wrapper.stream\n    return stream",
    "label": true
  },
  {
    "code": "def ratio_resolve(total: int, edges: Sequence[Edge]) -> List[int]:\n    sizes = [edge.size or None for edge in edges]\n    _Fraction = Fraction\n    while None in sizes:\n        flexible_edges = [(index, edge) for index, (size, edge) in enumerate(zip(sizes, edges)) if size is None]\n        remaining = total - sum((size or 0 for size in sizes))\n        if remaining <= 0:\n            return [edge.minimum_size or 1 if size is None else size for size, edge in zip(sizes, edges)]\n        portion = _Fraction(remaining, sum((edge.ratio or 1 for _, edge in flexible_edges)))\n        for index, edge in flexible_edges:\n            if portion * edge.ratio <= edge.minimum_size:\n                sizes[index] = edge.minimum_size\n                break\n        else:\n            remainder = _Fraction(0)\n            for index, edge in flexible_edges:\n                size, remainder = divmod(portion * edge.ratio + remainder, 1)\n                sizes[index] = size\n            break\n    return cast(List[int], sizes)",
    "label": true
  },
  {
    "code": "def python_entrypoint_reference(value: str) -> bool:\n    module, _, rest = value.partition(':')\n    if '[' in rest:\n        obj, _, extras_ = rest.partition('[')\n        if extras_.strip()[-1] != ']':\n            return False\n        extras = (x.strip() for x in extras_.strip(string.whitespace + '[]').split(','))\n        if not all((pep508_identifier(e) for e in extras)):\n            return False\n        _logger.warning(f'`{value}` - using extras for entry points is not recommended')\n    else:\n        obj = rest\n    module_parts = module.split('.')\n    identifiers = _chain(module_parts, obj.split('.')) if rest else module_parts\n    return all((python_identifier(i.strip()) for i in identifiers))",
    "label": true
  },
  {
    "code": "def random_permutation(iterable, r=None):\n    pool = tuple(iterable)\n    r = len(pool) if r is None else r\n    return tuple(sample(pool, r))",
    "label": true
  },
  {
    "code": "def collapse(iterable, base_type=None, levels=None):\n\n    def walk(node, level):\n        if levels is not None and level > levels or isinstance(node, (str, bytes)) or (base_type is not None and isinstance(node, base_type)):\n            yield node\n            return\n        try:\n            tree = iter(node)\n        except TypeError:\n            yield node\n            return\n        else:\n            for child in tree:\n                yield from walk(child, level + 1)\n    yield from walk(iterable, 0)",
    "label": true
  },
  {
    "code": "def platform_tags(arch: str) -> Iterator[str]:\n    sys_musl = _get_musl_version(sys.executable)\n    if sys_musl is None:\n        return\n    for minor in range(sys_musl.minor, -1, -1):\n        yield f'musllinux_{sys_musl.major}_{minor}_{arch}'",
    "label": true
  },
  {
    "code": "def regex_opt(strings, prefix='', suffix=''):\n    strings = sorted(strings)\n    return prefix + regex_opt_inner(strings, '(') + suffix",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def parse_one_line_basic_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1\n    return parse_basic_str(src, pos, multiline=False)",
    "label": true
  },
  {
    "code": "def get_all_lexers(plugins=True):\n    for item in LEXERS.values():\n        yield item[1:]\n    if plugins:\n        for lexer in find_plugin_lexers():\n            yield (lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes)",
    "label": true
  },
  {
    "code": "def check_label(label: Union[str, bytes, bytearray]) -> None:\n    if isinstance(label, (bytes, bytearray)):\n        label = label.decode('utf-8')\n    if len(label) == 0:\n        raise IDNAError('Empty Label')\n    check_nfc(label)\n    check_hyphen_ok(label)\n    check_initial_combiner(label)\n    for pos, cp in enumerate(label):\n        cp_value = ord(cp)\n        if intranges_contain(cp_value, idnadata.codepoint_classes['PVALID']):\n            continue\n        elif intranges_contain(cp_value, idnadata.codepoint_classes['CONTEXTJ']):\n            try:\n                if not valid_contextj(label, pos):\n                    raise InvalidCodepointContext('Joiner {} not allowed at position {} in {}'.format(_unot(cp_value), pos + 1, repr(label)))\n            except ValueError:\n                raise IDNAError('Unknown codepoint adjacent to joiner {} at position {} in {}'.format(_unot(cp_value), pos + 1, repr(label)))\n        elif intranges_contain(cp_value, idnadata.codepoint_classes['CONTEXTO']):\n            if not valid_contexto(label, pos):\n                raise InvalidCodepointContext('Codepoint {} not allowed at position {} in {}'.format(_unot(cp_value), pos + 1, repr(label)))\n        else:\n            raise InvalidCodepoint('Codepoint {} at position {} of {} not allowed'.format(_unot(cp_value), pos + 1, repr(label)))\n    check_bidi(label)",
    "label": true
  },
  {
    "code": "def extract_from_urllib3():\n    util.SSLContext = orig_util_SSLContext\n    util.ssl_.SSLContext = orig_util_SSLContext\n    util.HAS_SNI = orig_util_HAS_SNI\n    util.ssl_.HAS_SNI = orig_util_HAS_SNI\n    util.IS_SECURETRANSPORT = False\n    util.ssl_.IS_SECURETRANSPORT = False",
    "label": true
  },
  {
    "code": "def _supports_protocol_method(value: nodes.NodeNG, attr: str) -> bool:\n    try:\n        attributes = value.getattr(attr)\n    except astroid.NotFoundError:\n        return False\n    first = attributes[0]\n    if isinstance(first, nodes.AssignName):\n        this_assign_parent = get_node_first_ancestor_of_type(first, (nodes.Assign, nodes.NamedExpr))\n        if this_assign_parent is None:\n            return True\n        if isinstance(this_assign_parent.value, nodes.BaseContainer):\n            if all((isinstance(n, nodes.Const) for n in this_assign_parent.value.elts)):\n                return False\n        if isinstance(this_assign_parent.value, nodes.Const):\n            return False\n    return True",
    "label": true
  },
  {
    "code": "def _is_bad_name(name: str) -> str:\n    msg = ''\n    if name in BAD_NAMES:\n        msg = f'\"{name}\" is a name that should be avoided. Change to something less ambiguous and/or more descriptive.'\n    return msg",
    "label": true
  },
  {
    "code": "def _verify_one(req: InstallRequirement, wheel_path: str) -> None:\n    canonical_name = canonicalize_name(req.name or '')\n    w = Wheel(os.path.basename(wheel_path))\n    if canonicalize_name(w.name) != canonical_name:\n        raise InvalidWheelFilename('Wheel has unexpected file name: expected {!r}, got {!r}'.format(canonical_name, w.name))\n    dist = get_wheel_distribution(FilesystemWheel(wheel_path), canonical_name)\n    dist_verstr = str(dist.version)\n    if canonicalize_version(dist_verstr) != canonicalize_version(w.version):\n        raise InvalidWheelFilename('Wheel has unexpected file name: expected {!r}, got {!r}'.format(dist_verstr, w.version))\n    metadata_version_value = dist.metadata_version\n    if metadata_version_value is None:\n        raise UnsupportedWheel('Missing Metadata-Version')\n    try:\n        metadata_version = Version(metadata_version_value)\n    except InvalidVersion:\n        msg = f'Invalid Metadata-Version: {metadata_version_value}'\n        raise UnsupportedWheel(msg)\n    if metadata_version >= Version('1.2') and (not isinstance(dist.version, Version)):\n        raise UnsupportedWheel('Metadata 1.2 mandates PEP 440 version, but {!r} is not'.format(dist_verstr))",
    "label": true
  },
  {
    "code": "def pep561_stub_name(value: str) -> bool:\n    top, *children = value.split('.')\n    if not top.endswith('-stubs'):\n        return False\n    return python_module_name('.'.join([top[:-len('-stubs')], *children]))",
    "label": true
  },
  {
    "code": "def _is_ignored_file(element: str, ignore_list: list[str], ignore_list_re: list[Pattern[str]], ignore_list_paths_re: list[Pattern[str]]) -> bool:\n    element = os.path.normpath(element)\n    basename = os.path.basename(element)\n    return basename in ignore_list or _is_in_ignore_list_re(basename, ignore_list_re) or _is_in_ignore_list_re(element, ignore_list_paths_re)",
    "label": true
  },
  {
    "code": "def _find_local_config(curr_dir: AnyStr) -> Optional[AnyStr]:\n    if curr_dir.endswith('.py'):\n        curr_dir = os.path.dirname(curr_dir)\n    if os.path.exists(os.path.join(curr_dir, 'config', '.pylintrc')):\n        return os.path.join(curr_dir, 'config', '.pylintrc')\n    elif os.path.exists(os.path.join(curr_dir, 'config', 'pylintrc')):\n        return os.path.join(curr_dir, 'config', 'pylintrc')",
    "label": true
  },
  {
    "code": "def identify_python_interpreter(python: str) -> Optional[str]:\n    if os.path.exists(python):\n        if os.path.isdir(python):\n            for exe in ('bin/python', 'Scripts/python.exe'):\n                py = os.path.join(python, exe)\n                if os.path.exists(py):\n                    return py\n        else:\n            return python\n    return None",
    "label": true
  },
  {
    "code": "def _ensure_subclassable(mro_entries):\n\n    def inner(func):\n        if sys.implementation.name == 'pypy' and sys.version_info < (3, 9):\n            cls_dict = {'__call__': staticmethod(func), '__mro_entries__': staticmethod(mro_entries)}\n            t = type(func.__name__, (), cls_dict)\n            return functools.update_wrapper(t(), func)\n        else:\n            func.__mro_entries__ = mro_entries\n            return func\n    return inner",
    "label": true
  },
  {
    "code": "def get_bin_prefix() -> str:\n    if sys.platform[:6] == 'darwin' and sys.prefix[:16] == '/System/Library/':\n        return '/usr/local/bin'\n    return sysconfig.get_paths()['scripts']",
    "label": true
  },
  {
    "code": "def create_list_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 2\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n    out.flags.unset_all(key)\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.append_nest_to_list(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']]', pos):\n        raise suffixed_err(src, pos, \"Expected ']]' at the end of an array declaration\")\n    return (pos + 2, key)",
    "label": true
  },
  {
    "code": "def pass_none(func):\n\n    @functools.wraps(func)\n    def wrapper(param, *args, **kwargs):\n        if param is not None:\n            return func(param, *args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def _getattribute(obj, name):\n    for subpath in name.split('.'):\n        if subpath == '<locals>':\n            raise AttributeError(\"Can't get local attribute {!r} on {!r}\".format(name, obj))\n        try:\n            parent = obj\n            obj = getattr(obj, subpath)\n        except AttributeError:\n            raise AttributeError(\"Can't get attribute {!r} on {!r}\".format(name, obj))\n    return (obj, parent)",
    "label": true
  },
  {
    "code": "def _query_cpu() -> int | None:\n    cpu_quota, avail_cpu = (None, None)\n    if Path('/sys/fs/cgroup/cpu/cpu.cfs_quota_us').is_file():\n        with open('/sys/fs/cgroup/cpu/cpu.cfs_quota_us', encoding='utf-8') as file:\n            cpu_quota = int(file.read().rstrip())\n    if cpu_quota and cpu_quota != -1 and Path('/sys/fs/cgroup/cpu/cpu.cfs_period_us').is_file():\n        with open('/sys/fs/cgroup/cpu/cpu.cfs_period_us', encoding='utf-8') as file:\n            cpu_period = int(file.read().rstrip())\n        avail_cpu = int(cpu_quota / cpu_period)\n    elif Path('/sys/fs/cgroup/cpu/cpu.shares').is_file():\n        with open('/sys/fs/cgroup/cpu/cpu.shares', encoding='utf-8') as file:\n            cpu_shares = int(file.read().rstrip())\n        avail_cpu = int(cpu_shares / 1024)\n    if avail_cpu == 0:\n        avail_cpu = 1\n    return avail_cpu",
    "label": true
  },
  {
    "code": "def _always_object(classes):\n    if object not in classes:\n        return classes + (object,)\n    return classes",
    "label": true
  },
  {
    "code": "def test_globals():\n\n    def f():\n        a\n\n        def g():\n            b\n\n            def h():\n                c\n    assert globalvars(f) == dict(a=1, b=2, c=3)\n    res = globalvars(foo, recurse=True)\n    assert set(res) == set(['squared', 'a'])\n    res = globalvars(foo, recurse=False)\n    assert res == {}\n    zap = foo(2)\n    res = globalvars(zap, recurse=True)\n    assert set(res) == set(['squared', 'a'])\n    res = globalvars(zap, recurse=False)\n    assert set(res) == set(['squared'])\n    del zap\n    res = globalvars(squared)\n    assert set(res) == set(['a'])",
    "label": true
  },
  {
    "code": "def unzip(iterable):\n    head, iterable = spy(iter(iterable))\n    if not head:\n        return ()\n    head = head[0]\n    iterables = tee(iterable, len(head))\n\n    def itemgetter(i):\n\n        def getter(obj):\n            try:\n                return obj[i]\n            except IndexError:\n                raise StopIteration\n        return getter\n    return tuple((map(itemgetter(i), it) for i, it in enumerate(iterables)))",
    "label": true
  },
  {
    "code": "def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n    if tokenizer.check('VARIABLE'):\n        return process_env_var(tokenizer.read().text.replace('.', '_'))\n    elif tokenizer.check('QUOTED_STRING'):\n        return process_python_str(tokenizer.read().text)\n    else:\n        tokenizer.raise_syntax_error(message='Expected a marker variable or quoted string')",
    "label": true
  },
  {
    "code": "def _call_reprcompare(ops: Sequence[str], results: Sequence[bool], expls: Sequence[str], each_obj: Sequence[object]) -> str:\n    for i, res, expl in zip(range(len(ops)), results, expls):\n        try:\n            done = not res\n        except Exception:\n            done = True\n        if done:\n            break\n    if util._reprcompare is not None:\n        custom = util._reprcompare(ops[i], each_obj[i], each_obj[i + 1])\n        if custom is not None:\n            return custom\n    return expl",
    "label": true
  },
  {
    "code": "def _is_part_of_assignment_target(node: nodes.NodeNG) -> bool:\n    if isinstance(node.parent, nodes.Assign):\n        return node in node.parent.targets\n    if isinstance(node.parent, nodes.AugAssign):\n        return node == node.parent.target\n    if isinstance(node.parent, (nodes.Tuple, nodes.List)):\n        return _is_part_of_assignment_target(node.parent)\n    return False",
    "label": true
  },
  {
    "code": "def _parseline(path: str, line: str, lineno: int) -> tuple[str | None, str | None]:\n    if iscommentline(line):\n        line = ''\n    else:\n        line = line.rstrip()\n    if not line:\n        return (None, None)\n    if line[0] == '[':\n        realline = line\n        for c in COMMENTCHARS:\n            line = line.split(c)[0].rstrip()\n        if line[-1] == ']':\n            return (line[1:-1], None)\n        return (None, realline.strip())\n    elif not line[0].isspace():\n        try:\n            name, value = line.split('=', 1)\n            if ':' in name:\n                raise ValueError()\n        except ValueError:\n            try:\n                name, value = line.split(':', 1)\n            except ValueError:\n                raise ParseError(path, lineno, 'unexpected line: %r' % line)\n        return (name.strip(), value.strip())\n    else:\n        return (None, line.strip())",
    "label": true
  },
  {
    "code": "def parse_name_and_version(p):\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException(\"Ill-formed name/version string: '%s'\" % p)\n    d = m.groupdict()\n    return (d['name'].strip().lower(), d['ver'])",
    "label": true
  },
  {
    "code": "def deprecated(func):\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        warnings.warn(f'{func.__name__} is deprecated. Use files() instead. Refer to https://importlib-resources.readthedocs.io/en/latest/using.html#migrating-from-legacy for migration advice.', DeprecationWarning, stacklevel=2)\n        return func(*args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def unique_values(func):\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return unique_everseen(func(*args, **kwargs))\n    return wrapper",
    "label": true
  },
  {
    "code": "def _subst_compat(s):\n\n    def _subst(match):\n        return f'{{{match.group(1)}}}'\n    repl = re.sub('\\\\$([a-zA-Z_][a-zA-Z_0-9]*)', _subst, s)\n    if repl != s:\n        import warnings\n        warnings.warn('shell/Perl-style substitutions are deprecated', DeprecationWarning)\n    return repl",
    "label": true
  },
  {
    "code": "def _temporary_keychain():\n    random_bytes = os.urandom(40)\n    filename = base64.b16encode(random_bytes[:8]).decode('utf-8')\n    password = base64.b16encode(random_bytes[8:])\n    tempdirectory = tempfile.mkdtemp()\n    keychain_path = os.path.join(tempdirectory, filename).encode('utf-8')\n    keychain = Security.SecKeychainRef()\n    status = Security.SecKeychainCreate(keychain_path, len(password), password, False, None, ctypes.byref(keychain))\n    _assert_no_error(status)\n    return (keychain, tempdirectory)",
    "label": true
  },
  {
    "code": "def select_autoescape(enabled_extensions: t.Collection[str]=('html', 'htm', 'xml'), disabled_extensions: t.Collection[str]=(), default_for_string: bool=True, default: bool=False) -> t.Callable[[t.Optional[str]], bool]:\n    enabled_patterns = tuple((f\".{x.lstrip('.').lower()}\" for x in enabled_extensions))\n    disabled_patterns = tuple((f\".{x.lstrip('.').lower()}\" for x in disabled_extensions))\n\n    def autoescape(template_name: t.Optional[str]) -> bool:\n        if template_name is None:\n            return default_for_string\n        template_name = template_name.lower()\n        if template_name.endswith(enabled_patterns):\n            return True\n        if template_name.endswith(disabled_patterns):\n            return False\n        return default\n    return autoescape",
    "label": true
  },
  {
    "code": "def get_dependent_dists(dists, dist):\n    if dist not in dists:\n        raise DistlibException('given distribution %r is not a member of the list' % dist.name)\n    graph = make_graph(dists)\n    dep = [dist]\n    todo = graph.reverse_list[dist]\n    while todo:\n        d = todo.pop()\n        dep.append(d)\n        for succ in graph.reverse_list[d]:\n            if succ not in dep:\n                todo.append(succ)\n    dep.pop(0)\n    return dep",
    "label": true
  },
  {
    "code": "def show_tags(options: Values) -> None:\n    tag_limit = 10\n    target_python = make_target_python(options)\n    tags = target_python.get_tags()\n    formatted_target = target_python.format_given()\n    suffix = ''\n    if formatted_target:\n        suffix = f' (target: {formatted_target})'\n    msg = 'Compatible tags: {}{}'.format(len(tags), suffix)\n    logger.info(msg)\n    if options.verbose < 1 and len(tags) > tag_limit:\n        tags_limited = True\n        tags = tags[:tag_limit]\n    else:\n        tags_limited = False\n    with indent_log():\n        for tag in tags:\n            logger.info(str(tag))\n        if tags_limited:\n            msg = '...\\n[First {tag_limit} tags shown. Pass --verbose to show all.]'.format(tag_limit=tag_limit)\n            logger.info(msg)",
    "label": true
  },
  {
    "code": "def clean_span_data(raw_spans: str) -> list[float]:\n    span_lengths = raw_spans.split('=')\n    span_lengths.pop(0)\n    span_lengths.pop(0)\n    for i in range(len(span_lengths)):\n        span_lengths[i] = float(span_lengths[i][:span_lengths[i].index(';')])\n    return span_lengths",
    "label": true
  },
  {
    "code": "def _collect_type_vars(types, typevar_types=None):\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if isinstance(t, typevar_types) and t not in tvars and (not _is_unpack(t)):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)",
    "label": true
  },
  {
    "code": "def _padright(width, s):\n    fmt = '{0:<%ds}' % width\n    return fmt.format(s)",
    "label": true
  },
  {
    "code": "def get_empty_parameterset_mark(config: Config, argnames: Sequence[str], func) -> 'MarkDecorator':\n    from ..nodes import Collector\n    fs, lineno = getfslineno(func)\n    reason = 'got empty parameter set %r, function %s at %s:%d' % (argnames, func.__name__, fs, lineno)\n    requested_mark = config.getini(EMPTY_PARAMETERSET_OPTION)\n    if requested_mark in ('', None, 'skip'):\n        mark = MARK_GEN.skip(reason=reason)\n    elif requested_mark == 'xfail':\n        mark = MARK_GEN.xfail(reason=reason, run=False)\n    elif requested_mark == 'fail_at_collect':\n        f_name = func.__name__\n        _, lineno = getfslineno(func)\n        raise Collector.CollectError(\"Empty parameter set in '%s' at line %d\" % (f_name, lineno + 1))\n    else:\n        raise LookupError(requested_mark)\n    return mark",
    "label": true
  },
  {
    "code": "def ensure_directory(path):\n    dirname = os.path.dirname(path)\n    os.makedirs(dirname, exist_ok=True)",
    "label": true
  },
  {
    "code": "def _get_typedict_abc(obj, _dict, attrs, postproc_list):\n    if hasattr(abc, '_get_dump'):\n        registry, _, _, _ = abc._get_dump(obj)\n        register = obj.register\n        postproc_list.extend(((register, (reg(),)) for reg in registry))\n    elif hasattr(obj, '_abc_registry'):\n        registry = obj._abc_registry\n        register = obj.register\n        postproc_list.extend(((register, (reg,)) for reg in registry))\n    else:\n        raise PicklingError('Cannot find registry of ABC %s', obj)\n    if '_abc_registry' in _dict:\n        _dict.pop('_abc_registry', None)\n        _dict.pop('_abc_cache', None)\n        _dict.pop('_abc_negative_cache', None)\n    else:\n        _dict.pop('_abc_impl', None)\n    return (_dict, attrs)",
    "label": true
  },
  {
    "code": "def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:\n    sorted_list = sorted(list_)\n    ranges = []\n    last_write = -1\n    for i in range(len(sorted_list)):\n        if i + 1 < len(sorted_list):\n            if sorted_list[i] == sorted_list[i + 1] - 1:\n                continue\n        current_range = sorted_list[last_write + 1:i + 1]\n        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))\n        last_write = i\n    return tuple(ranges)",
    "label": true
  },
  {
    "code": "def _function2():\n    try:\n        raise\n    except Exception:\n        from sys import exc_info\n        e, er, tb = exc_info()\n        return (er, tb)",
    "label": true
  },
  {
    "code": "def get_similar_commands(name: str) -> Optional[str]:\n    from difflib import get_close_matches\n    name = name.lower()\n    close_commands = get_close_matches(name, commands_dict.keys())\n    if close_commands:\n        return close_commands[0]\n    else:\n        return None",
    "label": true
  },
  {
    "code": "def resolve_ssl_version(candidate):\n    if candidate is None:\n        return PROTOCOL_TLS\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'PROTOCOL_' + candidate)\n        return res\n    return candidate",
    "label": true
  },
  {
    "code": "def zip_item_is_executable(info: ZipInfo) -> bool:\n    mode = info.external_attr >> 16\n    return bool(mode and stat.S_ISREG(mode) and mode & 73)",
    "label": true
  },
  {
    "code": "def compress_for_output_listing(paths: Iterable[str]) -> Tuple[Set[str], Set[str]]:\n    will_remove = set(paths)\n    will_skip = set()\n    folders = set()\n    files = set()\n    for path in will_remove:\n        if path.endswith('.pyc'):\n            continue\n        if path.endswith('__init__.py') or '.dist-info' in path:\n            folders.add(os.path.dirname(path))\n        files.add(path)\n    _normcased_files = set(map(os.path.normcase, files))\n    folders = compact(folders)\n    for folder in folders:\n        for dirpath, _, dirfiles in os.walk(folder):\n            for fname in dirfiles:\n                if fname.endswith('.pyc'):\n                    continue\n                file_ = os.path.join(dirpath, fname)\n                if os.path.isfile(file_) and os.path.normcase(file_) not in _normcased_files:\n                    will_skip.add(file_)\n    will_remove = files | {os.path.join(folder, '*') for folder in folders}\n    return (will_remove, will_skip)",
    "label": true
  },
  {
    "code": "def _indented_config(config: Config, indent: str) -> Config:\n    if not indent:\n        return config\n    return Config(config=config, line_length=max(config.line_length - len(indent), 0), wrap_length=max(config.wrap_length - len(indent), 0), lines_after_imports=1, import_headings=config.import_headings if config.indented_import_headings else {}, import_footers=config.import_footers if config.indented_import_headings else {})",
    "label": true
  },
  {
    "code": "def get_source_file(filename: str, include_no_ext: bool=False) -> str:\n    filename = os.path.abspath(_path_from_filename(filename))\n    base, orig_ext = os.path.splitext(filename)\n    for ext in PY_SOURCE_EXTS:\n        source_path = f'{base}.{ext}'\n        if os.path.exists(source_path):\n            return source_path\n    if include_no_ext and (not orig_ext) and os.path.exists(base):\n        return base\n    raise NoSourceFile(filename)",
    "label": true
  },
  {
    "code": "def _legacy_cmpkey(version: str) -> LegacyCmpKey:\n    epoch = -1\n    parts: List[str] = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith('*'):\n            if part < '*final':\n                while parts and parts[-1] == '*final-':\n                    parts.pop()\n            while parts and parts[-1] == '00000000':\n                parts.pop()\n        parts.append(part)\n    return (epoch, tuple(parts))",
    "label": true
  },
  {
    "code": "def babel_extract(fileobj: t.BinaryIO, keywords: t.Sequence[str], comment_tags: t.Sequence[str], options: t.Dict[str, t.Any]) -> t.Iterator[t.Tuple[int, str, t.Union[t.Optional[str], t.Tuple[t.Optional[str], ...]], t.List[str]]]:\n    extensions: t.Dict[t.Type[Extension], None] = {}\n    for extension_name in options.get('extensions', '').split(','):\n        extension_name = extension_name.strip()\n        if not extension_name:\n            continue\n        extensions[import_string(extension_name)] = None\n    if InternationalizationExtension not in extensions:\n        extensions[InternationalizationExtension] = None\n\n    def getbool(options: t.Mapping[str, str], key: str, default: bool=False) -> bool:\n        return options.get(key, str(default)).lower() in {'1', 'on', 'yes', 'true'}\n    silent = getbool(options, 'silent', True)\n    environment = Environment(options.get('block_start_string', defaults.BLOCK_START_STRING), options.get('block_end_string', defaults.BLOCK_END_STRING), options.get('variable_start_string', defaults.VARIABLE_START_STRING), options.get('variable_end_string', defaults.VARIABLE_END_STRING), options.get('comment_start_string', defaults.COMMENT_START_STRING), options.get('comment_end_string', defaults.COMMENT_END_STRING), options.get('line_statement_prefix') or defaults.LINE_STATEMENT_PREFIX, options.get('line_comment_prefix') or defaults.LINE_COMMENT_PREFIX, getbool(options, 'trim_blocks', defaults.TRIM_BLOCKS), getbool(options, 'lstrip_blocks', defaults.LSTRIP_BLOCKS), defaults.NEWLINE_SEQUENCE, getbool(options, 'keep_trailing_newline', defaults.KEEP_TRAILING_NEWLINE), tuple(extensions), cache_size=0, auto_reload=False)\n    if getbool(options, 'trimmed'):\n        environment.policies['ext.i18n.trimmed'] = True\n    if getbool(options, 'newstyle_gettext'):\n        environment.newstyle_gettext = True\n    source = fileobj.read().decode(options.get('encoding', 'utf-8'))\n    try:\n        node = environment.parse(source)\n        tokens = list(environment.lex(environment.preprocess(source)))\n    except TemplateSyntaxError:\n        if not silent:\n            raise\n        return\n    finder = _CommentFinder(tokens, comment_tags)\n    for lineno, func, message in extract_from_ast(node, keywords):\n        yield (lineno, func, message, finder.find_comments(lineno))",
    "label": true
  },
  {
    "code": "def get_cache_dir(file_path: Path) -> Path:\n    if sys.version_info >= (3, 8) and sys.pycache_prefix:\n        return Path(sys.pycache_prefix) / Path(*file_path.parts[1:-1])\n    else:\n        return file_path.parent / '__pycache__'",
    "label": true
  },
  {
    "code": "def split_auth_from_netloc(netloc: str) -> NetlocTuple:\n    if '@' not in netloc:\n        return (netloc, (None, None))\n    auth, netloc = netloc.rsplit('@', 1)\n    pw: Optional[str] = None\n    if ':' in auth:\n        user, pw = auth.split(':', 1)\n    else:\n        user, pw = (auth, None)\n    user = urllib.parse.unquote(user)\n    if pw is not None:\n        pw = urllib.parse.unquote(pw)\n    return (netloc, (user, pw))",
    "label": true
  },
  {
    "code": "def invalid_marker(text):\n    try:\n        evaluate_marker(text)\n    except SyntaxError as e:\n        e.filename = None\n        e.lineno = None\n        return e\n    return False",
    "label": true
  },
  {
    "code": "def set_cell_size(text: str, total: int) -> str:\n    if _is_single_cell_widths(text):\n        size = len(text)\n        if size < total:\n            return text + ' ' * (total - size)\n        return text[:total]\n    if total <= 0:\n        return ''\n    cell_size = cell_len(text)\n    if cell_size == total:\n        return text\n    if cell_size < total:\n        return text + ' ' * (total - cell_size)\n    start = 0\n    end = len(text)\n    while True:\n        pos = (start + end) // 2\n        before = text[:pos + 1]\n        before_len = cell_len(before)\n        if before_len == total + 1 and cell_len(before[-1]) == 2:\n            return before[:-1] + ' '\n        if before_len == total:\n            return before\n        if before_len > total:\n            end = pos\n        else:\n            start = pos",
    "label": true
  },
  {
    "code": "def combination_index(element, iterable):\n    element = enumerate(element)\n    k, y = next(element, (None, None))\n    if k is None:\n        return 0\n    indexes = []\n    pool = enumerate(iterable)\n    for n, x in pool:\n        if x == y:\n            indexes.append(n)\n            tmp, y = next(element, (None, None))\n            if tmp is None:\n                break\n            else:\n                k = tmp\n    else:\n        raise ValueError('element is not a combination of iterable')\n    n, _ = last(pool, default=(n, None))\n    index = 1\n    for i, j in enumerate(reversed(indexes), start=1):\n        j = n - j\n        if i <= j:\n            index += factorial(j) // (factorial(i) * factorial(j - i))\n    return factorial(n + 1) // (factorial(k + 1) * factorial(n - k)) - index",
    "label": true
  },
  {
    "code": "def _default_key_normalizer(key_class, request_context):\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n    return key_class(**context)",
    "label": true
  },
  {
    "code": "def _rebuild_mod_path(orig_path, package_name, module):\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path",
    "label": true
  },
  {
    "code": "def grouper(iterable, n, incomplete='fill', fillvalue=None):\n    args = [iter(iterable)] * n\n    if incomplete == 'fill':\n        return zip_longest(*args, fillvalue=fillvalue)\n    if incomplete == 'strict':\n        return _zip_equal(*args)\n    if incomplete == 'ignore':\n        return zip(*args)\n    else:\n        raise ValueError('Expected fill, strict, or ignore')",
    "label": true
  },
  {
    "code": "def bind_context_to_node(context: InferenceContext | None, node) -> InferenceContext:\n    context = copy_context(context)\n    context.boundnode = node\n    return context",
    "label": true
  },
  {
    "code": "def is_terminating_func(node: nodes.Call) -> bool:\n    if not isinstance(node.func, nodes.Attribute) and (not isinstance(node.func, nodes.Name)) or isinstance(node.parent, nodes.Lambda):\n        return False\n    try:\n        for inferred in node.func.infer():\n            if hasattr(inferred, 'qname') and inferred.qname() in TERMINATING_FUNCS_QNAMES:\n                return True\n    except (StopIteration, astroid.InferenceError):\n        pass\n    return False",
    "label": true
  },
  {
    "code": "def sliced(seq, n, strict=False):\n    iterator = takewhile(len, (seq[i:i + n] for i in count(0, n)))\n    if strict:\n\n        def ret():\n            for _slice in iterator:\n                if len(_slice) != n:\n                    raise ValueError('seq is not divisible by n.')\n                yield _slice\n        return iter(ret())\n    else:\n        return iterator",
    "label": true
  },
  {
    "code": "def class_is_abstract(node: nodes.ClassDef) -> bool:\n    if is_protocol_class(node):\n        return True\n    meta = node.declared_metaclass()\n    if meta is not None:\n        if meta.name == 'ABCMeta' and meta.root().name in ABC_MODULES:\n            return True\n    for ancestor in node.ancestors():\n        if ancestor.name == 'ABC' and ancestor.root().name in ABC_MODULES:\n            return True\n    for method in node.methods():\n        if method.parent.frame(future=True) is node:\n            if method.is_abstract(pass_is_abstract=False):\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def report_similarities(sect: Section, stats: LinterStats, old_stats: LinterStats | None) -> None:\n    lines = ['', 'now', 'previous', 'difference']\n    lines += table_lines_from_stats(stats, old_stats, 'duplicated_lines')\n    sect.append(Table(children=lines, cols=4, rheaders=1, cheaders=1))",
    "label": true
  },
  {
    "code": "def pytest_runtest_protocol(item: Item, nextitem: Optional[Item]) -> bool:\n    ihook = item.ihook\n    ihook.pytest_runtest_logstart(nodeid=item.nodeid, location=item.location)\n    runtestprotocol(item, nextitem=nextitem)\n    ihook.pytest_runtest_logfinish(nodeid=item.nodeid, location=item.location)\n    return True",
    "label": true
  },
  {
    "code": "def _update_zipimporter_cache(normalized_path, cache, updater=None):\n    for p in _collect_zipimporter_cache_entries(normalized_path, cache):\n        old_entry = cache[p]\n        del cache[p]\n        new_entry = updater and updater(p, old_entry)\n        if new_entry is not None:\n            cache[p] = new_entry",
    "label": true
  },
  {
    "code": "def add_completion_class(cls: ShellCompleteType, name: t.Optional[str]=None) -> ShellCompleteType:\n    if name is None:\n        name = cls.name\n    _available_shells[name] = cls\n    return cls",
    "label": true
  },
  {
    "code": "def _basic_auth_str(username, password):\n    if not isinstance(username, basestring):\n        warnings.warn(\"Non-string usernames will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.\".format(username), category=DeprecationWarning)\n        username = str(username)\n    if not isinstance(password, basestring):\n        warnings.warn(\"Non-string passwords will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.\".format(type(password)), category=DeprecationWarning)\n        password = str(password)\n    if isinstance(username, str):\n        username = username.encode('latin1')\n    if isinstance(password, str):\n        password = password.encode('latin1')\n    authstr = 'Basic ' + to_native_string(b64encode(b':'.join((username, password))).strip())\n    return authstr",
    "label": true
  },
  {
    "code": "def build_url_from_netloc(netloc: str, scheme: str='https') -> str:\n    if netloc.count(':') >= 2 and '@' not in netloc and ('[' not in netloc):\n        netloc = f'[{netloc}]'\n    return f'{scheme}://{netloc}'",
    "label": true
  },
  {
    "code": "def _get_versions(s):\n    result = []\n    for m in _VERSION_PATTERN.finditer(s):\n        result.append(NV(m.groups()[0]))\n    return set(result)",
    "label": true
  },
  {
    "code": "def get_file_content(url: str, session: PipSession) -> Tuple[str, str]:\n    scheme = get_url_scheme(url)\n    if scheme in ['http', 'https', 'file']:\n        resp = session.get(url)\n        raise_for_status(resp)\n        return (resp.url, resp.text)\n    try:\n        with open(url, 'rb') as f:\n            content = auto_decode(f.read())\n    except OSError as exc:\n        raise InstallationError(f'Could not open requirements file: {exc}')\n    return (url, content)",
    "label": true
  },
  {
    "code": "def value_chain(*args):\n    for value in args:\n        if isinstance(value, (str, bytes)):\n            yield value\n            continue\n        try:\n            yield from value\n        except TypeError:\n            yield value",
    "label": true
  },
  {
    "code": "def _no_global_under_venv() -> bool:\n    cfg_lines = _get_pyvenv_cfg_lines()\n    if cfg_lines is None:\n        logger.warning(\"Could not access 'pyvenv.cfg' despite a virtual environment being active. Assuming global site-packages is not accessible in this environment.\")\n        return True\n    for line in cfg_lines:\n        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)\n        if match is not None and match.group('value') == 'false':\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def parse_key_part(src: str, pos: Pos) -> tuple[Pos, str]:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char in BARE_KEY_CHARS:\n        start_pos = pos\n        pos = skip_chars(src, pos, BARE_KEY_CHARS)\n        return (pos, src[start_pos:pos])\n    if char == \"'\":\n        return parse_literal_str(src, pos)\n    if char == '\"':\n        return parse_one_line_basic_str(src, pos)\n    raise suffixed_err(src, pos, 'Invalid initial character for a key part')",
    "label": true
  },
  {
    "code": "def returns_something(return_node: nodes.Return) -> bool:\n    returns = return_node.value\n    if returns is None:\n        return False\n    return not (isinstance(returns, nodes.Const) and returns.value is None)",
    "label": true
  },
  {
    "code": "def is_balanced(tree: BinarySearchTree) -> bool:\n    if tree.is_empty():\n        return True\n    else:\n        return all((abs(tree._left.height() - tree._right.height()) <= 1, is_balanced(tree._left), is_balanced(tree._right)))",
    "label": true
  },
  {
    "code": "def read_chunks(file: BinaryIO, size: int=io.DEFAULT_BUFFER_SIZE) -> Generator[bytes, None, None]:\n    while True:\n        chunk = file.read(size)\n        if not chunk:\n            break\n        yield chunk",
    "label": true
  },
  {
    "code": "def _pipepager(generator: t.Iterable[str], cmd: str, color: t.Optional[bool]) -> None:\n    import subprocess\n    env = dict(os.environ)\n    cmd_detail = cmd.rsplit('/', 1)[-1].split()\n    if color is None and cmd_detail[0] == 'less':\n        less_flags = f\"{os.environ.get('LESS', '')}{' '.join(cmd_detail[1:])}\"\n        if not less_flags:\n            env['LESS'] = '-R'\n            color = True\n        elif 'r' in less_flags or 'R' in less_flags:\n            color = True\n    c = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, env=env)\n    stdin = t.cast(t.BinaryIO, c.stdin)\n    encoding = get_best_encoding(stdin)\n    try:\n        for text in generator:\n            if not color:\n                text = strip_ansi(text)\n            stdin.write(text.encode(encoding, 'replace'))\n    except (OSError, KeyboardInterrupt):\n        pass\n    else:\n        stdin.close()\n    while True:\n        try:\n            c.wait()\n        except KeyboardInterrupt:\n            pass\n        else:\n            break",
    "label": true
  },
  {
    "code": "def iter_entry_points(group_name):\n    try:\n        from importlib.metadata import entry_points\n    except ImportError:\n        try:\n            from importlib_metadata import entry_points\n        except ImportError:\n            try:\n                from pip._vendor.pkg_resources import iter_entry_points\n            except (ImportError, OSError):\n                return []\n            else:\n                return iter_entry_points(group_name)\n    groups = entry_points()\n    if hasattr(groups, 'select'):\n        return groups.select(group=group_name)\n    else:\n        return groups.get(group_name, [])",
    "label": true
  },
  {
    "code": "def compress_file(in_file: str, out_file: str) -> None:\n    with open(in_file, 'rb') as f1:\n        text = f1.read()\n    freq = build_frequency_dict(text)\n    tree = build_huffman_tree(freq)\n    codes = get_codes(tree)\n    number_nodes(tree)\n    print('Bits per symbol:', avg_length(tree, freq))\n    result = tree.num_nodes_to_bytes() + tree_to_bytes(tree) + int32_to_bytes(len(text))\n    result += compress_bytes(text, codes)\n    with open(out_file, 'wb') as f2:\n        f2.write(result)",
    "label": true
  },
  {
    "code": "def object_build_methoddescriptor(node: nodes.Module | nodes.ClassDef, member: _FunctionTypes, localname: str) -> None:\n    func = build_function(getattr(member, '__name__', None) or localname, doc=member.__doc__)\n    node.add_local_node(func, localname)\n    _add_dunder_class(func, member)",
    "label": true
  },
  {
    "code": "def retry(*dargs: t.Any, **dkw: t.Any) -> t.Any:\n    if len(dargs) == 1 and callable(dargs[0]):\n        return retry()(dargs[0])\n    else:\n\n        def wrap(f: WrappedFn) -> WrappedFn:\n            if isinstance(f, retry_base):\n                warnings.warn(f'Got retry_base instance ({f.__class__.__name__}) as callable argument, this will probably hang indefinitely (did you mean retry={f.__class__.__name__}(...)?)')\n            r: 'BaseRetrying'\n            if iscoroutinefunction(f):\n                r = AsyncRetrying(*dargs, **dkw)\n            elif tornado and hasattr(tornado.gen, 'is_coroutine_function') and tornado.gen.is_coroutine_function(f):\n                r = TornadoRetrying(*dargs, **dkw)\n            else:\n                r = Retrying(*dargs, **dkw)\n            return r.wraps(f)\n        return wrap",
    "label": true
  },
  {
    "code": "def skip_comment(src: str, pos: Pos) -> Pos:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char == '#':\n        return skip_until(src, pos + 1, '\\n', error_on=ILLEGAL_COMMENT_CHARS, error_on_eof=False)\n    return pos",
    "label": true
  },
  {
    "code": "def interleave_evenly(iterables, lengths=None):\n    if lengths is None:\n        try:\n            lengths = [len(it) for it in iterables]\n        except TypeError:\n            raise ValueError('Iterable lengths could not be determined automatically. Specify them with the lengths keyword.')\n    elif len(iterables) != len(lengths):\n        raise ValueError('Mismatching number of iterables and lengths.')\n    dims = len(lengths)\n    lengths_permute = sorted(range(dims), key=lambda i: lengths[i], reverse=True)\n    lengths_desc = [lengths[i] for i in lengths_permute]\n    iters_desc = [iter(iterables[i]) for i in lengths_permute]\n    delta_primary, deltas_secondary = (lengths_desc[0], lengths_desc[1:])\n    iter_primary, iters_secondary = (iters_desc[0], iters_desc[1:])\n    errors = [delta_primary // dims] * len(deltas_secondary)\n    to_yield = sum(lengths)\n    while to_yield:\n        yield next(iter_primary)\n        to_yield -= 1\n        errors = [e - delta for e, delta in zip(errors, deltas_secondary)]\n        for i, e in enumerate(errors):\n            if e < 0:\n                yield next(iters_secondary[i])\n                to_yield -= 1\n                errors[i] += delta_primary",
    "label": true
  },
  {
    "code": "def response_chunks(response: Response, chunk_size: int=CONTENT_CHUNK_SIZE) -> Generator[bytes, None, None]:\n    try:\n        for chunk in response.raw.stream(chunk_size, decode_content=False):\n            yield chunk\n    except AttributeError:\n        while True:\n            chunk = response.raw.read(chunk_size)\n            if not chunk:\n                break\n            yield chunk",
    "label": true
  },
  {
    "code": "def _is_incomplete_argument(ctx: Context, param: Parameter) -> bool:\n    if not isinstance(param, Argument):\n        return False\n    assert param.name is not None\n    value = ctx.params.get(param.name)\n    return param.nargs == -1 or ctx.get_parameter_source(param.name) is not ParameterSource.COMMANDLINE or (param.nargs > 1 and isinstance(value, (tuple, list)) and (len(value) < param.nargs))",
    "label": true
  },
  {
    "code": "def _parse_specifier(tokenizer: Tokenizer) -> str:\n    with tokenizer.enclosing_tokens('LEFT_PARENTHESIS', 'RIGHT_PARENTHESIS', around='version specifier'):\n        tokenizer.consume('WS')\n        parsed_specifiers = _parse_version_many(tokenizer)\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def connection_requires_http_tunnel(proxy_url=None, proxy_config=None, destination_scheme=None):\n    if proxy_url is None:\n        return False\n    if destination_scheme == 'http':\n        return False\n    if proxy_url.scheme == 'https' and proxy_config and proxy_config.use_forwarding_for_https:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def make_safe_parse_float(parse_float: ParseFloat) -> ParseFloat:\n    if parse_float is float:\n        return float\n\n    def safe_parse_float(float_str: str) -> Any:\n        float_value = parse_float(float_str)\n        if isinstance(float_value, (dict, list)):\n            raise ValueError('parse_float must not return dicts or lists')\n        return float_value\n    return safe_parse_float",
    "label": true
  },
  {
    "code": "def deprecated_call(func: Optional[Callable[..., Any]]=None, *args: Any, **kwargs: Any) -> Union['WarningsRecorder', Any]:\n    __tracebackhide__ = True\n    if func is not None:\n        args = (func,) + args\n    return warns((DeprecationWarning, PendingDeprecationWarning), *args, **kwargs)",
    "label": true
  },
  {
    "code": "def _is_compatible(name: str, arch: str, version: _GLibCVersion) -> bool:\n    sys_glibc = _get_glibc_version()\n    if sys_glibc < version:\n        return False\n    try:\n        import _manylinux\n    except ImportError:\n        return True\n    if hasattr(_manylinux, 'manylinux_compatible'):\n        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n        if result is not None:\n            return bool(result)\n        return True\n    if version == _GLibCVersion(2, 5):\n        if hasattr(_manylinux, 'manylinux1_compatible'):\n            return bool(_manylinux.manylinux1_compatible)\n    if version == _GLibCVersion(2, 12):\n        if hasattr(_manylinux, 'manylinux2010_compatible'):\n            return bool(_manylinux.manylinux2010_compatible)\n    if version == _GLibCVersion(2, 17):\n        if hasattr(_manylinux, 'manylinux2014_compatible'):\n            return bool(_manylinux.manylinux2014_compatible)\n    return True",
    "label": true
  },
  {
    "code": "def allexcept(*args):\n    newcats = cats[:]\n    for arg in args:\n        newcats.remove(arg)\n    return ''.join((globals()[cat] for cat in newcats))",
    "label": true
  },
  {
    "code": "def apply_configuration(dist: 'Distribution', filepath: _Path, ignore_option_errors=False) -> 'Distribution':\n    config = read_configuration(filepath, True, ignore_option_errors, dist)\n    return _apply(dist, config, filepath)",
    "label": true
  },
  {
    "code": "def is_valid_cidr(string_network):\n    if string_network.count('/') == 1:\n        try:\n            mask = int(string_network.split('/')[1])\n        except ValueError:\n            return False\n        if mask < 1 or mask > 32:\n            return False\n        try:\n            socket.inet_aton(string_network.split('/')[0])\n        except OSError:\n            return False\n    else:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def _normalize(*values: str, key: str) -> Tuple[str, ...]:\n    if key == 'extra':\n        return tuple((canonicalize_name(v) for v in values))\n    return values",
    "label": true
  },
  {
    "code": "def warn_legacy_versions_and_specifiers(package_set: PackageSet) -> None:\n    for project_name, package_details in package_set.items():\n        if isinstance(package_details.version, LegacyVersion):\n            deprecated(reason=f'{project_name} {package_details.version} has a non-standard version number.', replacement=f'to upgrade to a newer version of {project_name} or contact the author to suggest that they release a version with a conforming version number', issue=12063, gone_in='23.3')\n        for dep in package_details.dependencies:\n            if any((isinstance(spec, LegacySpecifier) for spec in dep.specifier)):\n                deprecated(reason=f'{project_name} {package_details.version} has a non-standard dependency specifier {dep}.', replacement=f'to upgrade to a newer version of {project_name} or contact the author to suggest that they release a version with a conforming dependency specifiers', issue=12063, gone_in='23.3')",
    "label": true
  },
  {
    "code": "def rm_rf(path: Path) -> None:\n    path = ensure_extended_length_path(path)\n    onerror = partial(on_rm_rf_error, start_path=path)\n    if sys.version_info >= (3, 12):\n        shutil.rmtree(str(path), onexc=onerror)\n    else:\n        shutil.rmtree(str(path), onerror=onerror)",
    "label": true
  },
  {
    "code": "def iter_rows(rows: t.Iterable[t.Tuple[str, str]], col_count: int) -> t.Iterator[t.Tuple[str, ...]]:\n    for row in rows:\n        yield (row + ('',) * (col_count - len(row)))",
    "label": true
  },
  {
    "code": "def arguments_assigned_stmts(self: nodes.Arguments, node: node_classes.AssignedStmtsPossibleNode=None, context: InferenceContext | None=None, assign_path: list[int] | None=None) -> Any:\n    try:\n        node_name = node.name\n    except AttributeError:\n        node_name = None\n    if context and context.callcontext:\n        callee = context.callcontext.callee\n        while hasattr(callee, '_proxied'):\n            callee = callee._proxied\n    else:\n        return _arguments_infer_argname(self, node_name, context)\n    if node and getattr(callee, 'name', None) == node.frame(future=True).name:\n        callcontext = context.callcontext\n        context = copy_context(context)\n        context.callcontext = None\n        args = arguments.CallSite(callcontext, context=context)\n        return args.infer_argument(self.parent, node_name, context)\n    return _arguments_infer_argname(self, node_name, context)",
    "label": true
  },
  {
    "code": "def load_results(base: str) -> LinterStats | None:\n    from pylint.lint.caching import load_results as _real_load_results\n    warnings.warn(\"'pylint.config.load_results' is deprecated, please use 'pylint.lint.load_results' instead. This will be removed in 3.0.\", DeprecationWarning, stacklevel=2)\n    return _real_load_results(base, PYLINT_HOME)",
    "label": true
  },
  {
    "code": "def test_pickle_to_tempfile():\n    if not WINDOWS:\n        dumpfile = dump(x)\n        _x = load(dumpfile)\n        assert _x == x",
    "label": true
  },
  {
    "code": "def load_config_dict_from_file(filepath: Path) -> Optional[Dict[str, Union[str, List[str]]]]:\n    if filepath.suffix == '.ini':\n        iniconfig = _parse_ini_config(filepath)\n        if 'pytest' in iniconfig:\n            return dict(iniconfig['pytest'].items())\n        elif filepath.name == 'pytest.ini':\n            return {}\n    elif filepath.suffix == '.cfg':\n        iniconfig = _parse_ini_config(filepath)\n        if 'tool:pytest' in iniconfig.sections:\n            return dict(iniconfig['tool:pytest'].items())\n        elif 'pytest' in iniconfig.sections:\n            fail(CFG_PYTEST_SECTION.format(filename='setup.cfg'), pytrace=False)\n    elif filepath.suffix == '.toml':\n        if sys.version_info >= (3, 11):\n            import tomllib\n        else:\n            import tomli as tomllib\n        toml_text = filepath.read_text(encoding='utf-8')\n        try:\n            config = tomllib.loads(toml_text)\n        except tomllib.TOMLDecodeError as exc:\n            raise UsageError(f'{filepath}: {exc}') from exc\n        result = config.get('tool', {}).get('pytest', {}).get('ini_options', None)\n        if result is not None:\n\n            def make_scalar(v: object) -> Union[str, List[str]]:\n                return v if isinstance(v, list) else str(v)\n            return {k: make_scalar(v) for k, v in result.items()}\n    return None",
    "label": true
  },
  {
    "code": "def _find_top_level_modules(dist: Distribution) -> Iterator[str]:\n    py_modules = dist.py_modules or []\n    yield from (mod for mod in py_modules if '.' not in mod)\n    if not dist.ext_package:\n        ext_modules = dist.ext_modules or []\n        yield from (x.name for x in ext_modules if '.' not in x.name)",
    "label": true
  },
  {
    "code": "def shell_complete(cli: BaseCommand, ctx_args: t.MutableMapping[str, t.Any], prog_name: str, complete_var: str, instruction: str) -> int:\n    shell, _, instruction = instruction.partition('_')\n    comp_cls = get_completion_class(shell)\n    if comp_cls is None:\n        return 1\n    comp = comp_cls(cli, ctx_args, prog_name, complete_var)\n    if instruction == 'source':\n        echo(comp.source())\n        return 0\n    if instruction == 'complete':\n        echo(comp.complete())\n        return 0\n    return 1",
    "label": true
  },
  {
    "code": "def _have_working_poll():\n    try:\n        poll_obj = select.poll()\n        _retry_on_intr(poll_obj.poll, 0)\n    except (AttributeError, OSError):\n        return False\n    else:\n        return True",
    "label": true
  },
  {
    "code": "def _find_all_simple(path):\n    results = (os.path.join(base, file) for base, dirs, files in os.walk(path, followlinks=True) for file in files)\n    return filter(os.path.isfile, results)",
    "label": true
  },
  {
    "code": "def format_header_param_html5(name: str, value: _TYPE_FIELD_VALUE) -> str:\n    import warnings\n    warnings.warn(\"'format_header_param_html5' has been renamed to 'format_multipart_header_param'. The old name will be removed in urllib3 v2.1.0.\", DeprecationWarning, stacklevel=2)\n    return format_multipart_header_param(name, value)",
    "label": true
  },
  {
    "code": "def stagger(iterable, offsets=(-1, 0, 1), longest=False, fillvalue=None):\n    children = tee(iterable, len(offsets))\n    return zip_offset(*children, offsets=offsets, longest=longest, fillvalue=fillvalue)",
    "label": true
  },
  {
    "code": "def _create_ftype(ftypeobj, func, args, kwds):\n    if kwds is None:\n        kwds = {}\n    if args is None:\n        args = ()\n    return ftypeobj(func, *args, **kwds)",
    "label": true
  },
  {
    "code": "def _formatted_traceback(file_name) -> str:\n    exception_traceback = sys.exc_info()[2]\n    stack_size = len(traceback.extract_tb(exception_traceback))\n    exception_message = traceback.format_exc(limit=-(stack_size - 2))\n    main_path = os.path.abspath(file_name)\n    formatted_exception = exception_message.replace('<string>', main_path)\n    return formatted_exception",
    "label": true
  },
  {
    "code": "def first(iterable, default=_marker):\n    try:\n        return next(iter(iterable))\n    except StopIteration as e:\n        if default is _marker:\n            raise ValueError('first() was called on an empty iterable, and no default value was provided.') from e\n        return default",
    "label": true
  },
  {
    "code": "def get_text_stdout(encoding: t.Optional[str]=None, errors: t.Optional[str]=None) -> t.TextIO:\n    rv = _get_windows_console_stream(sys.stdout, encoding, errors)\n    if rv is not None:\n        return rv\n    return _force_correct_text_writer(sys.stdout, encoding, errors, force_writable=True)",
    "label": true
  },
  {
    "code": "def format_header_param_html5(name, value):\n    if isinstance(value, six.binary_type):\n        value = value.decode('utf-8')\n    value = _replace_multiple(value, _HTML5_REPLACEMENTS)\n    return u'%s=\"%s\"' % (name, value)",
    "label": true
  },
  {
    "code": "def format_header_param_rfc2231(name, value):\n    if isinstance(value, six.binary_type):\n        value = value.decode('utf-8')\n    if not any((ch in value for ch in '\"\\\\\\r\\n')):\n        result = u'%s=\"%s\"' % (name, value)\n        try:\n            result.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n        else:\n            return result\n    if six.PY2:\n        value = value.encode('utf-8')\n    value = email.utils.encode_rfc2231(value, 'utf-8')\n    value = '%s*=%s' % (name, value)\n    if six.PY2:\n        value = value.decode('utf-8')\n    return value",
    "label": true
  },
  {
    "code": "def _declare_state(vartype, **kw):\n    globals().update(kw)\n    _state_vars.update(dict.fromkeys(kw, vartype))",
    "label": true
  },
  {
    "code": "def make_setuptools_egg_info_args(setup_py_path: str, egg_info_dir: Optional[str], no_user_config: bool) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, no_user_config=no_user_config)\n    args += ['egg_info']\n    if egg_info_dir:\n        args += ['--egg-base', egg_info_dir]\n    return args",
    "label": true
  },
  {
    "code": "def _load_items_from_file(keychain, path):\n    certificates = []\n    identities = []\n    result_array = None\n    with open(path, 'rb') as f:\n        raw_filedata = f.read()\n    try:\n        filedata = CoreFoundation.CFDataCreate(CoreFoundation.kCFAllocatorDefault, raw_filedata, len(raw_filedata))\n        result_array = CoreFoundation.CFArrayRef()\n        result = Security.SecItemImport(filedata, None, None, None, 0, None, keychain, ctypes.byref(result_array))\n        _assert_no_error(result)\n        result_count = CoreFoundation.CFArrayGetCount(result_array)\n        for index in range(result_count):\n            item = CoreFoundation.CFArrayGetValueAtIndex(result_array, index)\n            item = ctypes.cast(item, CoreFoundation.CFTypeRef)\n            if _is_cert(item):\n                CoreFoundation.CFRetain(item)\n                certificates.append(item)\n            elif _is_identity(item):\n                CoreFoundation.CFRetain(item)\n                identities.append(item)\n    finally:\n        if result_array:\n            CoreFoundation.CFRelease(result_array)\n        CoreFoundation.CFRelease(filedata)\n    return (identities, certificates)",
    "label": true
  },
  {
    "code": "def _parse_project_urls(data: List[str]) -> Dict[str, str]:\n    urls = {}\n    for pair in data:\n        parts = [p.strip() for p in pair.split(',', 1)]\n        parts.extend([''] * max(0, 2 - len(parts)))\n        label, url = parts\n        if label in urls:\n            raise KeyError('duplicate labels in project urls')\n        urls[label] = url\n    return urls",
    "label": true
  },
  {
    "code": "def _resolve_tar_file_or_dir(tar_obj, tar_member_obj):\n    while tar_member_obj is not None and (tar_member_obj.islnk() or tar_member_obj.issym()):\n        linkpath = tar_member_obj.linkname\n        if tar_member_obj.issym():\n            base = posixpath.dirname(tar_member_obj.name)\n            linkpath = posixpath.join(base, linkpath)\n            linkpath = posixpath.normpath(linkpath)\n        tar_member_obj = tar_obj._getmember(linkpath)\n    is_file_or_dir = tar_member_obj is not None and (tar_member_obj.isfile() or tar_member_obj.isdir())\n    if is_file_or_dir:\n        return tar_member_obj\n    raise LookupError('Got unknown file type')",
    "label": true
  },
  {
    "code": "def save_socket(pickler, obj):\n    logger.trace(pickler, 'So: %s', obj)\n    pickler.save_reduce(*reduce_socket(obj))\n    logger.trace(pickler, '# So')\n    return",
    "label": true
  },
  {
    "code": "def pytest_sessionfinish(session: Session) -> None:\n    if not session.config.getoption('stepwise'):\n        assert session.config.cache is not None\n        if hasattr(session.config, 'workerinput'):\n            return\n        session.config.cache.set(STEPWISE_CACHE_DIR, [])",
    "label": true
  },
  {
    "code": "def hidden_prompt_func(prompt: str) -> str:\n    import getpass\n    return getpass.getpass(prompt)",
    "label": true
  },
  {
    "code": "def register(linter: PyLinter) -> None:\n    linter.register_checker(ClassChecker(linter))\n    linter.register_checker(SpecialMethodsChecker(linter))",
    "label": true
  },
  {
    "code": "def ansiformat(attr, text):\n    result = []\n    if attr[:1] == attr[-1:] == '+':\n        result.append(codes['blink'])\n        attr = attr[1:-1]\n    if attr[:1] == attr[-1:] == '*':\n        result.append(codes['bold'])\n        attr = attr[1:-1]\n    if attr[:1] == attr[-1:] == '_':\n        result.append(codes['underline'])\n        attr = attr[1:-1]\n    result.append(codes[attr])\n    result.append(text)\n    result.append(codes['reset'])\n    return ''.join(result)",
    "label": true
  },
  {
    "code": "def parse_credentials(netloc):\n    username = password = None\n    if '@' in netloc:\n        prefix, netloc = netloc.rsplit('@', 1)\n        if ':' not in prefix:\n            username = prefix\n        else:\n            username, password = prefix.split(':', 1)\n    if username:\n        username = unquote(username)\n    if password:\n        password = unquote(password)\n    return (username, password, netloc)",
    "label": true
  },
  {
    "code": "def _create_truststore_ssl_context() -> Optional['SSLContext']:\n    if sys.version_info < (3, 10):\n        raise CommandError('The truststore feature is only available for Python 3.10+')\n    try:\n        import ssl\n    except ImportError:\n        logger.warning('Disabling truststore since ssl support is missing')\n        return None\n    try:\n        import truststore\n    except ImportError:\n        raise CommandError(\"To use the truststore feature, 'truststore' must be installed into pip's current environment.\")\n    return truststore.SSLContext(ssl.PROTOCOL_TLS_CLIENT)",
    "label": true
  },
  {
    "code": "def pip_self_version_check(session: PipSession, options: optparse.Values) -> None:\n    installed_dist = get_default_environment().get_distribution('pip')\n    if not installed_dist:\n        return\n    try:\n        upgrade_prompt = _self_version_check_logic(state=SelfCheckState(cache_dir=options.cache_dir), current_time=datetime.datetime.utcnow(), local_version=installed_dist.version, get_remote_version=functools.partial(_get_current_remote_pip_version, session, options))\n        if upgrade_prompt is not None:\n            logger.warning('[present-rich] %s', upgrade_prompt)\n    except Exception:\n        logger.warning('There was an error checking the latest version of pip.')\n        logger.debug('See below for error', exc_info=True)",
    "label": true
  },
  {
    "code": "def show_compilers():\n    from ..ccompiler import show_compilers\n    show_compilers()",
    "label": true
  },
  {
    "code": "def _get_payload(msg: email.message.Message, source: Union[bytes, str]) -> str:\n    if isinstance(source, str):\n        payload: str = msg.get_payload()\n        return payload\n    else:\n        bpayload: bytes = msg.get_payload(decode=True)\n        try:\n            return bpayload.decode('utf8', 'strict')\n        except UnicodeDecodeError:\n            raise ValueError('payload in an invalid encoding')",
    "label": true
  },
  {
    "code": "def _make_new_pgettext(func: t.Callable[[str, str], str]) -> t.Callable[..., str]:\n\n    @pass_context\n    def pgettext(__context: Context, __string_ctx: str, __string: str, **variables: t.Any) -> str:\n        variables.setdefault('context', __string_ctx)\n        rv = __context.call(func, __string_ctx, __string)\n        if __context.eval_ctx.autoescape:\n            rv = Markup(rv)\n        return rv % variables\n    return pgettext",
    "label": true
  },
  {
    "code": "def _apply_diagram_item_enhancements(fn):\n\n    def _inner(element: pyparsing.ParserElement, parent: typing.Optional[EditablePartial], lookup: ConverterState=None, vertical: int=None, index: int=0, name_hint: str=None, show_results_names: bool=False, show_groups: bool=False) -> typing.Optional[EditablePartial]:\n        ret = fn(element, parent, lookup, vertical, index, name_hint, show_results_names, show_groups)\n        if show_results_names and ret is not None:\n            element_results_name = element.resultsName\n            if element_results_name:\n                element_results_name += '' if element.modalResults else '*'\n                ret = EditablePartial.from_call(railroad.Group, item=ret, label=element_results_name)\n        return ret\n    return _inner",
    "label": true
  },
  {
    "code": "def _ignore_import_failure(node: ImportNode, modname: str | None, ignored_modules: Sequence[str]) -> bool:\n    for submodule in _qualified_names(modname):\n        if submodule in ignored_modules:\n            return True\n    if in_type_checking_block(node):\n        return True\n    if isinstance(node.parent, nodes.If) and is_sys_guard(node.parent):\n        return True\n    return node_ignores_exception(node, ImportError)",
    "label": true
  },
  {
    "code": "def get_runnable_pip() -> str:\n    source = pathlib.Path(pip_location).resolve().parent\n    if not source.is_dir():\n        return str(source)\n    return os.fsdecode(source / '__pip-runner__.py')",
    "label": true
  },
  {
    "code": "def _prepare_attribute_parts(attr: t.Optional[t.Union[str, int]]) -> t.List[t.Union[str, int]]:\n    if attr is None:\n        return []\n    if isinstance(attr, str):\n        return [int(x) if x.isdigit() else x for x in attr.split('.')]\n    return [attr]",
    "label": true
  },
  {
    "code": "def nested_expr(opener: Union[str, ParserElement]='(', closer: Union[str, ParserElement]=')', content: typing.Optional[ParserElement]=None, ignore_expr: ParserElement=quoted_string(), *, ignoreExpr: ParserElement=quoted_string()) -> ParserElement:\n    if ignoreExpr != ignore_expr:\n        ignoreExpr = ignore_expr if ignoreExpr == quoted_string() else ignoreExpr\n    if opener == closer:\n        raise ValueError('opening and closing strings cannot be the same')\n    if content is None:\n        if isinstance(opener, str_type) and isinstance(closer, str_type):\n            opener = typing.cast(str, opener)\n            closer = typing.cast(str, closer)\n            if len(opener) == 1 and len(closer) == 1:\n                if ignoreExpr is not None:\n                    content = Combine(OneOrMore(~ignoreExpr + CharsNotIn(opener + closer + ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n                else:\n                    content = empty.copy() + CharsNotIn(opener + closer + ParserElement.DEFAULT_WHITE_CHARS).set_parse_action(lambda t: t[0].strip())\n            elif ignoreExpr is not None:\n                content = Combine(OneOrMore(~ignoreExpr + ~Literal(opener) + ~Literal(closer) + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n            else:\n                content = Combine(OneOrMore(~Literal(opener) + ~Literal(closer) + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))).set_parse_action(lambda t: t[0].strip())\n        else:\n            raise ValueError('opening and closing arguments must be strings if no content expression is given')\n    ret = Forward()\n    if ignoreExpr is not None:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer))\n    else:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ret | content) + Suppress(closer))\n    ret.set_name('nested %s%s expression' % (opener, closer))\n    return ret",
    "label": true
  },
  {
    "code": "def decompress_file(in_file: str, out_file: str) -> None:\n    with open(in_file, 'rb') as f:\n        num_nodes = f.read(1)[0]\n        buf = f.read(num_nodes * 4)\n        node_lst = bytes_to_nodes(buf)\n        tree = generate_tree_general(node_lst, num_nodes - 1)\n        size = bytes_to_int(f.read(4))\n        with open(out_file, 'wb') as g:\n            text = f.read()\n            g.write(decompress_bytes(tree, text, size))",
    "label": true
  },
  {
    "code": "def _pad_version(left: List[str], right: List[str]) -> Tuple[List[str], List[str]]:\n    left_split, right_split = ([], [])\n    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n    left_split.append(left[len(left_split[0]):])\n    right_split.append(right[len(right_split[0]):])\n    left_split.insert(1, ['0'] * max(0, len(right_split[0]) - len(left_split[0])))\n    right_split.insert(1, ['0'] * max(0, len(left_split[0]) - len(right_split[0])))\n    return (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))",
    "label": true
  },
  {
    "code": "def _error_message(func: callable, args: list, error: Exception) -> str:\n    args = str.join(',', map(str, args))\n    return f'The call {func.__name__}({args}) caused an error: {error}'",
    "label": true
  },
  {
    "code": "def _flatten(ll: list) -> list:\n    ret = []\n    for i in ll:\n        if isinstance(i, list):\n            ret.extend(_flatten(i))\n        else:\n            ret.append(i)\n    return ret",
    "label": true
  },
  {
    "code": "def _normalize(*values: str, key: str) -> Tuple[str, ...]:\n    if key == 'extra':\n        return tuple((canonicalize_name(v) for v in values))\n    return values",
    "label": true
  },
  {
    "code": "def _is_multiline(s):\n    if isinstance(s, str):\n        return bool(re.search(_multiline_codes, s))\n    else:\n        return bool(re.search(_multiline_codes_bytes, s))",
    "label": true
  },
  {
    "code": "def ep_matches(ep: EntryPoint, **params) -> bool:\n    try:\n        return ep.matches(**params)\n    except AttributeError:\n        from . import EntryPoint\n        return EntryPoint(ep.name, ep.value, ep.group).matches(**params)",
    "label": true
  },
  {
    "code": "def main(args: Optional[Union[List[str], 'os.PathLike[str]']]=None, plugins: Optional[Sequence[Union[str, _PluggyPlugin]]]=None) -> Union[int, ExitCode]:\n    try:\n        try:\n            config = _prepareconfig(args, plugins)\n        except ConftestImportFailure as e:\n            exc_info = ExceptionInfo.from_exc_info(e.excinfo)\n            tw = TerminalWriter(sys.stderr)\n            tw.line(f\"ImportError while loading conftest '{e.path}'.\", red=True)\n            exc_info.traceback = exc_info.traceback.filter(filter_traceback_for_conftest_import_failure)\n            exc_repr = exc_info.getrepr(style='short', chain=False) if exc_info.traceback else exc_info.exconly()\n            formatted_tb = str(exc_repr)\n            for line in formatted_tb.splitlines():\n                tw.line(line.rstrip(), red=True)\n            return ExitCode.USAGE_ERROR\n        else:\n            try:\n                ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(config=config)\n                try:\n                    return ExitCode(ret)\n                except ValueError:\n                    return ret\n            finally:\n                config._ensure_unconfigure()\n    except UsageError as e:\n        tw = TerminalWriter(sys.stderr)\n        for msg in e.args:\n            tw.line(f'ERROR: {msg}\\n', red=True)\n        return ExitCode.USAGE_ERROR",
    "label": true
  },
  {
    "code": "def make_analysator(f):\n\n    def text_analyse(text):\n        try:\n            rv = f(text)\n        except Exception:\n            return 0.0\n        if not rv:\n            return 0.0\n        try:\n            return min(1.0, max(0.0, float(rv)))\n        except (ValueError, TypeError):\n            return 0.0\n    text_analyse.__doc__ = f.__doc__\n    return staticmethod(text_analyse)",
    "label": true
  },
  {
    "code": "def prepare_map(context: 'Context', args: t.Tuple, kwargs: t.Dict[str, t.Any]) -> t.Callable[[t.Any], t.Any]:\n    if not args and 'attribute' in kwargs:\n        attribute = kwargs.pop('attribute')\n        default = kwargs.pop('default', None)\n        if kwargs:\n            raise FilterArgumentError(f'Unexpected keyword argument {next(iter(kwargs))!r}')\n        func = make_attrgetter(context.environment, attribute, default=default)\n    else:\n        try:\n            name = args[0]\n            args = args[1:]\n        except LookupError:\n            raise FilterArgumentError('map requires a filter argument') from None\n\n        def func(item: t.Any) -> t.Any:\n            return context.environment.call_filter(name, item, args, kwargs, context=context)\n    return func",
    "label": true
  },
  {
    "code": "def compile_type_hint(hint: str) -> CodeType:\n    parsed = parse(hint, '<string>', 'eval')\n    UnionTransformer().visit(parsed)\n    fix_missing_locations(parsed)\n    return compile(parsed, '<string>', 'eval', flags=0)",
    "label": true
  },
  {
    "code": "def _regexp_validator(_: Any, name: str, value: str | re.Pattern[str]) -> re.Pattern[str]:\n    if hasattr(value, 'pattern'):\n        return value\n    return re.compile(value)",
    "label": true
  },
  {
    "code": "def _get_decoder(mode):\n    if ',' in mode:\n        return MultiDecoder(mode)\n    if mode == 'gzip':\n        return GzipDecoder()\n    if brotli is not None and mode == 'br':\n        return BrotliDecoder()\n    return DeflateDecoder()",
    "label": true
  },
  {
    "code": "def _is_invalid_isinstance_type(arg: nodes.NodeNG) -> bool:\n    if PY310_PLUS and isinstance(arg, nodes.BinOp) and (arg.op == '|'):\n        return any((_is_invalid_isinstance_type(elt) and (not is_none(elt)) for elt in (arg.left, arg.right)))\n    inferred = utils.safe_infer(arg)\n    if not inferred:\n        return False\n    if isinstance(inferred, nodes.Tuple):\n        return any((_is_invalid_isinstance_type(elt) for elt in inferred.elts))\n    if isinstance(inferred, nodes.ClassDef):\n        return False\n    if isinstance(inferred, astroid.Instance) and inferred.qname() == BUILTIN_TUPLE:\n        return False\n    if PY310_PLUS and isinstance(inferred, bases.UnionType):\n        return any((_is_invalid_isinstance_type(elt) and (not is_none(elt)) for elt in (inferred.left, inferred.right)))\n    return True",
    "label": true
  },
  {
    "code": "def infer_special_alias(node: Call, ctx: context.InferenceContext | None=None) -> Iterator[ClassDef]:\n    if not (isinstance(node.parent, Assign) and len(node.parent.targets) == 1 and isinstance(node.parent.targets[0], AssignName)):\n        raise UseInferenceDefault\n    try:\n        res = next(node.args[0].infer(context=ctx))\n    except StopIteration as e:\n        raise InferenceError(node=node.args[0], context=ctx) from e\n    assign_name = node.parent.targets[0]\n    class_def = ClassDef(name=assign_name.name, parent=node.parent)\n    class_def.postinit(bases=[res], body=[], decorators=None)\n    func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n    class_def.locals['__class_getitem__'] = [func_to_add]\n    return iter([class_def])",
    "label": true
  },
  {
    "code": "def collection_function_recursion():\n    d = {}\n\n    def g():\n        return d\n    d['g'] = g\n    return g",
    "label": true
  },
  {
    "code": "def confirm(text: str, default: t.Optional[bool]=False, abort: bool=False, prompt_suffix: str=': ', show_default: bool=True, err: bool=False) -> bool:\n    prompt = _build_prompt(text, prompt_suffix, show_default, 'y/n' if default is None else 'Y/n' if default else 'y/N')\n    while True:\n        try:\n            echo(prompt.rstrip(' '), nl=False, err=err)\n            value = visible_prompt_func(' ').lower().strip()\n        except (KeyboardInterrupt, EOFError):\n            raise Abort() from None\n        if value in ('y', 'yes'):\n            rv = True\n        elif value in ('n', 'no'):\n            rv = False\n        elif default is not None and value == '':\n            rv = default\n        else:\n            echo(_('Error: invalid input'), err=err)\n            continue\n        break\n    if abort and (not rv):\n        raise Abort()\n    return rv",
    "label": true
  },
  {
    "code": "def _normalize_host(host, scheme):\n    if host:\n        if isinstance(host, six.binary_type):\n            host = six.ensure_str(host)\n        if scheme in NORMALIZABLE_SCHEMES:\n            is_ipv6 = IPV6_ADDRZ_RE.match(host)\n            if is_ipv6:\n                match = ZONE_ID_RE.search(host)\n                if match:\n                    start, end = match.span(1)\n                    zone_id = host[start:end]\n                    if zone_id.startswith('%25') and zone_id != '%25':\n                        zone_id = zone_id[3:]\n                    else:\n                        zone_id = zone_id[1:]\n                    zone_id = '%' + _encode_invalid_chars(zone_id, UNRESERVED_CHARS)\n                    return host[:start].lower() + zone_id + host[end:]\n                else:\n                    return host.lower()\n            elif not IPV4_RE.match(host):\n                return six.ensure_str(b'.'.join([_idna_encode(label) for label in host.split('.')]))\n    return host",
    "label": true
  },
  {
    "code": "def find_plugin_filters():\n    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):\n        yield (entrypoint.name, entrypoint.load())",
    "label": true
  },
  {
    "code": "def _version2fieldlist(version):\n    if version == '1.0':\n        return _241_FIELDS\n    elif version == '1.1':\n        return _314_FIELDS\n    elif version == '1.2':\n        return _345_FIELDS\n    elif version in ('1.3', '2.1'):\n        return _345_FIELDS + tuple((f for f in _566_FIELDS if f not in _345_FIELDS))\n    elif version == '2.0':\n        raise ValueError('Metadata 2.0 is withdrawn and not supported')\n    elif version == '2.2':\n        return _643_FIELDS\n    raise MetadataUnrecognizedVersionError(version)",
    "label": true
  },
  {
    "code": "def wrap_spec(package):\n    from . import _adapters\n    return _adapters.SpecLoaderAdapter(package.__spec__, TraversableResourcesLoader)",
    "label": true
  },
  {
    "code": "def create_dict_rule(src: str, pos: Pos, out: Output) -> Tuple[Pos, Key]:\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Can not declare {key} twice')\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.get_or_create_nest(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Can not overwrite a value')\n    if not src.startswith(']', pos):\n        raise suffixed_err(src, pos, 'Expected \"]\" at the end of a table declaration')\n    return (pos + 1, key)",
    "label": true
  },
  {
    "code": "def assertrepr_compare(config, op: str, left: Any, right: Any, use_ascii: bool=False) -> Optional[List[str]]:\n    verbose = config.getoption('verbose')\n    use_ascii = isinstance(left, str) and isinstance(right, str) and (normalize('NFD', left) == normalize('NFD', right))\n    if verbose > 1:\n        left_repr = saferepr_unlimited(left, use_ascii=use_ascii)\n        right_repr = saferepr_unlimited(right, use_ascii=use_ascii)\n    else:\n        maxsize = (80 - 15 - len(op) - 2) // 2\n        left_repr = saferepr(left, maxsize=maxsize, use_ascii=use_ascii)\n        right_repr = saferepr(right, maxsize=maxsize, use_ascii=use_ascii)\n    summary = f'{left_repr} {op} {right_repr}'\n    explanation = None\n    try:\n        if op == '==':\n            explanation = _compare_eq_any(left, right, verbose)\n        elif op == 'not in':\n            if istext(left) and istext(right):\n                explanation = _notin_text(left, right, verbose)\n    except outcomes.Exit:\n        raise\n    except Exception:\n        explanation = ['(pytest_assertion plugin: representation of details failed: {}.'.format(_pytest._code.ExceptionInfo.from_current()._getreprcrash()), ' Probably an object has a faulty __repr__.)']\n    if not explanation:\n        return None\n    return [summary] + explanation",
    "label": true
  },
  {
    "code": "def register_type(name: str, kind: type) -> Callable[[Callable[[Any, ISortPrettyPrinter], str]], Callable[[Any, ISortPrettyPrinter], str]]:\n\n    def wrap(function: Callable[[Any, ISortPrettyPrinter], str]) -> Callable[[Any, ISortPrettyPrinter], str]:\n        type_mapping[name] = (kind, function)\n        return function\n    return wrap",
    "label": true
  },
  {
    "code": "def _set_start_from_first_child(node):\n    try:\n        first_child = next(node.get_children())\n    except StopIteration:\n        pass\n    else:\n        node.fromlineno = first_child.fromlineno\n        node.col_offset = first_child.col_offset\n    return node",
    "label": true
  },
  {
    "code": "def _ipaddress_match(ipname, host_ip):\n    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())\n    return ip == host_ip",
    "label": true
  },
  {
    "code": "def _get_env() -> dict[str, str]:\n    env = dict(os.environ)\n    env['PYTHONPATH'] = os.pathsep.join(sys.path)\n    return env",
    "label": true
  },
  {
    "code": "def always_iterable(obj, base_type=(str, bytes)):\n    if obj is None:\n        return iter(())\n    if base_type is not None and isinstance(obj, base_type):\n        return iter((obj,))\n    try:\n        return iter(obj)\n    except TypeError:\n        return iter((obj,))",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def test_c2adder():\n    pc2adder = pickle.dumps(c2adder)\n    pc2add5 = pickle.loads(pc2adder)(x)\n    assert pc2add5(y) == x + y",
    "label": true
  },
  {
    "code": "def generate(url):\n    parts = ['\"\"\"\\n\\n    webencodings.labels\\n    ~~~~~~~~~~~~~~~~~~~\\n\\n    Map encoding labels to their name.\\n\\n    :copyright: Copyright 2012 by Simon Sapin\\n    :license: BSD, see LICENSE for details.\\n\\n\"\"\"\\n\\n# XXX Do not edit!\\n# This file is automatically generated by mklabels.py\\n\\nLABELS = {\\n']\n    labels = [(repr(assert_lower(label)).lstrip('u'), repr(encoding['name']).lstrip('u')) for category in json.loads(urlopen(url).read().decode('ascii')) for encoding in category['encodings'] for label in encoding['labels']]\n    max_len = max((len(label) for label, name in labels))\n    parts.extend(('    %s:%s %s,\\n' % (label, ' ' * (max_len - len(label)), name) for label, name in labels))\n    parts.append('}')\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def _move_install_requirements_markers(install_requires: Dict[str, Requirement], extras_require: Mapping[str, _Ordered[Requirement]]) -> Tuple[List[str], Dict[str, List[str]]]:\n    inst_reqs = install_requires.values()\n    simple_reqs = filter(_no_marker, inst_reqs)\n    complex_reqs = filterfalse(_no_marker, inst_reqs)\n    simple_install_requires = list(map(str, simple_reqs))\n    for r in complex_reqs:\n        extras_require[':' + str(r.marker)].setdefault(r)\n    expanded_extras = dict(((k, list(dict.fromkeys((str(r) for r in map(_clean_req, v))))) for k, v in extras_require.items()))\n    return (simple_install_requires, expanded_extras)",
    "label": true
  },
  {
    "code": "def get_environ_proxies(url, no_proxy=None):\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()",
    "label": true
  },
  {
    "code": "def _format_as_name_version(dist: BaseDistribution) -> str:\n    dist_version = dist.version\n    if isinstance(dist_version, Version):\n        return f'{dist.raw_name}=={dist_version}'\n    return f'{dist.raw_name}==={dist_version}'",
    "label": true
  },
  {
    "code": "def package_to_anchor(func):\n    undefined = object()\n\n    @functools.wraps(func)\n    def wrapper(anchor=undefined, package=undefined):\n        if package is not undefined:\n            if anchor is not undefined:\n                return func(anchor, package)\n            warnings.warn(\"First parameter to files is renamed to 'anchor'\", DeprecationWarning, stacklevel=2)\n            return func(package)\n        elif anchor is undefined:\n            return func()\n        return func(anchor)\n    return wrapper",
    "label": true
  },
  {
    "code": "def setup_logging(verbosity: int, no_color: bool, user_log_file: Optional[str]) -> int:\n    if verbosity >= 2:\n        level_number = logging.DEBUG\n    elif verbosity == 1:\n        level_number = VERBOSE\n    elif verbosity == -1:\n        level_number = logging.WARNING\n    elif verbosity == -2:\n        level_number = logging.ERROR\n    elif verbosity <= -3:\n        level_number = logging.CRITICAL\n    else:\n        level_number = logging.INFO\n    level = logging.getLevelName(level_number)\n    include_user_log = user_log_file is not None\n    if include_user_log:\n        additional_log_file = user_log_file\n        root_level = 'DEBUG'\n    else:\n        additional_log_file = '/dev/null'\n        root_level = level\n    vendored_log_level = 'WARNING' if level in ['INFO', 'ERROR'] else 'DEBUG'\n    log_streams = {'stdout': 'ext://sys.stdout', 'stderr': 'ext://sys.stderr'}\n    handler_classes = {'stream': 'pip._internal.utils.logging.RichPipStreamHandler', 'file': 'pip._internal.utils.logging.BetterRotatingFileHandler'}\n    handlers = ['console', 'console_errors', 'console_subprocess'] + (['user_log'] if include_user_log else [])\n    logging.config.dictConfig({'version': 1, 'disable_existing_loggers': False, 'filters': {'exclude_warnings': {'()': 'pip._internal.utils.logging.MaxLevelFilter', 'level': logging.WARNING}, 'restrict_to_subprocess': {'()': 'logging.Filter', 'name': subprocess_logger.name}, 'exclude_subprocess': {'()': 'pip._internal.utils.logging.ExcludeLoggerFilter', 'name': subprocess_logger.name}}, 'formatters': {'indent': {'()': IndentingFormatter, 'format': '%(message)s'}, 'indent_with_timestamp': {'()': IndentingFormatter, 'format': '%(message)s', 'add_timestamp': True}}, 'handlers': {'console': {'level': level, 'class': handler_classes['stream'], 'no_color': no_color, 'stream': log_streams['stdout'], 'filters': ['exclude_subprocess', 'exclude_warnings'], 'formatter': 'indent'}, 'console_errors': {'level': 'WARNING', 'class': handler_classes['stream'], 'no_color': no_color, 'stream': log_streams['stderr'], 'filters': ['exclude_subprocess'], 'formatter': 'indent'}, 'console_subprocess': {'level': level, 'class': handler_classes['stream'], 'stream': log_streams['stderr'], 'no_color': no_color, 'filters': ['restrict_to_subprocess'], 'formatter': 'indent'}, 'user_log': {'level': 'DEBUG', 'class': handler_classes['file'], 'filename': additional_log_file, 'encoding': 'utf-8', 'delay': True, 'formatter': 'indent_with_timestamp'}}, 'root': {'level': root_level, 'handlers': handlers}, 'loggers': {'pip._vendor': {'level': vendored_log_level}}})\n    return level_number",
    "label": true
  },
  {
    "code": "def virtualenv_no_global() -> bool:\n    if _running_under_venv():\n        return _no_global_under_venv()\n    if _running_under_legacy_virtualenv():\n        return _no_global_under_legacy_virtualenv()\n    return False",
    "label": true
  },
  {
    "code": "def pytest_collection_modifyitems(items: 'List[Item]', config: Config) -> None:\n    deselect_by_keyword(items, config)\n    deselect_by_mark(items, config)",
    "label": true
  },
  {
    "code": "def _container_generic_inference(node, context, node_type, transform):\n    args = node.args\n    if not args:\n        return node_type()\n    if len(node.args) > 1:\n        raise UseInferenceDefault()\n    arg, = args\n    transformed = transform(arg)\n    if not transformed:\n        try:\n            inferred = next(arg.infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n        if isinstance(inferred, util.UninferableBase):\n            raise UseInferenceDefault\n        transformed = transform(inferred)\n    if not transformed or isinstance(transformed, util.UninferableBase):\n        raise UseInferenceDefault\n    return transformed",
    "label": true
  },
  {
    "code": "def filename_arg(path: str, optname: str) -> str:\n    if os.path.isdir(path):\n        raise UsageError(f'{optname} must be a filename, given: {path}')\n    return path",
    "label": true
  },
  {
    "code": "def looks_like_xml(text):\n    if xml_decl_re.match(text):\n        return True\n    key = hash(text)\n    try:\n        return _looks_like_xml_cache[key]\n    except KeyError:\n        m = doctype_lookup_re.search(text)\n        if m is not None:\n            return True\n        rv = tag_re.search(text[:1000]) is not None\n        _looks_like_xml_cache[key] = rv\n        return rv",
    "label": true
  },
  {
    "code": "def _truncate_by_char_count(input_lines: List[str], max_chars: int) -> List[str]:\n    iterated_char_count = 0\n    for iterated_index, input_line in enumerate(input_lines):\n        if iterated_char_count + len(input_line) > max_chars:\n            break\n        iterated_char_count += len(input_line)\n    truncated_result = input_lines[:iterated_index]\n    final_line = input_lines[iterated_index]\n    if final_line:\n        final_line_truncate_point = max_chars - iterated_char_count\n        final_line = final_line[:final_line_truncate_point]\n    truncated_result.append(final_line)\n    return truncated_result",
    "label": true
  },
  {
    "code": "def get_platform():\n    if os.name != 'nt':\n        return get_host_platform()\n    cross_compilation_target = os.environ.get('VSCMD_ARG_TGT_ARCH')\n    if cross_compilation_target not in _TARGET_TO_PLAT:\n        return get_host_platform()\n    return _TARGET_TO_PLAT[cross_compilation_target]",
    "label": true
  },
  {
    "code": "def _get_wheel_metadata_from_wheel(whl_basename, metadata_directory, config_settings):\n    from zipfile import ZipFile\n    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):\n        pass\n    whl_file = os.path.join(metadata_directory, whl_basename)\n    with ZipFile(whl_file) as zipf:\n        dist_info = _dist_info_files(zipf)\n        zipf.extractall(path=metadata_directory, members=dist_info)\n    return dist_info[0].split('/')[0]",
    "label": true
  },
  {
    "code": "def parse_hex_char(src: str, pos: Pos, hex_len: int) -> tuple[Pos, str]:\n    hex_str = src[pos:pos + hex_len]\n    if len(hex_str) != hex_len or not HEXDIGIT_CHARS.issuperset(hex_str):\n        raise suffixed_err(src, pos, 'Invalid hex value')\n    pos += hex_len\n    hex_int = int(hex_str, 16)\n    if not is_unicode_scalar_value(hex_int):\n        raise suffixed_err(src, pos, 'Escaped character is not a Unicode scalar value')\n    return (pos, chr(hex_int))",
    "label": true
  },
  {
    "code": "def register(t):\n\n    def proxy(func):\n        Pickler.dispatch[t] = func\n        return func\n    return proxy",
    "label": true
  },
  {
    "code": "def get_host(url):\n    p = parse_url(url)\n    return (p.scheme or 'http', p.hostname, p.port)",
    "label": true
  },
  {
    "code": "def _mac_arch(arch: str, is_32bit: bool=_32_BIT_INTERPRETER) -> str:\n    if not is_32bit:\n        return arch\n    if arch.startswith('ppc'):\n        return 'ppc'\n    return 'i386'",
    "label": true
  },
  {
    "code": "def pack_f(args):\n    from .pack import pack\n    pack(args.directory, args.dest_dir, args.build_number)",
    "label": true
  },
  {
    "code": "def isfrommain(obj):\n    module = getmodule(obj)\n    if module and module.__name__ == '__main__':\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def get_stacklevel() -> int:\n    level = 1\n    frame = cast(FrameType, currentframe()).f_back\n    while frame and frame.f_globals.get('__name__', '').startswith('typeguard.'):\n        level += 1\n        frame = frame.f_back\n    return level",
    "label": true
  },
  {
    "code": "def _compare_eq_any(left: Any, right: Any, verbose: int=0) -> List[str]:\n    explanation = []\n    if istext(left) and istext(right):\n        explanation = _diff_text(left, right, verbose)\n    else:\n        from _pytest.python_api import ApproxBase\n        if isinstance(left, ApproxBase) or isinstance(right, ApproxBase):\n            approx_side = left if isinstance(left, ApproxBase) else right\n            other_side = right if isinstance(left, ApproxBase) else left\n            explanation = approx_side._repr_compare(other_side)\n        elif type(left) == type(right) and (isdatacls(left) or isattrs(left) or isnamedtuple(left)):\n            explanation = _compare_eq_cls(left, right, verbose)\n        elif issequence(left) and issequence(right):\n            explanation = _compare_eq_sequence(left, right, verbose)\n        elif isset(left) and isset(right):\n            explanation = _compare_eq_set(left, right, verbose)\n        elif isdict(left) and isdict(right):\n            explanation = _compare_eq_dict(left, right, verbose)\n        if isiterable(left) and isiterable(right):\n            expl = _compare_eq_iterable(left, right, verbose)\n            explanation.extend(expl)\n    return explanation",
    "label": true
  },
  {
    "code": "def mergesort2(lst: list, start: int=0, end: Optional[int]=None) -> None:\n    if end is None:\n        end = len(lst)\n    if start < end - 1:\n        mid = (start + end) // 2\n        mergesort2(lst, start, mid)\n        mergesort2(lst, mid, end)\n        _merge(lst, start, mid, end)",
    "label": true
  },
  {
    "code": "def language_callback(lexer, match):\n    lx = None\n    m = language_re.match(lexer.text[match.end():match.end() + 100])\n    if m is not None:\n        lx = lexer._get_lexer(m.group(1))\n    else:\n        m = list(language_re.finditer(lexer.text[max(0, match.start() - 100):match.start()]))\n        if m:\n            lx = lexer._get_lexer(m[-1].group(1))\n        else:\n            m = list(do_re.finditer(lexer.text[max(0, match.start() - 25):match.start()]))\n            if m:\n                lx = lexer._get_lexer('plpgsql')\n    yield (match.start(1), String, match.group(1))\n    yield (match.start(2), String.Delimiter, match.group(2))\n    yield (match.start(3), String, match.group(3))\n    if lx:\n        yield from lx.get_tokens_unprocessed(match.group(4))\n    else:\n        yield (match.start(4), String, match.group(4))\n    yield (match.start(5), String, match.group(5))\n    yield (match.start(6), String.Delimiter, match.group(6))\n    yield (match.start(7), String, match.group(7))",
    "label": true
  },
  {
    "code": "def _try_repr_or_str(obj: object) -> str:\n    try:\n        return repr(obj)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except BaseException:\n        return f'{type(obj).__name__}(\"{obj}\")'",
    "label": true
  },
  {
    "code": "def dumps(obj, protocol=None, byref=None, fmode=None, recurse=None, **kwds):\n    file = StringIO()\n    dump(obj, file, protocol, byref, fmode, recurse, **kwds)\n    return file.getvalue()",
    "label": true
  },
  {
    "code": "def value(raw: str) -> _Item:\n    parser = Parser(raw)\n    v = parser._parse_value()\n    if not parser.end():\n        raise parser.parse_error(UnexpectedCharError, char=parser._current)\n    return v",
    "label": true
  },
  {
    "code": "def ensure_local_distutils():\n    import importlib\n    clear_distutils()\n    with shim():\n        importlib.import_module('distutils')\n    core = importlib.import_module('distutils.core')\n    assert '_distutils' in core.__file__, core.__file__\n    assert 'setuptools._distutils.log' not in sys.modules",
    "label": true
  },
  {
    "code": "def _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n    try:\n        spec = Specifier(''.join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs, prereleases=True)\n    oper: Optional[Operator] = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(f'Undefined {op!r} on {lhs!r} and {rhs!r}.')\n    return oper(lhs, rhs)",
    "label": true
  },
  {
    "code": "def _parse_marker_op(tokenizer: Tokenizer) -> Op:\n    if tokenizer.check('IN'):\n        tokenizer.read()\n        return Op('in')\n    elif tokenizer.check('NOT'):\n        tokenizer.read()\n        tokenizer.expect('WS', expected=\"whitespace after 'not'\")\n        tokenizer.expect('IN', expected=\"'in' after 'not'\")\n        return Op('not in')\n    elif tokenizer.check('OP'):\n        return Op(tokenizer.read().text)\n    else:\n        return tokenizer.raise_syntax_error('Expected marker operator, one of <=, <, !=, ==, >=, >, ~=, ===, in, not in')",
    "label": true
  },
  {
    "code": "def password_option(*param_decls: str, **kwargs: t.Any) -> t.Callable[[FC], FC]:\n    if not param_decls:\n        param_decls = ('--password',)\n    kwargs.setdefault('prompt', True)\n    kwargs.setdefault('confirmation_prompt', True)\n    kwargs.setdefault('hide_input', True)\n    return option(*param_decls, **kwargs)",
    "label": true
  },
  {
    "code": "def _set_socket_options(sock, options):\n    if options is None:\n        return\n    for opt in options:\n        sock.setsockopt(*opt)",
    "label": true
  },
  {
    "code": "def _fix_dot_imports(not_consumed: dict[str, list[nodes.NodeNG]]) -> list[tuple[str, _base_nodes.ImportNode]]:\n    names: dict[str, _base_nodes.ImportNode] = {}\n    for name, stmts in not_consumed.items():\n        if any((isinstance(stmt, nodes.AssignName) and isinstance(stmt.assign_type(), nodes.AugAssign) for stmt in stmts)):\n            continue\n        for stmt in stmts:\n            if not isinstance(stmt, (nodes.ImportFrom, nodes.Import)):\n                continue\n            for imports in stmt.names:\n                second_name = None\n                import_module_name = imports[0]\n                if import_module_name == '*':\n                    second_name = name\n                else:\n                    name_matches_dotted_import = False\n                    if import_module_name.startswith(name) and import_module_name.find('.') > -1:\n                        name_matches_dotted_import = True\n                    if name_matches_dotted_import or name in imports:\n                        second_name = import_module_name\n                if second_name and second_name not in names:\n                    names[second_name] = stmt\n    return sorted(names.items(), key=lambda a: a[1].fromlineno)",
    "label": true
  },
  {
    "code": "def format_lines(var_name, seq, raw=False, indent_level=0):\n    lines = []\n    base_indent = ' ' * indent_level * 4\n    inner_indent = ' ' * (indent_level + 1) * 4\n    lines.append(base_indent + var_name + ' = (')\n    if raw:\n        for i in seq:\n            lines.append(inner_indent + i + ',')\n    else:\n        for i in seq:\n            r = repr(i + '\"')\n            lines.append(inner_indent + r[:-2] + r[-1] + ',')\n    lines.append(base_indent + ')')\n    return '\\n'.join(lines)",
    "label": true
  },
  {
    "code": "def test_pickle_to_stream():\n    dumpfile = dumpIO(x)\n    _x = loadIO(dumpfile)\n    assert _x == x",
    "label": true
  },
  {
    "code": "def rstrip(iterable, pred):\n    cache = []\n    cache_append = cache.append\n    cache_clear = cache.clear\n    for x in iterable:\n        if pred(x):\n            cache_append(x)\n        else:\n            yield from cache\n            cache_clear()\n            yield x",
    "label": true
  },
  {
    "code": "def _regexp_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    patterns: list[Pattern[str]] = []\n    for pattern in pylint_utils._check_regexp_csv(value):\n        patterns.append(_regex_transformer(pattern))\n    return patterns",
    "label": true
  },
  {
    "code": "def test_basic():\n    a = [0, 1, 2]\n    pa = pickle.dumps(a)\n    pmath = pickle.dumps(math)\n    pmap = pickle.dumps(map)\n    la = pickle.loads(pa)\n    lmath = pickle.loads(pmath)\n    lmap = pickle.loads(pmap)\n    assert list(map(math.sin, a)) == list(lmap(lmath.sin, la))",
    "label": true
  },
  {
    "code": "def expand_paths(inputs):\n    seen = {}\n    for dirname in inputs:\n        dirname = normalize_path(dirname)\n        if dirname in seen:\n            continue\n        seen[dirname] = 1\n        if not os.path.isdir(dirname):\n            continue\n        files = os.listdir(dirname)\n        yield (dirname, files)\n        for name in files:\n            if not name.endswith('.pth'):\n                continue\n            if name in ('easy-install.pth', 'setuptools.pth'):\n                continue\n            f = open(os.path.join(dirname, name))\n            lines = list(yield_lines(f))\n            f.close()\n            for line in lines:\n                if line.startswith('import'):\n                    continue\n                line = normalize_path(line.rstrip())\n                if line in seen:\n                    continue\n                seen[line] = 1\n                if not os.path.isdir(line):\n                    continue\n                yield (line, os.listdir(line))",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        yield from compatible_tags(interpreter='pp3')\n    else:\n        yield from compatible_tags()",
    "label": true
  },
  {
    "code": "def _hanging_indent_end_line(line: str) -> str:\n    if not line.endswith(' '):\n        line += ' '\n    return line + '\\\\'",
    "label": true
  },
  {
    "code": "def _make_binop(op: str) -> t.Callable[['CodeGenerator', nodes.BinExpr, 'Frame'], None]:\n\n    @optimizeconst\n    def visitor(self: 'CodeGenerator', node: nodes.BinExpr, frame: Frame) -> None:\n        if self.environment.sandboxed and op in self.environment.intercepted_binops:\n            self.write(f'environment.call_binop(context, {op!r}, ')\n            self.visit(node.left, frame)\n            self.write(', ')\n            self.visit(node.right, frame)\n        else:\n            self.write('(')\n            self.visit(node.left, frame)\n            self.write(f' {op} ')\n            self.visit(node.right, frame)\n        self.write(')')\n    return visitor",
    "label": true
  },
  {
    "code": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s",
    "label": true
  },
  {
    "code": "def split_auth_netloc_from_url(url: str) -> Tuple[str, str, Tuple[Optional[str], Optional[str]]]:\n    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)\n    return (url_without_auth, netloc, auth)",
    "label": true
  },
  {
    "code": "def get_all_lexers(plugins=True):\n    for item in LEXERS.values():\n        yield item[1:]\n    if plugins:\n        for lexer in find_plugin_lexers():\n            yield (lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes)",
    "label": true
  },
  {
    "code": "def _parse_options(o_strs):\n    opts = {}\n    if not o_strs:\n        return opts\n    for o_str in o_strs:\n        if not o_str.strip():\n            continue\n        o_args = o_str.split(',')\n        for o_arg in o_args:\n            o_arg = o_arg.strip()\n            try:\n                o_key, o_val = o_arg.split('=', 1)\n                o_key = o_key.strip()\n                o_val = o_val.strip()\n            except ValueError:\n                opts[o_arg] = True\n            else:\n                opts[o_key] = o_val\n    return opts",
    "label": true
  },
  {
    "code": "def _enclose(object, alias=''):\n    dummy = '__this_is_a_big_dummy_enclosing_function__'\n    stub = '__this_is_a_stub_variable__'\n    code = 'def %s():\\n' % dummy\n    code += indent(getsource(object, alias=stub, lstrip=True, force=True))\n    code += indent('return %s\\n' % stub)\n    if alias:\n        code += '%s = ' % alias\n    code += '%s(); del %s\\n' % (dummy, dummy)\n    return code",
    "label": true
  },
  {
    "code": "def datetime(raw: str) -> DateTime:\n    value = parse_rfc3339(raw)\n    if not isinstance(value, _datetime.datetime):\n        raise ValueError('datetime() only accepts datetime strings.')\n    return item(value)",
    "label": true
  },
  {
    "code": "def _find_statement_by_line(node: nodes.NodeNG, line: int) -> nodes.NodeNG | None:\n    if isinstance(node, (nodes.ClassDef, nodes.FunctionDef, nodes.MatchCase)):\n        node_line = node.fromlineno\n    else:\n        node_line = node.lineno\n    if node_line == line:\n        return node\n    for child in node.get_children():\n        result = _find_statement_by_line(child, line)\n        if result:\n            return result\n    return None",
    "label": true
  },
  {
    "code": "def platform_tags(linux: str, arch: str) -> Iterator[str]:\n    if not _have_compatible_abi(sys.executable, arch):\n        return\n    too_old_glibc2 = _GLibCVersion(2, 16)\n    if arch in {'x86_64', 'i686'}:\n        too_old_glibc2 = _GLibCVersion(2, 4)\n    current_glibc = _GLibCVersion(*_get_glibc_version())\n    glibc_max_list = [current_glibc]\n    for glibc_major in range(current_glibc.major - 1, 1, -1):\n        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n    for glibc_max in glibc_max_list:\n        if glibc_max.major == too_old_glibc2.major:\n            min_minor = too_old_glibc2.minor\n        else:\n            min_minor = -1\n        for glibc_minor in range(glibc_max.minor, min_minor, -1):\n            glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n            tag = 'manylinux_{}_{}'.format(*glibc_version)\n            if _is_compatible(tag, arch, glibc_version):\n                yield linux.replace('linux', tag)\n            if glibc_version in _LEGACY_MANYLINUX_MAP:\n                legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n                if _is_compatible(legacy_tag, arch, glibc_version):\n                    yield linux.replace('linux', legacy_tag)",
    "label": true
  },
  {
    "code": "def ensure_binary(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def group(name: t.Union[str, _AnyCallable, None]=None, cls: t.Optional[t.Type[GrpType]]=None, **attrs: t.Any) -> t.Union[Group, t.Callable[[_AnyCallable], t.Union[Group, GrpType]]]:\n    if cls is None:\n        cls = t.cast(t.Type[GrpType], Group)\n    if callable(name):\n        return command(cls=cls, **attrs)(name)\n    return command(name, cls, **attrs)",
    "label": true
  },
  {
    "code": "def _is_quote_delimiter_chosen_freely(string_token: str) -> bool:\n    quote_delimiter = _get_quote_delimiter(string_token)\n    unchosen_delimiter = '\"' if quote_delimiter == \"'\" else \"'\"\n    return bool(quote_delimiter and (not _is_long_string(string_token)) and (unchosen_delimiter not in str_eval(string_token)))",
    "label": true
  },
  {
    "code": "def identify_python_interpreter(python: str) -> Optional[str]:\n    if os.path.exists(python):\n        if os.path.isdir(python):\n            for exe in ('bin/python', 'Scripts/python.exe'):\n                py = os.path.join(python, exe)\n                if os.path.exists(py):\n                    return py\n        else:\n            return python\n    return None",
    "label": true
  },
  {
    "code": "def badtypes(obj, depth=0, exact=False, safe=False):\n    from dill import pickles\n    if not depth:\n        if pickles(obj, exact, safe):\n            return None\n        return type(obj)\n    return dict(((attr, badtypes(getattr(obj, attr), depth - 1, exact, safe)) for attr in dir(obj) if not pickles(getattr(obj, attr), exact, safe)))",
    "label": true
  },
  {
    "code": "def platform_tags() -> Iterator[str]:\n    if platform.system() == 'Darwin':\n        return mac_platforms()\n    elif platform.system() == 'Linux':\n        return _linux_platforms()\n    else:\n        return _generic_platforms()",
    "label": true
  },
  {
    "code": "def write_safety_flag(egg_dir, safe):\n    for flag, fn in safety_flags.items():\n        fn = os.path.join(egg_dir, fn)\n        if os.path.exists(fn):\n            if safe is None or bool(safe) != flag:\n                os.unlink(fn)\n        elif safe is not None and bool(safe) == flag:\n            f = open(fn, 'wt')\n            f.write('\\n')\n            f.close()",
    "label": true
  },
  {
    "code": "def show_formats():\n    from ..fancy_getopt import FancyGetopt\n    from ..archive_util import ARCHIVE_FORMATS\n    formats = []\n    for format in ARCHIVE_FORMATS.keys():\n        formats.append(('formats=' + format, None, ARCHIVE_FORMATS[format][2]))\n    formats.sort()\n    FancyGetopt(formats).print_help('List of available source distribution formats:')",
    "label": true
  },
  {
    "code": "def normalize_hookimpl_opts(opts: HookimplOpts) -> None:\n    opts.setdefault('tryfirst', False)\n    opts.setdefault('trylast', False)\n    opts.setdefault('wrapper', False)\n    opts.setdefault('hookwrapper', False)\n    opts.setdefault('optionalhook', False)\n    opts.setdefault('specname', None)",
    "label": true
  },
  {
    "code": "def test_dynamic():\n    assert likely_import(add) == 'from %s import add\\n' % __name__\n    assert likely_import(squared) == 'from %s import squared\\n' % __name__",
    "label": true
  },
  {
    "code": "def _check_extra(extra, reqs):\n    name, sep, marker = extra.partition(':')\n    try:\n        _check_marker(marker)\n    except InvalidMarker:\n        msg = f'Invalid environment marker: {marker} ({extra!r})'\n        raise DistutilsSetupError(msg) from None\n    list(_reqs.parse(reqs))",
    "label": true
  },
  {
    "code": "def get_platlib() -> str:\n    new = _sysconfig.get_platlib()\n    if _USE_SYSCONFIG:\n        return new\n    from . import _distutils\n    old = _distutils.get_platlib()\n    if _looks_like_deb_system_dist_packages(old):\n        return old\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key='platlib'):\n        _log_context()\n    return old",
    "label": true
  },
  {
    "code": "def get_code_complexity(code, threshold=7, filename='stdin'):\n    try:\n        tree = compile(code, filename, 'exec', ast.PyCF_ONLY_AST)\n    except SyntaxError:\n        e = sys.exc_info()[1]\n        sys.stderr.write('Unable to parse %s: %s\\n' % (filename, e))\n        return 0\n    complx = []\n    McCabeChecker.max_complexity = threshold\n    for lineno, offset, text, check in McCabeChecker(tree, filename).run():\n        complx.append('%s:%d:1: %s' % (filename, lineno, text))\n    if len(complx) == 0:\n        return 0\n    print('\\n'.join(complx))\n    return len(complx)",
    "label": true
  },
  {
    "code": "def _parse_letter_version(letter: Optional[str], number: Union[str, bytes, SupportsInt, None]) -> Optional[Tuple[str, int]]:\n    if letter:\n        if number is None:\n            number = 0\n        letter = letter.lower()\n        if letter == 'alpha':\n            letter = 'a'\n        elif letter == 'beta':\n            letter = 'b'\n        elif letter in ['c', 'pre', 'preview']:\n            letter = 'rc'\n        elif letter in ['rev', 'r']:\n            letter = 'post'\n        return (letter, int(number))\n    if not letter and number:\n        letter = 'post'\n        return (letter, int(number))\n    return None",
    "label": true
  },
  {
    "code": "def safe_version(version: str) -> str:\n    v = version.replace(' ', '.')\n    try:\n        return str(packaging.version.Version(v))\n    except packaging.version.InvalidVersion:\n        attempt = _UNSAFE_NAME_CHARS.sub('-', v)\n        return str(packaging.version.Version(attempt))",
    "label": true
  },
  {
    "code": "def get_src_prefix() -> str:\n    if running_under_virtualenv():\n        src_prefix = os.path.join(sys.prefix, 'src')\n    else:\n        try:\n            src_prefix = os.path.join(os.getcwd(), 'src')\n        except OSError:\n            sys.exit('The folder you are executing pip from can no longer be found.')\n    return os.path.abspath(src_prefix)",
    "label": true
  },
  {
    "code": "def ratio_distribute(total: int, ratios: List[int], minimums: Optional[List[int]]=None) -> List[int]:\n    if minimums:\n        ratios = [ratio if _min else 0 for ratio, _min in zip(ratios, minimums)]\n    total_ratio = sum(ratios)\n    assert total_ratio > 0, 'Sum of ratios must be > 0'\n    total_remaining = total\n    distributed_total: List[int] = []\n    append = distributed_total.append\n    if minimums is None:\n        _minimums = [0] * len(ratios)\n    else:\n        _minimums = minimums\n    for ratio, minimum in zip(ratios, _minimums):\n        if total_ratio > 0:\n            distributed = max(minimum, ceil(ratio * total_remaining / total_ratio))\n        else:\n            distributed = total_remaining\n        append(distributed)\n        total_ratio -= ratio\n        total_remaining -= distributed\n    return distributed_total",
    "label": true
  },
  {
    "code": "def nth_combination(iterable, r, index):\n    pool = tuple(iterable)\n    n = len(pool)\n    if r < 0 or r > n:\n        raise ValueError\n    c = 1\n    k = min(r, n - r)\n    for i in range(1, k + 1):\n        c = c * (n - k + i) // i\n    if index < 0:\n        index += c\n    if index < 0 or index >= c:\n        raise IndexError\n    result = []\n    while r:\n        c, n, r = (c * r // n, n - 1, r - 1)\n        while index >= c:\n            index -= c\n            c, n = (c * (n - r) // n, n - 1)\n        result.append(pool[-1 - n])\n    return tuple(result)",
    "label": true
  },
  {
    "code": "def render_context(start, stop, source_lines):\n    start, stop = (max(start, 1), min(stop, len(source_lines)))\n    yield from ((line, slice(None, None), LineType.CONTEXT, source_lines[line - 1]) for line in range(start, stop))",
    "label": true
  },
  {
    "code": "def wheel_dist_info_dir(source: ZipFile, name: str) -> str:\n    subdirs = {p.split('/', 1)[0] for p in source.namelist()}\n    info_dirs = [s for s in subdirs if s.endswith('.dist-info')]\n    if not info_dirs:\n        raise UnsupportedWheel('.dist-info directory not found')\n    if len(info_dirs) > 1:\n        raise UnsupportedWheel('multiple .dist-info directories found: {}'.format(', '.join(info_dirs)))\n    info_dir = info_dirs[0]\n    info_dir_name = canonicalize_name(info_dir)\n    canonical_name = canonicalize_name(name)\n    if not info_dir_name.startswith(canonical_name):\n        raise UnsupportedWheel('.dist-info directory {!r} does not start with {!r}'.format(info_dir, canonical_name))\n    return info_dir",
    "label": true
  },
  {
    "code": "def test_class_descriptors():\n    d = _d.__dict__\n    for i in d.values():\n        ok = dill.pickles(i)\n        if verbose:\n            print('%s: %s, %s' % (ok, type(i), i))\n        assert ok\n    if verbose:\n        print('')\n    od = _newclass.__dict__\n    for i in od.values():\n        ok = dill.pickles(i)\n        if verbose:\n            print('%s: %s, %s' % (ok, type(i), i))\n        assert ok\n    if verbose:\n        print('')",
    "label": true
  },
  {
    "code": "def _swap_items(tree: HuffmanTree, code: str, item: int) -> Optional[int]:\n    location = tree\n    for i in code:\n        if i == '1':\n            location = location.right\n        else:\n            location = location.left\n    if location.symbol == item:\n        return None\n    else:\n        out = location.symbol\n        location.symbol = item\n        return out",
    "label": true
  },
  {
    "code": "def _pad_version(left: List[str], right: List[str]) -> Tuple[List[str], List[str]]:\n    left_split, right_split = ([], [])\n    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n    left_split.append(left[len(left_split[0]):])\n    right_split.append(right[len(right_split[0]):])\n    left_split.insert(1, ['0'] * max(0, len(right_split[0]) - len(left_split[0])))\n    right_split.insert(1, ['0'] * max(0, len(left_split[0]) - len(right_split[0])))\n    return (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))",
    "label": true
  },
  {
    "code": "def _swap_items(tree: HuffmanTree, code: str, item: int) -> Optional[int]:\n    location = tree\n    for i in code:\n        if i == '1':\n            location = location.right\n        else:\n            location = location.left\n    if location.symbol == item:\n        return None\n    else:\n        out = location.symbol\n        location.symbol = item\n        return out",
    "label": true
  },
  {
    "code": "def apply_configuration(dist: 'Distribution', filepath: _Path) -> 'Distribution':\n    _apply(dist, filepath)\n    dist._finalize_requires()\n    return dist",
    "label": true
  },
  {
    "code": "def write_setup_requirements(cmd, basename, filename):\n    data = io.StringIO()\n    _write_requirements(data, cmd.distribution.setup_requires)\n    cmd.write_or_delete_file('setup-requirements', filename, data.getvalue())",
    "label": true
  },
  {
    "code": "def decompress_bytes(tree: HuffmanTree, text: bytes, size: int) -> bytes:\n    if not text:\n        if size > 0:\n            raise ValueError('empty <text> parameter but size was given')\n        return bytes()\n    decompressed_len = 0\n    decompressed = []\n    location = tree\n    next_text = iter(text)\n    text = format(next(next_text), '0=8b')\n    text_i = 0\n    while decompressed_len < size:\n        if text[text_i] == '1':\n            location = location.right\n        else:\n            location = location.left\n        text_i += 1\n        if location.is_leaf():\n            decompressed.append(location.symbol)\n            decompressed_len += 1\n            location = tree\n        if text_i == 8:\n            text_i = 0\n            try:\n                text = format(next(next_text), '0=8b')\n            except StopIteration:\n                pass\n    return bytes(decompressed)",
    "label": true
  },
  {
    "code": "def parts(s: str) -> Set[str]:\n    parts = s.split(sep)\n    return {sep.join(parts[:i + 1]) or sep for i in range(len(parts))}",
    "label": true
  },
  {
    "code": "def create_os_error_message(error: OSError, show_traceback: bool, using_user_site: bool) -> str:\n    parts = []\n    parts.append('Could not install packages due to an OSError')\n    if not show_traceback:\n        parts.append(': ')\n        parts.append(str(error))\n    else:\n        parts.append('.')\n    parts[-1] += '\\n'\n    if error.errno == errno.EACCES:\n        user_option_part = 'Consider using the `--user` option'\n        permissions_part = 'Check the permissions'\n        if not running_under_virtualenv() and (not using_user_site):\n            parts.extend([user_option_part, ' or ', permissions_part.lower()])\n        else:\n            parts.append(permissions_part)\n        parts.append('.\\n')\n    if WINDOWS and error.errno == errno.ENOENT and error.filename and (len(error.filename) > 260):\n        parts.append('HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\\n')\n    return ''.join(parts).strip() + '\\n'",
    "label": true
  },
  {
    "code": "def _update_with_replacement(lhs_dict: dict[SuccessfulInferenceResult, SuccessfulInferenceResult], rhs_dict: dict[SuccessfulInferenceResult, SuccessfulInferenceResult]) -> dict[SuccessfulInferenceResult, SuccessfulInferenceResult]:\n    combined_dict = itertools.chain(lhs_dict.items(), rhs_dict.items())\n    string_map = {key.as_string(): (key, value) for key, value in combined_dict}\n    return dict(string_map.values())",
    "label": true
  },
  {
    "code": "def _load_client_cert_chain(keychain: SecKeychainRef, *paths: str | None) -> CFArray:\n    certificates = []\n    identities = []\n    filtered_paths = (path for path in paths if path)\n    try:\n        for file_path in filtered_paths:\n            new_identities, new_certs = _load_items_from_file(keychain, file_path)\n            identities.extend(new_identities)\n            certificates.extend(new_certs)\n        if not identities:\n            new_identity = Security.SecIdentityRef()\n            status = Security.SecIdentityCreateWithCertificate(keychain, certificates[0], ctypes.byref(new_identity))\n            _assert_no_error(status)\n            identities.append(new_identity)\n            CoreFoundation.CFRelease(certificates.pop(0))\n        trust_chain = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        for item in itertools.chain(identities, certificates):\n            CoreFoundation.CFArrayAppendValue(trust_chain, item)\n        return trust_chain\n    finally:\n        for obj in itertools.chain(identities, certificates):\n            CoreFoundation.CFRelease(obj)",
    "label": true
  },
  {
    "code": "def _filter_operation_errors(self: _T, infer_callable: Callable[[_T, InferenceContext | None], Generator[InferenceResult | util.BadOperationMessage, None, None]], context: InferenceContext | None, error: type[util.BadOperationMessage]) -> Generator[InferenceResult, None, None]:\n    for result in infer_callable(self, context):\n        if isinstance(result, error):\n            yield util.Uninferable\n        else:\n            yield result",
    "label": true
  },
  {
    "code": "def attr_attributes_transform(node: ClassDef) -> None:\n    node.locals['__attrs_attrs__'] = [Unknown(parent=node)]\n    for cdef_body_node in node.body:\n        if not isinstance(cdef_body_node, (Assign, AnnAssign)):\n            continue\n        if isinstance(cdef_body_node.value, Call):\n            if cdef_body_node.value.func.as_string() not in ATTRIB_NAMES:\n                continue\n        else:\n            continue\n        targets = cdef_body_node.targets if hasattr(cdef_body_node, 'targets') else [cdef_body_node.target]\n        for target in targets:\n            rhs_node = Unknown(lineno=cdef_body_node.lineno, col_offset=cdef_body_node.col_offset, parent=cdef_body_node)\n            if isinstance(target, AssignName):\n                node.locals[target.name] = [rhs_node]\n                node.instance_attrs[target.name] = [rhs_node]",
    "label": true
  },
  {
    "code": "def _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:\n    if not value:\n        return (None, None)\n    parts = value.split('.')\n    if len(parts) > 3:\n        return ((), 'at most three version parts are allowed')\n    if len(parts) == 1:\n        value = parts[0]\n        if len(value) > 1:\n            parts = [value[0], value[1:]]\n    try:\n        version_info = tuple((int(part) for part in parts))\n    except ValueError:\n        return ((), 'each version part must be an integer')\n    return (version_info, None)",
    "label": true
  },
  {
    "code": "def register(linter: PyLinter) -> None:\n    linter.register_checker(BasicErrorChecker(linter))\n    linter.register_checker(BasicChecker(linter))\n    linter.register_checker(NameChecker(linter))\n    linter.register_checker(DocStringChecker(linter))\n    linter.register_checker(PassChecker(linter))\n    linter.register_checker(ComparisonChecker(linter))",
    "label": true
  },
  {
    "code": "def check_hyphen_ok(label: str) -> bool:\n    if label[2:4] == '--':\n        raise IDNAError('Label has disallowed hyphens in 3rd and 4th position')\n    if label[0] == '-' or label[-1] == '-':\n        raise IDNAError('Label must not start or end with a hyphen')\n    return True",
    "label": true
  },
  {
    "code": "def _declare_state(vartype, **kw):\n    globals().update(kw)\n    _state_vars.update(dict.fromkeys(kw, vartype))",
    "label": true
  },
  {
    "code": "def extract_from_urllib3():\n    util.SSLContext = orig_util_SSLContext\n    util.ssl_.SSLContext = orig_util_SSLContext\n    util.HAS_SNI = orig_util_HAS_SNI\n    util.ssl_.HAS_SNI = orig_util_HAS_SNI\n    util.IS_SECURETRANSPORT = False\n    util.ssl_.IS_SECURETRANSPORT = False",
    "label": true
  },
  {
    "code": "def _multiply_seq_by_int(self: _TupleListNodeT, opnode: nodes.AugAssign | nodes.BinOp, value: int, context: InferenceContext) -> _TupleListNodeT:\n    node = self.__class__(parent=opnode)\n    if value > 100000000.0:\n        node.elts = [util.Uninferable]\n        return node\n    filtered_elts = (helpers.safe_infer(elt, context) or util.Uninferable for elt in self.elts if not isinstance(elt, util.UninferableBase))\n    node.elts = list(filtered_elts) * value\n    return node",
    "label": true
  },
  {
    "code": "def print_dist_installation_info(name: str, latest: str) -> None:\n    env = get_default_environment()\n    dist = env.get_distribution(name)\n    if dist is not None:\n        with indent_log():\n            if dist.version == latest:\n                write_output('INSTALLED: %s (latest)', dist.version)\n            else:\n                write_output('INSTALLED: %s', dist.version)\n                if parse_version(latest).pre:\n                    write_output('LATEST:    %s (pre-release; install with `pip install --pre`)', latest)\n                else:\n                    write_output('LATEST:    %s', latest)",
    "label": true
  },
  {
    "code": "def rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, 'seek', None)\n    if body_seek is not None and isinstance(prepared_request._body_position, integer_types):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError('An error occurred when rewinding request body for redirect.')\n    else:\n        raise UnrewindableBodyError('Unable to rewind request body for redirect.')",
    "label": true
  },
  {
    "code": "def _parse_local_version(local: str) -> Optional[LocalType]:\n    if local is not None:\n        return tuple((part.lower() if not part.isdigit() else int(part) for part in _local_version_separators.split(local)))\n    return None",
    "label": true
  },
  {
    "code": "def target_info_from_filename(filename: str) -> tuple[str, str, str]:\n    basename = os.path.basename(filename)\n    storedir = os.path.dirname(os.path.abspath(filename))\n    target = os.path.splitext(filename)[-1][1:]\n    return (storedir, basename, target)",
    "label": true
  },
  {
    "code": "def _non_numeric_type_error(value, at: Optional[str]) -> TypeError:\n    at_str = f' at {at}' if at else ''\n    return TypeError('cannot make approximate comparisons to non-numeric values: {!r} {}'.format(value, at_str))",
    "label": true
  },
  {
    "code": "def detect_all(byte_str: Union[bytes, bytearray], ignore_threshold: bool=False, should_rename_legacy: bool=False) -> List[ResultDict]:\n    if not isinstance(byte_str, bytearray):\n        if not isinstance(byte_str, bytes):\n            raise TypeError(f'Expected object of type bytes or bytearray, got: {type(byte_str)}')\n        byte_str = bytearray(byte_str)\n    detector = UniversalDetector(should_rename_legacy=should_rename_legacy)\n    detector.feed(byte_str)\n    detector.close()\n    if detector.input_state == InputState.HIGH_BYTE:\n        results: List[ResultDict] = []\n        probers: List[CharSetProber] = []\n        for prober in detector.charset_probers:\n            if isinstance(prober, CharSetGroupProber):\n                probers.extend((p for p in prober.probers))\n            else:\n                probers.append(prober)\n        for prober in probers:\n            if ignore_threshold or prober.get_confidence() > detector.MINIMUM_THRESHOLD:\n                charset_name = prober.charset_name or ''\n                lower_charset_name = charset_name.lower()\n                if lower_charset_name.startswith('iso-8859') and detector.has_win_bytes:\n                    charset_name = detector.ISO_WIN_MAP.get(lower_charset_name, charset_name)\n                if should_rename_legacy:\n                    charset_name = detector.LEGACY_MAP.get(charset_name.lower(), charset_name)\n                results.append({'encoding': charset_name, 'confidence': prober.get_confidence(), 'language': prober.language})\n        if len(results) > 0:\n            return sorted(results, key=lambda result: -result['confidence'])\n    return [detector.result]",
    "label": true
  },
  {
    "code": "def _parse_letter_version(letter: str, number: Union[str, bytes, SupportsInt]) -> Optional[Tuple[str, int]]:\n    if letter:\n        if number is None:\n            number = 0\n        letter = letter.lower()\n        if letter == 'alpha':\n            letter = 'a'\n        elif letter == 'beta':\n            letter = 'b'\n        elif letter in ['c', 'pre', 'preview']:\n            letter = 'rc'\n        elif letter in ['rev', 'r']:\n            letter = 'post'\n        return (letter, int(number))\n    if not letter and number:\n        letter = 'post'\n        return (letter, int(number))\n    return None",
    "label": true
  },
  {
    "code": "def validate_basetemp(path: str) -> str:\n    msg = 'basetemp must not be empty, the current working directory or any parent directory of it'\n    if not path:\n        raise argparse.ArgumentTypeError(msg)\n\n    def is_ancestor(base: Path, query: Path) -> bool:\n        \"\"\"Return whether query is an ancestor of base.\"\"\"\n        if base == query:\n            return True\n        return query in base.parents\n    if is_ancestor(Path.cwd(), Path(path).absolute()):\n        raise argparse.ArgumentTypeError(msg)\n    if is_ancestor(Path.cwd().resolve(), Path(path).resolve()):\n        raise argparse.ArgumentTypeError(msg)\n    return path",
    "label": true
  },
  {
    "code": "def zip_item_is_executable(info: ZipInfo) -> bool:\n    mode = info.external_attr >> 16\n    return bool(mode and stat.S_ISREG(mode) and mode & 73)",
    "label": true
  },
  {
    "code": "def get_win_folder_via_ctypes(csidl_name: str) -> str:\n    csidl_const = {'CSIDL_APPDATA': 26, 'CSIDL_COMMON_APPDATA': 35, 'CSIDL_LOCAL_APPDATA': 28, 'CSIDL_PERSONAL': 5, 'CSIDL_MYPICTURES': 39, 'CSIDL_MYVIDEO': 14, 'CSIDL_MYMUSIC': 13, 'CSIDL_DOWNLOADS': 40}.get(csidl_name)\n    if csidl_const is None:\n        msg = f'Unknown CSIDL name: {csidl_name}'\n        raise ValueError(msg)\n    buf = ctypes.create_unicode_buffer(1024)\n    windll = getattr(ctypes, 'windll')\n    windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n    if any((ord(c) > 255 for c in buf)):\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n    if csidl_name == 'CSIDL_DOWNLOADS':\n        return os.path.join(buf.value, 'Downloads')\n    return buf.value",
    "label": true
  },
  {
    "code": "def infer_super(node, context: InferenceContext | None=None):\n    if len(node.args) == 1:\n        raise UseInferenceDefault\n    scope = node.scope()\n    if not isinstance(scope, nodes.FunctionDef):\n        raise UseInferenceDefault\n    if scope.type not in ('classmethod', 'method'):\n        raise UseInferenceDefault\n    cls = scoped_nodes.get_wrapping_class(scope)\n    if not node.args:\n        mro_pointer = cls\n        if scope.type == 'classmethod':\n            mro_type = cls\n        else:\n            mro_type = cls.instantiate_class()\n    else:\n        try:\n            mro_pointer = next(node.args[0].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n        try:\n            mro_type = next(node.args[1].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n    if isinstance(mro_pointer, util.UninferableBase) or isinstance(mro_type, util.UninferableBase):\n        raise UseInferenceDefault\n    super_obj = objects.Super(mro_pointer=mro_pointer, mro_type=mro_type, self_class=cls, scope=scope)\n    super_obj.parent = node\n    return super_obj",
    "label": true
  },
  {
    "code": "def ca_bundle_info(config: Configuration) -> str:\n    levels = set()\n    for key, _ in config.items():\n        levels.add(key.split('.')[0])\n    if not levels:\n        return 'Not specified'\n    levels_that_override_global = ['install', 'wheel', 'download']\n    global_overriding_level = [level for level in levels if level in levels_that_override_global]\n    if not global_overriding_level:\n        return 'global'\n    if 'global' in levels:\n        levels.remove('global')\n    return ', '.join(levels)",
    "label": true
  },
  {
    "code": "def find_packages(*, namespaces=True, fill_package_dir: Optional[Dict[str, str]]=None, root_dir: Optional[_Path]=None, **kwargs) -> List[str]:\n    from setuptools.discovery import construct_package_dir\n    from setuptools.extern.more_itertools import unique_everseen, always_iterable\n    if namespaces:\n        from setuptools.discovery import PEP420PackageFinder as PackageFinder\n    else:\n        from setuptools.discovery import PackageFinder\n    root_dir = root_dir or os.curdir\n    where = kwargs.pop('where', ['.'])\n    packages: List[str] = []\n    fill_package_dir = {} if fill_package_dir is None else fill_package_dir\n    search = list(unique_everseen(always_iterable(where)))\n    if len(search) == 1 and all((not _same_path(search[0], x) for x in ('.', root_dir))):\n        fill_package_dir.setdefault('', search[0])\n    for path in search:\n        package_path = _nest_path(root_dir, path)\n        pkgs = PackageFinder.find(package_path, **kwargs)\n        packages.extend(pkgs)\n        if pkgs and (not (fill_package_dir.get('') == path or os.path.samefile(package_path, root_dir))):\n            fill_package_dir.update(construct_package_dir(pkgs, path))\n    return packages",
    "label": true
  },
  {
    "code": "def get_resource_reader(package: types.ModuleType) -> Optional[ResourceReader]:\n    spec = package.__spec__\n    reader = getattr(spec.loader, 'get_resource_reader', None)\n    if reader is None:\n        return None\n    return reader(spec.name)",
    "label": true
  },
  {
    "code": "def write_entries(cmd, basename, filename):\n    eps = _entry_points.load(cmd.distribution.entry_points)\n    defn = _entry_points.render(eps)\n    cmd.write_or_delete_file('entry points', filename, defn, True)",
    "label": true
  },
  {
    "code": "def check_initial_combiner(label: str) -> bool:\n    if unicodedata.category(label[0])[0] == 'M':\n        raise IDNAError('Label begins with an illegal combining character')\n    return True",
    "label": true
  },
  {
    "code": "def _should_build(req: InstallRequirement, need_wheel: bool) -> bool:\n    if req.constraint:\n        return False\n    if req.is_wheel:\n        if need_wheel:\n            logger.info('Skipping %s, due to already being wheel.', req.name)\n        return False\n    if need_wheel:\n        return True\n    if not req.source_dir:\n        return False\n    if req.editable:\n        return req.supports_pyproject_editable()\n    return True",
    "label": true
  },
  {
    "code": "def parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:\n    if multiline:\n        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape_multiline\n    else:\n        error_on = ILLEGAL_BASIC_STR_CHARS\n        parse_escapes = parse_basic_str_escape\n    result = ''\n    start_pos = pos\n    while True:\n        try:\n            char = src[pos]\n        except IndexError:\n            raise suffixed_err(src, pos, 'Unterminated string') from None\n        if char == '\"':\n            if not multiline:\n                return (pos + 1, result + src[start_pos:pos])\n            if src.startswith('\"\"\"', pos):\n                return (pos + 3, result + src[start_pos:pos])\n            pos += 1\n            continue\n        if char == '\\\\':\n            result += src[start_pos:pos]\n            pos, parsed_escape = parse_escapes(src, pos)\n            result += parsed_escape\n            start_pos = pos\n            continue\n        if char in error_on:\n            raise suffixed_err(src, pos, f'Illegal character {char!r}')\n        pos += 1",
    "label": true
  },
  {
    "code": "def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    if cookiejar is None:\n        cookiejar = RequestsCookieJar()\n    if cookie_dict is not None:\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if overwrite or name not in names_from_jar:\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n    return cookiejar",
    "label": true
  },
  {
    "code": "def _is_target_name_in_binop_side(target: nodes.AssignName | nodes.AssignAttr, side: nodes.NodeNG | None) -> bool:\n    if isinstance(side, nodes.Name):\n        if isinstance(target, nodes.AssignName):\n            return target.name == side.name\n        return False\n    if isinstance(side, nodes.Attribute) and isinstance(target, nodes.AssignAttr):\n        return target.as_string() == side.as_string()\n    return False",
    "label": true
  },
  {
    "code": "def get_module_files(src_directory: str, blacklist: Sequence[str], list_all: bool=False) -> list[str]:\n    files: list[str] = []\n    for directory, dirnames, filenames in os.walk(src_directory):\n        if directory in blacklist:\n            continue\n        _handle_blacklist(blacklist, dirnames, filenames)\n        if not list_all and '__init__.py' not in filenames:\n            dirnames[:] = ()\n            continue\n        for filename in filenames:\n            if _is_python_file(filename):\n                src = os.path.join(directory, filename)\n                files.append(src)\n    return files",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    import faulthandler\n    config.stash[fault_handler_stderr_fd_key] = os.dup(get_stderr_fileno())\n    config.stash[fault_handler_originally_enabled_key] = faulthandler.is_enabled()\n    faulthandler.enable(file=config.stash[fault_handler_stderr_fd_key])",
    "label": true
  },
  {
    "code": "def parse_header_links(value):\n    links = []\n    replace_chars = ' \\'\"'\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(', *<', value):\n        try:\n            url, params = val.split(';', 1)\n        except ValueError:\n            url, params = (val, '')\n        link = {'url': url.strip('<> \\'\"')}\n        for param in params.split(';'):\n            try:\n                key, value = param.split('=')\n            except ValueError:\n                break\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n        links.append(link)\n    return links",
    "label": true
  },
  {
    "code": "def _basic_auth_str(username, password):\n    if not isinstance(username, basestring):\n        warnings.warn(\"Non-string usernames will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.\".format(username), category=DeprecationWarning)\n        username = str(username)\n    if not isinstance(password, basestring):\n        warnings.warn(\"Non-string passwords will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.\".format(type(password)), category=DeprecationWarning)\n        password = str(password)\n    if isinstance(username, str):\n        username = username.encode('latin1')\n    if isinstance(password, str):\n        password = password.encode('latin1')\n    authstr = 'Basic ' + to_native_string(b64encode(b':'.join((username, password))).strip())\n    return authstr",
    "label": true
  },
  {
    "code": "def get_module_from_module_name(module_name: str) -> ModuleType:\n    module_name = module_name.lower().replace('-', '_')\n    if module_name == 'setuptools':\n        module_name = 'pkg_resources'\n    __import__(f'pip._vendor.{module_name}', globals(), locals(), level=0)\n    return getattr(pip._vendor, module_name)",
    "label": true
  },
  {
    "code": "def assign_annassigned_stmts(self: nodes.AnnAssign, node: node_classes.AssignedStmtsPossibleNode=None, context: InferenceContext | None=None, assign_path: list[int] | None=None) -> Any:\n    for inferred in assign_assigned_stmts(self, node, context, assign_path):\n        if inferred is None:\n            yield util.Uninferable\n        else:\n            yield inferred",
    "label": true
  },
  {
    "code": "def _no_global_under_legacy_virtualenv() -> bool:\n    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))\n    no_global_site_packages_file = os.path.join(site_mod_dir, 'no-global-site-packages.txt')\n    return os.path.exists(no_global_site_packages_file)",
    "label": true
  },
  {
    "code": "def _main(config: Config, session: 'Session') -> Optional[Union[int, ExitCode]]:\n    config.hook.pytest_collection(session=session)\n    config.hook.pytest_runtestloop(session=session)\n    if session.testsfailed:\n        return ExitCode.TESTS_FAILED\n    elif session.testscollected == 0:\n        return ExitCode.NO_TESTS_COLLECTED\n    return None",
    "label": true
  },
  {
    "code": "def only_binary() -> Option:\n    format_control = FormatControl(set(), set())\n    return Option('--only-binary', dest='format_control', action='callback', callback=_handle_only_binary, type='str', default=format_control, help='Do not use source packages. Can be supplied multiple times, and each time adds to the existing value. Accepts either \":all:\" to disable all source packages, \":none:\" to empty the set, or one or more package names with commas between them. Packages without binary distributions will fail to install when this option is used on them.')",
    "label": true
  },
  {
    "code": "def tabulate(rows: Iterable[Iterable[Any]]) -> Tuple[List[str], List[int]]:\n    rows = [tuple(map(str, row)) for row in rows]\n    sizes = [max(map(len, col)) for col in zip_longest(*rows, fillvalue='')]\n    table = [' '.join(map(str.ljust, row, sizes)).rstrip() for row in rows]\n    return (table, sizes)",
    "label": true
  },
  {
    "code": "def _signals_enum() -> str:\n    signals_enum = '\\n    import enum\\n    class Signals(enum.IntEnum):\\n        SIGABRT   = enum.auto()\\n        SIGEMT    = enum.auto()\\n        SIGFPE    = enum.auto()\\n        SIGILL    = enum.auto()\\n        SIGINFO   = enum.auto()\\n        SIGINT    = enum.auto()\\n        SIGSEGV   = enum.auto()\\n        SIGTERM   = enum.auto()\\n    '\n    if sys.platform != 'win32':\n        signals_enum += '\\n        SIGALRM   = enum.auto()\\n        SIGBUS    = enum.auto()\\n        SIGCHLD   = enum.auto()\\n        SIGCONT   = enum.auto()\\n        SIGHUP    = enum.auto()\\n        SIGIO     = enum.auto()\\n        SIGIOT    = enum.auto()\\n        SIGKILL   = enum.auto()\\n        SIGPIPE   = enum.auto()\\n        SIGPROF   = enum.auto()\\n        SIGQUIT   = enum.auto()\\n        SIGSTOP   = enum.auto()\\n        SIGSYS    = enum.auto()\\n        SIGTRAP   = enum.auto()\\n        SIGTSTP   = enum.auto()\\n        SIGTTIN   = enum.auto()\\n        SIGTTOU   = enum.auto()\\n        SIGURG    = enum.auto()\\n        SIGUSR1   = enum.auto()\\n        SIGUSR2   = enum.auto()\\n        SIGVTALRM = enum.auto()\\n        SIGWINCH  = enum.auto()\\n        SIGXCPU   = enum.auto()\\n        SIGXFSZ   = enum.auto()\\n        '\n    if sys.platform == 'win32':\n        signals_enum += '\\n        SIGBREAK  = enum.auto()\\n        '\n    if sys.platform not in ('darwin', 'win32'):\n        signals_enum += '\\n        SIGCLD    = enum.auto()\\n        SIGPOLL   = enum.auto()\\n        SIGPWR    = enum.auto()\\n        SIGRTMAX  = enum.auto()\\n        SIGRTMIN  = enum.auto()\\n        '\n    return signals_enum",
    "label": true
  },
  {
    "code": "def symlink_or_skip(src, dst, **kwargs):\n    try:\n        os.symlink(str(src), str(dst), **kwargs)\n    except OSError as e:\n        skip(f'symlinks not supported: {e}')",
    "label": true
  },
  {
    "code": "def check_testcase_implements_trial_reporter(done: List[int]=[]) -> None:\n    if done:\n        return\n    from zope.interface import classImplements\n    from twisted.trial.itrial import IReporter\n    classImplements(TestCaseFunction, IReporter)\n    done.append(1)",
    "label": true
  },
  {
    "code": "def chop_cells(text: str, max_size: int, position: int=0) -> List[str]:\n    _get_character_cell_size = get_character_cell_size\n    characters = [(character, _get_character_cell_size(character)) for character in text]\n    total_size = position\n    lines: List[List[str]] = [[]]\n    append = lines[-1].append\n    for character, size in reversed(characters):\n        if total_size + size > max_size:\n            lines.append([character])\n            append = lines[-1].append\n            total_size = size\n        else:\n            total_size += size\n            append(character)\n    return [''.join(line) for line in lines]",
    "label": true
  },
  {
    "code": "def _shortened(word):\n    dpos = word.find('$')\n    return '|'.join((word[:dpos] + word[dpos + 1:i] + '\\\\b' for i in range(len(word), dpos, -1)))",
    "label": true
  },
  {
    "code": "def deprecated(*, reason: str, replacement: Optional[str], gone_in: Optional[str], feature_flag: Optional[str]=None, issue: Optional[int]=None) -> None:\n    is_gone = gone_in is not None and parse(current_version) >= parse(gone_in)\n    message_parts = [(reason, f'{DEPRECATION_MSG_PREFIX}{{}}'), (gone_in, 'pip {} will enforce this behaviour change.' if not is_gone else 'Since pip {}, this is no longer supported.'), (replacement, 'A possible replacement is {}.'), (feature_flag, 'You can use the flag --use-feature={} to test the upcoming behaviour.' if not is_gone else None), (issue, 'Discussion can be found at https://github.com/pypa/pip/issues/{}')]\n    message = ' '.join((format_str.format(value) for value, format_str in message_parts if format_str is not None and value is not None))\n    if is_gone:\n        raise PipDeprecationWarning(message)\n    warnings.warn(message, category=PipDeprecationWarning, stacklevel=2)",
    "label": true
  },
  {
    "code": "def unique_to_each(*iterables):\n    pool = [list(it) for it in iterables]\n    counts = Counter(chain.from_iterable(map(set, pool)))\n    uniques = {element for element in counts if counts[element] == 1}\n    return [list(filter(uniques.__contains__, it)) for it in pool]",
    "label": true
  },
  {
    "code": "def generate_tree_general(node_lst: list[ReadNode], root_index: int) -> HuffmanTree:\n    node = node_lst[root_index]\n    tree = _generate_tree_from_node(node)\n    if node.l_type == 1:\n        tree.left = generate_tree_general(node_lst, node.l_data)\n    if node.r_type == 1:\n        tree.right = generate_tree_general(node_lst, node.r_data)\n    return tree",
    "label": true
  },
  {
    "code": "def get_parser(prog='pycodestyle', version=__version__):\n    parser = OptionParser(prog=prog, version=version, usage='%prog [options] input ...')\n    parser.config_options = ['exclude', 'filename', 'select', 'ignore', 'max-line-length', 'max-doc-length', 'indent-size', 'hang-closing', 'count', 'format', 'quiet', 'show-pep8', 'show-source', 'statistics', 'verbose']\n    parser.add_option('-v', '--verbose', default=0, action='count', help='print status messages, or debug with -vv')\n    parser.add_option('-q', '--quiet', default=0, action='count', help='report only file names, or nothing with -qq')\n    parser.add_option('-r', '--repeat', default=True, action='store_true', help='(obsolete) show all occurrences of the same error')\n    parser.add_option('--first', action='store_false', dest='repeat', help='show first occurrence of each error')\n    parser.add_option('--exclude', metavar='patterns', default=DEFAULT_EXCLUDE, help='exclude files or directories which match these comma separated patterns (default: %default)')\n    parser.add_option('--filename', metavar='patterns', default='*.py', help='when parsing directories, only check filenames matching these comma separated patterns (default: %default)')\n    parser.add_option('--select', metavar='errors', default='', help='select errors and warnings (e.g. E,W6)')\n    parser.add_option('--ignore', metavar='errors', default='', help='skip errors and warnings (e.g. E4,W) (default: %s)' % DEFAULT_IGNORE)\n    parser.add_option('--show-source', action='store_true', help='show source code for each error')\n    parser.add_option('--show-pep8', action='store_true', help='show text of PEP 8 for each error (implies --first)')\n    parser.add_option('--statistics', action='store_true', help='count errors and warnings')\n    parser.add_option('--count', action='store_true', help='print total number of errors and warnings to standard error and set exit code to 1 if total is not null')\n    parser.add_option('--max-line-length', type='int', metavar='n', default=MAX_LINE_LENGTH, help='set maximum allowed line length (default: %default)')\n    parser.add_option('--max-doc-length', type='int', metavar='n', default=None, help='set maximum allowed doc line length and perform these checks (unchecked if not set)')\n    parser.add_option('--indent-size', type='int', metavar='n', default=INDENT_SIZE, help='set how many spaces make up an indent (default: %default)')\n    parser.add_option('--hang-closing', action='store_true', help=\"hang closing bracket instead of matching indentation of opening bracket's line\")\n    parser.add_option('--format', metavar='format', default='default', help='set the error format [default|pylint|<custom>]')\n    parser.add_option('--diff', action='store_true', help='report changes only within line number ranges in the unified diff received on STDIN')\n    group = parser.add_option_group('Testing Options')\n    group.add_option('--benchmark', action='store_true', help='measure processing speed')\n    return parser",
    "label": true
  },
  {
    "code": "def infer_typing_typevar_or_newtype(node: Call, context_itton: context.InferenceContext | None=None) -> Iterator[ClassDef]:\n    try:\n        func = next(node.func.infer(context=context_itton))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if func.qname() not in TYPING_TYPEVARS_QUALIFIED:\n        raise UseInferenceDefault\n    if not node.args:\n        raise UseInferenceDefault\n    if isinstance(node.args[0], JoinedStr):\n        raise UseInferenceDefault\n    typename = node.args[0].as_string().strip(\"'\")\n    try:\n        node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    except AstroidSyntaxError as exc:\n        raise InferenceError from exc\n    return node.infer(context=context_itton)",
    "label": true
  },
  {
    "code": "def disable_stdlib_finder():\n\n    def matches(finder):\n        return getattr(finder, '__module__', None) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')\n    for finder in filter(matches, sys.meta_path):\n        del finder.find_distributions",
    "label": true
  },
  {
    "code": "def _build_proxy_class(cls_name: str, builtins: nodes.Module) -> nodes.ClassDef:\n    proxy = raw_building.build_class(cls_name)\n    proxy.parent = builtins\n    return proxy",
    "label": true
  },
  {
    "code": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s",
    "label": true
  },
  {
    "code": "def _enum_has_attribute(owner: astroid.Instance | nodes.ClassDef, node: nodes.Attribute) -> bool:\n    if isinstance(owner, astroid.Instance):\n        enum_def = next((b.parent for b in owner.bases if isinstance(b.parent, nodes.ClassDef)), None)\n        if enum_def is None:\n            enum_def = node\n            while enum_def is not None and (not isinstance(enum_def, nodes.ClassDef)):\n                enum_def = enum_def.parent\n        assert enum_def is not None, 'enum_def unexpectedly None'\n    else:\n        enum_def = owner\n    dunder_new = next((m for m in enum_def.methods() if m.name == '__new__'), None)\n    dunder_init = next((m for m in enum_def.methods() if m.name == '__init__'), None)\n    enum_attributes: set[str] = set()\n    if dunder_new:\n        returned_obj_name = next((c.value for c in dunder_new.get_children() if isinstance(c, nodes.Return)), None)\n        if isinstance(returned_obj_name, nodes.Name):\n            enum_attributes |= _get_all_attribute_assignments(dunder_new, returned_obj_name.name)\n    if dunder_init and dunder_init.body and dunder_init.args:\n        enum_attributes |= _get_all_attribute_assignments(dunder_init, dunder_init.args.arguments[0].name)\n    return node.attrname in enum_attributes",
    "label": true
  },
  {
    "code": "def deinit():\n    if orig_stdout is not None:\n        sys.stdout = orig_stdout\n    if orig_stderr is not None:\n        sys.stderr = orig_stderr",
    "label": true
  },
  {
    "code": "def _redefines_import(node: nodes.AssignName) -> bool:\n    current = node\n    while current and (not isinstance(current.parent, nodes.ExceptHandler)):\n        current = current.parent\n    if not current or not utils.error_of_type(current.parent, ImportError):\n        return False\n    try_block = current.parent.parent\n    for import_node in try_block.nodes_of_class((nodes.ImportFrom, nodes.Import)):\n        for name, alias in import_node.names:\n            if alias:\n                if alias == node.name:\n                    return True\n            elif name == node.name:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def findall(dir=os.curdir):\n    files = _find_all_simple(dir)\n    if dir == os.curdir:\n        make_rel = functools.partial(os.path.relpath, start=dir)\n        files = map(make_rel, files)\n    return list(files)",
    "label": true
  },
  {
    "code": "def init(autoreset=False, convert=None, strip=None, wrap=True):\n    if not wrap and any([autoreset, convert, strip]):\n        raise ValueError('wrap=False conflicts with any other arg=True')\n    global wrapped_stdout, wrapped_stderr\n    global orig_stdout, orig_stderr\n    orig_stdout = sys.stdout\n    orig_stderr = sys.stderr\n    if sys.stdout is None:\n        wrapped_stdout = None\n    else:\n        sys.stdout = wrapped_stdout = wrap_stream(orig_stdout, convert, strip, autoreset, wrap)\n    if sys.stderr is None:\n        wrapped_stderr = None\n    else:\n        sys.stderr = wrapped_stderr = wrap_stream(orig_stderr, convert, strip, autoreset, wrap)\n    global atexit_done\n    if not atexit_done:\n        atexit.register(reset_all)\n        atexit_done = True",
    "label": true
  },
  {
    "code": "def getuserid(user):\n    import pwd\n    if not isinstance(user, int):\n        user = pwd.getpwnam(user)[2]\n    return user",
    "label": true
  },
  {
    "code": "def find_files(path: str, pattern: str) -> List[str]:\n    result: List[str] = []\n    for root, _, files in os.walk(path):\n        matches = fnmatch.filter(files, pattern)\n        result.extend((os.path.join(root, f) for f in matches))\n    return result",
    "label": true
  },
  {
    "code": "def _print_list_as_json(requested_items):\n    import json\n    result = {}\n    if 'lexer' in requested_items:\n        info = {}\n        for fullname, names, filenames, mimetypes in get_all_lexers():\n            info[fullname] = {'aliases': names, 'filenames': filenames, 'mimetypes': mimetypes}\n        result['lexers'] = info\n    if 'formatter' in requested_items:\n        info = {}\n        for cls in get_all_formatters():\n            doc = docstring_headline(cls)\n            info[cls.name] = {'aliases': cls.aliases, 'filenames': cls.filenames, 'doc': doc}\n        result['formatters'] = info\n    if 'filter' in requested_items:\n        info = {}\n        for name in get_all_filters():\n            cls = find_filter_class(name)\n            info[name] = {'doc': docstring_headline(cls)}\n        result['filters'] = info\n    if 'style' in requested_items:\n        info = {}\n        for name in get_all_styles():\n            cls = get_style_by_name(name)\n            info[name] = {'doc': docstring_headline(cls)}\n        result['styles'] = info\n    json.dump(result, sys.stdout)",
    "label": true
  },
  {
    "code": "def check_set(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if origin_type is frozenset:\n        if not isinstance(value, frozenset):\n            raise TypeCheckError('is not a frozenset')\n    elif not isinstance(value, AbstractSet):\n        raise TypeCheckError('is not a set')\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for v in samples:\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f'[{v}]')\n                raise",
    "label": true
  },
  {
    "code": "def _string_distance(seq1: str, seq2: str) -> int:\n    seq2_length = len(seq2)\n    row = list(range(1, seq2_length + 1)) + [0]\n    for seq1_index, seq1_char in enumerate(seq1):\n        last_row = row\n        row = [0] * seq2_length + [seq1_index + 1]\n        for seq2_index, seq2_char in enumerate(seq2):\n            row[seq2_index] = min(last_row[seq2_index] + 1, row[seq2_index - 1] + 1, last_row[seq2_index - 1] + (seq1_char != seq2_char))\n    return row[seq2_length - 1]",
    "label": true
  },
  {
    "code": "def get_http_url(link: Link, download: Downloader, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> File:\n    temp_dir = TempDirectory(kind='unpack', globally_managed=True)\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n        content_type = None\n    else:\n        from_path, content_type = download(link, temp_dir.path)\n        if hashes:\n            hashes.check_against_path(from_path)\n    return File(from_path, content_type)",
    "label": true
  },
  {
    "code": "def merge_coherence_ratios(results: List[CoherenceMatches]) -> CoherenceMatches:\n    per_language_ratios: Dict[str, List[float]] = {}\n    for result in results:\n        for sub_result in result:\n            language, ratio = sub_result\n            if language not in per_language_ratios:\n                per_language_ratios[language] = [ratio]\n                continue\n            per_language_ratios[language].append(ratio)\n    merge = [(language, round(sum(per_language_ratios[language]) / len(per_language_ratios[language]), 4)) for language in per_language_ratios]\n    return sorted(merge, key=lambda x: x[1], reverse=True)",
    "label": true
  },
  {
    "code": "def find_lexer_class_for_filename(_fn, code=None):\n    matches = []\n    fn = basename(_fn)\n    for modname, name, _, filenames, _ in LEXERS.values():\n        for filename in filenames:\n            if _fn_matches(fn, filename):\n                if name not in _lexer_cache:\n                    _load_lexers(modname)\n                matches.append((_lexer_cache[name], filename))\n    for cls in find_plugin_lexers():\n        for filename in cls.filenames:\n            if _fn_matches(fn, filename):\n                matches.append((cls, filename))\n    if isinstance(code, bytes):\n        code = guess_decode(code)\n\n    def get_rating(info):\n        cls, filename = info\n        bonus = '*' not in filename and 0.5 or 0\n        if code:\n            return (cls.analyse_text(code) + bonus, cls.__name__)\n        return (cls.priority + bonus, cls.__name__)\n    if matches:\n        matches.sort(key=get_rating)\n        return matches[-1][0]",
    "label": true
  },
  {
    "code": "def repeat_last(iterable, default=None):\n    item = _marker\n    for item in iterable:\n        yield item\n    final = default if item is _marker else item\n    yield from repeat(final)",
    "label": true
  },
  {
    "code": "def parse_one_line_basic_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1\n    return parse_basic_str(src, pos, multiline=False)",
    "label": true
  },
  {
    "code": "def check_code_string(code: str, show_diff: Union[bool, TextIO]=False, extension: Optional[str]=None, config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, disregard_skip: bool=False, **config_kwargs: Any) -> bool:\n    config = _config(path=file_path, config=config, **config_kwargs)\n    return check_stream(StringIO(code), show_diff=show_diff, extension=extension, config=config, file_path=file_path, disregard_skip=disregard_skip)",
    "label": true
  },
  {
    "code": "def assert_bool(dist, attr, value):\n    if bool(value) != value:\n        tmpl = '{attr!r} must be a boolean value (got {value!r})'\n        raise DistutilsSetupError(tmpl.format(attr=attr, value=value))",
    "label": true
  },
  {
    "code": "def unpack_vcs_link(link: Link, location: str, verbosity: int) -> None:\n    vcs_backend = vcs.get_backend_for_scheme(link.scheme)\n    assert vcs_backend is not None\n    vcs_backend.unpack(location, url=hide_url(link.url), verbosity=verbosity)",
    "label": true
  },
  {
    "code": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s",
    "label": true
  },
  {
    "code": "def test_filters() -> None:\n    customers = create_customers(test_dict)\n    process_event_history(test_dict, customers)\n    calls = []\n    hist = customers[0].get_history()\n    calls.extend(hist[0])\n    filters = [DurationFilter(), CustomerFilter(), ResetFilter()]\n    filter_strings = [['L050', 'G010', 'L000', '50', 'AA', ''], ['7777', '1111', '9999', 'aaaaaaaa', ''], ['rrrr', '']]\n    expected_return_lengths = [[1, 2, 0, 3, 3, 3], [3, 3, 3, 3, 3], [3, 3]]\n    for i in range(len(filters)):\n        for j in range(len(filter_strings[i])):\n            result = filters[i].apply(customers, calls, filter_strings[i][j])\n            assert len(result) == expected_return_lengths[i][j]",
    "label": true
  },
  {
    "code": "def set_file_position(body: typing.Any, pos: _TYPE_BODY_POSITION | None) -> _TYPE_BODY_POSITION | None:\n    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except OSError:\n            pos = _FAILEDTELL\n    return pos",
    "label": true
  },
  {
    "code": "def suffixed_err(src: str, pos: Pos, msg: str) -> TOMLDecodeError:\n\n    def coord_repr(src: str, pos: Pos) -> str:\n        if pos >= len(src):\n            return 'end of document'\n        line = src.count('\\n', 0, pos) + 1\n        if line == 1:\n            column = pos + 1\n        else:\n            column = pos - src.rindex('\\n', 0, pos)\n        return f'line {line}, column {column}'\n    return TOMLDecodeError(f'{msg} (at {coord_repr(src, pos)})')",
    "label": true
  },
  {
    "code": "def parse_one_line_basic_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1\n    return parse_basic_str(src, pos, multiline=False)",
    "label": true
  },
  {
    "code": "def end_setter_from_source(source_code, pred, only_consumables=False):\n\n    def set_endings_from_source(node):\n        if not hasattr(node, 'end_col_offset') or isinstance(node, nodes.Tuple):\n            set_from_last_child(node)\n        end_col_offset, lineno = (node.end_col_offset, node.end_lineno - 1)\n        for j in range(end_col_offset, len(source_code[lineno])):\n            if source_code[lineno][j] == '#':\n                break\n            if pred(source_code[lineno], j, node):\n                node.end_col_offset = j + 1\n                return node\n            elif only_consumables and source_code[lineno][j] not in CONSUMABLES:\n                return node\n        for i in range(lineno + 1, len(source_code)):\n            for j in range(len(source_code[i])):\n                if source_code[i][j] == '#':\n                    break\n                if pred(source_code[i], j, node):\n                    node.end_col_offset, node.end_lineno = (j + 1, i + 1)\n                    return node\n                elif source_code[i][j] not in CONSUMABLES:\n                    return node\n        return node\n    return set_endings_from_source",
    "label": true
  },
  {
    "code": "def _parent_path(pkg, pkg_path):\n    parent = pkg_path[:-len(pkg)] if pkg_path.endswith(pkg) else pkg_path\n    return parent.rstrip('/' + os.sep)",
    "label": true
  },
  {
    "code": "def parse_wheel_filename(filename: str) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n    if not filename.endswith('.whl'):\n        raise InvalidWheelFilename(f\"Invalid wheel filename (extension must be '.whl'): {filename}\")\n    filename = filename[:-4]\n    dashes = filename.count('-')\n    if dashes not in (4, 5):\n        raise InvalidWheelFilename(f'Invalid wheel filename (wrong number of parts): {filename}')\n    parts = filename.split('-', dashes - 2)\n    name_part = parts[0]\n    if '__' in name_part or re.match('^[\\\\w\\\\d._]*$', name_part, re.UNICODE) is None:\n        raise InvalidWheelFilename(f'Invalid project name: {filename}')\n    name = canonicalize_name(name_part)\n    try:\n        version = Version(parts[1])\n    except InvalidVersion as e:\n        raise InvalidWheelFilename(f'Invalid wheel filename (invalid version): {filename}') from e\n    if dashes == 5:\n        build_part = parts[2]\n        build_match = _build_tag_regex.match(build_part)\n        if build_match is None:\n            raise InvalidWheelFilename(f\"Invalid build number: {build_part} in '{filename}'\")\n        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))\n    else:\n        build = ()\n    tags = parse_tag(parts[-1])\n    return (name, version, build, tags)",
    "label": true
  },
  {
    "code": "def check_type_strict(argname: str, value: Any, expected_type: type) -> None:\n    if ENABLE_CONTRACT_CHECKING:\n        if type(value) is int and expected_type is float or (type(value) is bool and expected_type is int):\n            raise TypeError(f'type of {argname} must be {expected_type}; got {value} instead')\n        check_type(value, expected_type, collection_check_strategy=CollectionCheckStrategy.ALL_ITEMS)",
    "label": true
  },
  {
    "code": "def _get_required(d: Dict[str, Any], expected_type: Type[T], key: str, default: Optional[T]=None) -> T:\n    value = _get(d, expected_type, key, default)\n    if value is None:\n        raise DirectUrlValidationError(f'{key} must have a value')\n    return value",
    "label": true
  },
  {
    "code": "def _handle_src(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    value = os.path.abspath(value)\n    setattr(parser.values, option.dest, value)",
    "label": true
  },
  {
    "code": "def lazy_descriptor(obj):\n\n    class DescriptorProxy(lazy_object_proxy.Proxy):\n\n        def __get__(self, instance, owner=None):\n            return self.__class__.__get__(self, instance)\n    return DescriptorProxy(obj)",
    "label": true
  },
  {
    "code": "def _keyword_search(keyword):\n\n    def _is_keyword(s, index, node):\n        \"\"\"Search for a keyword. Right-to-left.\n        @type s: string\n        @type index: int\n        @type node: Astroid node\n        @rtype: bool\n        \"\"\"\n        return s[index:index + len(keyword)] == keyword\n    return _is_keyword",
    "label": true
  },
  {
    "code": "def reconfigure(*args: Any, **kwargs: Any) -> None:\n    from pip._vendor.rich.console import Console\n    new_console = Console(*args, **kwargs)\n    _console = get_console()\n    _console.__dict__ = new_console.__dict__",
    "label": true
  },
  {
    "code": "def parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n    if filename.endswith('.tar.gz'):\n        file_stem = filename[:-len('.tar.gz')]\n    elif filename.endswith('.zip'):\n        file_stem = filename[:-len('.zip')]\n    else:\n        raise InvalidSdistFilename(f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'): {filename}\")\n    name_part, sep, version_part = file_stem.rpartition('-')\n    if not sep:\n        raise InvalidSdistFilename(f'Invalid sdist filename: {filename}')\n    name = canonicalize_name(name_part)\n    version = Version(version_part)\n    return (name, version)",
    "label": true
  },
  {
    "code": "def _find_func_form_arguments(node, context):\n\n    def _extract_namedtuple_arg_or_keyword(position, key_name=None):\n        if len(args) > position:\n            return _infer_first(args[position], context)\n        if key_name and key_name in found_keywords:\n            return _infer_first(found_keywords[key_name], context)\n    args = node.args\n    keywords = node.keywords\n    found_keywords = {keyword.arg: keyword.value for keyword in keywords} if keywords else {}\n    name = _extract_namedtuple_arg_or_keyword(position=0, key_name='typename')\n    names = _extract_namedtuple_arg_or_keyword(position=1, key_name='field_names')\n    if name and names:\n        return (name.value, names)\n    raise UseInferenceDefault()",
    "label": true
  },
  {
    "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    if session_setting is None:\n        return request_setting\n    if request_setting is None:\n        return session_setting\n    if not (isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)):\n        return request_setting\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n    none_keys = [k for k, v in merged_setting.items() if v is None]\n    for key in none_keys:\n        del merged_setting[key]\n    return merged_setting",
    "label": true
  },
  {
    "code": "def _get_valid_file_path(mod: str='') -> Optional[str]:\n    if mod == '':\n        m = sys.modules['__main__']\n        spec = importlib.util.spec_from_file_location(m.__name__, m.__file__)\n        mod = spec.origin\n    elif not isinstance(mod, str):\n        print('No CFG generated. Input to check, `{}`, has invalid type, must be a string.'.format(mod))\n        return\n    if not os.path.isfile(mod):\n        print('Could not find the file called, `{}`\\n'.format(mod))\n        return\n    return os.path.abspath(mod)",
    "label": true
  },
  {
    "code": "def find_lexer_class_by_name(_alias):\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name]\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls\n    raise ClassNotFound('no lexer for alias %r found' % _alias)",
    "label": true
  },
  {
    "code": "def dotted_netmask(mask):\n    bits = 4294967295 ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack('>I', bits))",
    "label": true
  },
  {
    "code": "def warn_if_run_as_root() -> None:\n    if running_under_virtualenv():\n        return\n    if not hasattr(os, 'getuid'):\n        return\n    if sys.platform == 'win32' or sys.platform == 'cygwin':\n        return\n    if os.getuid() != 0:\n        return\n    logger.warning(\"Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\")",
    "label": true
  },
  {
    "code": "def compress_file(in_file: str, out_file: str) -> None:\n    with open(in_file, 'rb') as f1:\n        text = f1.read()\n    freq = build_frequency_dict(text)\n    tree = build_huffman_tree(freq)\n    codes = get_codes(tree)\n    number_nodes(tree)\n    print('Bits per symbol:', avg_length(tree, freq))\n    result = tree.num_nodes_to_bytes() + tree_to_bytes(tree) + int32_to_bytes(len(text))\n    result += compress_bytes(text, codes)\n    with open(out_file, 'wb') as f2:\n        f2.write(result)",
    "label": true
  },
  {
    "code": "def _is_in_snake_case(name: str) -> bool:\n    pattern = '(_?[a-z][a-z0-9_]*)$'\n    return re.match(pattern, name) is not None",
    "label": true
  },
  {
    "code": "def start_setter_from_source(source_code, pred):\n\n    def set_start_from_source(node):\n        col_offset, lineno = (node.col_offset, node.fromlineno - 1)\n        for j in range(min(len(source_code[lineno]) - 1, col_offset), -1, -1):\n            if pred(source_code[lineno], j, node):\n                node.col_offset = j\n                return node\n        for i in range(lineno - 1, -1, -1):\n            for j in range(len(source_code[i]) - 1, -1, -1):\n                if pred(source_code[i], j, node):\n                    node.end_col_offset, node.end_lineno = (j, i + 1)\n                    return node\n                elif source_code[i][j] not in CONSUMABLES:\n                    return node\n        return node\n    return set_start_from_source",
    "label": true
  },
  {
    "code": "def _convert_option_to_argument(opt: str, optdict: dict[str, Any]) -> _StoreArgument | _StoreTrueArgument | _CallableArgument | _StoreOldNamesArgument | _StoreNewNamesArgument | _ExtendArgument:\n    if 'level' in optdict and 'hide' not in optdict:\n        warnings.warn(f\"The 'level' key in optdicts has been deprecated. Use 'hide' with a boolean to hide an option from the help message. optdict={optdict}\", DeprecationWarning)\n    flags = [f'--{opt}']\n    if 'short' in optdict:\n        flags += [f\"-{optdict['short']}\"]\n    action = optdict.get('action', 'store')\n    if action == 'store_true':\n        return _StoreTrueArgument(flags=flags, action=action, default=optdict.get('default', True), arg_help=optdict.get('help', ''), hide_help=optdict.get('hide', False), section=optdict.get('group', None))\n    if not isinstance(action, str) and issubclass(action, _CallbackAction):\n        return _CallableArgument(flags=flags, action=action, arg_help=optdict.get('help', ''), kwargs=optdict.get('kwargs', {}), hide_help=optdict.get('hide', False), section=optdict.get('group', None), metavar=optdict.get('metavar', None))\n    try:\n        default = optdict['default']\n    except KeyError:\n        warnings.warn(f\"An option dictionary should have a 'default' key to specify the option's default value. This key will be required in pylint 3.0. It is not required for 'store_true' and callable actions. optdict={optdict}\", DeprecationWarning)\n        default = None\n    if action == 'extend':\n        return _ExtendArgument(flags=flags, action=action, default=[] if default is None else default, arg_type=optdict['type'], choices=optdict.get('choices', None), arg_help=optdict.get('help', ''), metavar=optdict.get('metavar', ''), hide_help=optdict.get('hide', False), section=optdict.get('group', None), dest=optdict.get('dest', None))\n    if 'kwargs' in optdict:\n        if 'old_names' in optdict['kwargs']:\n            return _StoreOldNamesArgument(flags=flags, default=default, arg_type=optdict['type'], choices=optdict.get('choices', None), arg_help=optdict.get('help', ''), metavar=optdict.get('metavar', ''), hide_help=optdict.get('hide', False), kwargs=optdict.get('kwargs', {}), section=optdict.get('group', None))\n        if 'new_names' in optdict['kwargs']:\n            return _StoreNewNamesArgument(flags=flags, default=default, arg_type=optdict['type'], choices=optdict.get('choices', None), arg_help=optdict.get('help', ''), metavar=optdict.get('metavar', ''), hide_help=optdict.get('hide', False), kwargs=optdict.get('kwargs', {}), section=optdict.get('group', None))\n    if 'dest' in optdict:\n        return _StoreOldNamesArgument(flags=flags, default=default, arg_type=optdict['type'], choices=optdict.get('choices', None), arg_help=optdict.get('help', ''), metavar=optdict.get('metavar', ''), hide_help=optdict.get('hide', False), kwargs={'old_names': [optdict['dest']]}, section=optdict.get('group', None))\n    return _StoreArgument(flags=flags, action=action, default=default, arg_type=optdict['type'], choices=optdict.get('choices', None), arg_help=optdict.get('help', ''), metavar=optdict.get('metavar', ''), hide_help=optdict.get('hide', False), section=optdict.get('group', None))",
    "label": true
  },
  {
    "code": "def generate_tree_general(node_lst: list[ReadNode], root_index: int) -> HuffmanTree:\n    node = node_lst[root_index]\n    tree = _generate_tree_from_node(node)\n    if node.l_type == 1:\n        tree.left = generate_tree_general(node_lst, node.l_data)\n    if node.r_type == 1:\n        tree.right = generate_tree_general(node_lst, node.r_data)\n    return tree",
    "label": true
  },
  {
    "code": "def improve_tree(tree: HuffmanTree, freq_dict: dict[int, int]) -> None:\n    leafs, levels = _get_leafs_and_levels(tree)\n    codes = get_codes(tree)\n    current = 0\n    for span in levels:\n        big = (current, leafs, freq_dict, span, tree, codes)\n        _improve_tree_helper(big)\n        current += span",
    "label": true
  },
  {
    "code": "def _parse_options(o_strs):\n    opts = {}\n    if not o_strs:\n        return opts\n    for o_str in o_strs:\n        if not o_str.strip():\n            continue\n        o_args = o_str.split(',')\n        for o_arg in o_args:\n            o_arg = o_arg.strip()\n            try:\n                o_key, o_val = o_arg.split('=', 1)\n                o_key = o_key.strip()\n                o_val = o_val.strip()\n            except ValueError:\n                opts[o_arg] = True\n            else:\n                opts[o_key] = o_val\n    return opts",
    "label": true
  },
  {
    "code": "def get_pip_version() -> str:\n    pip_pkg_dir = os.path.join(os.path.dirname(__file__), '..', '..')\n    pip_pkg_dir = os.path.abspath(pip_pkg_dir)\n    return 'pip {} from {} (python {})'.format(__version__, pip_pkg_dir, get_major_minor_version())",
    "label": true
  },
  {
    "code": "def unique_to_each(*iterables):\n    pool = [list(it) for it in iterables]\n    counts = Counter(chain.from_iterable(map(set, pool)))\n    uniques = {element for element in counts if counts[element] == 1}\n    return [list(filter(uniques.__contains__, it)) for it in pool]",
    "label": true
  },
  {
    "code": "def padded(iterable, fillvalue=None, n=None, next_multiple=False):\n    it = iter(iterable)\n    if n is None:\n        yield from chain(it, repeat(fillvalue))\n    elif n < 1:\n        raise ValueError('n must be at least 1')\n    else:\n        item_count = 0\n        for item in it:\n            yield item\n            item_count += 1\n        remaining = (n - item_count) % n if next_multiple else n - item_count\n        for _ in range(remaining):\n            yield fillvalue",
    "label": true
  },
  {
    "code": "def egg_link_path_from_location(raw_name: str) -> Optional[str]:\n    sites: List[str] = []\n    if running_under_virtualenv():\n        sites.append(site_packages)\n        if not virtualenv_no_global() and user_site:\n            sites.append(user_site)\n    else:\n        if user_site:\n            sites.append(user_site)\n        sites.append(site_packages)\n    egg_link_name = _egg_link_name(raw_name)\n    for site in sites:\n        egglink = os.path.join(site, egg_link_name)\n        if os.path.isfile(egglink):\n            return egglink\n    return None",
    "label": true
  },
  {
    "code": "def strtobool(val: str) -> int:\n    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(f'invalid truth value {val!r}')",
    "label": true
  },
  {
    "code": "def _set_module(typevarlike):\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod",
    "label": true
  },
  {
    "code": "def _call_assertion_pass(lineno: int, orig: str, expl: str) -> None:\n    if util._assertion_pass is not None:\n        util._assertion_pass(lineno, orig, expl)",
    "label": true
  },
  {
    "code": "def handle_line(line: ParsedLine, options: Optional[optparse.Values]=None, finder: Optional['PackageFinder']=None, session: Optional[PipSession]=None) -> Optional[ParsedRequirement]:\n    if line.is_requirement:\n        parsed_req = handle_requirement_line(line, options)\n        return parsed_req\n    else:\n        handle_option_line(line.opts, line.filename, line.lineno, finder, options, session)\n        return None",
    "label": true
  },
  {
    "code": "def _strip_extras(path: str) -> Tuple[str, Optional[str]]:\n    m = re.match('^(.+)(\\\\[[^\\\\]]+\\\\])$', path)\n    extras = None\n    if m:\n        path_no_extras = m.group(1)\n        extras = m.group(2)\n    else:\n        path_no_extras = path\n    return (path_no_extras, extras)",
    "label": true
  },
  {
    "code": "def _chunked_even_finite(iterable, N, n):\n    if N < 1:\n        return\n    q, r = divmod(N, n)\n    num_lists = q + (1 if r > 0 else 0)\n    q, r = divmod(N, num_lists)\n    full_size = q + (1 if r > 0 else 0)\n    partial_size = full_size - 1\n    num_full = N - partial_size * num_lists\n    num_partial = num_lists - num_full\n    buffer = []\n    iterator = iter(iterable)\n    for x in iterator:\n        buffer.append(x)\n        if len(buffer) == full_size:\n            yield buffer\n            buffer = []\n            num_full -= 1\n            if num_full <= 0:\n                break\n    for x in iterator:\n        buffer.append(x)\n        if len(buffer) == partial_size:\n            yield buffer\n            buffer = []\n            num_partial -= 1",
    "label": true
  },
  {
    "code": "def check_instance(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, origin_type):\n        raise TypeCheckError(f'is not an instance of {qualified_name(origin_type)}')",
    "label": true
  },
  {
    "code": "def _legacy_key(s):\n\n    def get_parts(s):\n        result = []\n        for p in _VERSION_PART.split(s.lower()):\n            p = _VERSION_REPLACE.get(p, p)\n            if p:\n                if '0' <= p[:1] <= '9':\n                    p = p.zfill(8)\n                else:\n                    p = '*' + p\n                result.append(p)\n        result.append('*final')\n        return result\n    result = []\n    for p in get_parts(s):\n        if p.startswith('*'):\n            if p < '*final':\n                while result and result[-1] == '*final-':\n                    result.pop()\n            while result and result[-1] == '00000000':\n                result.pop()\n        result.append(p)\n    return tuple(result)",
    "label": true
  },
  {
    "code": "def find_plugin_lexers():\n    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):\n        yield entrypoint.load()",
    "label": true
  },
  {
    "code": "def pick_unit_and_suffix(size: int, suffixes: List[str], base: int) -> Tuple[int, str]:\n    for i, suffix in enumerate(suffixes):\n        unit = base ** i\n        if size < unit * base:\n            break\n    return (unit, suffix)",
    "label": true
  },
  {
    "code": "def ansiformat(attr, text):\n    result = []\n    if attr[:1] == attr[-1:] == '+':\n        result.append(codes['blink'])\n        attr = attr[1:-1]\n    if attr[:1] == attr[-1:] == '*':\n        result.append(codes['bold'])\n        attr = attr[1:-1]\n    if attr[:1] == attr[-1:] == '_':\n        result.append(codes['underline'])\n        attr = attr[1:-1]\n    result.append(codes[attr])\n    result.append(text)\n    result.append(codes['reset'])\n    return ''.join(result)",
    "label": true
  },
  {
    "code": "def register(linter: PyLinter) -> None:\n    linter.register_checker(StringFormatChecker(linter))\n    linter.register_checker(StringConstantChecker(linter))",
    "label": true
  },
  {
    "code": "def strictly_n(iterable, n, too_short=None, too_long=None):\n    if too_short is None:\n        too_short = lambda item_count: raise_(ValueError, 'Too few items in iterable (got {})'.format(item_count))\n    if too_long is None:\n        too_long = lambda item_count: raise_(ValueError, 'Too many items in iterable (got at least {})'.format(item_count))\n    it = iter(iterable)\n    for i in range(n):\n        try:\n            item = next(it)\n        except StopIteration:\n            too_short(i)\n            return\n        else:\n            yield item\n    try:\n        next(it)\n    except StopIteration:\n        pass\n    else:\n        too_long(n + 1)",
    "label": true
  },
  {
    "code": "def sync_do_slice(value: 't.Collection[V]', slices: int, fill_with: 't.Optional[V]'=None) -> 't.Iterator[t.List[V]]':\n    seq = list(value)\n    length = len(seq)\n    items_per_slice = length // slices\n    slices_with_extra = length % slices\n    offset = 0\n    for slice_number in range(slices):\n        start = offset + slice_number * items_per_slice\n        if slice_number < slices_with_extra:\n            offset += 1\n        end = offset + (slice_number + 1) * items_per_slice\n        tmp = seq[start:end]\n        if fill_with is not None and slice_number >= slices_with_extra:\n            tmp.append(fill_with)\n        yield tmp",
    "label": true
  },
  {
    "code": "def getstatementrange_ast(lineno: int, source: Source, assertion: bool=False, astnode: Optional[ast.AST]=None) -> Tuple[ast.AST, int, int]:\n    if astnode is None:\n        content = str(source)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            astnode = ast.parse(content, 'source', 'exec')\n    start, end = get_statement_startend2(lineno, astnode)\n    if end is None:\n        end = len(source.lines)\n    if end > start + 1:\n        block_finder = inspect.BlockFinder()\n        block_finder.started = source.lines[start][0].isspace()\n        it = (x + '\\n' for x in source.lines[start:end])\n        try:\n            for tok in tokenize.generate_tokens(lambda: next(it)):\n                block_finder.tokeneater(*tok)\n        except (inspect.EndOfBlock, IndentationError):\n            end = block_finder.last + start\n        except Exception:\n            pass\n    while end:\n        line = source.lines[end - 1].lstrip()\n        if line.startswith('#') or not line:\n            end -= 1\n        else:\n            break\n    return (astnode, start, end)",
    "label": true
  },
  {
    "code": "def file_size(path: str) -> Union[int, float]:\n    if os.path.islink(path):\n        return 0\n    return os.path.getsize(path)",
    "label": true
  },
  {
    "code": "def _get_allow_bytes_flag() -> int:\n    import doctest\n    return doctest.register_optionflag('ALLOW_BYTES')",
    "label": true
  },
  {
    "code": "def _create_basic_elements(value: Iterable[Any], node: List | Set | Tuple) -> list[NodeNG]:\n    elements: list[NodeNG] = []\n    for element in value:\n        element_node = const_factory(element)\n        element_node.parent = node\n        elements.append(element_node)\n    return elements",
    "label": true
  },
  {
    "code": "def test_all_labels():\n    for label in LABELS:\n        assert decode(b'', label) == ('', lookup(label))\n        assert encode('', label) == b''\n        for repeat in [0, 1, 12]:\n            output, _ = iter_decode([b''] * repeat, label)\n            assert list(output) == []\n            assert list(iter_encode([''] * repeat, label)) == []\n        decoder = IncrementalDecoder(label)\n        assert decoder.decode(b'') == ''\n        assert decoder.decode(b'', final=True) == ''\n        encoder = IncrementalEncoder(label)\n        assert encoder.encode('') == b''\n        assert encoder.encode('', final=True) == b''\n    for name in set(LABELS.values()):\n        assert lookup(name).name == name",
    "label": true
  },
  {
    "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)",
    "label": true
  },
  {
    "code": "def failable_map(fn: Callable[[TypeVar('T')], Failable], itr: Iterable[Failable]) -> Failable:\n    lst = list(itr)\n    if lst == []:\n        return Failable([])\n    return fn(lst[0]) >> (lambda fst: failable_map(fn, lst[1:]) >> (lambda rest: Failable([fst] + rest)))",
    "label": true
  },
  {
    "code": "def get_style_by_name(name):\n    if name in STYLE_MAP:\n        mod, cls = STYLE_MAP[name].split('::')\n        builtin = 'yes'\n    else:\n        for found_name, style in find_plugin_styles():\n            if name == found_name:\n                return style\n        builtin = ''\n        mod = name\n        cls = name.title() + 'Style'\n    try:\n        mod = __import__('pygments.styles.' + mod, None, None, [cls])\n    except ImportError:\n        raise ClassNotFound('Could not find style module %r' % mod + (builtin and ', though it should be builtin') + '.')\n    try:\n        return getattr(mod, cls)\n    except AttributeError:\n        raise ClassNotFound('Could not find style class %r in style module.' % cls)",
    "label": true
  },
  {
    "code": "def get_scheme(dist_name: str, user: bool=False, home: Optional[str]=None, root: Optional[str]=None, isolated: bool=False, prefix: Optional[str]=None) -> Scheme:\n    scheme = distutils_scheme(dist_name, user, home, root, isolated, prefix)\n    return Scheme(platlib=scheme['platlib'], purelib=scheme['purelib'], headers=scheme['headers'], scripts=scheme['scripts'], data=scheme['data'])",
    "label": true
  },
  {
    "code": "def isimportable(name):\n    if name and (name[0].isalpha() or name[0] == '_'):\n        name = name.replace('_', '')\n        return not name or name.isalnum()",
    "label": true
  },
  {
    "code": "def _hashlib_transform():\n    maybe_usedforsecurity = ', usedforsecurity=True' if PY39_PLUS else ''\n    init_signature = f\"value=''{maybe_usedforsecurity}\"\n    digest_signature = 'self'\n    shake_digest_signature = 'self, length'\n    template = \"\\n    class %(name)s:\\n        def __init__(self, %(init_signature)s): pass\\n        def digest(%(digest_signature)s):\\n            return %(digest)s\\n        def copy(self):\\n            return self\\n        def update(self, value): pass\\n        def hexdigest(%(digest_signature)s):\\n            return ''\\n        @property\\n        def name(self):\\n            return %(name)r\\n        @property\\n        def block_size(self):\\n            return 1\\n        @property\\n        def digest_size(self):\\n            return 1\\n    \"\n    algorithms_with_signature = dict.fromkeys(['md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512', 'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512'], (init_signature, digest_signature))\n    blake2b_signature = f\"data=b'', *, digest_size=64, key=b'', salt=b'', person=b'', fanout=1, depth=1, leaf_size=0, node_offset=0, node_depth=0, inner_size=0, last_node=False{maybe_usedforsecurity}\"\n    blake2s_signature = f\"data=b'', *, digest_size=32, key=b'', salt=b'', person=b'', fanout=1, depth=1, leaf_size=0, node_offset=0, node_depth=0, inner_size=0, last_node=False{maybe_usedforsecurity}\"\n    shake_algorithms = dict.fromkeys(['shake_128', 'shake_256'], (init_signature, shake_digest_signature))\n    algorithms_with_signature.update(shake_algorithms)\n    algorithms_with_signature.update({'blake2b': (blake2b_signature, digest_signature), 'blake2s': (blake2s_signature, digest_signature)})\n    classes = ''.join((template % {'name': hashfunc, 'digest': 'b\"\"', 'init_signature': init_signature, 'digest_signature': digest_signature} for hashfunc, (init_signature, digest_signature) in algorithms_with_signature.items()))\n    return parse(classes)",
    "label": true
  },
  {
    "code": "def find_files(path: str, pattern: str) -> List[str]:\n    result: List[str] = []\n    for root, _, files in os.walk(path):\n        matches = fnmatch.filter(files, pattern)\n        result.extend((os.path.join(root, f) for f in matches))\n    return result",
    "label": true
  },
  {
    "code": "def _have_working_poll() -> bool:\n    try:\n        poll_obj = select.poll()\n        poll_obj.poll(0)\n    except (AttributeError, OSError):\n        return False\n    else:\n        return True",
    "label": true
  },
  {
    "code": "def divide_line(text: str, width: int, fold: bool=True) -> List[int]:\n    divides: List[int] = []\n    append = divides.append\n    line_position = 0\n    _cell_len = cell_len\n    for start, _end, word in words(text):\n        word_length = _cell_len(word.rstrip())\n        if line_position + word_length > width:\n            if word_length > width:\n                if fold:\n                    chopped_words = chop_cells(word, max_size=width, position=0)\n                    for last, line in loop_last(chopped_words):\n                        if start:\n                            append(start)\n                        if last:\n                            line_position = _cell_len(line)\n                        else:\n                            start += len(line)\n                else:\n                    if start:\n                        append(start)\n                    line_position = _cell_len(word)\n            elif line_position and start:\n                append(start)\n                line_position = _cell_len(word)\n        else:\n            line_position += _cell_len(word)\n    return divides",
    "label": true
  },
  {
    "code": "def init_logging() -> None:\n    logging.setLoggerClass(VerboseLogger)\n    logging.addLevelName(VERBOSE, 'VERBOSE')",
    "label": true
  },
  {
    "code": "def _get_cycles(graph_dict: dict[str, set[str]], path: list[str], visited: set[str], result: list[list[str]], vertice: str) -> None:\n    if vertice in path:\n        cycle = [vertice]\n        for node in path[::-1]:\n            if node == vertice:\n                break\n            cycle.insert(0, node)\n        start_from = min(cycle)\n        index = cycle.index(start_from)\n        cycle = cycle[index:] + cycle[0:index]\n        if cycle not in result:\n            result.append(cycle)\n        return\n    path.append(vertice)\n    try:\n        for node in graph_dict[vertice]:\n            if node not in visited:\n                _get_cycles(graph_dict, path, visited, result, node)\n                visited.add(node)\n    except KeyError:\n        pass\n    path.pop()",
    "label": true
  },
  {
    "code": "def _http_get_download(session: PipSession, link: Link) -> Response:\n    target_url = link.url.split('#', 1)[0]\n    resp = session.get(target_url, headers=HEADERS, stream=True)\n    raise_for_status(resp)\n    return resp",
    "label": true
  },
  {
    "code": "def split_sections(s):\n    section = None\n    content = []\n    for line in yield_lines(s):\n        if line.startswith('['):\n            if line.endswith(']'):\n                if section or content:\n                    yield (section, content)\n                section = line[1:-1].strip()\n                content = []\n            else:\n                raise ValueError('Invalid section heading', line)\n        else:\n            content.append(line)\n    yield (section, content)",
    "label": true
  },
  {
    "code": "def duplicates_removed(it, already_seen=()):\n    lst = []\n    seen = set()\n    for i in it:\n        if i in seen or i in already_seen:\n            continue\n        lst.append(i)\n        seen.add(i)\n    return lst",
    "label": true
  },
  {
    "code": "def create_cookie(name, value, **kwargs):\n    result = {'version': 0, 'name': name, 'value': value, 'port': None, 'domain': '', 'path': '/', 'secure': False, 'expires': None, 'discard': True, 'comment': None, 'comment_url': None, 'rest': {'HttpOnly': None}, 'rfc2109': False}\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        raise TypeError(f'create_cookie() got unexpected keyword arguments: {list(badargs)}')\n    result.update(kwargs)\n    result['port_specified'] = bool(result['port'])\n    result['domain_specified'] = bool(result['domain'])\n    result['domain_initial_dot'] = result['domain'].startswith('.')\n    result['path_specified'] = bool(result['path'])\n    return cookielib.Cookie(**result)",
    "label": true
  },
  {
    "code": "def returns_tuple_of(func: callable, args: list, tp: tuple):\n    result = type_check_simple(func, args, tuple)\n    if not result[0]:\n        return (False, result[1])\n    tuple_format = ', '.join([item.__name__ for item in tp])\n    msg = type_error_message(func.__name__, 'tuple of ({})'.format(tuple_format), result[1])\n    if len(result[1]) != len(tp):\n        return (False, msg)\n    for i in range(len(tp)):\n        if not isinstance(result[1][i], tp[i]):\n            return (False, msg)\n    return (True, result[1])",
    "label": true
  },
  {
    "code": "def _get_str_to_type_converter(setting_name: str) -> Union[Callable[[str], Any], Type[Any]]:\n    type_converter: Union[Callable[[str], Any], Type[Any]] = type(_DEFAULT_SETTINGS.get(setting_name, ''))\n    if type_converter == WrapModes:\n        type_converter = wrap_mode_from_string\n    return type_converter",
    "label": true
  },
  {
    "code": "def _glibc_version_string_confstr() -> Optional[str]:\n    try:\n        version_string: str = getattr(os, 'confstr')('CS_GNU_LIBC_VERSION')\n        assert version_string is not None\n        _, version = version_string.rsplit()\n    except (AssertionError, AttributeError, OSError, ValueError):\n        return None\n    return version",
    "label": true
  },
  {
    "code": "def reconfigure(*args: Any, **kwargs: Any) -> None:\n    from pip._vendor.rich.console import Console\n    new_console = Console(*args, **kwargs)\n    _console = get_console()\n    _console.__dict__ = new_console.__dict__",
    "label": true
  },
  {
    "code": "def parse_bdist_wininst(name):\n    lower = name.lower()\n    base, py_ver, plat = (None, None, None)\n    if lower.endswith('.exe'):\n        if lower.endswith('.win32.exe'):\n            base = name[:-10]\n            plat = 'win32'\n        elif lower.startswith('.win32-py', -16):\n            py_ver = name[-7:-4]\n            base = name[:-16]\n            plat = 'win32'\n        elif lower.endswith('.win-amd64.exe'):\n            base = name[:-14]\n            plat = 'win-amd64'\n        elif lower.startswith('.win-amd64-py', -20):\n            py_ver = name[-7:-4]\n            base = name[:-20]\n            plat = 'win-amd64'\n    return (base, py_ver, plat)",
    "label": true
  },
  {
    "code": "def _caller(depth=2):\n    try:\n        return sys._getframe(depth).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        return None",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e261(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, len(source_lines[line - 1])), LineType.ERROR, source_lines[line - 1] + \"  # INSERT TWO SPACES BEFORE THE '#'\")\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _find_spec(module_name: str, module_path: Optional[_Path]) -> ModuleSpec:\n    spec = importlib.util.spec_from_file_location(module_name, module_path)\n    spec = spec or importlib.util.find_spec(module_name)\n    if spec is None:\n        raise ModuleNotFoundError(module_name)\n    return spec",
    "label": true
  },
  {
    "code": "def _emoji_replace(text: str, default_variant: Optional[str]=None, _emoji_sub: _EmojiSubMethod=re.compile('(:(\\\\S*?)(?:(?:\\\\-)(emoji|text))?:)').sub) -> str:\n    get_emoji = EMOJI.__getitem__\n    variants = {'text': '\ufe0e', 'emoji': '\ufe0f'}\n    get_variant = variants.get\n    default_variant_code = variants.get(default_variant, '') if default_variant else ''\n\n    def do_replace(match: Match[str]) -> str:\n        emoji_code, emoji_name, variant = match.groups()\n        try:\n            return get_emoji(emoji_name.lower()) + get_variant(variant, default_variant_code)\n        except KeyError:\n            return emoji_code\n    return _emoji_sub(do_replace, text)",
    "label": true
  },
  {
    "code": "def canonicalize_version(version: Union[Version, str], *, strip_trailing_zero: bool=True) -> str:\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            return version\n    else:\n        parsed = version\n    parts = []\n    if parsed.epoch != 0:\n        parts.append(f'{parsed.epoch}!')\n    release_segment = '.'.join((str(x) for x in parsed.release))\n    if strip_trailing_zero:\n        release_segment = re.sub('(\\\\.0)+$', '', release_segment)\n    parts.append(release_segment)\n    if parsed.pre is not None:\n        parts.append(''.join((str(x) for x in parsed.pre)))\n    if parsed.post is not None:\n        parts.append(f'.post{parsed.post}')\n    if parsed.dev is not None:\n        parts.append(f'.dev{parsed.dev}')\n    if parsed.local is not None:\n        parts.append(f'+{parsed.local}')\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def _patch_sys_path(args: Sequence[str]) -> list[str]:\n    warnings.warn('_patch_sys_path has been deprecated because it relies on auto-magic package path discovery which is implemented by get_python_path that is deprecated. Use _augment_sys_path and pass additional sys.path entries as an argument obtained from discover_package_path.', DeprecationWarning, stacklevel=2)\n    return _augment_sys_path([discover_package_path(arg, []) for arg in args])",
    "label": true
  },
  {
    "code": "def _instancecheck(cls, other) -> bool:\n    wrapped = cls.__wrapped__\n    other_cls = other.__class__\n    is_instance_of = wrapped is other_cls or issubclass(other_cls, wrapped)\n    warnings.warn('%r is deprecated and slated for removal in astroid 2.0, use %r instead' % (cls.__class__.__name__, wrapped.__name__), PendingDeprecationWarning, stacklevel=2)\n    return is_instance_of",
    "label": true
  },
  {
    "code": "def _should_enable_warnings(cmd_line_warn_options: typing.Iterable[str], warn_env_var: typing.Optional[str]) -> bool:\n    enable = bool(warn_env_var)\n    for warn_opt in cmd_line_warn_options:\n        w_action, w_message, w_category, w_module, w_line = (warn_opt + '::::').split(':')[:5]\n        if not w_action.lower().startswith('i') and (not (w_message or w_category or w_module) or w_module == 'pyparsing'):\n            enable = True\n        elif w_action.lower().startswith('i') and w_module in ('pyparsing', ''):\n            enable = False\n    return enable",
    "label": true
  },
  {
    "code": "def resolve_egg_link(path):\n    referenced_paths = non_empty_lines(path)\n    resolved_paths = (os.path.join(os.path.dirname(path), ref) for ref in referenced_paths)\n    dist_groups = map(find_distributions, resolved_paths)\n    return next(dist_groups, ())",
    "label": true
  },
  {
    "code": "def symbols_for_node(node: nodes.Node, parent_symbols: t.Optional['Symbols']=None) -> 'Symbols':\n    sym = Symbols(parent=parent_symbols)\n    sym.analyze_node(node)\n    return sym",
    "label": true
  },
  {
    "code": "def cpython_tags(python_version: Optional[PythonVersion]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    interpreter = f'cp{_version_nodot(python_version[:2])}'\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    for explicit_abi in ('abi3', 'none'):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    if _abi3_applies(python_version):\n        yield from (Tag(interpreter, 'abi3', platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, 'none', platform_) for platform_ in platforms)\n    if _abi3_applies(python_version):\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = 'cp{version}'.format(version=_version_nodot((python_version[0], minor_version)))\n                yield Tag(interpreter, 'abi3', platform_)",
    "label": true
  },
  {
    "code": "def get_subparser_help(linter: PyLinter, command: str) -> str:\n    assert linter._arg_parser._subparsers\n    subparser_action = linter._arg_parser._subparsers._group_actions[0]\n    assert isinstance(subparser_action, argparse._SubParsersAction)\n    for name, subparser in subparser_action.choices.items():\n        assert isinstance(subparser, argparse.ArgumentParser)\n        if name == command:\n            return subparser.format_help()[:-1]\n    return ''",
    "label": true
  },
  {
    "code": "def _set_invariants(klass: type) -> None:\n    rep_invariants: List[Tuple[str, CodeType]] = []\n    for cls in reversed(klass.__mro__):\n        if '__representation_invariants__' in cls.__dict__:\n            rep_invariants.extend(cls.__representation_invariants__)\n        elif cls.__module__ != 'builtins':\n            assertions = parse_assertions(cls, parse_token='Representation Invariant')\n            for assertion in assertions:\n                try:\n                    compiled = compile(assertion, '<string>', 'eval')\n                except:\n                    _debug(f'Warning: representation invariant {assertion} could not be parsed as a valid Python expression')\n                    continue\n                rep_invariants.append((assertion, compiled))\n    setattr(klass, '__representation_invariants__', rep_invariants)",
    "label": true
  },
  {
    "code": "def _get(d: Dict[str, Any], expected_type: Type[T], key: str, default: Optional[T]=None) -> Optional[T]:\n    if key not in d:\n        return default\n    value = d[key]\n    if not isinstance(value, expected_type):\n        raise DirectUrlValidationError('{!r} has unexpected type for {} (expected {})'.format(value, key, expected_type))\n    return value",
    "label": true
  },
  {
    "code": "def _check_cryptography(cryptography_version):\n    try:\n        cryptography_version = list(map(int, cryptography_version.split('.')))\n    except ValueError:\n        return\n    if cryptography_version < [1, 3, 4]:\n        warning = 'Old version of cryptography ({}) may cause slowdown.'.format(cryptography_version)\n        warnings.warn(warning, RequestsDependencyWarning)",
    "label": true
  },
  {
    "code": "def infer_callable(node, context: InferenceContext | None=None):\n    if len(node.args) != 1:\n        raise UseInferenceDefault\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if isinstance(inferred, util.UninferableBase):\n        return util.Uninferable\n    return nodes.Const(inferred.callable())",
    "label": true
  },
  {
    "code": "def evaluate_marker(text, extra=None):\n    try:\n        marker = packaging.markers.Marker(text)\n        return marker.evaluate()\n    except packaging.markers.InvalidMarker as e:\n        raise SyntaxError(e) from e",
    "label": true
  },
  {
    "code": "def _match_prefix(x, y):\n    x = str(x)\n    y = str(y)\n    if x == y:\n        return True\n    if not x.startswith(y):\n        return False\n    n = len(y)\n    return x[n] == '.'",
    "label": true
  },
  {
    "code": "def enquote_executable(executable):\n    if ' ' in executable:\n        if executable.startswith('/usr/bin/env '):\n            env, _executable = executable.split(' ', 1)\n            if ' ' in _executable and (not _executable.startswith('\"')):\n                executable = '%s \"%s\"' % (env, _executable)\n        elif not executable.startswith('\"'):\n            executable = '\"%s\"' % executable\n    return executable",
    "label": true
  },
  {
    "code": "def f():\n\n    def g():\n        return g\n    return g",
    "label": true
  },
  {
    "code": "def get_prog() -> str:\n    try:\n        prog = os.path.basename(sys.argv[0])\n        if prog in ('__main__.py', '-c'):\n            return f'{sys.executable} -m pip'\n        else:\n            return prog\n    except (AttributeError, TypeError, IndexError):\n        pass\n    return 'pip'",
    "label": true
  },
  {
    "code": "def create_cookie(name, value, **kwargs):\n    result = {'version': 0, 'name': name, 'value': value, 'port': None, 'domain': '', 'path': '/', 'secure': False, 'expires': None, 'discard': True, 'comment': None, 'comment_url': None, 'rest': {'HttpOnly': None}, 'rfc2109': False}\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        raise TypeError(f'create_cookie() got unexpected keyword arguments: {list(badargs)}')\n    result.update(kwargs)\n    result['port_specified'] = bool(result['port'])\n    result['domain_specified'] = bool(result['domain'])\n    result['domain_initial_dot'] = result['domain'].startswith('.')\n    result['path_specified'] = bool(result['path'])\n    return cookielib.Cookie(**result)",
    "label": true
  },
  {
    "code": "def install_req_from_line(name: str, comes_from: Optional[Union[str, InstallRequirement]]=None, *, use_pep517: Optional[bool]=None, isolated: bool=False, global_options: Optional[List[str]]=None, hash_options: Optional[Dict[str, List[str]]]=None, constraint: bool=False, line_source: Optional[str]=None, user_supplied: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    parts = parse_req_from_line(name, line_source)\n    return InstallRequirement(parts.requirement, comes_from, link=parts.link, markers=parts.markers, use_pep517=use_pep517, isolated=isolated, global_options=global_options, hash_options=hash_options, config_settings=config_settings, constraint=constraint, extras=parts.extras, user_supplied=user_supplied)",
    "label": true
  },
  {
    "code": "def parse_uri(uri):\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])",
    "label": true
  },
  {
    "code": "def to_native_string(string, encoding='ascii'):\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n    return out",
    "label": true
  },
  {
    "code": "def _create_whitelist(would_be_installed: Set[NormalizedName], package_set: PackageSet) -> Set[NormalizedName]:\n    packages_affected = set(would_be_installed)\n    for package_name in package_set:\n        if package_name in packages_affected:\n            continue\n        for req in package_set[package_name].dependencies:\n            if canonicalize_name(req.name) in packages_affected:\n                packages_affected.add(package_name)\n                break\n    return packages_affected",
    "label": true
  },
  {
    "code": "def _cfstr(py_bstr):\n    c_str = ctypes.c_char_p(py_bstr)\n    cf_str = CoreFoundation.CFStringCreateWithCString(CoreFoundation.kCFAllocatorDefault, c_str, CFConst.kCFStringEncodingUTF8)\n    return cf_str",
    "label": true
  },
  {
    "code": "def with_iter(context_manager):\n    with context_manager as iterable:\n        yield from iterable",
    "label": true
  },
  {
    "code": "def install_given_reqs(requirements: List[InstallRequirement], global_options: Sequence[str], root: Optional[str], home: Optional[str], prefix: Optional[str], warn_script_location: bool, use_user_site: bool, pycompile: bool) -> List[InstallationResult]:\n    to_install = collections.OrderedDict(_validate_requirements(requirements))\n    if to_install:\n        logger.info('Installing collected packages: %s', ', '.join(to_install.keys()))\n    installed = []\n    with indent_log():\n        for req_name, requirement in to_install.items():\n            if requirement.should_reinstall:\n                logger.info('Attempting uninstall: %s', req_name)\n                with indent_log():\n                    uninstalled_pathset = requirement.uninstall(auto_confirm=True)\n            else:\n                uninstalled_pathset = None\n            try:\n                requirement.install(global_options, root=root, home=home, prefix=prefix, warn_script_location=warn_script_location, use_user_site=use_user_site, pycompile=pycompile)\n            except Exception:\n                if uninstalled_pathset and (not requirement.install_succeeded):\n                    uninstalled_pathset.rollback()\n                raise\n            else:\n                if uninstalled_pathset and requirement.install_succeeded:\n                    uninstalled_pathset.commit()\n            installed.append(InstallationResult(req_name))\n    return installed",
    "label": true
  },
  {
    "code": "def assert_string_list(dist, attr, value):\n    try:\n        assert isinstance(value, (list, tuple))\n        assert ''.join(value) != value\n    except (TypeError, ValueError, AttributeError, AssertionError) as e:\n        raise DistutilsSetupError('%r must be a list of strings (got %r)' % (attr, value)) from e",
    "label": true
  },
  {
    "code": "def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:\n    tokenizer.consume('WS')\n    if tokenizer.check('LEFT_PARENTHESIS', peek=True):\n        with tokenizer.enclosing_tokens('LEFT_PARENTHESIS', 'RIGHT_PARENTHESIS', around='marker expression'):\n            tokenizer.consume('WS')\n            marker: MarkerAtom = _parse_marker(tokenizer)\n            tokenizer.consume('WS')\n    else:\n        marker = _parse_marker_item(tokenizer)\n    tokenizer.consume('WS')\n    return marker",
    "label": true
  },
  {
    "code": "def declare_namespace(packageName):\n    msg = f'Deprecated call to `pkg_resources.declare_namespace({packageName!r})`.\\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages'\n    warnings.warn(msg, DeprecationWarning, stacklevel=2)\n    _imp.acquire_lock()\n    try:\n        if packageName in _namespace_packages:\n            return\n        path = sys.path\n        parent, _, _ = packageName.rpartition('.')\n        if parent:\n            declare_namespace(parent)\n            if parent not in _namespace_packages:\n                __import__(parent)\n            try:\n                path = sys.modules[parent].__path__\n            except AttributeError as e:\n                raise TypeError('Not a package:', parent) from e\n        _namespace_packages.setdefault(parent or None, []).append(packageName)\n        _namespace_packages.setdefault(packageName, [])\n        for path_item in path:\n            _handle_ns(packageName, path_item)\n    finally:\n        _imp.release_lock()",
    "label": true
  },
  {
    "code": "def get_requires_for_build_wheel(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_wheel\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)",
    "label": true
  },
  {
    "code": "def create_single_customer_with_all_lines() -> Customer:\n    contracts = [TermContract(start=datetime.date(year=2017, month=12, day=25), end=datetime.date(year=2019, month=6, day=25)), MTMContract(start=datetime.date(year=2017, month=12, day=25)), PrepaidContract(start=datetime.date(year=2017, month=12, day=25), balance=100)]\n    numbers = ['867-5309', '273-8255', '649-2568']\n    customer = Customer(cid=7777)\n    for i in range(len(contracts)):\n        customer.add_phone_line(PhoneLine(numbers[i], contracts[i]))\n    customer.new_month(12, 2017)\n    return customer",
    "label": true
  },
  {
    "code": "def _check_csv(value: list[str] | tuple[str] | str) -> Sequence[str]:\n    if isinstance(value, (list, tuple)):\n        return value\n    return _splitstrip(value)",
    "label": true
  },
  {
    "code": "def get_list_opt(options, optname, default=None):\n    val = options.get(optname, default)\n    if isinstance(val, str):\n        return val.split()\n    elif isinstance(val, (list, tuple)):\n        return list(val)\n    else:\n        raise OptionError('Invalid type %r for option %s; you must give a list value' % (val, optname))",
    "label": true
  },
  {
    "code": "def copy_cache(cache: t.Optional[t.MutableMapping]) -> t.Optional[t.MutableMapping[t.Tuple[weakref.ref, str], 'Template']]:\n    if cache is None:\n        return None\n    if type(cache) is dict:\n        return {}\n    return LRUCache(cache.capacity)",
    "label": true
  },
  {
    "code": "def _choose_width_fn(has_invisible, enable_widechars, is_multiline):\n    if has_invisible:\n        line_width_fn = _visible_width\n    elif enable_widechars:\n        line_width_fn = wcwidth.wcswidth\n    else:\n        line_width_fn = len\n    if is_multiline:\n        width_fn = lambda s: _multiline_width(s, line_width_fn)\n    else:\n        width_fn = line_width_fn\n    return width_fn",
    "label": true
  },
  {
    "code": "def check_uniontype(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    errors: dict[str, TypeCheckError] = {}\n    for type_ in args:\n        try:\n            check_type_internal(value, type_, memo)\n            return\n        except TypeCheckError as exc:\n            errors[get_type_name(type_)] = exc\n    formatted_errors = indent('\\n'.join((f'{key}: {error}' for key, error in errors.items())), '  ')\n    raise TypeCheckError(f'did not match any element in the union:\\n{formatted_errors}')",
    "label": true
  },
  {
    "code": "def _create_cfstring_array(lst: list[bytes]) -> CFMutableArray:\n    cf_arr = None\n    try:\n        cf_arr = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        if not cf_arr:\n            raise MemoryError('Unable to allocate memory!')\n        for item in lst:\n            cf_str = _cfstr(item)\n            if not cf_str:\n                raise MemoryError('Unable to allocate memory!')\n            try:\n                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)\n            finally:\n                CoreFoundation.CFRelease(cf_str)\n    except BaseException as e:\n        if cf_arr:\n            CoreFoundation.CFRelease(cf_arr)\n        raise ssl.SSLError(f'Unable to allocate array: {e}') from None\n    return cf_arr",
    "label": true
  },
  {
    "code": "def _parse_specifier(tokenizer: Tokenizer) -> str:\n    with tokenizer.enclosing_tokens('LEFT_PARENTHESIS', 'RIGHT_PARENTHESIS', around='version specifier'):\n        tokenizer.consume('WS')\n        parsed_specifiers = _parse_version_many(tokenizer)\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def _import_module(name):\n    __import__(name)\n    return sys.modules[name]",
    "label": true
  },
  {
    "code": "def runner_with_spinner_message(message: str) -> Callable[..., None]:\n\n    def runner(cmd: List[str], cwd: Optional[str]=None, extra_environ: Optional[Mapping[str, Any]]=None) -> None:\n        with open_spinner(message) as spinner:\n            call_subprocess(cmd, command_desc=message, cwd=cwd, extra_environ=extra_environ, spinner=spinner)\n    return runner",
    "label": true
  },
  {
    "code": "def read_data(filename: str) -> list[list]:\n    with open(filename) as csv_file:\n        lines = list(csv.reader(csv_file))\n        start_year = int(lines[1][COLUMN_BCI + 1])\n        data = lines[2:]\n        clean_data(data, start_year)\n    return data",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    mp = MonkeyPatch()\n    config.add_cleanup(mp.undo)\n    _tmp_path_factory = TempPathFactory.from_config(config, _ispytest=True)\n    mp.setattr(config, '_tmp_path_factory', _tmp_path_factory, raising=False)",
    "label": true
  },
  {
    "code": "def range_scan(decoded_sequence: str) -> List[str]:\n    ranges: Set[str] = set()\n    for character in decoded_sequence:\n        character_range: Optional[str] = unicode_range(character)\n        if character_range is None:\n            continue\n        ranges.add(character_range)\n    return list(ranges)",
    "label": true
  },
  {
    "code": "def assert_fingerprint(cert, fingerprint):\n    fingerprint = fingerprint.replace(':', '').lower()\n    digest_length = len(fingerprint)\n    hashfunc = HASHFUNC_MAP.get(digest_length)\n    if not hashfunc:\n        raise SSLError('Fingerprint of invalid length: {0}'.format(fingerprint))\n    fingerprint_bytes = unhexlify(fingerprint.encode())\n    cert_digest = hashfunc(cert).digest()\n    if not _const_compare_digest(cert_digest, fingerprint_bytes):\n        raise SSLError('Fingerprints did not match. Expected \"{0}\", got \"{1}\".'.format(fingerprint, hexlify(cert_digest)))",
    "label": true
  },
  {
    "code": "def _is_unnecessary_indexing(node: Union[nodes.For, nodes.Comprehension]) -> bool:\n    index_nodes = []\n    for assign_name_node in node.target.nodes_of_class((nodes.AssignName, nodes.Name)):\n        index_nodes.extend(_index_name_nodes(assign_name_node.name, node))\n    return all((_is_redundant(index_node, node) for index_node in index_nodes)) and index_nodes",
    "label": true
  },
  {
    "code": "def _disassemble_key(name: str) -> List[str]:\n    if '.' not in name:\n        error_message = \"Key does not contain dot separated section and key. Perhaps you wanted to use 'global.{}' instead?\".format(name)\n        raise ConfigurationError(error_message)\n    return name.split('.', 1)",
    "label": true
  },
  {
    "code": "def clean_data(data: list[list], start_year: int) -> None:\n    next_id = 1\n    for row in data:\n        row[COLUMN_ID] = next_id\n        next_id = next_id + 1\n        row[COLUMN_LAT] = float(row[COLUMN_LAT])\n        row[COLUMN_LON] = float(row[COLUMN_LON])\n        row[COLUMN_NUM_SPANS] = int(row[COLUMN_NUM_SPANS])\n        row[COLUMN_DECK_LENGTH] = clean_length_data(row[COLUMN_DECK_LENGTH])\n        row[COLUMN_SPAN_DETAILS] = clean_span_data(row[COLUMN_SPAN_DETAILS])\n        bci_years = []\n        bci_scores = row[COLUMN_BCI + 1:]\n        clean_bci_data(bci_years, start_year, bci_scores)\n        row[COLUMN_BCI] = [bci_years, bci_scores]\n        trim_from_end(row, len(row) - COLUMN_BCI - 1)",
    "label": true
  },
  {
    "code": "def _load_formatters(module_name):\n    mod = __import__(module_name, None, None, ['__all__'])\n    for formatter_name in mod.__all__:\n        cls = getattr(mod, formatter_name)\n        _formatter_cache[cls.name] = cls",
    "label": true
  },
  {
    "code": "def parse_content_disposition(content_disposition: str, default_filename: str) -> str:\n    m = email.message.Message()\n    m['content-type'] = content_disposition\n    filename = m.get_param('filename')\n    if filename:\n        filename = sanitize_content_filename(str(filename))\n    return filename or default_filename",
    "label": true
  },
  {
    "code": "def time_queue() -> tuple[list[int], list[float], list[float]]:\n    queue_sizes = [1000, 2000]\n    trials = 200\n\n    def short(n):\n        a = 2\n        for i in range(n):\n            a += 1\n    enqueue_times = []\n    for queue_size in queue_sizes:\n        queue = _setup_queue(queue_size)\n        time = 0\n        for _ in range(trials):\n            time += timeit(f'short({queue_size})', number=1, globals=locals())\n        print(f'short: Queue size {queue_size:>7}, time {time}')\n        enqueue_times.append(time)\n\n    def long(n):\n        n = n ** 165\n        a = 2\n        for i in range(n):\n            a += 1\n    dequeue_times = []\n    for queue_size in queue_sizes:\n        time = 0\n        for _ in range(trials):\n            time += timeit(f'long({queue_size})', number=1, globals=locals())\n        print(f'long: Queue size {queue_size:>7}, time {time}')\n        dequeue_times.append(time)\n    return (queue_sizes, [], dequeue_times)",
    "label": true
  },
  {
    "code": "def Getattr(object, name, default=__NO_DEFAULT):\n    if default is Getattr.NO_DEFAULT:\n        reduction = (getattr, (object, name))\n    else:\n        reduction = (getattr, (object, name, default))\n    return Reduce(*reduction, is_callable=callable(default))",
    "label": true
  },
  {
    "code": "def valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:\n    if len(label) > (254 if trailing_dot else 253):\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def looks_like_typing_typevar_or_newtype(node) -> bool:\n    func = node.func\n    if isinstance(func, Attribute):\n        return func.attrname in TYPING_TYPEVARS\n    if isinstance(func, Name):\n        return func.name in TYPING_TYPEVARS\n    return False",
    "label": true
  },
  {
    "code": "def test_iter_decode():\n\n    def iter_decode_to_string(input, fallback_encoding):\n        output, _encoding = iter_decode(input, fallback_encoding)\n        return ''.join(output)\n    assert iter_decode_to_string([], 'latin1') == ''\n    assert iter_decode_to_string([b''], 'latin1') == ''\n    assert iter_decode_to_string([b'\\xe9'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'hello'], 'latin1') == 'hello'\n    assert iter_decode_to_string([b'he', b'llo'], 'latin1') == 'hello'\n    assert iter_decode_to_string([b'hell', b'o'], 'latin1') == 'hello'\n    assert iter_decode_to_string([b'\\xc3\\xa9'], 'latin1') == '\u00c3\u00a9'\n    assert iter_decode_to_string([b'\\xef\\xbb\\xbf\\xc3\\xa9'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'\\xef\\xbb\\xbf', b'\\xc3', b'\\xa9'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'\\xef\\xbb\\xbf', b'a', b'\\xc3'], 'latin1') == 'a\ufffd'\n    assert iter_decode_to_string([b'', b'\\xef', b'', b'', b'\\xbb\\xbf\\xc3', b'\\xa9'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'\\xef\\xbb\\xbf'], 'latin1') == ''\n    assert iter_decode_to_string([b'\\xef\\xbb'], 'latin1') == '\u00ef\u00bb'\n    assert iter_decode_to_string([b'\\xfe\\xff\\x00\\xe9'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'\\xff\\xfe\\xe9\\x00'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'', b'\\xff', b'', b'', b'\\xfe\\xe9', b'\\x00'], 'latin1') == '\u00e9'\n    assert iter_decode_to_string([b'', b'h\\xe9', b'llo'], 'x-user-defined') == 'h\\uf7e9llo'",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e125_and_e129(msg, _node, source_lines=None):\n    line = msg.line\n    curr_idx = len(source_lines[line - 1]) - len(source_lines[line - 1].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(curr_idx, len(source_lines[line - 1])), LineType.ERROR, source_lines[line - 1] + ' ' * 2 + '# INDENT THIS LINE')\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _filter_existing_files(filepaths: Iterable[_Path]) -> Iterator[_Path]:\n    for path in filepaths:\n        if os.path.isfile(path):\n            yield path\n        else:\n            SetuptoolsWarning.emit(f'File {path!r} cannot be found')",
    "label": true
  },
  {
    "code": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {'implementation_name': implementation_name, 'implementation_version': iver, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'python_full_version': platform.python_version(), 'platform_python_implementation': platform.python_implementation(), 'python_version': '.'.join(platform.python_version_tuple()[:2]), 'sys_platform': sys.platform}",
    "label": true
  },
  {
    "code": "def _pad_version(left: List[str], right: List[str]) -> Tuple[List[str], List[str]]:\n    left_split, right_split = ([], [])\n    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n    left_split.append(left[len(left_split[0]):])\n    right_split.append(right[len(right_split[0]):])\n    left_split.insert(1, ['0'] * max(0, len(right_split[0]) - len(left_split[0])))\n    right_split.insert(1, ['0'] * max(0, len(left_split[0]) - len(right_split[0])))\n    return (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))",
    "label": true
  },
  {
    "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    if session_setting is None:\n        return request_setting\n    if request_setting is None:\n        return session_setting\n    if not (isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)):\n        return request_setting\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n    none_keys = [k for k, v in merged_setting.items() if v is None]\n    for key in none_keys:\n        del merged_setting[key]\n    return merged_setting",
    "label": true
  },
  {
    "code": "def validate_https___packaging_python_org_en_latest_specifications_declaring_project_metadata___definitions_entry_point_group(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, dict):\n        raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must be object', value=data, name='' + (name_prefix or 'data') + '', definition={'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        for data_key, data_val in data.items():\n            if REGEX_PATTERNS['^.+$'].search(data_key):\n                if data_key in data_keys:\n                    data_keys.remove(data_key)\n                if not isinstance(data_val, str):\n                    raise JsonSchemaValueException('' + (name_prefix or 'data') + '.{data_key}'.format(**locals()) + ' must be string', value=data_val, name='' + (name_prefix or 'data') + '.{data_key}'.format(**locals()) + '', definition={'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}, rule='type')\n                if isinstance(data_val, str):\n                    if not custom_formats['python-entrypoint-reference'](data_val):\n                        raise JsonSchemaValueException('' + (name_prefix or 'data') + '.{data_key}'.format(**locals()) + ' must be python-entrypoint-reference', value=data_val, name='' + (name_prefix or 'data') + '.{data_key}'.format(**locals()) + '', definition={'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}, rule='format')\n        if data_keys:\n            raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must not contain ' + str(data_keys) + ' properties', value=data, name='' + (name_prefix or 'data') + '', definition={'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, rule='additionalProperties')\n        data_len = len(data)\n        if data_len != 0:\n            data_property_names = True\n            for data_key in data:\n                try:\n                    if isinstance(data_key, str):\n                        if not custom_formats['python-entrypoint-name'](data_key):\n                            raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must be python-entrypoint-name', value=data_key, name='' + (name_prefix or 'data') + '', definition={'format': 'python-entrypoint-name'}, rule='format')\n                except JsonSchemaValueException:\n                    data_property_names = False\n            if not data_property_names:\n                raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must be named by propertyName definition', value=data, name='' + (name_prefix or 'data') + '', definition={'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, rule='propertyNames')\n    return data",
    "label": true
  },
  {
    "code": "def get_auth_from_url(url):\n    parsed = urlparse(url)\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = ('', '')\n    return auth",
    "label": true
  },
  {
    "code": "def _register_require_version(node):\n    try:\n        import gi\n        gi.require_version(node.args[0].value, node.args[1].value)\n    except Exception:\n        pass\n    return node",
    "label": true
  },
  {
    "code": "def break_args_options(line: str) -> Tuple[str, str]:\n    tokens = line.split(' ')\n    args = []\n    options = tokens[:]\n    for token in tokens:\n        if token.startswith('-') or token.startswith('--'):\n            break\n        else:\n            args.append(token)\n            options.pop(0)\n    return (' '.join(args), ' '.join(options))",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e202(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def trace(arg: Union[bool, TextIO, str, os.PathLike]=None, *, mode: str='a') -> None:\n    if not isinstance(arg, bool):\n        return TraceManager(file=arg, mode=mode)\n    logger.setLevel(logging.INFO if arg else logging.WARNING)",
    "label": true
  },
  {
    "code": "def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n    if len(py_version) > 1:\n        yield f'py{_version_nodot(py_version[:2])}'\n    yield f'py{py_version[0]}'\n    if len(py_version) > 1:\n        for minor in range(py_version[1] - 1, -1, -1):\n            yield f'py{_version_nodot((py_version[0], minor))}'",
    "label": true
  },
  {
    "code": "def get_object_types_mro(obj: Union[object, Type[Any]]) -> Tuple[type, ...]:\n    if not hasattr(obj, '__mro__'):\n        obj = type(obj)\n    return getattr(obj, '__mro__', ())",
    "label": true
  },
  {
    "code": "def _set_verbose_mode(run: Run, value: str | None) -> None:\n    assert value is None\n    run.verbose = True",
    "label": true
  },
  {
    "code": "def _multiple_choice_validator(choices: list[Any], name: str, value: Any) -> Any:\n    values = utils._check_csv(value)\n    for csv_value in values:\n        if csv_value not in choices:\n            msg = 'option %s: invalid value: %r, should be in %s'\n            raise optparse.OptionValueError(msg % (name, csv_value, choices))\n    return values",
    "label": true
  },
  {
    "code": "def is_connection_dropped(conn):\n    sock = getattr(conn, 'sock', False)\n    if sock is False:\n        return False\n    if sock is None:\n        return True\n    try:\n        return wait_for_read(sock, timeout=0.0)\n    except NoWayToWaitForSocketError:\n        return False",
    "label": true
  },
  {
    "code": "def get_msvcr():\n    match = re.search('MSC v\\\\.(\\\\d{4})', sys.version)\n    try:\n        msc_ver = int(match.group(1))\n    except AttributeError:\n        return\n    try:\n        return _msvcr_lookup[msc_ver]\n    except KeyError:\n        raise ValueError('Unknown MS Compiler version %s ' % msc_ver)",
    "label": true
  },
  {
    "code": "def do_default(value: V, default_value: V='', boolean: bool=False) -> V:\n    if isinstance(value, Undefined) or (boolean and (not value)):\n        return default_value\n    return value",
    "label": true
  },
  {
    "code": "def _iter_built_with_prepended(installed: Candidate, infos: Iterator[IndexCandidateInfo]) -> Iterator[Candidate]:\n    yield installed\n    versions_found: Set[_BaseVersion] = {installed.version}\n    for version, func in infos:\n        if version in versions_found:\n            continue\n        candidate = func()\n        if candidate is None:\n            continue\n        yield candidate\n        versions_found.add(version)",
    "label": true
  },
  {
    "code": "def _slice_value(index, context: InferenceContext | None=None):\n    if isinstance(index, Const):\n        if isinstance(index.value, (int, type(None))):\n            return index.value\n    elif index is None:\n        return None\n    else:\n        try:\n            inferred = next(index.infer(context=context))\n        except (InferenceError, StopIteration):\n            pass\n        else:\n            if isinstance(inferred, Const):\n                if isinstance(inferred.value, (int, type(None))):\n                    return inferred.value\n    return _SLICE_SENTINEL",
    "label": true
  },
  {
    "code": "def add_stderr_logger(level=logging.DEBUG):\n    logger = logging.getLogger(__name__)\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n    logger.addHandler(handler)\n    logger.setLevel(level)\n    logger.debug('Added a stderr logging handler to logger: %s', __name__)\n    return handler",
    "label": true
  },
  {
    "code": "def _is_linux_i686(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.I386)",
    "label": true
  },
  {
    "code": "def check_label(label: Union[str, bytes, bytearray]) -> None:\n    if isinstance(label, (bytes, bytearray)):\n        label = label.decode('utf-8')\n    if len(label) == 0:\n        raise IDNAError('Empty Label')\n    check_nfc(label)\n    check_hyphen_ok(label)\n    check_initial_combiner(label)\n    for pos, cp in enumerate(label):\n        cp_value = ord(cp)\n        if intranges_contain(cp_value, idnadata.codepoint_classes['PVALID']):\n            continue\n        elif intranges_contain(cp_value, idnadata.codepoint_classes['CONTEXTJ']):\n            try:\n                if not valid_contextj(label, pos):\n                    raise InvalidCodepointContext('Joiner {} not allowed at position {} in {}'.format(_unot(cp_value), pos + 1, repr(label)))\n            except ValueError:\n                raise IDNAError('Unknown codepoint adjacent to joiner {} at position {} in {}'.format(_unot(cp_value), pos + 1, repr(label)))\n        elif intranges_contain(cp_value, idnadata.codepoint_classes['CONTEXTO']):\n            if not valid_contexto(label, pos):\n                raise InvalidCodepointContext('Codepoint {} not allowed at position {} in {}'.format(_unot(cp_value), pos + 1, repr(label)))\n        else:\n            raise InvalidCodepoint('Codepoint {} at position {} of {} not allowed'.format(_unot(cp_value), pos + 1, repr(label)))\n    check_bidi(label)",
    "label": true
  },
  {
    "code": "def test_session_other():\n    import test_classdef as module\n    atexit.register(_clean_up_cache, module)\n    module.selfref = module\n    dict_objects = [obj for obj in module.__dict__.keys() if not obj.startswith('__')]\n    session_buffer = BytesIO()\n    dill.dump_module(session_buffer, module)\n    for obj in dict_objects:\n        del module.__dict__[obj]\n    session_buffer.seek(0)\n    dill.load_module(session_buffer, module)\n    assert all((obj in module.__dict__ for obj in dict_objects))\n    assert module.selfref is module",
    "label": true
  },
  {
    "code": "def findall(dir=os.curdir):\n    files = _find_all_simple(dir)\n    if dir == os.curdir:\n        make_rel = functools.partial(os.path.relpath, start=dir)\n        files = map(make_rel, files)\n    return list(files)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e101(msg, _node, source_lines=None):\n    line = msg.line\n    curr_idx = len(source_lines[line - 1]) - len(source_lines[line - 1].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(0, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def test_dataclasses():\n\n    @dataclasses.dataclass\n    class A:\n        x: int\n        y: str\n\n    @dataclasses.dataclass\n    class B:\n        a: A\n    a = A(1, 'test')\n    before = B(a)\n    save = dill.dumps(before)\n    after = dill.loads(save)\n    assert before != after\n    assert before == B(A(**dataclasses.asdict(after.a)))\n    assert dataclasses.asdict(before) == dataclasses.asdict(after)",
    "label": true
  },
  {
    "code": "def _rebuild_mod_path(orig_path, package_name, module):\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path",
    "label": true
  },
  {
    "code": "def brainless_manager():\n    m = manager.AstroidManager()\n    m.__dict__ = {}\n    m._failed_import_hooks = []\n    m.astroid_cache = {}\n    m._mod_file_cache = {}\n    m._transform = transforms.TransformVisitor()\n    m.extension_package_whitelist = set()\n    return m",
    "label": true
  },
  {
    "code": "def get_global_option(checker: BaseChecker, option: GLOBAL_OPTION_NAMES, default: T_GlobalOptionReturnTypes | None=None) -> T_GlobalOptionReturnTypes | None | Any:\n    warnings.warn('get_global_option has been deprecated. You can use checker.linter.config to get all global options instead.', DeprecationWarning, stacklevel=2)\n    return getattr(checker.linter.config, option.replace('-', '_'))",
    "label": true
  },
  {
    "code": "def with_attribute(*args, **attr_dict):\n    if args:\n        attrs = args[:]\n    else:\n        attrs = attr_dict.items()\n    attrs = [(k, v) for k, v in attrs]\n\n    def pa(s, l, tokens):\n        for attrName, attrValue in attrs:\n            if attrName not in tokens:\n                raise ParseException(s, l, 'no matching attribute ' + attrName)\n            if attrValue != with_attribute.ANY_VALUE and tokens[attrName] != attrValue:\n                raise ParseException(s, l, f'attribute {attrName!r} has value {tokens[attrName]!r}, must be {attrValue!r}')\n    return pa",
    "label": true
  },
  {
    "code": "def test_partial():\n    assert copy(Machine(), byref=True)\n    assert copy(Machine(), byref=True, recurse=True)\n    assert copy(Machine(), recurse=True)\n    assert copy(Machine())",
    "label": true
  },
  {
    "code": "def _parse_string(data: str, type_comments: bool=True) -> tuple[ast.Module, ParserModule]:\n    parser_module = get_parser_module(type_comments=type_comments)\n    try:\n        parsed = parser_module.parse(data + '\\n', type_comments=type_comments)\n    except SyntaxError as exc:\n        if exc.args[0] != MISPLACED_TYPE_ANNOTATION_ERROR or not type_comments:\n            raise\n        parser_module = get_parser_module(type_comments=False)\n        parsed = parser_module.parse(data + '\\n', type_comments=False)\n    return (parsed, parser_module)",
    "label": true
  },
  {
    "code": "def _semantic_key(s):\n\n    def make_tuple(s, absent):\n        if s is None:\n            result = (absent,)\n        else:\n            parts = s[1:].split('.')\n            result = tuple([p.zfill(8) if p.isdigit() else p for p in parts])\n        return result\n    m = is_semver(s)\n    if not m:\n        raise UnsupportedVersionError(s)\n    groups = m.groups()\n    major, minor, patch = [int(i) for i in groups[:3]]\n    pre, build = (make_tuple(groups[3], '|'), make_tuple(groups[5], '*'))\n    return ((major, minor, patch), pre, build)",
    "label": true
  },
  {
    "code": "def _called_in_methods(func: LocalsDictNodeNG, klass: nodes.ClassDef, methods: Sequence[str]) -> bool:\n    if not isinstance(func, nodes.FunctionDef):\n        return False\n    for method in methods:\n        try:\n            inferred = klass.getattr(method)\n        except astroid.NotFoundError:\n            continue\n        for infer_method in inferred:\n            for call in infer_method.nodes_of_class(nodes.Call):\n                try:\n                    bound = next(call.func.infer())\n                except (astroid.InferenceError, StopIteration):\n                    continue\n                if not isinstance(bound, astroid.BoundMethod):\n                    continue\n                func_obj = bound._proxied\n                if isinstance(func_obj, astroid.UnboundMethod):\n                    func_obj = func_obj._proxied\n                if func_obj.name == func.name:\n                    return True\n    return False",
    "label": true
  },
  {
    "code": "def _get(d: Dict[str, Any], expected_type: Type[T], key: str, default: Optional[T]=None) -> Optional[T]:\n    if key not in d:\n        return default\n    value = d[key]\n    if not isinstance(value, expected_type):\n        raise DirectUrlValidationError('{!r} has unexpected type for {} (expected {})'.format(value, key, expected_type))\n    return value",
    "label": true
  },
  {
    "code": "def doctype_matches(text, regex):\n    m = doctype_lookup_re.search(text)\n    if m is None:\n        return False\n    doctype = m.group(1)\n    return re.compile(regex, re.I).match(doctype.strip()) is not None",
    "label": true
  },
  {
    "code": "def find_pylintrc() -> str | None:\n    warnings.warn(\"find_pylintrc and the PYLINTRC constant have been deprecated. Use find_default_config_files if you want access to pylint's configuration file finding logic.\", DeprecationWarning, stacklevel=2)\n    for config_file in find_default_config_files():\n        if str(config_file).endswith('pylintrc'):\n            return str(config_file)\n    return None",
    "label": true
  },
  {
    "code": "def parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n    if filename.endswith('.tar.gz'):\n        file_stem = filename[:-len('.tar.gz')]\n    elif filename.endswith('.zip'):\n        file_stem = filename[:-len('.zip')]\n    else:\n        raise InvalidSdistFilename(f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'): {filename}\")\n    name_part, sep, version_part = file_stem.rpartition('-')\n    if not sep:\n        raise InvalidSdistFilename(f'Invalid sdist filename: {filename}')\n    name = canonicalize_name(name_part)\n    version = Version(version_part)\n    return (name, version)",
    "label": true
  },
  {
    "code": "def _check_all_skipped(test: 'doctest.DocTest') -> None:\n    import doctest\n    all_skipped = all((x.options.get(doctest.SKIP, False) for x in test.examples))\n    if all_skipped:\n        skip('all tests skipped by +SKIP option')",
    "label": true
  },
  {
    "code": "def _parse_requirement_details(tokenizer: Tokenizer) -> Tuple[str, str, Optional[MarkerList]]:\n    specifier = ''\n    url = ''\n    marker = None\n    if tokenizer.check('AT'):\n        tokenizer.read()\n        tokenizer.consume('WS')\n        url_start = tokenizer.position\n        url = tokenizer.expect('URL', expected='URL after @').text\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        tokenizer.expect('WS', expected='whitespace after URL')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=url_start, after='URL and whitespace')\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume('WS')\n        if tokenizer.check('END', peek=True):\n            return (url, specifier, marker)\n        marker = _parse_requirement_marker(tokenizer, span_start=specifier_start, after='version specifier' if specifier else 'name and no valid version specifier')\n    return (url, specifier, marker)",
    "label": true
  },
  {
    "code": "def guess_json_utf(data):\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return 'utf-32'\n    if sample[:3] == codecs.BOM_UTF8:\n        return 'utf-8-sig'\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return 'utf-16'\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return 'utf-8'\n    if nullcount == 2:\n        if sample[::2] == _null2:\n            return 'utf-16-be'\n        if sample[1::2] == _null2:\n            return 'utf-16-le'\n    if nullcount == 3:\n        if sample[:3] == _null3:\n            return 'utf-32-be'\n        if sample[1:] == _null3:\n            return 'utf-32-le'\n    return None",
    "label": true
  },
  {
    "code": "def make_target_python(options: Values) -> TargetPython:\n    target_python = TargetPython(platforms=options.platforms, py_version_info=options.python_version, abis=options.abis, implementation=options.implementation)\n    return target_python",
    "label": true
  },
  {
    "code": "def _dict_method_all(dict_method: F) -> F:\n\n    @functools.wraps(dict_method)\n    def f_all(self: 'Context') -> t.Any:\n        return dict_method(self.get_all())\n    return t.cast(F, f_all)",
    "label": true
  },
  {
    "code": "def get_platlib() -> str:\n    new = _sysconfig.get_platlib()\n    if _USE_SYSCONFIG:\n        return new\n    from . import _distutils\n    old = _distutils.get_platlib()\n    if _looks_like_deb_system_dist_packages(old):\n        return old\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key='platlib'):\n        _log_context()\n    return old",
    "label": true
  },
  {
    "code": "def read_configuration(filepath: _Path, expand=True, ignore_option_errors=False, dist: Optional['Distribution']=None):\n    filepath = os.path.abspath(filepath)\n    if not os.path.isfile(filepath):\n        raise FileError(f'Configuration file {filepath!r} does not exist.')\n    asdict = load_file(filepath) or {}\n    project_table = asdict.get('project', {})\n    tool_table = asdict.get('tool', {})\n    setuptools_table = tool_table.get('setuptools', {})\n    if not asdict or not (project_table or setuptools_table):\n        return {}\n    if 'distutils' in tool_table:\n        _ExperimentalConfiguration.emit(subject='[tool.distutils]')\n    if dist and getattr(dist, 'include_package_data', None) is not None:\n        setuptools_table.setdefault('include-package-data', dist.include_package_data)\n    else:\n        setuptools_table.setdefault('include-package-data', True)\n    asdict['tool'] = tool_table\n    tool_table['setuptools'] = setuptools_table\n    with _ignore_errors(ignore_option_errors):\n        subset = {'project': project_table, 'tool': {'setuptools': setuptools_table}}\n        validate(subset, filepath)\n    if expand:\n        root_dir = os.path.dirname(filepath)\n        return expand_configuration(asdict, root_dir, ignore_option_errors, dist)\n    return asdict",
    "label": true
  },
  {
    "code": "def _parse_letter_version(letter: str, number: Union[str, bytes, SupportsInt]) -> Optional[Tuple[str, int]]:\n    if letter:\n        if number is None:\n            number = 0\n        letter = letter.lower()\n        if letter == 'alpha':\n            letter = 'a'\n        elif letter == 'beta':\n            letter = 'b'\n        elif letter in ['c', 'pre', 'preview']:\n            letter = 'rc'\n        elif letter in ['rev', 'r']:\n            letter = 'post'\n        return (letter, int(number))\n    if not letter and number:\n        letter = 'post'\n        return (letter, int(number))\n    return None",
    "label": true
  },
  {
    "code": "def _safe_infer_call_result(node: nodes.FunctionDef, caller: nodes.FunctionDef, context: InferenceContext | None=None) -> InferenceResult | None:\n    try:\n        inferit = node.infer_call_result(caller, context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    except StopIteration:\n        return None\n    try:\n        next(inferit)\n        return None\n    except astroid.InferenceError:\n        return None\n    except StopIteration:\n        return value",
    "label": true
  },
  {
    "code": "def get_win_folder_if_csidl_name_not_env_var(csidl_name: str) -> str | None:\n    if csidl_name == 'CSIDL_PERSONAL':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Documents')\n    if csidl_name == 'CSIDL_DOWNLOADS':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Downloads')\n    if csidl_name == 'CSIDL_MYPICTURES':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Pictures')\n    if csidl_name == 'CSIDL_MYVIDEO':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Videos')\n    if csidl_name == 'CSIDL_MYMUSIC':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Music')\n    return None",
    "label": true
  },
  {
    "code": "def _remove_path_dot_segments(path):\n    segments = path.split('/')\n    output = []\n    for segment in segments:\n        if segment == '.':\n            continue\n        elif segment != '..':\n            output.append(segment)\n        elif output:\n            output.pop()\n    if path.startswith('/') and (not output or output[0]):\n        output.insert(0, '')\n    if path.endswith(('/.', '/..')):\n        output.append('')\n    return '/'.join(output)",
    "label": true
  },
  {
    "code": "def dispatch_hook(key, hooks, hook_data, **kwargs):\n    hooks = hooks or {}\n    hooks = hooks.get(key)\n    if hooks:\n        if hasattr(hooks, '__call__'):\n            hooks = [hooks]\n        for hook in hooks:\n            _hook_data = hook(hook_data, **kwargs)\n            if _hook_data is not None:\n                hook_data = _hook_data\n    return hook_data",
    "label": true
  },
  {
    "code": "def _align_cell_veritically(text_lines, num_lines, column_width, row_alignment):\n    delta_lines = num_lines - len(text_lines)\n    blank = [' ' * column_width]\n    if row_alignment == 'bottom':\n        return blank * delta_lines + text_lines\n    elif row_alignment == 'center':\n        top_delta = delta_lines // 2\n        bottom_delta = delta_lines - top_delta\n        return top_delta * blank + text_lines + bottom_delta * blank\n    else:\n        return text_lines + blank * delta_lines",
    "label": true
  },
  {
    "code": "def get_formatter_by_name(_alias, **options):\n    cls = find_formatter_class(_alias)\n    if cls is None:\n        raise ClassNotFound('no formatter found for name %r' % _alias)\n    return cls(**options)",
    "label": true
  },
  {
    "code": "def _clean_link(link: Link) -> _CleanResult:\n    parsed = link._parsed_url\n    netloc = parsed.netloc.rsplit('@', 1)[-1]\n    if parsed.scheme == 'file' and (not netloc):\n        netloc = 'localhost'\n    fragment = urllib.parse.parse_qs(parsed.fragment)\n    if 'egg' in fragment:\n        logger.debug('Ignoring egg= fragment in %s', link)\n    try:\n        subdirectory = fragment['subdirectory'][0]\n    except (IndexError, KeyError):\n        subdirectory = ''\n    hashes = {k: fragment[k][0] for k in _SUPPORTED_HASHES if k in fragment}\n    return _CleanResult(parsed=parsed._replace(netloc=netloc, query='', fragment=''), query=urllib.parse.parse_qs(parsed.query), subdirectory=subdirectory, hashes=hashes)",
    "label": true
  },
  {
    "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n    netaddr, bits = net.split('/')\n    netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n    return ipaddr & netmask == network & netmask",
    "label": true
  },
  {
    "code": "def _write_pyc(state: 'AssertionState', co: types.CodeType, source_stat: os.stat_result, pyc: Path) -> bool:\n    proc_pyc = f'{pyc}.{os.getpid()}'\n    try:\n        with open(proc_pyc, 'wb') as fp:\n            _write_pyc_fp(fp, source_stat, co)\n    except OSError as e:\n        state.trace(f'error writing pyc file at {proc_pyc}: errno={e.errno}')\n        return False\n    try:\n        os.replace(proc_pyc, pyc)\n    except OSError as e:\n        state.trace(f'error writing pyc file at {pyc}: {e}')\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def test_dictviews():\n    x = {'a': 1}\n    assert dill.copy(x.keys())\n    assert dill.copy(x.values())\n    assert dill.copy(x.items())",
    "label": true
  },
  {
    "code": "def _collect_type_vars(types, typevar_types=None):\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if isinstance(t, typevar_types) and t not in tvars and (not _is_unpack(t)):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)",
    "label": true
  },
  {
    "code": "def object_type(node: SuccessfulInferenceResult, context: InferenceContext | None=None) -> InferenceResult | None:\n    try:\n        types = set(_object_type(node, context))\n    except InferenceError:\n        return util.Uninferable\n    if len(types) > 1 or not types:\n        return util.Uninferable\n    return list(types)[0]",
    "label": true
  },
  {
    "code": "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    tokenizer.consume('WS')\n    name_token = tokenizer.expect('IDENTIFIER', expected='package name at the start of dependency specifier')\n    name = name_token.text\n    tokenizer.consume('WS')\n    extras = _parse_extras(tokenizer)\n    tokenizer.consume('WS')\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n    tokenizer.expect('END', expected='end of dependency specifier')\n    return ParsedRequirement(name, url, extras, specifier, marker)",
    "label": true
  },
  {
    "code": "def register_assert_rewrite(*names: str) -> None:\n    for name in names:\n        if not isinstance(name, str):\n            msg = 'expected module names as *args, got {0} instead'\n            raise TypeError(msg.format(repr(names)))\n    for hook in sys.meta_path:\n        if isinstance(hook, rewrite.AssertionRewritingHook):\n            importhook = hook\n            break\n    else:\n        importhook = DummyRewriteHook()\n    importhook.mark_rewrite(*names)",
    "label": true
  },
  {
    "code": "def _handle_no_cache_dir(option: Option, opt: str, value: str, parser: OptionParser) -> None:\n    if value is not None:\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n    parser.values.cache_dir = False",
    "label": true
  },
  {
    "code": "def get_prog() -> str:\n    try:\n        prog = os.path.basename(sys.argv[0])\n        if prog in ('__main__.py', '-c'):\n            return f'{sys.executable} -m pip'\n        else:\n            return prog\n    except (AttributeError, TypeError, IndexError):\n        pass\n    return 'pip'",
    "label": true
  },
  {
    "code": "def _glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def test_session_main(refimported):\n    extra_objects = {}\n    if refimported:\n        from sys import flags\n        extra_objects['flags'] = flags\n    with TestNamespace(**extra_objects) as ns:\n        try:\n            dill.dump_module(session_file % refimported, refimported=refimported)\n            from dill.tests.__main__ import python, shell, sp\n            error = sp.call([python, __file__, '--child', str(refimported)], shell=shell)\n            if error:\n                sys.exit(error)\n        finally:\n            with suppress(OSError):\n                os.remove(session_file % refimported)\n        session_buffer = BytesIO()\n        dill.dump_module(session_buffer, refimported=refimported)\n        session_buffer.seek(0)\n        dill.load_module(session_buffer, module='__main__')\n        ns.backup['_test_objects'](__main__, ns.backup, refimported)",
    "label": true
  },
  {
    "code": "def copytree(source: Path, target: Path) -> None:\n    assert source.is_dir()\n    for entry in visit(source, recurse=lambda entry: not entry.is_symlink()):\n        x = Path(entry)\n        relpath = x.relative_to(source)\n        newx = target / relpath\n        newx.parent.mkdir(exist_ok=True)\n        if x.is_symlink():\n            newx.symlink_to(os.readlink(x))\n        elif x.is_file():\n            shutil.copyfile(x, newx)\n        elif x.is_dir():\n            newx.mkdir(exist_ok=True)",
    "label": true
  },
  {
    "code": "def pytest_sessionfinish(session: 'Session') -> None:\n    assertstate = session.config.stash.get(assertstate_key, None)\n    if assertstate:\n        if assertstate.hook is not None:\n            assertstate.hook.set_session(None)",
    "label": true
  },
  {
    "code": "def modifies_known_mutable(obj: t.Any, attr: str) -> bool:\n    for typespec, unsafe in _mutable_spec:\n        if isinstance(obj, typespec):\n            return attr in unsafe\n    return False",
    "label": true
  },
  {
    "code": "def _merge(lst: list, start: int, mid: int, end: int) -> None:\n    result = []\n    left = start\n    right = mid\n    while left < mid and right < end:\n        if lst[left] < lst[right]:\n            result.append(lst[left])\n            left += 1\n        else:\n            result.append(lst[right])\n            right += 1\n    lst[start:end] = result + lst[left:mid] + lst[right:end]",
    "label": true
  },
  {
    "code": "def tabulate(rows: Iterable[Iterable[Any]]) -> Tuple[List[str], List[int]]:\n    rows = [tuple(map(str, row)) for row in rows]\n    sizes = [max(map(len, col)) for col in zip_longest(*rows, fillvalue='')]\n    table = [' '.join(map(str.ljust, row, sizes)).rstrip() for row in rows]\n    return (table, sizes)",
    "label": true
  },
  {
    "code": "def parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, list]:\n    pos += 1\n    array: list = []\n    pos = skip_comments_and_array_ws(src, pos)\n    if src.startswith(']', pos):\n        return (pos + 1, array)\n    while True:\n        pos, val = parse_value(src, pos, parse_float)\n        array.append(val)\n        pos = skip_comments_and_array_ws(src, pos)\n        c = src[pos:pos + 1]\n        if c == ']':\n            return (pos + 1, array)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed array')\n        pos += 1\n        pos = skip_comments_and_array_ws(src, pos)\n        if src.startswith(']', pos):\n            return (pos + 1, array)",
    "label": true
  },
  {
    "code": "def _write_requirements(stream, reqs):\n    lines = yield_lines(reqs or ())\n\n    def append_cr(line):\n        return line + '\\n'\n    lines = map(append_cr, lines)\n    stream.writelines(lines)",
    "label": true
  },
  {
    "code": "def _validate_header_part(header, header_part, header_validator_index):\n    if isinstance(header_part, str):\n        validator = _HEADER_VALIDATORS_STR[header_validator_index]\n    elif isinstance(header_part, bytes):\n        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]\n    else:\n        raise InvalidHeader(f'Header part ({header_part!r}) from {header} must be of type str or bytes, not {type(header_part)}')\n    if not validator.match(header_part):\n        header_kind = 'name' if header_validator_index == 0 else 'value'\n        raise InvalidHeader(f'Invalid leading whitespace, reserved character(s), or returncharacter(s) in header {header_kind}: {header_part!r}')",
    "label": true
  },
  {
    "code": "def apply(transform):\n\n    def wrap(func):\n        return functools.wraps(func)(compose(transform, func))\n    return wrap",
    "label": true
  },
  {
    "code": "def _pretty_fixture_path(func) -> str:\n    cwd = Path.cwd()\n    loc = Path(getlocation(func, str(cwd)))\n    prefix = Path('...', '_pytest')\n    try:\n        return str(prefix / loc.relative_to(_PYTEST_DIR))\n    except ValueError:\n        return bestrelpath(cwd, loc)",
    "label": true
  },
  {
    "code": "def _override_attribute_defined_outside_init():\n    old_leave_classdef = ClassChecker.leave_classdef\n\n    def new_leave_classdef(self, cnode):\n        for attr, nodes in cnode.instance_attrs.items():\n            setter = _get_attribute_property_setter(attr, cnode)\n            if setter is not None:\n                self.linter.config.defining_attr_methods = self.linter.config.defining_attr_methods + (setter,)\n        old_leave_classdef(self, cnode)\n    ClassChecker.leave_classdef = new_leave_classdef",
    "label": true
  },
  {
    "code": "def locate_config(args: Iterable[Path]) -> Tuple[Optional[Path], Optional[Path], Dict[str, Union[str, List[str]]]]:\n    config_names = ['pytest.ini', '.pytest.ini', 'pyproject.toml', 'tox.ini', 'setup.cfg']\n    args = [x for x in args if not str(x).startswith('-')]\n    if not args:\n        args = [Path.cwd()]\n    for arg in args:\n        argpath = absolutepath(arg)\n        for base in (argpath, *argpath.parents):\n            for config_name in config_names:\n                p = base / config_name\n                if p.is_file():\n                    ini_config = load_config_dict_from_file(p)\n                    if ini_config is not None:\n                        return (base, p, ini_config)\n    return (None, None, {})",
    "label": true
  },
  {
    "code": "def __getstate__():\n    state = {}\n    g = globals()\n    for k, v in _state_vars.items():\n        state[k] = g['_sget_' + v](g[k])\n    return state",
    "label": true
  },
  {
    "code": "def convert_path(pathname):\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n    paths = pathname.split('/')\n    while os.curdir in paths:\n        paths.remove(os.curdir)\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)",
    "label": true
  },
  {
    "code": "def _match_prefix(x, y):\n    x = str(x)\n    y = str(y)\n    if x == y:\n        return True\n    if not x.startswith(y):\n        return False\n    n = len(y)\n    return x[n] == '.'",
    "label": true
  },
  {
    "code": "def get_config(args: Optional[List[str]]=None, plugins: Optional[Sequence[Union[str, _PluggyPlugin]]]=None) -> 'Config':\n    pluginmanager = PytestPluginManager()\n    config = Config(pluginmanager, invocation_params=Config.InvocationParams(args=args or (), plugins=plugins, dir=Path.cwd()))\n    if args is not None:\n        pluginmanager.consider_preparse(args, exclude_only=True)\n    for spec in default_plugins:\n        pluginmanager.import_plugin(spec)\n    return config",
    "label": true
  },
  {
    "code": "def get_path_uid(path: str) -> int:\n    if hasattr(os, 'O_NOFOLLOW'):\n        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)\n        file_uid = os.fstat(fd).st_uid\n        os.close(fd)\n    elif not os.path.islink(path):\n        file_uid = os.stat(path).st_uid\n    else:\n        raise OSError(f'{path} is a symlink; Will not return uid for symlinks')\n    return file_uid",
    "label": true
  },
  {
    "code": "def evaluate_marker(text, extra=None):\n    try:\n        marker = packaging.markers.Marker(text)\n        return marker.evaluate()\n    except packaging.markers.InvalidMarker as e:\n        raise SyntaxError(e) from e",
    "label": true
  },
  {
    "code": "def was_installed_by_pip(pkg: str) -> bool:\n    dist = get_default_environment().get_distribution(pkg)\n    return dist is not None and 'pip' == dist.installer",
    "label": true
  },
  {
    "code": "def proceed(prompt, allowed_chars, error_prompt=None, default=None):\n    p = prompt\n    while True:\n        s = raw_input(p)\n        p = prompt\n        if not s and default:\n            s = default\n        if s:\n            c = s[0].lower()\n            if c in allowed_chars:\n                break\n            if error_prompt:\n                p = '%c: %s\\n%s' % (c, error_prompt, prompt)\n    return c",
    "label": true
  },
  {
    "code": "def pytest_runtestloop(session: 'Session') -> bool:\n    if session.testsfailed and (not session.config.option.continue_on_collection_errors):\n        raise session.Interrupted('%d error%s during collection' % (session.testsfailed, 's' if session.testsfailed != 1 else ''))\n    if session.config.option.collectonly:\n        return True\n    for i, item in enumerate(session.items):\n        nextitem = session.items[i + 1] if i + 1 < len(session.items) else None\n        item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\n        if session.shouldfail:\n            raise session.Failed(session.shouldfail)\n        if session.shouldstop:\n            raise session.Interrupted(session.shouldstop)\n    return True",
    "label": true
  },
  {
    "code": "def _have_compatible_abi(executable: str, arch: str) -> bool:\n    if arch == 'armv7l':\n        return _is_linux_armhf(executable)\n    if arch == 'i686':\n        return _is_linux_i686(executable)\n    return arch in {'x86_64', 'aarch64', 'ppc64', 'ppc64le', 's390x'}",
    "label": true
  },
  {
    "code": "def _parse_options(o_strs):\n    opts = {}\n    if not o_strs:\n        return opts\n    for o_str in o_strs:\n        if not o_str.strip():\n            continue\n        o_args = o_str.split(',')\n        for o_arg in o_args:\n            o_arg = o_arg.strip()\n            try:\n                o_key, o_val = o_arg.split('=', 1)\n                o_key = o_key.strip()\n                o_val = o_val.strip()\n            except ValueError:\n                opts[o_arg] = True\n            else:\n                opts[o_key] = o_val\n    return opts",
    "label": true
  },
  {
    "code": "def terminal_encoding(term):\n    if getattr(term, 'encoding', None):\n        return term.encoding\n    import locale\n    return locale.getpreferredencoding()",
    "label": true
  },
  {
    "code": "def removeDuplicates(variable):\n    oldList = variable.split(os.pathsep)\n    newList = []\n    for i in oldList:\n        if i not in newList:\n            newList.append(i)\n    newVariable = os.pathsep.join(newList)\n    return newVariable",
    "label": true
  },
  {
    "code": "def pick_unit_and_suffix(size: int, suffixes: List[str], base: int) -> Tuple[int, str]:\n    for i, suffix in enumerate(suffixes):\n        unit = base ** i\n        if size < unit * base:\n            break\n    return (unit, suffix)",
    "label": true
  },
  {
    "code": "def _glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def _is_property_decorator(decorator: nodes.Name) -> bool:\n    for inferred in decorator.infer():\n        if isinstance(inferred, nodes.ClassDef):\n            if inferred.qname() in {'builtins.property', 'functools.cached_property'}:\n                return True\n            for ancestor in inferred.ancestors():\n                if ancestor.name == 'property' and ancestor.root().name == 'builtins':\n                    return True\n        elif isinstance(inferred, nodes.FunctionDef):\n            returns: list[nodes.Return] = list(inferred._get_return_nodes_skip_functions())\n            if len(returns) == 1 and isinstance(returns[0].value, (nodes.Name, nodes.Attribute)):\n                inferred = safe_infer(returns[0].value)\n                if inferred and isinstance(inferred, astroid.objects.Property) and isinstance(inferred.function, nodes.FunctionDef):\n                    return decorated_with_property(inferred.function)\n    return False",
    "label": true
  },
  {
    "code": "def _tempfilepager(generator: t.Iterable[str], cmd: str, color: t.Optional[bool]) -> None:\n    import tempfile\n    fd, filename = tempfile.mkstemp()\n    text = ''.join(generator)\n    if not color:\n        text = strip_ansi(text)\n    encoding = get_best_encoding(sys.stdout)\n    with open_stream(filename, 'wb')[0] as f:\n        f.write(text.encode(encoding))\n    try:\n        os.system(f'{cmd} \"{filename}\"')\n    finally:\n        os.close(fd)\n        os.unlink(filename)",
    "label": true
  },
  {
    "code": "def resolve_cert_reqs(candidate):\n    if candidate is None:\n        return CERT_REQUIRED\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'CERT_' + candidate)\n        return res\n    return candidate",
    "label": true
  },
  {
    "code": "def interpreter_version(*, warn: bool=False) -> str:\n    version = _get_config_var('py_version_nodot', warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
    "label": true
  },
  {
    "code": "def _folded_skips(startpath: Path, skipped: Sequence[CollectReport]) -> List[Tuple[int, str, Optional[int], str]]:\n    d: Dict[Tuple[str, Optional[int], str], List[CollectReport]] = {}\n    for event in skipped:\n        assert event.longrepr is not None\n        assert isinstance(event.longrepr, tuple), (event, event.longrepr)\n        assert len(event.longrepr) == 3, (event, event.longrepr)\n        fspath, lineno, reason = event.longrepr\n        fspath = bestrelpath(startpath, Path(fspath))\n        keywords = getattr(event, 'keywords', {})\n        if event.when == 'setup' and 'skip' in keywords and ('pytestmark' not in keywords):\n            key: Tuple[str, Optional[int], str] = (fspath, None, reason)\n        else:\n            key = (fspath, lineno, reason)\n        d.setdefault(key, []).append(event)\n    values: List[Tuple[int, str, Optional[int], str]] = []\n    for key, events in d.items():\n        values.append((len(events), *key))\n    return values",
    "label": true
  },
  {
    "code": "def parse_multiline_str(src: str, pos: Pos, *, literal: bool) -> tuple[Pos, str]:\n    pos += 3\n    if src.startswith('\\n', pos):\n        pos += 1\n    if literal:\n        delim = \"'\"\n        end_pos = skip_until(src, pos, \"'''\", error_on=ILLEGAL_MULTILINE_LITERAL_STR_CHARS, error_on_eof=True)\n        result = src[pos:end_pos]\n        pos = end_pos + 3\n    else:\n        delim = '\"'\n        pos, result = parse_basic_str(src, pos, multiline=True)\n    if not src.startswith(delim, pos):\n        return (pos, result)\n    pos += 1\n    if not src.startswith(delim, pos):\n        return (pos, result + delim)\n    pos += 1\n    return (pos, result + delim * 2)",
    "label": true
  },
  {
    "code": "def find_worst_bci(bridges: list[list], bridge_ids: list[int]) -> int:\n    current_lowest = 101\n    id_of_lowest = None\n    for bridge_id in bridge_ids:\n        bci_score = get_bridge_condition(bridges, bridge_id)\n        if bci_score < current_lowest:\n            id_of_lowest = bridge_id\n            current_lowest = bci_score\n        elif bci_score == current_lowest:\n            id_of_lowest = min(bridge_id, id_of_lowest)\n    return id_of_lowest",
    "label": true
  },
  {
    "code": "def filter_noncode_lines(ls_1: LineSet, stindex_1: Index, ls_2: LineSet, stindex_2: Index, common_lines_nb: int) -> int:\n    stripped_l1 = [lspecif.text for lspecif in ls_1.stripped_lines[stindex_1:stindex_1 + common_lines_nb] if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)]\n    stripped_l2 = [lspecif.text for lspecif in ls_2.stripped_lines[stindex_2:stindex_2 + common_lines_nb] if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)]\n    return sum((sline_1 == sline_2 for sline_1, sline_2 in zip(stripped_l1, stripped_l2)))",
    "label": true
  },
  {
    "code": "def format_lines(var_name, seq, raw=False, indent_level=0):\n    lines = []\n    base_indent = ' ' * indent_level * 4\n    inner_indent = ' ' * (indent_level + 1) * 4\n    lines.append(base_indent + var_name + ' = (')\n    if raw:\n        for i in seq:\n            lines.append(inner_indent + i + ',')\n    else:\n        for i in seq:\n            r = repr(i + '\"')\n            lines.append(inner_indent + r[:-2] + r[-1] + ',')\n    lines.append(base_indent + ')')\n    return '\\n'.join(lines)",
    "label": true
  },
  {
    "code": "def get_host_platform():\n    if sys.version_info < (3, 8):\n        if os.name == 'nt':\n            if '(arm)' in sys.version.lower():\n                return 'win-arm32'\n            if '(arm64)' in sys.version.lower():\n                return 'win-arm64'\n    if sys.version_info < (3, 9):\n        if os.name == 'posix' and hasattr(os, 'uname'):\n            osname, host, release, version, machine = os.uname()\n            if osname[:3] == 'aix':\n                from .py38compat import aix_platform\n                return aix_platform(osname, version, release)\n    return sysconfig.get_platform()",
    "label": true
  },
  {
    "code": "def redact_netloc(netloc: str) -> str:\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    if password is None:\n        user = '****'\n        password = ''\n    else:\n        user = urllib.parse.quote(user)\n        password = ':****'\n    return '{user}{password}@{netloc}'.format(user=user, password=password, netloc=netloc)",
    "label": true
  },
  {
    "code": "def allexcept(*args):\n    newcats = cats[:]\n    for arg in args:\n        newcats.remove(arg)\n    return ''.join((globals()[cat] for cat in newcats))",
    "label": true
  },
  {
    "code": "def duplicates_removed(it, already_seen=()):\n    lst = []\n    seen = set()\n    for i in it:\n        if i in seen or i in already_seen:\n            continue\n        lst.append(i)\n        seen.add(i)\n    return lst",
    "label": true
  },
  {
    "code": "def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:\n    tuple_ = _encode_range(int_, 0)\n    pos = bisect.bisect_left(ranges, tuple_)\n    if pos > 0:\n        left, right = _decode_range(ranges[pos - 1])\n        if left <= int_ < right:\n            return True\n    if pos < len(ranges):\n        left, _ = _decode_range(ranges[pos])\n        if left == int_:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def transform_six_add_metaclass(node):\n    if not node.decorators:\n        return\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, nodes.Call):\n            continue\n        try:\n            func = next(decorator.func.infer())\n        except (InferenceError, StopIteration):\n            continue\n        if func.qname() == SIX_ADD_METACLASS and decorator.args:\n            metaclass = decorator.args[0]\n            node._metaclass = metaclass\n            return node\n    return",
    "label": true
  },
  {
    "code": "def _comment(string: str) -> str:\n    lines = [line.strip() for line in string.splitlines()]\n    sep = '\\n'\n    return '# ' + f'{sep}# '.join(lines)",
    "label": true
  },
  {
    "code": "def guess_decode(text):\n    try:\n        text = text.decode('utf-8')\n        return (text, 'utf-8')\n    except UnicodeDecodeError:\n        try:\n            import locale\n            prefencoding = locale.getpreferredencoding()\n            text = text.decode()\n            return (text, prefencoding)\n        except (UnicodeDecodeError, LookupError):\n            text = text.decode('latin1')\n            return (text, 'latin1')",
    "label": true
  },
  {
    "code": "def get_runnable_pip() -> str:\n    source = pathlib.Path(pip_location).resolve().parent\n    if not source.is_dir():\n        return str(source)\n    return os.fsdecode(source / '__pip-runner__.py')",
    "label": true
  },
  {
    "code": "def _nose_tools_transform(node):\n    for method_name, method in _nose_tools_functions():\n        node.locals[method_name] = [method]",
    "label": true
  },
  {
    "code": "def _emoji_replace(text: str, default_variant: Optional[str]=None, _emoji_sub: _EmojiSubMethod=re.compile('(:(\\\\S*?)(?:(?:\\\\-)(emoji|text))?:)').sub) -> str:\n    get_emoji = EMOJI.__getitem__\n    variants = {'text': '\ufe0e', 'emoji': '\ufe0f'}\n    get_variant = variants.get\n    default_variant_code = variants.get(default_variant, '') if default_variant else ''\n\n    def do_replace(match: Match[str]) -> str:\n        emoji_code, emoji_name, variant = match.groups()\n        try:\n            return get_emoji(emoji_name.lower()) + get_variant(variant, default_variant_code)\n        except KeyError:\n            return emoji_code\n    return _emoji_sub(do_replace, text)",
    "label": true
  },
  {
    "code": "def parse_editable(editable_req: str) -> Tuple[Optional[str], str, Set[str]]:\n    url = editable_req\n    url_no_extras, extras = _strip_extras(url)\n    if os.path.isdir(url_no_extras):\n        url_no_extras = path_to_url(url_no_extras)\n    if url_no_extras.lower().startswith('file:'):\n        package_name = Link(url_no_extras).egg_fragment\n        if extras:\n            return (package_name, url_no_extras, get_requirement('placeholder' + extras.lower()).extras)\n        else:\n            return (package_name, url_no_extras, set())\n    for version_control in vcs:\n        if url.lower().startswith(f'{version_control}:'):\n            url = f'{version_control}+{url}'\n            break\n    link = Link(url)\n    if not link.is_vcs:\n        backends = ', '.join(vcs.all_schemes)\n        raise InstallationError(f'{editable_req} is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with {backends}).')\n    package_name = link.egg_fragment\n    if not package_name:\n        raise InstallationError(\"Could not detect requirement name for '{}', please specify one with #egg=your_package_name\".format(editable_req))\n    return (package_name, url, set())",
    "label": true
  },
  {
    "code": "def get_subj_alt_name(peer_cert: X509) -> list[tuple[str, str]]:\n    cert = peer_cert.to_cryptography()\n    try:\n        ext = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName).value\n    except x509.ExtensionNotFound:\n        return []\n    except (x509.DuplicateExtension, UnsupportedExtension, x509.UnsupportedGeneralNameType, UnicodeError) as e:\n        log.warning('A problem was encountered with the certificate that prevented urllib3 from finding the SubjectAlternativeName field. This can affect certificate validation. The error was %s', e)\n        return []\n    names = [('DNS', name) for name in map(_dnsname_to_stdlib, ext.get_values_for_type(x509.DNSName)) if name is not None]\n    names.extend((('IP Address', str(name)) for name in ext.get_values_for_type(x509.IPAddress)))\n    return names",
    "label": true
  },
  {
    "code": "def loop_first(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\n    iter_values = iter(values)\n    try:\n        value = next(iter_values)\n    except StopIteration:\n        return\n    yield (True, value)\n    for value in iter_values:\n        yield (False, value)",
    "label": true
  },
  {
    "code": "def _colorama_workaround() -> None:\n    if sys.platform.startswith('win32'):\n        try:\n            import colorama\n        except ImportError:\n            pass",
    "label": true
  },
  {
    "code": "def reinit():\n    if wrapped_stdout is not None:\n        sys.stdout = wrapped_stdout\n    if wrapped_stderr is not None:\n        sys.stderr = wrapped_stderr",
    "label": true
  },
  {
    "code": "def direct_url_from_link(link: Link, source_dir: Optional[str]=None, link_is_in_wheel_cache: bool=False) -> DirectUrl:\n    if link.is_vcs:\n        vcs_backend = vcs.get_backend_for_scheme(link.scheme)\n        assert vcs_backend\n        url, requested_revision, _ = vcs_backend.get_url_rev_and_auth(link.url_without_fragment)\n        if link_is_in_wheel_cache:\n            assert requested_revision\n            commit_id = requested_revision\n        else:\n            assert source_dir\n            commit_id = vcs_backend.get_revision(source_dir)\n        return DirectUrl(url=url, info=VcsInfo(vcs=vcs_backend.name, commit_id=commit_id, requested_revision=requested_revision), subdirectory=link.subdirectory_fragment)\n    elif link.is_existing_dir():\n        return DirectUrl(url=link.url_without_fragment, info=DirInfo(), subdirectory=link.subdirectory_fragment)\n    else:\n        hash = None\n        hash_name = link.hash_name\n        if hash_name:\n            hash = f'{hash_name}={link.hash}'\n        return DirectUrl(url=link.url_without_fragment, info=ArchiveInfo(hash=hash), subdirectory=link.subdirectory_fragment)",
    "label": true
  },
  {
    "code": "def install_warning_logger() -> None:\n    warnings.simplefilter('default', PipDeprecationWarning, append=True)\n    global _original_showwarning\n    if _original_showwarning is None:\n        _original_showwarning = warnings.showwarning\n        warnings.showwarning = _showwarning",
    "label": true
  },
  {
    "code": "def _known_pattern(name: str, config: Config) -> Optional[Tuple[str, str]]:\n    parts = name.split('.')\n    module_names_to_check = ('.'.join(parts[:first_k]) for first_k in range(len(parts), 0, -1))\n    for module_name_to_check in module_names_to_check:\n        for pattern, placement in config.known_patterns:\n            if placement in config.sections and pattern.match(module_name_to_check):\n                return (placement, f'Matched configured known pattern {pattern}')\n    return None",
    "label": true
  },
  {
    "code": "def assert_fingerprint(cert: bytes | None, fingerprint: str) -> None:\n    if cert is None:\n        raise SSLError('No certificate for the peer.')\n    fingerprint = fingerprint.replace(':', '').lower()\n    digest_length = len(fingerprint)\n    hashfunc = HASHFUNC_MAP.get(digest_length)\n    if not hashfunc:\n        raise SSLError(f'Fingerprint of invalid length: {fingerprint}')\n    fingerprint_bytes = unhexlify(fingerprint.encode())\n    cert_digest = hashfunc(cert).digest()\n    if not hmac.compare_digest(cert_digest, fingerprint_bytes):\n        raise SSLError(f'Fingerprints did not match. Expected \"{fingerprint}\", got \"{cert_digest.hex()}\"')",
    "label": true
  },
  {
    "code": "def ask_whether_to_apply_changes_to_file(file_path: str) -> bool:\n    answer = None\n    while answer not in ('yes', 'y', 'no', 'n', 'quit', 'q'):\n        answer = input(f\"Apply suggested changes to '{file_path}' [y/n/q]? \")\n        answer = answer.lower()\n        if answer in ('no', 'n'):\n            return False\n        if answer in ('quit', 'q'):\n            sys.exit(1)\n    return True",
    "label": true
  },
  {
    "code": "def loop_first_last(values: Iterable[T]) -> Iterable[Tuple[bool, bool, T]]:\n    iter_values = iter(values)\n    try:\n        previous_value = next(iter_values)\n    except StopIteration:\n        return\n    first = True\n    for value in iter_values:\n        yield (first, False, previous_value)\n        first = False\n        previous_value = value\n    yield (first, True, previous_value)",
    "label": true
  },
  {
    "code": "def ensure_slash(s):\n    if not s.endswith('/'):\n        return s + '/'\n    return s",
    "label": true
  },
  {
    "code": "def unicode_is_ascii(u_string):\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode('ascii')\n        return True\n    except UnicodeEncodeError:\n        return False",
    "label": true
  },
  {
    "code": "def preprocess(content: str) -> ReqFileLines:\n    lines_enum: ReqFileLines = enumerate(content.splitlines(), start=1)\n    lines_enum = join_lines(lines_enum)\n    lines_enum = ignore_comments(lines_enum)\n    lines_enum = expand_env_variables(lines_enum)\n    return lines_enum",
    "label": true
  },
  {
    "code": "def check_messages(*messages: str) -> Callable[[AstCallbackMethod[_CheckerT, _NodeT]], AstCallbackMethod[_CheckerT, _NodeT]]:\n    warnings.warn('utils.check_messages will be removed in favour of calling utils.only_required_for_messages in pylint 3.0', DeprecationWarning, stacklevel=2)\n    return only_required_for_messages(*messages)",
    "label": true
  },
  {
    "code": "def test_getsource():\n    assert getsource(f) == 'f = lambda x: x**2\\n'\n    assert getsource(g) == 'def g(x): return f(x) - x\\n'\n    assert getsource(h) == 'def h(x):\\n  def g(x): return x\\n  return g(x) - x\\n'\n    assert getname(f) == 'f'\n    assert getname(g) == 'g'\n    assert getname(h) == 'h'\n    assert _wrap(f)(4) == 16\n    assert _wrap(g)(4) == 12\n    assert _wrap(h)(4) == 0\n    assert getname(Foo) == 'Foo'\n    assert getname(Bar) == 'Bar'\n    assert getsource(Bar) == 'class Bar:\\n  pass\\n'\n    assert getsource(Foo) == 'class Foo(object):\\n  def bar(self, x):\\n    return x*x+x\\n'",
    "label": true
  },
  {
    "code": "def auto_complete_paths(current: str, completion_type: str) -> Iterable[str]:\n    directory, filename = os.path.split(current)\n    current_path = os.path.abspath(directory)\n    if not os.access(current_path, os.R_OK):\n        return\n    filename = os.path.normcase(filename)\n    file_list = (x for x in os.listdir(current_path) if os.path.normcase(x).startswith(filename))\n    for f in file_list:\n        opt = os.path.join(current_path, f)\n        comp_file = os.path.normcase(os.path.join(directory, f))\n        if completion_type != 'dir' and os.path.isfile(opt):\n            yield comp_file\n        elif os.path.isdir(opt):\n            yield os.path.join(comp_file, '')",
    "label": true
  },
  {
    "code": "def __getstate__():\n    state = {}\n    g = globals()\n    for k, v in _state_vars.items():\n        state[k] = g['_sget_' + v](g[k])\n    return state",
    "label": true
  },
  {
    "code": "def _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version",
    "label": true
  },
  {
    "code": "def _encode_invalid_chars(component, allowed_chars, encoding='utf-8'):\n    if component is None:\n        return component\n    component = six.ensure_text(component)\n    component, percent_encodings = PERCENT_RE.subn(lambda match: match.group(0).upper(), component)\n    uri_bytes = component.encode('utf-8', 'surrogatepass')\n    is_percent_encoded = percent_encodings == uri_bytes.count(b'%')\n    encoded_component = bytearray()\n    for i in range(0, len(uri_bytes)):\n        byte = uri_bytes[i:i + 1]\n        byte_ord = ord(byte)\n        if is_percent_encoded and byte == b'%' or (byte_ord < 128 and byte.decode() in allowed_chars):\n            encoded_component += byte\n            continue\n        encoded_component.extend(b'%' + hex(byte_ord)[2:].encode().zfill(2).upper())\n    return encoded_component.decode(encoding)",
    "label": true
  },
  {
    "code": "def parse_dict_header(value):\n    result = {}\n    for item in _parse_list_header(value):\n        if '=' not in item:\n            result[item] = None\n            continue\n        name, value = item.split('=', 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result",
    "label": true
  },
  {
    "code": "def default_subprocess_runner(cmd, cwd=None, extra_environ=None):\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    check_call(cmd, cwd=cwd, env=env)",
    "label": true
  },
  {
    "code": "def normalize_newlines(text):\n    newlines = ['\\r\\n', '\\r', '\\n', '\\x85', '\\u2028', '\\u2029']\n    pattern = '|'.join(newlines)\n    return re.sub(pattern, '\\n', text)",
    "label": true
  },
  {
    "code": "def _update_globals():\n    if not sys.platform.startswith('java') and sys.platform != 'cli':\n        return\n    incompatible = ('extract_constant', 'get_module_constant')\n    for name in incompatible:\n        del globals()[name]\n        __all__.remove(name)",
    "label": true
  },
  {
    "code": "def file_ns_handler(importer, path_item, packageName, module):\n    subpath = os.path.join(path_item, packageName.split('.')[-1])\n    normalized = _normalize_cached(subpath)\n    for item in module.__path__:\n        if _normalize_cached(item) == normalized:\n            break\n    else:\n        return subpath",
    "label": true
  },
  {
    "code": "def _augment_sys_path(additional_paths: Sequence[str]) -> list[str]:\n    original = list(sys.path)\n    changes = []\n    seen = set()\n    for additional_path in additional_paths:\n        if additional_path not in seen:\n            changes.append(additional_path)\n            seen.add(additional_path)\n    sys.path[:] = changes + sys.path\n    return original",
    "label": true
  },
  {
    "code": "def counted_array(expr: ParserElement, int_expr: typing.Optional[ParserElement]=None, *, intExpr: typing.Optional[ParserElement]=None) -> ParserElement:\n    intExpr = intExpr or int_expr\n    array_expr = Forward()\n\n    def count_field_parse_action(s, l, t):\n        nonlocal array_expr\n        n = t[0]\n        array_expr <<= expr * n if n else Empty()\n        del t[:]\n    if intExpr is None:\n        intExpr = Word(nums).set_parse_action(lambda t: int(t[0]))\n    else:\n        intExpr = intExpr.copy()\n    intExpr.set_name('arrayLen')\n    intExpr.add_parse_action(count_field_parse_action, call_during_try=True)\n    return (intExpr + array_expr).set_name('(len) ' + str(expr) + '...')",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e265(msg, _node, source_lines=None):\n    line = msg.line\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(0, len(source_lines[line - 1])), LineType.ERROR, source_lines[line - 1] + \"  # INSERT SPACE AFTER THE '#'\")\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _get_continue_on_failure(config):\n    continue_on_failure = config.getvalue('doctest_continue_on_failure')\n    if continue_on_failure:\n        if config.getvalue('usepdb'):\n            continue_on_failure = False\n    return continue_on_failure",
    "label": true
  },
  {
    "code": "def patch_function_wrapper(module, name):\n\n    def _wrapper(wrapper):\n        return wrap_object(module, name, FunctionWrapper, (wrapper,))\n    return _wrapper",
    "label": true
  },
  {
    "code": "def guess_lexer(_text, **options):\n    if not isinstance(_text, str):\n        inencoding = options.get('inencoding', options.get('encoding'))\n        if inencoding:\n            _text = _text.decode(inencoding or 'utf8')\n        else:\n            _text, _ = guess_decode(_text)\n    ft = get_filetype_from_buffer(_text)\n    if ft is not None:\n        try:\n            return get_lexer_by_name(ft, **options)\n        except ClassNotFound:\n            pass\n    best_lexer = [0.0, None]\n    for lexer in _iter_lexerclasses():\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        if rv > best_lexer[0]:\n            best_lexer[:] = (rv, lexer)\n    if not best_lexer[0] or best_lexer[1] is None:\n        raise ClassNotFound('no lexer matching the text found')\n    return best_lexer[1](**options)",
    "label": true
  },
  {
    "code": "def _is_doctest(config: Config, path: Path, parent: Collector) -> bool:\n    if path.suffix in ('.txt', '.rst') and parent.session.isinitpath(path):\n        return True\n    globs = config.getoption('doctestglob') or ['test*.txt']\n    return any((fnmatch_ex(glob, path) for glob in globs))",
    "label": true
  },
  {
    "code": "def format_for_json(packages: '_ProcessedDists', options: Values) -> str:\n    data = []\n    for dist in packages:\n        info = {'name': dist.raw_name, 'version': str(dist.version)}\n        if options.verbose >= 1:\n            info['location'] = dist.location or ''\n            info['installer'] = dist.installer\n        if options.outdated:\n            info['latest_version'] = str(dist.latest_version)\n            info['latest_filetype'] = dist.latest_filetype\n        editable_project_location = dist.editable_project_location\n        if editable_project_location:\n            info['editable_project_location'] = editable_project_location\n        data.append(info)\n    return json.dumps(data)",
    "label": true
  },
  {
    "code": "def main(argv: Optional[List[str]]=None) -> None:\n    parser = argparse.ArgumentParser(description='Takes one or more file paths and reports their detected encodings')\n    parser.add_argument('input', help='File whose encoding we would like to determine. (default: stdin)', type=argparse.FileType('rb'), nargs='*', default=[sys.stdin.buffer])\n    parser.add_argument('--minimal', help='Print only the encoding to standard output', action='store_true')\n    parser.add_argument('-l', '--legacy', help='Rename legacy encodings to more modern ones.', action='store_true')\n    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')\n    args = parser.parse_args(argv)\n    for f in args.input:\n        if f.isatty():\n            print('You are running chardetect interactively. Press CTRL-D twice at the start of a blank line to signal the end of your input. If you want help, run chardetect --help\\n', file=sys.stderr)\n        print(description_of(f, f.name, minimal=args.minimal, should_rename_legacy=args.legacy))",
    "label": true
  },
  {
    "code": "def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]:\n    if hasattr(report, 'wasxfail'):\n        if report.skipped:\n            return ('xfailed', 'x', 'XFAIL')\n        elif report.passed:\n            return ('xpassed', 'X', 'XPASS')\n    return None",
    "label": true
  },
  {
    "code": "def platform_tags() -> Iterator[str]:\n    if platform.system() == 'Darwin':\n        return mac_platforms()\n    elif platform.system() == 'Linux':\n        return _linux_platforms()\n    else:\n        return _generic_platforms()",
    "label": true
  },
  {
    "code": "def _extend_builtins(class_transforms):\n    builtin_ast = AstroidManager().builtins_module\n    for class_name, transform in class_transforms.items():\n        transform(builtin_ast[class_name])",
    "label": true
  },
  {
    "code": "def make_target_python(options: Values) -> TargetPython:\n    target_python = TargetPython(platforms=options.platforms, py_version_info=options.python_version, abis=options.abis, implementation=options.implementation)\n    return target_python",
    "label": true
  },
  {
    "code": "def get_best_encoding(stream: t.IO[t.Any]) -> str:\n    rv = getattr(stream, 'encoding', None) or sys.getdefaultencoding()\n    if is_ascii_encoding(rv):\n        return 'utf-8'\n    return rv",
    "label": true
  },
  {
    "code": "def fail_fixturefunc(fixturefunc, msg: str) -> NoReturn:\n    fs, lineno = getfslineno(fixturefunc)\n    location = f'{fs}:{lineno + 1}'\n    source = _pytest._code.Source(fixturefunc)\n    fail(msg + ':\\n\\n' + str(source.indent()) + '\\n' + location, pytrace=False)",
    "label": true
  },
  {
    "code": "def _looks_like_bpo_44860() -> bool:\n    from distutils.command.install import INSTALL_SCHEMES\n    try:\n        unix_user_platlib = INSTALL_SCHEMES['unix_user']['platlib']\n    except KeyError:\n        return False\n    return unix_user_platlib == '$usersite'",
    "label": true
  },
  {
    "code": "def analyze_egg(egg_dir, stubs):\n    for flag, fn in safety_flags.items():\n        if os.path.exists(os.path.join(egg_dir, 'EGG-INFO', fn)):\n            return flag\n    if not can_scan():\n        return False\n    safe = True\n    for base, dirs, files in walk_egg(egg_dir):\n        for name in files:\n            if name.endswith('.py') or name.endswith('.pyw'):\n                continue\n            elif name.endswith('.pyc') or name.endswith('.pyo'):\n                safe = scan_module(egg_dir, base, name, stubs) and safe\n    return safe",
    "label": true
  },
  {
    "code": "def safe_isclass(obj: object) -> bool:\n    try:\n        return inspect.isclass(obj)\n    except Exception:\n        return False",
    "label": true
  },
  {
    "code": "def _get_all_attribute_assignments(node: nodes.FunctionDef, name: str | None=None) -> set[str]:\n    attributes: set[str] = set()\n    for child in node.nodes_of_class((nodes.Assign, nodes.AnnAssign)):\n        targets = []\n        if isinstance(child, nodes.Assign):\n            targets = child.targets\n        elif isinstance(child, nodes.AnnAssign):\n            targets = [child.target]\n        for assign_target in targets:\n            if isinstance(assign_target, nodes.Tuple):\n                targets.extend(assign_target.elts)\n                continue\n            if isinstance(assign_target, nodes.AssignAttr) and isinstance(assign_target.expr, nodes.Name) and (name is None or assign_target.expr.name == name):\n                attributes.add(assign_target.attrname)\n    return attributes",
    "label": true
  },
  {
    "code": "def bygroups(*args):\n\n    def callback(lexer, match, ctx=None):\n        for i, action in enumerate(args):\n            if action is None:\n                continue\n            elif type(action) is _TokenType:\n                data = match.group(i + 1)\n                if data:\n                    yield (match.start(i + 1), action, data)\n            else:\n                data = match.group(i + 1)\n                if data is not None:\n                    if ctx:\n                        ctx.pos = match.start(i + 1)\n                    for item in action(lexer, _PseudoMatch(match.start(i + 1), data), ctx):\n                        if item:\n                            yield item\n        if ctx:\n            ctx.pos = match.end()\n    return callback",
    "label": true
  },
  {
    "code": "def skip_comments_and_array_ws(src: str, pos: Pos) -> Pos:\n    while True:\n        pos_before_skip = pos\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        pos = skip_comment(src, pos)\n        if pos == pos_before_skip:\n            return pos",
    "label": true
  },
  {
    "code": "def get_lines(command: List[str]) -> List[str]:\n    stdout = get_output(command)\n    return [line.strip() for line in stdout.splitlines()]",
    "label": true
  },
  {
    "code": "def _check_invariants(instance, klass: type, global_scope: dict) -> None:\n    rep_invariants = getattr(klass, '__representation_invariants__', set())\n    for invariant, compiled in rep_invariants:\n        try:\n            _debug(f'Checking representation invariant for {instance.__class__.__qualname__}: {invariant}')\n            check = eval(compiled, {**global_scope, 'self': instance})\n        except:\n            _debug(f'Warning: could not evaluate representation invariant: {invariant}')\n        else:\n            if not check:\n                curr_attributes = ', '.join((f'{k}: {_display_value(v)}' for k, v in vars(instance).items()))\n                curr_attributes = '{' + curr_attributes + '}'\n                raise PyTAContractError(f'\"{instance.__class__.__name__}\" representation invariant \"{invariant}\" was violated for instance attributes {curr_attributes}')",
    "label": true
  },
  {
    "code": "def test_iter_encode():\n    assert b''.join(iter_encode([], 'latin1')) == b''\n    assert b''.join(iter_encode([''], 'latin1')) == b''\n    assert b''.join(iter_encode(['\u00e9'], 'latin1')) == b'\\xe9'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'latin1')) == b'\\xe9'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16')) == b'\\xe9\\x00'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16le')) == b'\\xe9\\x00'\n    assert b''.join(iter_encode(['', '\u00e9', '', ''], 'utf-16be')) == b'\\x00\\xe9'\n    assert b''.join(iter_encode(['', 'h\\uf7e9', '', 'llo'], 'x-user-defined')) == b'h\\xe9llo'",
    "label": true
  },
  {
    "code": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {'implementation_name': implementation_name, 'implementation_version': iver, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'python_full_version': platform.python_version(), 'platform_python_implementation': platform.python_implementation(), 'python_version': '.'.join(platform.python_version_tuple()[:2]), 'sys_platform': sys.platform}",
    "label": true
  },
  {
    "code": "def interleave_longest(*iterables):\n    i = chain.from_iterable(zip_longest(*iterables, fillvalue=_marker))\n    return (x for x in i if x is not _marker)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('typeguard')\n    group.addoption('--typeguard-packages', action='store', help='comma separated name list of packages and modules to instrument for type checking, or :all: to instrument all modules loaded after typeguard')\n    group.addoption('--typeguard-debug-instrumentation', action='store_true', help='print all instrumented code to stderr')\n    group.addoption('--typeguard-typecheck-fail-callback', action='store', help='a module:varname (e.g. typeguard:warn_on_error) reference to a function that is called (with the exception, and memo object as arguments) to handle a TypeCheckError')\n    group.addoption('--typeguard-forward-ref-policy', action='store', choices=list(ForwardRefPolicy.__members__), help='determines how to deal with unresolveable forward references in type annotations')\n    group.addoption('--typeguard-collection-check-strategy', action='store', choices=list(CollectionCheckStrategy.__members__), help='determines how thoroughly to check collections (list, dict, etc)')",
    "label": true
  },
  {
    "code": "def _is_namespace_package(path: Path, src_extensions: FrozenSet[str]) -> bool:\n    if not _is_package(path):\n        return False\n    init_file = path / '__init__.py'\n    if not init_file.exists():\n        filenames = [filepath for filepath in path.iterdir() if filepath.suffix.lstrip('.') in src_extensions or filepath.name.lower() in ('setup.cfg', 'pyproject.toml')]\n        if filenames:\n            return False\n    else:\n        with init_file.open('rb') as open_init_file:\n            file_start = open_init_file.read(4096)\n            if b\"__import__('pkg_resources').declare_namespace(__name__)\" not in file_start and b'__import__(\"pkg_resources\").declare_namespace(__name__)' not in file_start and (b\"__path__ = __import__('pkgutil').extend_path(__path__, __name__)\" not in file_start) and (b'__path__ = __import__(\"pkgutil\").extend_path(__path__, __name__)' not in file_start):\n                return False\n    return True",
    "label": true
  },
  {
    "code": "def get_vendor_version_from_module(module_name: str) -> Optional[str]:\n    module = get_module_from_module_name(module_name)\n    version = getattr(module, '__version__', None)\n    if not version:\n        assert module.__file__ is not None\n        env = get_environment([os.path.dirname(module.__file__)])\n        dist = env.get_distribution(module_name)\n        if dist:\n            version = str(dist.version)\n    return version",
    "label": true
  },
  {
    "code": "def _cmpkey(epoch: int, release: Tuple[int, ...], pre: Optional[Tuple[str, int]], post: Optional[Tuple[str, int]], dev: Optional[Tuple[str, int]], local: Optional[LocalType]) -> CmpKey:\n    _release = tuple(reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release)))))\n    if pre is None and post is None and (dev is not None):\n        _pre: CmpPrePostDevType = NegativeInfinity\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n    if post is None:\n        _post: CmpPrePostDevType = NegativeInfinity\n    else:\n        _post = post\n    if dev is None:\n        _dev: CmpPrePostDevType = Infinity\n    else:\n        _dev = dev\n    if local is None:\n        _local: CmpLocalType = NegativeInfinity\n    else:\n        _local = tuple(((i, '') if isinstance(i, int) else (NegativeInfinity, i) for i in local))\n    return (epoch, _release, _pre, _post, _dev, _local)",
    "label": true
  },
  {
    "code": "def getworkerinfoline(node):\n    try:\n        return node._workerinfocache\n    except AttributeError:\n        d = node.workerinfo\n        ver = '%s.%s.%s' % d['version_info'][:3]\n        node._workerinfocache = s = '[{}] {} -- Python {} {}'.format(d['id'], d['sysplatform'], ver, d['executable'])\n        return s",
    "label": true
  },
  {
    "code": "def _remove_nested(pkg_roots: Dict[str, str]) -> Dict[str, str]:\n    output = dict(pkg_roots.copy())\n    for pkg, path in reversed(list(pkg_roots.items())):\n        if any((pkg != other and _is_nested(pkg, path, other, other_path) for other, other_path in pkg_roots.items())):\n            output.pop(pkg)\n    return output",
    "label": true
  },
  {
    "code": "def get_bin_prefix() -> str:\n    new = _sysconfig.get_bin_prefix()\n    if _USE_SYSCONFIG:\n        return new\n    old = _distutils.get_bin_prefix()\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key='bin_prefix'):\n        _log_context()\n    return old",
    "label": true
  },
  {
    "code": "def resolve_egg_link(path):\n    referenced_paths = non_empty_lines(path)\n    resolved_paths = (os.path.join(os.path.dirname(path), ref) for ref in referenced_paths)\n    dist_groups = map(find_distributions, resolved_paths)\n    return next(dist_groups, ())",
    "label": true
  },
  {
    "code": "def get_list_opt(options, optname, default=None):\n    val = options.get(optname, default)\n    if isinstance(val, str):\n        return val.split()\n    elif isinstance(val, (list, tuple)):\n        return list(val)\n    else:\n        raise OptionError('Invalid type %r for option %s; you must give a list value' % (val, optname))",
    "label": true
  },
  {
    "code": "def create_terminal_printer(color: bool, output: Optional[TextIO]=None, error: str='', success: str='') -> BasicPrinter:\n    if color and colorama_unavailable:\n        no_colorama_message = '\\nSorry, but to use --color (color_output) the colorama python package is required.\\n\\nReference: https://pypi.org/project/colorama/\\n\\nYou can either install it separately on your system or as the colors extra for isort. Ex: \\n\\n$ pip install isort[colors]\\n'\n        print(no_colorama_message, file=sys.stderr)\n        sys.exit(1)\n    if not colorama_unavailable:\n        colorama.init(strip=False)\n    return ColoramaPrinter(error, success, output) if color else BasicPrinter(error, success, output)",
    "label": true
  },
  {
    "code": "def get_choice_opt(options, optname, allowed, default=None, normcase=False):\n    string = options.get(optname, default)\n    if normcase:\n        string = string.lower()\n    if string not in allowed:\n        raise OptionError('Value for option %s must be one of %s' % (optname, ', '.join(map(str, allowed))))\n    return string",
    "label": true
  },
  {
    "code": "def _get_required(d: Dict[str, Any], expected_type: Type[T], key: str, default: Optional[T]=None) -> T:\n    value = _get(d, expected_type, key, default)\n    if value is None:\n        raise DirectUrlValidationError(f'{key} must have a value')\n    return value",
    "label": true
  },
  {
    "code": "def GetConsoleScreenBufferInfo(std_handle: wintypes.HANDLE) -> CONSOLE_SCREEN_BUFFER_INFO:\n    console_screen_buffer_info = CONSOLE_SCREEN_BUFFER_INFO()\n    _GetConsoleScreenBufferInfo(std_handle, byref(console_screen_buffer_info))\n    return console_screen_buffer_info",
    "label": true
  },
  {
    "code": "def _check_initialpaths_for_relpath(session: 'Session', path: Path) -> Optional[str]:\n    for initial_path in session._initialpaths:\n        if commonpath(path, initial_path) == initial_path:\n            rel = str(path.relative_to(initial_path))\n            return '' if rel == '.' else rel\n    return None",
    "label": true
  },
  {
    "code": "def _warn_accidental_env_marker_misconfig(label: str, orig_value: str, parsed: list):\n    if '\\n' in orig_value or len(parsed) != 2:\n        return\n    markers = marker_env().keys()\n    try:\n        req = Requirement(parsed[1])\n        if req.name in markers:\n            _AmbiguousMarker.emit(field=label, req=parsed[1])\n    except InvalidRequirement as ex:\n        if any((parsed[1].startswith(marker) for marker in markers)):\n            msg = _AmbiguousMarker.message(field=label, req=parsed[1])\n            raise InvalidRequirement(msg) from ex",
    "label": true
  },
  {
    "code": "def resolve_fixture_function(fixturedef: FixtureDef[FixtureValue], request: FixtureRequest) -> '_FixtureFunc[FixtureValue]':\n    fixturefunc = fixturedef.func\n    if fixturedef.unittest:\n        if request.instance is not None:\n            fixturefunc = fixturedef.func.__get__(request.instance)\n    elif request.instance is not None:\n        if hasattr(fixturefunc, '__self__') and (not isinstance(request.instance, fixturefunc.__self__.__class__)):\n            return fixturefunc\n        fixturefunc = getimfunc(fixturedef.func)\n        if fixturefunc != fixturedef.func:\n            fixturefunc = fixturefunc.__get__(request.instance)\n    return fixturefunc",
    "label": true
  },
  {
    "code": "def get_type_name(type_: Any) -> str:\n    name: str\n    for attrname in ('__name__', '_name', '__forward_arg__'):\n        candidate = getattr(type_, attrname, None)\n        if isinstance(candidate, str):\n            name = candidate\n            break\n    else:\n        origin = get_origin(type_)\n        candidate = getattr(origin, '_name', None)\n        if candidate is None:\n            candidate = type_.__class__.__name__.strip('_')\n        if isinstance(candidate, str):\n            name = candidate\n        else:\n            return '(unknown)'\n    args = get_args(type_)\n    if args:\n        if name == 'Literal':\n            formatted_args = ', '.join((repr(arg) for arg in args))\n        else:\n            formatted_args = ', '.join((get_type_name(arg) for arg in args))\n        name += f'[{formatted_args}]'\n    module = getattr(type_, '__module__', None)\n    if module and module not in (None, 'typing', 'typing_extensions', 'builtins'):\n        name = module + '.' + name\n    return name",
    "label": true
  },
  {
    "code": "def _has_data_descriptor(cls: nodes.ClassDef, attr: str) -> bool:\n    attributes = cls.getattr(attr)\n    for attribute in attributes:\n        try:\n            for inferred in attribute.infer():\n                if isinstance(inferred, astroid.Instance):\n                    try:\n                        inferred.getattr('__get__')\n                        inferred.getattr('__set__')\n                    except astroid.NotFoundError:\n                        continue\n                    else:\n                        return True\n        except astroid.InferenceError:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _read_config(config_file: Path) -> TestFileOptions:\n    config = configparser.ConfigParser()\n    config.read(str(config_file))\n    source_roots = config.get('testoptions', 'source_roots', fallback=None)\n    return {'source_roots': source_roots.split(',') if source_roots else [], 'output_formats': config.get('testoptions', 'output_formats', fallback='mmd').split(','), 'command_line_args': shlex.split(config.get('testoptions', 'command_line_args', fallback=''))}",
    "label": true
  },
  {
    "code": "def convert_mbcs(s):\n    dec = getattr(s, 'decode', None)\n    if dec is not None:\n        try:\n            s = dec('mbcs')\n        except UnicodeError:\n            pass\n    return s",
    "label": true
  },
  {
    "code": "def _clean_up_cache(module):\n    cached = module.__file__.split('.', 1)[0] + '.pyc'\n    cached = module.__cached__ if hasattr(module, '__cached__') else cached\n    pycache = os.path.join(os.path.dirname(module.__file__), '__pycache__')\n    for remove, file in [(os.remove, cached), (os.removedirs, pycache)]:\n        with suppress(OSError):\n            remove(file)",
    "label": true
  },
  {
    "code": "def select_wait_for_socket(sock, read=False, write=False, timeout=None):\n    if not read and (not write):\n        raise RuntimeError('must specify at least one of read=True, write=True')\n    rcheck = []\n    wcheck = []\n    if read:\n        rcheck.append(sock)\n    if write:\n        wcheck.append(sock)\n    fn = partial(select.select, rcheck, wcheck, wcheck)\n    rready, wready, xready = _retry_on_intr(fn, timeout)\n    return bool(rready or wready or xready)",
    "label": true
  },
  {
    "code": "def trace_parse_action(f: ParseAction) -> ParseAction:\n    f = _trim_arity(f)\n\n    def z(*paArgs):\n        thisFunc = f.__name__\n        s, l, t = paArgs[-3:]\n        if len(paArgs) > 3:\n            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc\n        sys.stderr.write(f'>>entering {thisFunc}(line: {line(l, s)!r}, {l}, {t!r})\\n')\n        try:\n            ret = f(*paArgs)\n        except Exception as exc:\n            sys.stderr.write(f'<<leaving {thisFunc} (exception: {exc})\\n')\n            raise\n        sys.stderr.write(f'<<leaving {thisFunc} (ret: {ret!r})\\n')\n        return ret\n    z.__name__ = f.__name__\n    return z",
    "label": true
  },
  {
    "code": "def skip_comment(src: str, pos: Pos) -> Pos:\n    try:\n        char: str | None = src[pos]\n    except IndexError:\n        char = None\n    if char == '#':\n        return skip_until(src, pos + 1, '\\n', error_on=ILLEGAL_COMMENT_CHARS, error_on_eof=False)\n    return pos",
    "label": true
  },
  {
    "code": "def _positional_parameters(method: nodes.FunctionDef) -> list[nodes.AssignName]:\n    positional = method.args.args\n    if method.is_bound() and method.type in {'classmethod', 'method'}:\n        positional = positional[1:]\n    return positional",
    "label": true
  },
  {
    "code": "def read_json(path):\n    with open(path, encoding='utf-8') as f:\n        return json.load(f)",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def alabel(label: str) -> bytes:\n    try:\n        label_bytes = label.encode('ascii')\n        ulabel(label_bytes)\n        if not valid_label_length(label_bytes):\n            raise IDNAError('Label too long')\n        return label_bytes\n    except UnicodeEncodeError:\n        pass\n    if not label:\n        raise IDNAError('No Input')\n    label = str(label)\n    check_label(label)\n    label_bytes = _punycode(label)\n    label_bytes = _alabel_prefix + label_bytes\n    if not valid_label_length(label_bytes):\n        raise IDNAError('Label too long')\n    return label_bytes",
    "label": true
  },
  {
    "code": "def _raise_for_invalid_entrypoint(specification: str) -> None:\n    entry = get_export_entry(specification)\n    if entry is not None and entry.suffix is None:\n        raise MissingCallableSuffix(str(entry))",
    "label": true
  },
  {
    "code": "def _is_compatible(name: str, arch: str, version: _GLibCVersion) -> bool:\n    sys_glibc = _get_glibc_version()\n    if sys_glibc < version:\n        return False\n    try:\n        import _manylinux\n    except ImportError:\n        return True\n    if hasattr(_manylinux, 'manylinux_compatible'):\n        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n        if result is not None:\n            return bool(result)\n        return True\n    if version == _GLibCVersion(2, 5):\n        if hasattr(_manylinux, 'manylinux1_compatible'):\n            return bool(_manylinux.manylinux1_compatible)\n    if version == _GLibCVersion(2, 12):\n        if hasattr(_manylinux, 'manylinux2010_compatible'):\n            return bool(_manylinux.manylinux2010_compatible)\n    if version == _GLibCVersion(2, 17):\n        if hasattr(_manylinux, 'manylinux2014_compatible'):\n            return bool(_manylinux.manylinux2014_compatible)\n    return True",
    "label": true
  },
  {
    "code": "def module_in_path(modname: str, path: str | Iterable[str]) -> bool:\n    modname = modname.split('.')[0]\n    try:\n        filename = file_from_modpath([modname])\n    except ImportError:\n        return False\n    if filename is None:\n        return False\n    filename = _normalize_path(filename)\n    if isinstance(path, str):\n        return filename.startswith(_cache_normalize_path(path))\n    return any((filename.startswith(_cache_normalize_path(entry)) for entry in path))",
    "label": true
  },
  {
    "code": "def get_url_scheme(url: str) -> Optional[str]:\n    if ':' not in url:\n        return None\n    return url.split(':', 1)[0].lower()",
    "label": true
  },
  {
    "code": "def _has_init(directory: str) -> str | None:\n    mod_or_pack = os.path.join(directory, '__init__')\n    for ext in PY_SOURCE_EXTS + ('pyc', 'pyo'):\n        if os.path.exists(mod_or_pack + '.' + ext):\n            return mod_or_pack + '.' + ext\n    return None",
    "label": true
  },
  {
    "code": "def check_list_path_option(options: Values) -> None:\n    if options.path and (options.user or options.local):\n        raise CommandError(\"Cannot combine '--path' with '--user' or '--local'\")",
    "label": true
  },
  {
    "code": "def finder_for_path(path):\n    result = None\n    pkgutil.get_importer(path)\n    loader = sys.path_importer_cache.get(path)\n    finder = _finder_registry.get(type(loader))\n    if finder:\n        module = _dummy_module\n        module.__file__ = os.path.join(path, '')\n        module.__loader__ = loader\n        result = finder(module)\n    return result",
    "label": true
  },
  {
    "code": "def get_abi3_suffix():\n    for suffix in EXTENSION_SUFFIXES:\n        if '.abi3' in suffix:\n            return suffix\n        elif suffix == '.pyd':\n            return suffix",
    "label": true
  },
  {
    "code": "def get_src_prefix() -> str:\n    if running_under_virtualenv():\n        src_prefix = os.path.join(sys.prefix, 'src')\n    else:\n        try:\n            src_prefix = os.path.join(os.getcwd(), 'src')\n        except OSError:\n            sys.exit('The folder you are executing pip from can no longer be found.')\n    return os.path.abspath(src_prefix)",
    "label": true
  },
  {
    "code": "def _is_linux_i686(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.I386)",
    "label": true
  },
  {
    "code": "def last(iterable, default=_marker):\n    try:\n        if isinstance(iterable, Sequence):\n            return iterable[-1]\n        elif hasattr(iterable, '__reversed__') and hexversion != 50856176:\n            return next(reversed(iterable))\n        else:\n            return deque(iterable, maxlen=1)[-1]\n    except (IndexError, TypeError, StopIteration):\n        if default is _marker:\n            raise ValueError('last() was called on an empty iterable, and no default was provided.')\n        return default",
    "label": true
  },
  {
    "code": "def _parse_local_version(local: str) -> Optional[LocalType]:\n    if local is not None:\n        return tuple((part.lower() if not part.isdigit() else int(part) for part in _local_version_separators.split(local)))\n    return None",
    "label": true
  },
  {
    "code": "def _temporary_keychain():\n    random_bytes = os.urandom(40)\n    filename = base64.b16encode(random_bytes[:8]).decode('utf-8')\n    password = base64.b16encode(random_bytes[8:])\n    tempdirectory = tempfile.mkdtemp()\n    keychain_path = os.path.join(tempdirectory, filename).encode('utf-8')\n    keychain = Security.SecKeychainRef()\n    status = Security.SecKeychainCreate(keychain_path, len(password), password, False, None, ctypes.byref(keychain))\n    _assert_no_error(status)\n    return (keychain, tempdirectory)",
    "label": true
  },
  {
    "code": "def sys_tags(*, warn: bool=False) -> Iterator[Tag]:\n    interp_name = interpreter_name()\n    if interp_name == 'cp':\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n    if interp_name == 'pp':\n        interp = 'pp3'\n    elif interp_name == 'cp':\n        interp = 'cp' + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)",
    "label": true
  },
  {
    "code": "def safe_exists(p: Path) -> bool:\n    try:\n        return p.exists()\n    except (ValueError, OSError):\n        return False",
    "label": true
  },
  {
    "code": "def _parse_glibc_version(version_str: str) -> Tuple[int, int]:\n    m = re.match('(?P<major>[0-9]+)\\\\.(?P<minor>[0-9]+)', version_str)\n    if not m:\n        warnings.warn(f'Expected glibc version with 2 components major.minor, got: {version_str}', RuntimeWarning)\n        return (-1, -1)\n    return (int(m.group('major')), int(m.group('minor')))",
    "label": true
  },
  {
    "code": "def filter_traceback(entry: TracebackEntry) -> bool:\n    raw_filename = entry.frame.code.raw.co_filename\n    is_generated = '<' in raw_filename and '>' in raw_filename\n    if is_generated:\n        return False\n    p = Path(entry.path)\n    parents = p.parents\n    if _PLUGGY_DIR in parents:\n        return False\n    if _PYTEST_DIR in parents:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def _check_mode_str(mode: Any) -> bool:\n    if not isinstance(mode, str):\n        return False\n    modes = set(mode)\n    _mode = 'rwatb+Ux'\n    creating = 'x' in modes\n    if modes - set(_mode) or len(mode) > len(modes):\n        return False\n    reading = 'r' in modes\n    writing = 'w' in modes\n    appending = 'a' in modes\n    text = 't' in modes\n    binary = 'b' in modes\n    if 'U' in modes:\n        if writing or appending or creating:\n            return False\n        reading = True\n    if text and binary:\n        return False\n    total = reading + writing + appending + creating\n    if total > 1:\n        return False\n    if not (reading or writing or appending or creating):\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def is_ipv4_address(string_ip):\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def load_formatter_from_file(filename, formattername='CustomFormatter', **options):\n    try:\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        if formattername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' % (formattername, filename))\n        formatter_class = custom_namespace[formattername]\n        return formatter_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom formatter: %s' % err)",
    "label": true
  },
  {
    "code": "def display_path(path: str) -> str:\n    path = os.path.normcase(os.path.abspath(path))\n    if path.startswith(os.getcwd() + os.path.sep):\n        path = '.' + path[len(os.getcwd()):]\n    return path",
    "label": true
  },
  {
    "code": "def get_best_invocation_for_this_pip() -> str:\n    binary_directory = 'Scripts' if WINDOWS else 'bin'\n    binary_prefix = os.path.join(sys.prefix, binary_directory)\n    path_parts = os.path.normcase(os.environ.get('PATH', '')).split(os.pathsep)\n    exe_are_in_PATH = os.path.normcase(binary_prefix) in path_parts\n    if exe_are_in_PATH:\n        for exe_name in _EXECUTABLE_NAMES:\n            found_executable = shutil.which(exe_name)\n            binary_executable = os.path.join(binary_prefix, exe_name)\n            if found_executable and os.path.exists(binary_executable) and os.path.samefile(found_executable, binary_executable):\n                return exe_name\n    return f'{get_best_invocation_for_this_python()} -m pip'",
    "label": true
  },
  {
    "code": "def parse_requirements(filename: str, session: PipSession, finder: Optional['PackageFinder']=None, options: Optional[optparse.Values]=None, constraint: bool=False) -> Generator[ParsedRequirement, None, None]:\n    line_parser = get_line_parser(finder)\n    parser = RequirementsFileParser(session, line_parser)\n    for parsed_line in parser.parse(filename, constraint):\n        parsed_req = handle_line(parsed_line, options=options, finder=finder, session=session)\n        if parsed_req is not None:\n            yield parsed_req",
    "label": true
  },
  {
    "code": "def module_key(module_name: str, config: Config, sub_imports: bool=False, ignore_case: bool=False, section_name: Optional[Any]=None, straight_import: Optional[bool]=False) -> str:\n    match = re.match('^(\\\\.+)\\\\s*(.*)', module_name)\n    if match:\n        sep = ' ' if config.reverse_relative else '_'\n        module_name = sep.join(match.groups())\n    prefix = ''\n    if ignore_case:\n        module_name = str(module_name).lower()\n    else:\n        module_name = str(module_name)\n    if sub_imports and config.order_by_type:\n        if module_name in config.constants:\n            prefix = 'A'\n        elif module_name in config.classes:\n            prefix = 'B'\n        elif module_name in config.variables:\n            prefix = 'C'\n        elif module_name.isupper() and len(module_name) > 1:\n            prefix = 'A'\n        elif module_name in config.classes or module_name[0:1].isupper():\n            prefix = 'B'\n        else:\n            prefix = 'C'\n    if not config.case_sensitive:\n        module_name = module_name.lower()\n    length_sort = config.length_sort or (config.length_sort_straight and straight_import) or str(section_name).lower() in config.length_sort_sections\n    _length_sort_maybe = str(len(module_name)) + ':' + module_name if length_sort else module_name\n    return f\"{module_name in config.force_to_top and 'A' or 'B'}{prefix}{_length_sort_maybe}\"",
    "label": true
  },
  {
    "code": "def find_lexer_class_by_name(_alias):\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name]\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls\n    raise ClassNotFound('no lexer for alias %r found' % _alias)",
    "label": true
  },
  {
    "code": "def install_req_from_parsed_requirement(parsed_req: ParsedRequirement, isolated: bool=False, use_pep517: Optional[bool]=None, user_supplied: bool=False, config_settings: Optional[Dict[str, Union[str, List[str]]]]=None) -> InstallRequirement:\n    if parsed_req.is_editable:\n        req = install_req_from_editable(parsed_req.requirement, comes_from=parsed_req.comes_from, use_pep517=use_pep517, constraint=parsed_req.constraint, isolated=isolated, user_supplied=user_supplied, config_settings=config_settings)\n    else:\n        req = install_req_from_line(parsed_req.requirement, comes_from=parsed_req.comes_from, use_pep517=use_pep517, isolated=isolated, global_options=parsed_req.options.get('global_options', []) if parsed_req.options else [], hash_options=parsed_req.options.get('hashes', {}) if parsed_req.options else {}, constraint=parsed_req.constraint, line_source=parsed_req.line_source, user_supplied=user_supplied, config_settings=config_settings)\n    return req",
    "label": true
  },
  {
    "code": "def _import_module(name):\n    __import__(name)\n    return sys.modules[name]",
    "label": true
  },
  {
    "code": "def map_except(function, iterable, *exceptions):\n    for item in iterable:\n        try:\n            yield function(item)\n        except exceptions:\n            pass",
    "label": true
  },
  {
    "code": "def build_editable(wheel_directory, config_settings, metadata_directory=None):\n    backend = _build_backend()\n    try:\n        hook = backend.build_editable\n    except AttributeError:\n        raise HookMissing()\n    else:\n        prebuilt_whl = _find_already_built_wheel(metadata_directory)\n        if prebuilt_whl:\n            shutil.copy2(prebuilt_whl, wheel_directory)\n            return os.path.basename(prebuilt_whl)\n        return hook(wheel_directory, config_settings, metadata_directory)",
    "label": true
  },
  {
    "code": "def has_known_bases(klass, context: InferenceContext | None=None) -> bool:\n    try:\n        return klass._all_bases_known\n    except AttributeError:\n        pass\n    for base in klass.bases:\n        result = safe_infer(base, context=context)\n        if not isinstance(result, scoped_nodes.ClassDef) or result is klass or (not has_known_bases(result, context=context)):\n            klass._all_bases_known = False\n            return False\n    klass._all_bases_known = True\n    return True",
    "label": true
  },
  {
    "code": "def info():\n    try:\n        platform_info = {'system': platform.system(), 'release': platform.release()}\n    except OSError:\n        platform_info = {'system': 'Unknown', 'release': 'Unknown'}\n    implementation_info = _implementation()\n    urllib3_info = {'version': urllib3.__version__}\n    charset_normalizer_info = {'version': None}\n    chardet_info = {'version': None}\n    if charset_normalizer:\n        charset_normalizer_info = {'version': charset_normalizer.__version__}\n    if chardet:\n        chardet_info = {'version': chardet.__version__}\n    pyopenssl_info = {'version': None, 'openssl_version': ''}\n    if OpenSSL:\n        pyopenssl_info = {'version': OpenSSL.__version__, 'openssl_version': f'{OpenSSL.SSL.OPENSSL_VERSION_NUMBER:x}'}\n    cryptography_info = {'version': getattr(cryptography, '__version__', '')}\n    idna_info = {'version': getattr(idna, '__version__', '')}\n    system_ssl = ssl.OPENSSL_VERSION_NUMBER\n    system_ssl_info = {'version': f'{system_ssl:x}' if system_ssl is not None else ''}\n    return {'platform': platform_info, 'implementation': implementation_info, 'system_ssl': system_ssl_info, 'using_pyopenssl': pyopenssl is not None, 'using_charset_normalizer': chardet is None, 'pyOpenSSL': pyopenssl_info, 'urllib3': urllib3_info, 'chardet': chardet_info, 'charset_normalizer': charset_normalizer_info, 'cryptography': cryptography_info, 'idna': idna_info, 'requests': {'version': requests_version}}",
    "label": true
  },
  {
    "code": "def infer_typing_alias(node: Call, ctx: context.InferenceContext | None=None) -> Iterator[ClassDef]:\n    if not isinstance(node.parent, Assign) or not len(node.parent.targets) == 1 or (not isinstance(node.parent.targets[0], AssignName)):\n        raise UseInferenceDefault\n    try:\n        res = next(node.args[0].infer(context=ctx))\n    except StopIteration as e:\n        raise InferenceError(node=node.args[0], context=ctx) from e\n    assign_name = node.parent.targets[0]\n    class_def = ClassDef(name=assign_name.name, lineno=assign_name.lineno, col_offset=assign_name.col_offset, parent=node.parent)\n    if isinstance(res, ClassDef):\n        class_def.postinit(bases=[res], body=[], decorators=None)\n    maybe_type_var = node.args[1]\n    if not PY39_PLUS and (not (isinstance(maybe_type_var, Tuple) and (not maybe_type_var.elts))) or (PY39_PLUS and isinstance(maybe_type_var, Const) and (maybe_type_var.value > 0)):\n        func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n        class_def.locals['__class_getitem__'] = [func_to_add]\n    else:\n        _forbid_class_getitem_access(class_def)\n    return iter([class_def])",
    "label": true
  },
  {
    "code": "def assign_params(func, namespace):\n    sig = inspect.signature(func)\n    params = sig.parameters.keys()\n    call_ns = {k: namespace[k] for k in params if k in namespace}\n    return functools.partial(func, **call_ns)",
    "label": true
  },
  {
    "code": "def get_lexer_for_filename(_fn, code=None, **options):\n    res = find_lexer_class_for_filename(_fn, code)\n    if not res:\n        raise ClassNotFound('no lexer for filename %r found' % _fn)\n    return res(**options)",
    "label": true
  },
  {
    "code": "def extract_by_key(d, keys):\n    if isinstance(keys, string_types):\n        keys = keys.split()\n    result = {}\n    for key in keys:\n        if key in d:\n            result[key] = d[key]\n    return result",
    "label": true
  },
  {
    "code": "def naturally(to_sort: Iterable[str], key: Optional[Callable[[str], Any]]=None, reverse: bool=False) -> List[str]:\n    if key is None:\n        key_callback = _natural_keys\n    else:\n\n        def key_callback(text: str) -> List[Any]:\n            return _natural_keys(key(text))\n    return sorted(to_sort, key=key_callback, reverse=reverse)",
    "label": true
  },
  {
    "code": "def _get_user_media_dir(env_var: str, fallback_tilde_path: str) -> str:\n    media_dir = _get_user_dirs_folder(env_var)\n    if media_dir is None:\n        media_dir = os.environ.get(env_var, '').strip()\n        if not media_dir:\n            media_dir = os.path.expanduser(fallback_tilde_path)\n    return media_dir",
    "label": true
  },
  {
    "code": "def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n    formats = [cpu_arch]\n    if cpu_arch == 'x86_64':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat64', 'fat32'])\n    elif cpu_arch == 'i386':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat32', 'fat'])\n    elif cpu_arch == 'ppc64':\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append('fat64')\n    elif cpu_arch == 'ppc':\n        if version > (10, 6):\n            return []\n        formats.extend(['fat32', 'fat'])\n    if cpu_arch in {'arm64', 'x86_64'}:\n        formats.append('universal2')\n    if cpu_arch in {'x86_64', 'i386', 'ppc64', 'ppc', 'intel'}:\n        formats.append('universal')\n    return formats",
    "label": true
  },
  {
    "code": "def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split('.')\n    assert urllib3_version != ['dev']\n    if len(urllib3_version) == 2:\n        urllib3_version.append('0')\n    major, minor, patch = urllib3_version\n    major, minor, patch = (int(major), int(minor), int(patch))\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n    if chardet_version:\n        major, minor, patch = chardet_version.split('.')[:3]\n        major, minor, patch = (int(major), int(minor), int(patch))\n        assert (3, 0, 2) <= (major, minor, patch) < (6, 0, 0)\n    elif charset_normalizer_version:\n        major, minor, patch = charset_normalizer_version.split('.')[:3]\n        major, minor, patch = (int(major), int(minor), int(patch))\n        assert (2, 0, 0) <= (major, minor, patch) < (4, 0, 0)\n    else:\n        raise Exception('You need either charset_normalizer or chardet installed')",
    "label": true
  },
  {
    "code": "def get_distribution(dist):\n    if isinstance(dist, str):\n        dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement):\n        dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError('Expected string, Requirement, or Distribution', dist)\n    return dist",
    "label": true
  },
  {
    "code": "def _get_user_media_dir(env_var: str, fallback_tilde_path: str) -> str:\n    media_dir = _get_user_dirs_folder(env_var)\n    if media_dir is None:\n        media_dir = os.environ.get(env_var, '').strip()\n        if not media_dir:\n            media_dir = os.path.expanduser(fallback_tilde_path)\n    return media_dir",
    "label": true
  },
  {
    "code": "def has_changed(*args, **kwds):\n    kwds['simple'] = True\n    return whats_changed(*args, **kwds)",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    config.addinivalue_line('markers', \"parametrize(argnames, argvalues): call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/stable/how-to/parametrize.html for more info and examples.\")\n    config.addinivalue_line('markers', 'usefixtures(fixturename1, fixturename2, ...): mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/stable/explanation/fixtures.html#usefixtures ')",
    "label": true
  },
  {
    "code": "def count_cycle(iterable, n=None):\n    iterable = tuple(iterable)\n    if not iterable:\n        return iter(())\n    counter = count() if n is None else range(n)\n    return ((i, item) for i in counter for item in iterable)",
    "label": true
  },
  {
    "code": "def _looks_like_decorated_with_six_add_metaclass(node) -> bool:\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, nodes.Call):\n            continue\n        if decorator.func.as_string() == SIX_ADD_METACLASS:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _nose_tools_functions():\n    module = _BUILDER.string_build(textwrap.dedent('\\n    import unittest\\n\\n    class Test(unittest.TestCase):\\n        pass\\n    a = Test()\\n    '))\n    try:\n        case = next(module['a'].infer())\n    except (InferenceError, StopIteration):\n        return\n    for method in case.methods():\n        if method.name.startswith('assert') and '_' not in method.name:\n            pep8_name = _pep8(method.name)\n            yield (pep8_name, astroid.BoundMethod(method, case))\n        if method.name == 'assertEqual':\n            yield ('assert_equals', astroid.BoundMethod(method, case))",
    "label": true
  },
  {
    "code": "def _strictly_valid_num(n):\n    n = n.strip()\n    if not n:\n        return False\n    if n[0] == '_':\n        return False\n    if n[-1] == '_':\n        return False\n    if '_.' in n or '._' in n:\n        return False\n    if len(n) == 1:\n        return True\n    if n[0] == '0' and n[1] not in ['.', 'o', 'b', 'x']:\n        return False\n    if n[0] == '+' or n[0] == '-':\n        n = n[1:]\n        if len(n) > 1 and n[0] == '0' and (n[1] != '.'):\n            return False\n    if '__' in n:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def file_info_from_modpath(modpath: list[str], path: Sequence[str] | None=None, context_file: str | None=None) -> spec.ModuleSpec:\n    if context_file is not None:\n        context: str | None = os.path.dirname(context_file)\n    else:\n        context = context_file\n    if modpath[0] == 'xml':\n        try:\n            return _spec_from_modpath(['_xmlplus'] + modpath[1:], path, context)\n        except ImportError:\n            return _spec_from_modpath(modpath, path, context)\n    elif modpath == ['os', 'path']:\n        return spec.ModuleSpec(name='os.path', location=os.path.__file__, type=spec.ModuleType.PY_SOURCE)\n    return _spec_from_modpath(modpath, path, context)",
    "label": true
  },
  {
    "code": "def blend_rgb(color1: ColorTriplet, color2: ColorTriplet, cross_fade: float=0.5) -> ColorTriplet:\n    r1, g1, b1 = color1\n    r2, g2, b2 = color2\n    new_color = ColorTriplet(int(r1 + (r2 - r1) * cross_fade), int(g1 + (g2 - g1) * cross_fade), int(b1 + (b2 - b1) * cross_fade))\n    return new_color",
    "label": true
  },
  {
    "code": "def reraise(tp: type[BaseException] | None, value: BaseException, tb: TracebackType | None=None) -> typing.NoReturn:\n    try:\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n    finally:\n        value = None\n        tb = None",
    "label": true
  },
  {
    "code": "def normalize_paths(value, parent=os.curdir):\n    if not value:\n        return []\n    if isinstance(value, list):\n        return value\n    paths = []\n    for path in value.split(','):\n        path = path.strip()\n        if '/' in path:\n            path = os.path.abspath(os.path.join(parent, path))\n        paths.append(path.rstrip('/'))\n    return paths",
    "label": true
  },
  {
    "code": "def bygroups(*args):\n\n    def callback(lexer, match, ctx=None):\n        for i, action in enumerate(args):\n            if action is None:\n                continue\n            elif type(action) is _TokenType:\n                data = match.group(i + 1)\n                if data:\n                    yield (match.start(i + 1), action, data)\n            else:\n                data = match.group(i + 1)\n                if data is not None:\n                    if ctx:\n                        ctx.pos = match.start(i + 1)\n                    for item in action(lexer, _PseudoMatch(match.start(i + 1), data), ctx):\n                        if item:\n                            yield item\n        if ctx:\n            ctx.pos = match.end()\n    return callback",
    "label": true
  },
  {
    "code": "def _param_memo(f: t.Callable[..., t.Any], param: Parameter) -> None:\n    if isinstance(f, Command):\n        f.params.append(param)\n    else:\n        if not hasattr(f, '__click_params__'):\n            f.__click_params__ = []\n        f.__click_params__.append(param)",
    "label": true
  },
  {
    "code": "def deselect_by_keyword(items: 'List[Item]', config: Config) -> None:\n    keywordexpr = config.option.keyword.lstrip()\n    if not keywordexpr:\n        return\n    expr = _parse_expression(keywordexpr, \"Wrong expression passed to '-k'\")\n    remaining = []\n    deselected = []\n    for colitem in items:\n        if not expr.evaluate(KeywordMatcher.from_item(colitem)):\n            deselected.append(colitem)\n        else:\n            remaining.append(colitem)\n    if deselected:\n        config.hook.pytest_deselected(items=deselected)\n        items[:] = remaining",
    "label": true
  },
  {
    "code": "def canonic_data_files(data_files: Union[list, dict], root_dir: Optional[_Path]=None) -> List[Tuple[str, List[str]]]:\n    if isinstance(data_files, list):\n        return data_files\n    return [(dest, glob_relative(patterns, root_dir)) for dest, patterns in data_files.items()]",
    "label": true
  },
  {
    "code": "def get_all_filters():\n    yield from FILTERS\n    for name, _ in find_plugin_filters():\n        yield name",
    "label": true
  },
  {
    "code": "def interleave_longest(*iterables):\n    i = chain.from_iterable(zip_longest(*iterables, fillvalue=_marker))\n    return (x for x in i if x is not _marker)",
    "label": true
  },
  {
    "code": "def _req_set_item_sorter(item: Tuple[str, InstallRequirement], weights: Dict[Optional[str], int]) -> Tuple[int, str]:\n    name = canonicalize_name(item[0])\n    return (weights[name], name)",
    "label": true
  },
  {
    "code": "def inject_into_urllib3():\n    _validate_dependencies_met()\n    util.SSLContext = PyOpenSSLContext\n    util.ssl_.SSLContext = PyOpenSSLContext\n    util.HAS_SNI = HAS_SNI\n    util.ssl_.HAS_SNI = HAS_SNI\n    util.IS_PYOPENSSL = True\n    util.ssl_.IS_PYOPENSSL = True",
    "label": true
  },
  {
    "code": "def pprint(_object: Any, *, console: Optional['Console']=None, indent_guides: bool=True, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> None:\n    _console = get_console() if console is None else console\n    _console.print(Pretty(_object, max_length=max_length, max_string=max_string, max_depth=max_depth, indent_guides=indent_guides, expand_all=expand_all, overflow='ignore'), soft_wrap=True)",
    "label": true
  },
  {
    "code": "def normalized_name(dist: Distribution) -> Optional[str]:\n    try:\n        return dist._normalized_name\n    except AttributeError:\n        from . import Prepared\n        return Prepared.normalize(getattr(dist, 'name', None) or dist.metadata['Name'])",
    "label": true
  },
  {
    "code": "def fetch_build_egg(dist, req):\n    _DeprecatedInstaller.emit()\n    _warn_wheel_not_available(dist)\n    return _fetch_build_egg_no_warn(dist, req)",
    "label": true
  },
  {
    "code": "def _tree_to_bytes_helper(tree: HuffmanTree) -> list:\n    data = []\n    left_leaf = 0\n    left_info = tree.left.symbol\n    if not tree.left.is_leaf():\n        left_leaf = 1\n        left_info = tree.left.number\n        data.extend(_tree_to_bytes_helper(tree.left))\n    right_leaf = 0\n    right_info = tree.right.symbol\n    if not tree.right.is_leaf():\n        right_leaf = 1\n        right_info = tree.right.number\n        data.extend(_tree_to_bytes_helper(tree.right))\n    data.extend([left_leaf, left_info, right_leaf, right_info])\n    return data",
    "label": true
  },
  {
    "code": "def get_binary_stderr() -> t.BinaryIO:\n    writer = _find_binary_writer(sys.stderr)\n    if writer is None:\n        raise RuntimeError('Was not able to determine binary stream for sys.stderr.')\n    return writer",
    "label": true
  },
  {
    "code": "def before_sleep_log(logger: 'logging.Logger', log_level: int, exc_info: bool=False) -> typing.Callable[['RetryCallState'], None]:\n\n    def log_it(retry_state: 'RetryCallState') -> None:\n        local_exc_info: BaseException | bool | None\n        if retry_state.outcome is None:\n            raise RuntimeError('log_it() called before outcome was set')\n        if retry_state.next_action is None:\n            raise RuntimeError('log_it() called before next_action was set')\n        if retry_state.outcome.failed:\n            ex = retry_state.outcome.exception()\n            verb, value = ('raised', f'{ex.__class__.__name__}: {ex}')\n            if exc_info:\n                local_exc_info = retry_state.outcome.exception()\n            else:\n                local_exc_info = False\n        else:\n            verb, value = ('returned', retry_state.outcome.result())\n            local_exc_info = False\n        if retry_state.fn is None:\n            fn_name = '<unknown>'\n        else:\n            fn_name = _utils.get_callback_name(retry_state.fn)\n        logger.log(log_level, f'Retrying {fn_name} in {retry_state.next_action.sleep} seconds as it {verb} {value}.', exc_info=local_exc_info)\n    return log_it",
    "label": true
  },
  {
    "code": "def find_plugin_filters():\n    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):\n        yield (entrypoint.name, entrypoint.load())",
    "label": true
  },
  {
    "code": "def make_option_group(group: Dict[str, Any], parser: ConfigOptionParser) -> OptionGroup:\n    option_group = OptionGroup(parser, group['name'])\n    for option in group['options']:\n        option_group.add_option(option())\n    return option_group",
    "label": true
  },
  {
    "code": "def sample(iterable, k, weights=None):\n    if k == 0:\n        return []\n    iterable = iter(iterable)\n    if weights is None:\n        return _sample_unweighted(iterable, k)\n    else:\n        weights = iter(weights)\n        return _sample_weighted(iterable, k, weights)",
    "label": true
  },
  {
    "code": "def get_bin_prefix() -> str:\n    if sys.platform[:6] == 'darwin' and sys.prefix[:16] == '/System/Library/':\n        return '/usr/local/bin'\n    return sysconfig.get_paths()['scripts']",
    "label": true
  },
  {
    "code": "def skip_comments_and_array_ws(src: str, pos: Pos) -> Pos:\n    while True:\n        pos_before_skip = pos\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        pos = skip_comment(src, pos)\n        if pos == pos_before_skip:\n            return pos",
    "label": true
  },
  {
    "code": "def build_frequency_dict(text: bytes) -> dict[int, int]:\n    out = {}\n    for byte in text:\n        if byte not in out:\n            out[byte] = 0\n        out[byte] += 1\n    return out",
    "label": true
  },
  {
    "code": "def was_installed_by_pip(pkg: str) -> bool:\n    dist = get_default_environment().get_distribution(pkg)\n    return dist is not None and 'pip' == dist.installer",
    "label": true
  },
  {
    "code": "def _assert_no_error(error, exception_class=None):\n    if error == 0:\n        return\n    cf_error_string = Security.SecCopyErrorMessageString(error, None)\n    output = _cf_string_to_unicode(cf_error_string)\n    CoreFoundation.CFRelease(cf_error_string)\n    if output is None or output == u'':\n        output = u'OSStatus %s' % error\n    if exception_class is None:\n        exception_class = ssl.SSLError\n    raise exception_class(output)",
    "label": true
  },
  {
    "code": "def pytest_fixture_setup(fixturedef: FixtureDef[FixtureValue], request: SubRequest) -> FixtureValue:\n    kwargs = {}\n    for argname in fixturedef.argnames:\n        fixdef = request._get_active_fixturedef(argname)\n        assert fixdef.cached_result is not None\n        result, arg_cache_key, exc = fixdef.cached_result\n        request._check_scope(argname, request._scope, fixdef._scope)\n        kwargs[argname] = result\n    fixturefunc = resolve_fixture_function(fixturedef, request)\n    my_cache_key = fixturedef.cache_key(request)\n    try:\n        result = call_fixture_func(fixturefunc, request, kwargs)\n    except TEST_OUTCOME:\n        exc_info = sys.exc_info()\n        assert exc_info[0] is not None\n        if isinstance(exc_info[1], skip.Exception) and (not fixturefunc.__name__.startswith('xunit_setup')):\n            exc_info[1]._use_item_location = True\n        fixturedef.cached_result = (None, my_cache_key, exc_info)\n        raise\n    fixturedef.cached_result = (result, my_cache_key, None)\n    return result",
    "label": true
  },
  {
    "code": "def get_abi_tag():\n    soabi = sysconfig.get_config_var('SOABI')\n    impl = tags.interpreter_name()\n    if not soabi and impl in ('cp', 'pp') and hasattr(sys, 'maxunicode'):\n        d = ''\n        m = ''\n        u = ''\n        if get_flag('Py_DEBUG', hasattr(sys, 'gettotalrefcount'), warn=impl == 'cp'):\n            d = 'd'\n        if get_flag('WITH_PYMALLOC', impl == 'cp', warn=impl == 'cp' and sys.version_info < (3, 8)) and sys.version_info < (3, 8):\n            m = 'm'\n        abi = f'{impl}{tags.interpreter_version()}{d}{m}{u}'\n    elif soabi and impl == 'cp':\n        abi = 'cp' + soabi.split('-')[1]\n    elif soabi and impl == 'pp':\n        abi = '-'.join(soabi.split('-')[:2])\n        abi = abi.replace('.', '_').replace('-', '_')\n    elif soabi and impl == 'graalpy':\n        abi = '-'.join(soabi.split('-')[:3])\n        abi = abi.replace('.', '_').replace('-', '_')\n    elif soabi:\n        abi = soabi.replace('.', '_').replace('-', '_')\n    else:\n        abi = None\n    return abi",
    "label": true
  },
  {
    "code": "def parse_assertions(obj: Any, parse_token: str='Precondition') -> List[str]:\n    if hasattr(obj, 'doc_node'):\n        docstring = obj.doc_node.value\n    else:\n        docstring = getattr(obj, '__doc__') or ''\n    lines = [line.strip() for line in docstring.split('\\n')]\n    assertion_lines = [i for i, line in enumerate(lines) if line.lower().startswith(parse_token.lower())]\n    if assertion_lines == []:\n        return []\n    first = assertion_lines[0]\n    if lines[first].startswith(parse_token + ':'):\n        return [lines[first][len(parse_token + ':'):].strip()]\n    elif lines[first].startswith(parse_token + 's:'):\n        assertions = []\n        for line in lines[first + 1:]:\n            if line.startswith('-'):\n                assertion = line[1:].strip()\n                if hasattr(obj, '__qualname__'):\n                    _debug(f'Adding assertion to {obj.__qualname__}: {assertion}')\n                assertions.append(assertion)\n            elif line != '':\n                break\n        return assertions\n    else:\n        return []",
    "label": true
  },
  {
    "code": "def _wrapper(args: Optional[List[str]]=None) -> int:\n    sys.stderr.write(\"WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\\n\")\n    return main(args)",
    "label": true
  },
  {
    "code": "def inference_tip(infer_function: InferFn, raise_on_overwrite: bool=False) -> InferFn:\n\n    def transform(node: NodeNG, infer_function: InferFn=infer_function) -> NodeNG:\n        if raise_on_overwrite and node._explicit_inference is not None and (node._explicit_inference is not infer_function):\n            raise InferenceOverwriteError('Inference already set to {existing_inference}. Trying to overwrite with {new_inference} for {node}'.format(existing_inference=infer_function, new_inference=node._explicit_inference, node=node))\n        node._explicit_inference = _inference_tip_cached(infer_function)\n        return node\n    return transform",
    "label": true
  },
  {
    "code": "def auto_decode(data: bytes) -> str:\n    for bom, encoding in BOMS:\n        if data.startswith(bom):\n            return data[len(bom):].decode(encoding)\n    for line in data.split(b'\\n')[:2]:\n        if line[0:1] == b'#' and ENCODING_RE.search(line):\n            result = ENCODING_RE.search(line)\n            assert result is not None\n            encoding = result.groups()[0].decode('ascii')\n            return data.decode(encoding)\n    return data.decode(locale.getpreferredencoding(False) or sys.getdefaultencoding())",
    "label": true
  },
  {
    "code": "def get_terminal_width() -> int:\n    width, _ = shutil.get_terminal_size(fallback=(80, 24))\n    if width < 40:\n        width = 80\n    return width",
    "label": true
  },
  {
    "code": "def to_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n    if isinstance(value, Mapping):\n        value = value.items()\n    return list(value)",
    "label": true
  },
  {
    "code": "def format_for_columns(pkgs: '_ProcessedDists', options: Values) -> Tuple[List[List[str]], List[str]]:\n    header = ['Package', 'Version']\n    running_outdated = options.outdated\n    if running_outdated:\n        header.extend(['Latest', 'Type'])\n    has_editables = any((x.editable for x in pkgs))\n    if has_editables:\n        header.append('Editable project location')\n    if options.verbose >= 1:\n        header.append('Location')\n    if options.verbose >= 1:\n        header.append('Installer')\n    data = []\n    for proj in pkgs:\n        row = [proj.raw_name, str(proj.version)]\n        if running_outdated:\n            row.append(str(proj.latest_version))\n            row.append(proj.latest_filetype)\n        if has_editables:\n            row.append(proj.editable_project_location or '')\n        if options.verbose >= 1:\n            row.append(proj.location or '')\n        if options.verbose >= 1:\n            row.append(proj.installer)\n        data.append(row)\n    return (data, header)",
    "label": true
  },
  {
    "code": "def _parse_local_version(local: str) -> Optional[LocalType]:\n    if local is not None:\n        return tuple((part.lower() if not part.isdigit() else int(part) for part in _local_version_separators.split(local)))\n    return None",
    "label": true
  },
  {
    "code": "def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:\n    if not tokenizer.check('SEMICOLON'):\n        tokenizer.raise_syntax_error(f'Expected end or semicolon (after {after})', span_start=span_start)\n    tokenizer.read()\n    marker = _parse_marker(tokenizer)\n    tokenizer.consume('WS')\n    return marker",
    "label": true
  },
  {
    "code": "def _format(val, valtype, floatfmt, intfmt, missingval='', has_invisible=True):\n    if val is None:\n        return missingval\n    if valtype is str:\n        return f'{val}'\n    elif valtype is int:\n        return format(val, intfmt)\n    elif valtype is bytes:\n        try:\n            return str(val, 'ascii')\n        except (TypeError, UnicodeDecodeError):\n            return str(val)\n    elif valtype is float:\n        is_a_colored_number = has_invisible and isinstance(val, (str, bytes))\n        if is_a_colored_number:\n            raw_val = _strip_ansi(val)\n            formatted_val = format(float(raw_val), floatfmt)\n            return val.replace(raw_val, formatted_val)\n        else:\n            return format(float(val), floatfmt)\n    else:\n        return f'{val}'",
    "label": true
  },
  {
    "code": "def _basic_auth_str(username, password):\n    if not isinstance(username, basestring):\n        warnings.warn(\"Non-string usernames will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.\".format(username), category=DeprecationWarning)\n        username = str(username)\n    if not isinstance(password, basestring):\n        warnings.warn(\"Non-string passwords will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.\".format(type(password)), category=DeprecationWarning)\n        password = str(password)\n    if isinstance(username, str):\n        username = username.encode('latin1')\n    if isinstance(password, str):\n        password = password.encode('latin1')\n    authstr = 'Basic ' + to_native_string(b64encode(b':'.join((username, password))).strip())\n    return authstr",
    "label": true
  },
  {
    "code": "def _zip_equal_generator(iterables):\n    for combo in zip_longest(*iterables, fillvalue=_marker):\n        for val in combo:\n            if val is _marker:\n                raise UnequalIterablesError()\n        yield combo",
    "label": true
  },
  {
    "code": "def _normalize(*values: str, key: str) -> Tuple[str, ...]:\n    if key == 'extra':\n        return tuple((canonicalize_name(v) for v in values))\n    return values",
    "label": true
  },
  {
    "code": "def poll_wait_for_socket(sock: socket.socket, read: bool=False, write: bool=False, timeout: float | None=None) -> bool:\n    if not read and (not write):\n        raise RuntimeError('must specify at least one of read=True, write=True')\n    mask = 0\n    if read:\n        mask |= select.POLLIN\n    if write:\n        mask |= select.POLLOUT\n    poll_obj = select.poll()\n    poll_obj.register(sock, mask)\n\n    def do_poll(t: float | None) -> list[tuple[int, int]]:\n        if t is not None:\n            t *= 1000\n        return poll_obj.poll(t)\n    return bool(do_poll(timeout))",
    "label": true
  },
  {
    "code": "def add_target_python_options(cmd_opts: OptionGroup) -> None:\n    cmd_opts.add_option(platforms())\n    cmd_opts.add_option(python_version())\n    cmd_opts.add_option(implementation())\n    cmd_opts.add_option(abis())",
    "label": true
  },
  {
    "code": "def _main():\n    import signal\n    try:\n        signal.signal(signal.SIGPIPE, lambda signum, frame: sys.exit(1))\n    except AttributeError:\n        pass\n    style_guide = StyleGuide(parse_argv=True)\n    options = style_guide.options\n    report = style_guide.check_files()\n    if options.statistics:\n        report.print_statistics()\n    if options.benchmark:\n        report.print_benchmark()\n    if report.total_errors:\n        if options.count:\n            sys.stderr.write(str(report.total_errors) + '\\n')\n        sys.exit(1)",
    "label": true
  },
  {
    "code": "def python_qualified_identifier(value: str) -> bool:\n    if value.startswith('.') or value.endswith('.'):\n        return False\n    return all((python_identifier(m) for m in value.split('.')))",
    "label": true
  },
  {
    "code": "def resolve_from_str(input: str, rootpath: Path) -> Path:\n    input = expanduser(input)\n    input = expandvars(input)\n    if isabs(input):\n        return Path(input)\n    else:\n        return rootpath.joinpath(input)",
    "label": true
  },
  {
    "code": "def _ensure_quoted_url(url: str) -> str:\n    result = urllib.parse.urlparse(url)\n    is_local_path = not result.netloc\n    path = _clean_url_path(result.path, is_local_path=is_local_path)\n    return urllib.parse.urlunparse(result._replace(path=path))",
    "label": true
  },
  {
    "code": "def allowed_gai_family():\n    family = socket.AF_INET\n    if HAS_IPV6:\n        family = socket.AF_UNSPEC\n    return family",
    "label": true
  },
  {
    "code": "def _make_unop(op: str) -> t.Callable[['CodeGenerator', nodes.UnaryExpr, 'Frame'], None]:\n\n    @optimizeconst\n    def visitor(self: 'CodeGenerator', node: nodes.UnaryExpr, frame: Frame) -> None:\n        if self.environment.sandboxed and op in self.environment.intercepted_unops:\n            self.write(f'environment.call_unop(context, {op!r}, ')\n            self.visit(node.node, frame)\n        else:\n            self.write('(' + op)\n            self.visit(node.node, frame)\n        self.write(')')\n    return visitor",
    "label": true
  },
  {
    "code": "def docstring_headline(obj):\n    if not obj.__doc__:\n        return ''\n    res = []\n    for line in obj.__doc__.strip().splitlines():\n        if line.strip():\n            res.append(' ' + line.strip())\n        else:\n            break\n    return ''.join(res).lstrip()",
    "label": true
  },
  {
    "code": "def parse_key_value_pair(src: str, pos: Pos, parse_float: ParseFloat) -> Tuple[Pos, Key, Any]:\n    pos, key = parse_key(src, pos)\n    try:\n        char: Optional[str] = src[pos]\n    except IndexError:\n        char = None\n    if char != '=':\n        raise suffixed_err(src, pos, 'Expected \"=\" after a key in a key/value pair')\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, value = parse_value(src, pos, parse_float)\n    return (pos, key, value)",
    "label": true
  },
  {
    "code": "def detect_all(byte_str: Union[bytes, bytearray], ignore_threshold: bool=False, should_rename_legacy: bool=False) -> List[ResultDict]:\n    if not isinstance(byte_str, bytearray):\n        if not isinstance(byte_str, bytes):\n            raise TypeError(f'Expected object of type bytes or bytearray, got: {type(byte_str)}')\n        byte_str = bytearray(byte_str)\n    detector = UniversalDetector(should_rename_legacy=should_rename_legacy)\n    detector.feed(byte_str)\n    detector.close()\n    if detector.input_state == InputState.HIGH_BYTE:\n        results: List[ResultDict] = []\n        probers: List[CharSetProber] = []\n        for prober in detector.charset_probers:\n            if isinstance(prober, CharSetGroupProber):\n                probers.extend((p for p in prober.probers))\n            else:\n                probers.append(prober)\n        for prober in probers:\n            if ignore_threshold or prober.get_confidence() > detector.MINIMUM_THRESHOLD:\n                charset_name = prober.charset_name or ''\n                lower_charset_name = charset_name.lower()\n                if lower_charset_name.startswith('iso-8859') and detector.has_win_bytes:\n                    charset_name = detector.ISO_WIN_MAP.get(lower_charset_name, charset_name)\n                if should_rename_legacy:\n                    charset_name = detector.LEGACY_MAP.get(charset_name.lower(), charset_name)\n                results.append({'encoding': charset_name, 'confidence': prober.get_confidence(), 'language': prober.language})\n        if len(results) > 0:\n            return sorted(results, key=lambda result: -result['confidence'])\n    return [detector.result]",
    "label": true
  },
  {
    "code": "def node_type(node: nodes.NodeNG) -> SuccessfulInferenceResult | None:\n    types: set[SuccessfulInferenceResult] = set()\n    try:\n        for var_type in node.infer():\n            if isinstance(var_type, util.UninferableBase) or is_none(var_type):\n                continue\n            types.add(var_type)\n            if len(types) > 1:\n                return None\n    except astroid.InferenceError:\n        return None\n    return types.pop() if types else None",
    "label": true
  },
  {
    "code": "def iterate(func, start):\n    while True:\n        yield start\n        start = func(start)",
    "label": true
  },
  {
    "code": "def ensure_extended_length_path(path: Path) -> Path:\n    if sys.platform.startswith('win32'):\n        path = path.resolve()\n        path = Path(get_extended_length_path_str(str(path)))\n    return path",
    "label": true
  },
  {
    "code": "def _generic_abi() -> List[str]:\n    ext_suffix = _get_config_var('EXT_SUFFIX', warn=True)\n    if not isinstance(ext_suffix, str) or ext_suffix[0] != '.':\n        raise SystemError(\"invalid sysconfig.get_config_var('EXT_SUFFIX')\")\n    parts = ext_suffix.split('.')\n    if len(parts) < 3:\n        return _cpython_abis(sys.version_info[:2])\n    soabi = parts[1]\n    if soabi.startswith('cpython'):\n        abi = 'cp' + soabi.split('-')[1]\n    elif soabi.startswith('cp'):\n        abi = soabi.split('-')[0]\n    elif soabi.startswith('pypy'):\n        abi = '-'.join(soabi.split('-')[:2])\n    elif soabi.startswith('graalpy'):\n        abi = '-'.join(soabi.split('-')[:3])\n    elif soabi:\n        abi = soabi\n    else:\n        return []\n    return [_normalize_string(abi)]",
    "label": true
  },
  {
    "code": "def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n    if len(py_version) > 1:\n        yield f'py{_version_nodot(py_version[:2])}'\n    yield f'py{py_version[0]}'\n    if len(py_version) > 1:\n        for minor in range(py_version[1] - 1, -1, -1):\n            yield f'py{_version_nodot((py_version[0], minor))}'",
    "label": true
  },
  {
    "code": "def _re_transform() -> nodes.Module:\n    if PY311_PLUS:\n        import_compiler = 'import re._compiler as _compiler'\n    else:\n        import_compiler = 'import sre_compile as _compiler'\n    return parse(f'\\n    {import_compiler}\\n    NOFLAG = 0\\n    ASCII = _compiler.SRE_FLAG_ASCII\\n    IGNORECASE = _compiler.SRE_FLAG_IGNORECASE\\n    LOCALE = _compiler.SRE_FLAG_LOCALE\\n    UNICODE = _compiler.SRE_FLAG_UNICODE\\n    MULTILINE = _compiler.SRE_FLAG_MULTILINE\\n    DOTALL = _compiler.SRE_FLAG_DOTALL\\n    VERBOSE = _compiler.SRE_FLAG_VERBOSE\\n    TEMPLATE = _compiler.SRE_FLAG_TEMPLATE\\n    DEBUG = _compiler.SRE_FLAG_DEBUG\\n    A = ASCII\\n    I = IGNORECASE\\n    L = LOCALE\\n    U = UNICODE\\n    M = MULTILINE\\n    S = DOTALL\\n    X = VERBOSE\\n    T = TEMPLATE\\n    ')",
    "label": true
  },
  {
    "code": "def valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def directory_arg(path: str, optname: str) -> str:\n    if not os.path.isdir(path):\n        raise UsageError(f'{optname} must be a directory, given: {path}')\n    return path",
    "label": true
  },
  {
    "code": "def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:\n    position = 0\n    sgr: Optional[str]\n    osc: Optional[str]\n    for match in re_ansi.finditer(ansi_text):\n        start, end = match.span(0)\n        osc, sgr = match.groups()\n        if start > position:\n            yield _AnsiToken(ansi_text[position:start])\n        if sgr:\n            if sgr == '(':\n                position = end + 1\n                continue\n            if sgr.endswith('m'):\n                yield _AnsiToken('', sgr[1:-1], osc)\n        else:\n            yield _AnsiToken('', sgr, osc)\n        position = end\n    if position < len(ansi_text):\n        yield _AnsiToken(ansi_text[position:])",
    "label": true
  },
  {
    "code": "def iter_field_objects(fields: _TYPE_FIELDS) -> typing.Iterable[RequestField]:\n    iterable: typing.Iterable[RequestField | tuple[str, _TYPE_FIELD_VALUE_TUPLE]]\n    if isinstance(fields, typing.Mapping):\n        iterable = fields.items()\n    else:\n        iterable = fields\n    for field in iterable:\n        if isinstance(field, RequestField):\n            yield field\n        else:\n            yield RequestField.from_tuples(*field)",
    "label": true
  },
  {
    "code": "def _handle_python_version(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    version_info, error_msg = _convert_python_version(value)\n    if error_msg is not None:\n        msg = 'invalid --python-version value: {!r}: {}'.format(value, error_msg)\n        raise_option_error(parser, option=option, msg=msg)\n    parser.values.python_version = version_info",
    "label": true
  },
  {
    "code": "def find_eggs_in_zip(importer, path_item, only=False):\n    if importer.archive.endswith('.whl'):\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        return\n    for subitem in metadata.resource_listdir(''):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith(('.dist-info', '.egg-info')):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)",
    "label": true
  },
  {
    "code": "def trim_from_end(raw_data: list, count: int) -> None:\n    for dummy_i in range(count):\n        raw_data.pop()",
    "label": true
  },
  {
    "code": "def filter_unallowed_hashes(candidates: List[InstallationCandidate], hashes: Optional[Hashes], project_name: str) -> List[InstallationCandidate]:\n    if not hashes:\n        logger.debug('Given no hashes to check %s links for project %r: discarding no candidates', len(candidates), project_name)\n        return list(candidates)\n    matches_or_no_digest = []\n    non_matches = []\n    match_count = 0\n    for candidate in candidates:\n        link = candidate.link\n        if not link.has_hash:\n            pass\n        elif link.is_hash_allowed(hashes=hashes):\n            match_count += 1\n        else:\n            non_matches.append(candidate)\n            continue\n        matches_or_no_digest.append(candidate)\n    if match_count:\n        filtered = matches_or_no_digest\n    else:\n        filtered = list(candidates)\n    if len(filtered) == len(candidates):\n        discard_message = 'discarding no candidates'\n    else:\n        discard_message = 'discarding {} non-matches:\\n  {}'.format(len(non_matches), '\\n  '.join((str(candidate.link) for candidate in non_matches)))\n    logger.debug('Checked %s links for project %r against %s hashes (%s matches, %s no digest): %s', len(candidates), project_name, hashes.digest_count, match_count, len(matches_or_no_digest) - match_count, discard_message)\n    return filtered",
    "label": true
  },
  {
    "code": "def expire_after(delta, date=None):\n    date = date or datetime.utcnow()\n    return date + delta",
    "label": true
  },
  {
    "code": "def _normalize_host(host, scheme):\n    host = normalize_host(host, scheme)\n    if host.startswith('[') and host.endswith(']'):\n        host = host[1:-1]\n    return host",
    "label": true
  },
  {
    "code": "def url_quote(obj: t.Any, charset: str='utf-8', for_qs: bool=False) -> str:\n    if not isinstance(obj, bytes):\n        if not isinstance(obj, str):\n            obj = str(obj)\n        obj = obj.encode(charset)\n    safe = b'' if for_qs else b'/'\n    rv = quote_from_bytes(obj, safe)\n    if for_qs:\n        rv = rv.replace('%20', '+')\n    return rv",
    "label": true
  },
  {
    "code": "def _make_tree_defs(mod_files_list: ItemsView[str, set[str]]) -> _ImportTree:\n    tree_defs: _ImportTree = {}\n    for mod, files in mod_files_list:\n        node: list[_ImportTree | list[str]] = [tree_defs, []]\n        for prefix in mod.split('.'):\n            assert isinstance(node[0], dict)\n            node = node[0].setdefault(prefix, ({}, []))\n        assert isinstance(node[1], list)\n        node[1].extend(files)\n    return tree_defs",
    "label": true
  },
  {
    "code": "def _looks_like_typing_subscript(node) -> bool:\n    if isinstance(node, Name):\n        return node.name in TYPING_MEMBERS\n    if isinstance(node, Attribute):\n        return node.attrname in TYPING_MEMBERS\n    if isinstance(node, Subscript):\n        return _looks_like_typing_subscript(node.value)\n    return False",
    "label": true
  },
  {
    "code": "def get_build_architecture():\n    prefix = ' bit ('\n    i = sys.version.find(prefix)\n    if i == -1:\n        return 'Intel'\n    j = sys.version.find(')', i)\n    return sys.version[i + len(prefix):j]",
    "label": true
  },
  {
    "code": "def _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version",
    "label": true
  },
  {
    "code": "def _has_different_keyword_only_parameters(original: list[nodes.AssignName], overridden: list[nodes.AssignName]) -> list[str]:\n    original_names = [i.name for i in original]\n    overridden_names = [i.name for i in overridden]\n    if any((name not in overridden_names for name in original_names)):\n        return ['Number of parameters ']\n    for name in overridden_names:\n        if name in original_names:\n            continue\n        try:\n            overridden[0].parent.default_value(name)\n        except astroid.NoDefault:\n            return ['Number of parameters ']\n    return []",
    "label": true
  },
  {
    "code": "def _split_env(cmd):\n    pivot = 0\n    if os.path.basename(cmd[0]) == 'env':\n        pivot = 1\n        while '=' in cmd[pivot]:\n            pivot += 1\n    return (cmd[:pivot], cmd[pivot:])",
    "label": true
  },
  {
    "code": "def deinit():\n    if orig_stdout is not None:\n        sys.stdout = orig_stdout\n    if orig_stderr is not None:\n        sys.stderr = orig_stderr",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('logging')\n\n    def add_option_ini(option, dest, default=None, type=None, **kwargs):\n        parser.addini(dest, default=default, type=type, help='Default value for ' + option)\n        group.addoption(option, dest=dest, **kwargs)\n    add_option_ini('--log-level', dest='log_level', default=None, metavar='LEVEL', help='Level of messages to catch/display. Not set by default, so it depends on the root/parent log handler\\'s effective level, where it is \"WARNING\" by default.')\n    add_option_ini('--log-format', dest='log_format', default=DEFAULT_LOG_FORMAT, help='Log format used by the logging module')\n    add_option_ini('--log-date-format', dest='log_date_format', default=DEFAULT_LOG_DATE_FORMAT, help='Log date format used by the logging module')\n    parser.addini('log_cli', default=False, type='bool', help='Enable log display during test run (also known as \"live logging\")')\n    add_option_ini('--log-cli-level', dest='log_cli_level', default=None, help='CLI logging level')\n    add_option_ini('--log-cli-format', dest='log_cli_format', default=None, help='Log format used by the logging module')\n    add_option_ini('--log-cli-date-format', dest='log_cli_date_format', default=None, help='Log date format used by the logging module')\n    add_option_ini('--log-file', dest='log_file', default=None, help='Path to a file when logging will be written to')\n    add_option_ini('--log-file-level', dest='log_file_level', default=None, help='Log file logging level')\n    add_option_ini('--log-file-format', dest='log_file_format', default=DEFAULT_LOG_FORMAT, help='Log format used by the logging module')\n    add_option_ini('--log-file-date-format', dest='log_file_date_format', default=DEFAULT_LOG_DATE_FORMAT, help='Log date format used by the logging module')\n    add_option_ini('--log-auto-indent', dest='log_auto_indent', default=None, help='Auto-indent multiline messages passed to the logging module. Accepts true|on, false|off or an integer.')\n    group.addoption('--log-disable', action='append', default=[], dest='logger_disable', help='Disable a logger by name. Can be passed multiple times.')",
    "label": true
  },
  {
    "code": "def path_wrapper(func):\n\n    @functools.wraps(func)\n    def wrapped(node, context: InferenceContext | None=None, _func=func, **kwargs) -> Generator:\n        \"\"\"Wrapper function handling context.\"\"\"\n        if context is None:\n            context = InferenceContext()\n        if context.push(node):\n            return\n        yielded = set()\n        for res in _func(node, context, **kwargs):\n            if res.__class__.__name__ == 'Instance':\n                ares = res._proxied\n            else:\n                ares = res\n            if ares not in yielded:\n                yield res\n                yielded.add(ares)\n    return wrapped",
    "label": true
  },
  {
    "code": "def _check_download_dir(link: Link, download_dir: str, hashes: Optional[Hashes], warn_on_hash_mismatch: bool=True) -> Optional[str]:\n    download_path = os.path.join(download_dir, link.filename)\n    if not os.path.exists(download_path):\n        return None\n    logger.info('File was already downloaded %s', download_path)\n    if hashes:\n        try:\n            hashes.check_against_path(download_path)\n        except HashMismatch:\n            if warn_on_hash_mismatch:\n                logger.warning('Previously-downloaded file %s has bad hash. Re-downloading.', download_path)\n            os.unlink(download_path)\n            return None\n    return download_path",
    "label": true
  },
  {
    "code": "def get_all_styles():\n    yield from STYLE_MAP\n    for name, _ in find_plugin_styles():\n        yield name",
    "label": true
  },
  {
    "code": "def _teardown_yield_fixture(fixturefunc, it) -> None:\n    try:\n        next(it)\n    except StopIteration:\n        pass\n    else:\n        fail_fixturefunc(fixturefunc, \"fixture function has more than one 'yield'\")",
    "label": true
  },
  {
    "code": "def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\n    iter_values = iter(values)\n    try:\n        previous_value = next(iter_values)\n    except StopIteration:\n        return\n    for value in iter_values:\n        yield (False, previous_value)\n        previous_value = value\n    yield (True, previous_value)",
    "label": true
  },
  {
    "code": "def create_list_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 2\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n    out.flags.unset_all(key)\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.append_nest_to_list(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']]', pos):\n        raise suffixed_err(src, pos, \"Expected ']]' at the end of an array declaration\")\n    return (pos + 2, key)",
    "label": true
  },
  {
    "code": "def product(integer_queue: Queue) -> int:\n    manjyot = 1\n    while not integer_queue.is_empty():\n        manjyot *= integer_queue.dequeue()\n    return manjyot",
    "label": true
  },
  {
    "code": "def strip_marker(req):\n    import pkg_resources\n    req = pkg_resources.Requirement.parse(str(req))\n    req.marker = None\n    return req",
    "label": true
  },
  {
    "code": "def load_module_from_file(filepath: str) -> types.ModuleType:\n    modpath = modpath_from_file(filepath)\n    return load_module_from_modpath(modpath)",
    "label": true
  },
  {
    "code": "def _wrap_io_open(file: t.Union[str, 'os.PathLike[str]', int], mode: str, encoding: t.Optional[str], errors: t.Optional[str]) -> t.IO[t.Any]:\n    if 'b' in mode:\n        return open(file, mode)\n    return open(file, mode, encoding=encoding, errors=errors)",
    "label": true
  },
  {
    "code": "def _default_start_debug_action(instring: str, loc: int, expr: 'ParserElement', cache_hit: bool=False):\n    cache_hit_str = '*' if cache_hit else ''\n    print(f\"{cache_hit_str}Match {expr} at loc {loc}({lineno(loc, instring)},{col(loc, instring)})\\n  {line(loc, instring)}\\n  {' ' * (col(loc, instring) - 1)}^\")",
    "label": true
  },
  {
    "code": "def _require_version_compare(fn: Callable[['Specifier', ParsedVersion, str], bool]) -> Callable[['Specifier', ParsedVersion, str], bool]:\n\n    @functools.wraps(fn)\n    def wrapped(self: 'Specifier', prospective: ParsedVersion, spec: str) -> bool:\n        if not isinstance(prospective, Version):\n            return False\n        return fn(self, prospective, spec)\n    return wrapped",
    "label": true
  },
  {
    "code": "def find_inferred_fn_from_register(node: nodes.NodeNG) -> nodes.FunctionDef | None:\n    if isinstance(node, nodes.Call):\n        func = node.func\n    elif isinstance(node, nodes.Attribute):\n        func = node\n    else:\n        return None\n    if not isinstance(func, nodes.Attribute) or func.attrname != 'register':\n        return None\n    func_def = safe_infer(func.expr)\n    if not isinstance(func_def, nodes.FunctionDef):\n        return None\n    return func_def",
    "label": true
  },
  {
    "code": "def get_expected_output(configuration_path: str | Path, user_specific_path: Path) -> tuple[int, str]:\n    exit_code = 0\n    msg = \"we expect a single file of the form 'filename.32.out' where 'filename' represents the name of the configuration file, and '32' the expected error code.\"\n    possible_out_files = get_related_files(configuration_path, suffix_filter='out')\n    if len(possible_out_files) > 1:\n        logging.error('Too much .out files for %s %s.', configuration_path, msg)\n        return (-1, 'out file is broken')\n    if not possible_out_files:\n        logging.info('.out file does not exists, so the expected exit code is 0')\n        return (0, '')\n    path = possible_out_files[0]\n    try:\n        exit_code = int(str(path.stem).rsplit('.', maxsplit=1)[-1])\n    except Exception as e:\n        logging.error('Wrong format for .out file name for %s %s: %s', configuration_path, msg, e)\n        return (-1, 'out file is broken')\n    output = get_expected_or_default(configuration_path, suffix=f'{exit_code}.out', default='')\n    logging.info('Output exists for %s so the expected exit code is %s', configuration_path, exit_code)\n    return (exit_code, output.format(abspath=configuration_path, relpath=Path(configuration_path).relative_to(user_specific_path)))",
    "label": true
  },
  {
    "code": "def ilen(iterable):\n    counter = count()\n    deque(zip(iterable, counter), maxlen=0)\n    return next(counter)",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e304(msg, _node, source_lines=None):\n    line = msg.line - 1\n    while source_lines[line - 1].strip() == '':\n        line -= 1\n    yield from render_context(line - 1, line + 1, source_lines)\n    yield from ((curr_line, slice(None, None), LineType.ERROR, ' ' * 28) for curr_line in range(line + 1, msg.line))\n    yield from render_context(msg.line, msg.line + 2, source_lines)",
    "label": true
  },
  {
    "code": "def _extend_string_class(class_node, code, rvalue):\n    code = code.format(rvalue=rvalue)\n    fake = AstroidBuilder(AstroidManager()).string_build(code)['whatever']\n    for method in fake.mymethods():\n        method.parent = class_node\n        method.lineno = None\n        method.col_offset = None\n        if '__class__' in method.locals:\n            method.locals['__class__'] = [class_node]\n        class_node.locals[method.name] = [method]\n        method.parent = class_node",
    "label": true
  },
  {
    "code": "def get_export_entry(specification):\n    m = ENTRY_RE.search(specification)\n    if not m:\n        result = None\n        if '[' in specification or ']' in specification:\n            raise DistlibException(\"Invalid specification '%s'\" % specification)\n    else:\n        d = m.groupdict()\n        name = d['name']\n        path = d['callable']\n        colons = path.count(':')\n        if colons == 0:\n            prefix, suffix = (path, None)\n        else:\n            if colons != 1:\n                raise DistlibException(\"Invalid specification '%s'\" % specification)\n            prefix, suffix = path.split(':')\n        flags = d['flags']\n        if flags is None:\n            if '[' in specification or ']' in specification:\n                raise DistlibException(\"Invalid specification '%s'\" % specification)\n            flags = []\n        else:\n            flags = [f.strip() for f in flags.split(',')]\n        result = ExportEntry(name, prefix, suffix, flags)\n    return result",
    "label": true
  },
  {
    "code": "def find_undeclared_variables(ast: nodes.Template) -> t.Set[str]:\n    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers",
    "label": true
  },
  {
    "code": "def match_previous_literal(expr: ParserElement) -> ParserElement:\n    rep = Forward()\n\n    def copy_token_to_repeater(s, l, t):\n        if t:\n            if len(t) == 1:\n                rep << t[0]\n            else:\n                tflat = _flatten(t.as_list())\n                rep << And((Literal(tt) for tt in tflat))\n        else:\n            rep << Empty()\n    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)\n    rep.set_name('(prev) ' + str(expr))\n    return rep",
    "label": true
  },
  {
    "code": "def _surrounding_parens_on_own_lines(lines: List[str]) -> None:\n    opening = lines[0][:1]\n    if opening in ['(', '[', '{']:\n        lines[0] = ' ' + lines[0][1:]\n        lines[:] = [opening] + lines\n    closing = lines[-1][-1:]\n    if closing in [')', ']', '}']:\n        lines[-1] = lines[-1][:-1] + ','\n        lines[:] = lines + [closing]",
    "label": true
  },
  {
    "code": "def _get_dataclass_attributes(node: nodes.ClassDef, init: bool=False) -> Iterator[nodes.AnnAssign]:\n    for assign_node in node.body:\n        if not isinstance(assign_node, nodes.AnnAssign) or not isinstance(assign_node.target, nodes.AssignName):\n            continue\n        if _is_class_var(assign_node.annotation):\n            continue\n        if _is_keyword_only_sentinel(assign_node.annotation):\n            continue\n        if not init and _is_init_var(assign_node.annotation):\n            continue\n        yield assign_node",
    "label": true
  },
  {
    "code": "def dict_to_sequence(d):\n    if hasattr(d, 'items'):\n        d = d.items()\n    return d",
    "label": true
  },
  {
    "code": "def get_win_folder_from_env_vars(csidl_name: str) -> str:\n    if csidl_name == 'CSIDL_PERSONAL':\n        return os.path.join(os.path.normpath(os.environ['USERPROFILE']), 'Documents')\n    env_var_name = {'CSIDL_APPDATA': 'APPDATA', 'CSIDL_COMMON_APPDATA': 'ALLUSERSPROFILE', 'CSIDL_LOCAL_APPDATA': 'LOCALAPPDATA'}.get(csidl_name)\n    if env_var_name is None:\n        raise ValueError(f'Unknown CSIDL name: {csidl_name}')\n    result = os.environ.get(env_var_name)\n    if result is None:\n        raise ValueError(f'Unset environment variable: {env_var_name}')\n    return result",
    "label": true
  },
  {
    "code": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {'implementation_name': implementation_name, 'implementation_version': iver, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'python_full_version': platform.python_version(), 'platform_python_implementation': platform.python_implementation(), 'python_version': '.'.join(platform.python_version_tuple()[:2]), 'sys_platform': sys.platform}",
    "label": true
  },
  {
    "code": "def _handle_get_simple_fail(link: Link, reason: Union[str, Exception], meth: Optional[Callable[..., None]]=None) -> None:\n    if meth is None:\n        meth = logger.debug\n    meth('Could not fetch URL %s: %s - skipping', link, reason)",
    "label": true
  },
  {
    "code": "def write_file(filename, contents):\n    f = open(filename, 'w')\n    try:\n        for line in contents:\n            f.write(line + '\\n')\n    finally:\n        f.close()",
    "label": true
  },
  {
    "code": "def parse_wheel_filename(filename: str) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n    if not filename.endswith('.whl'):\n        raise InvalidWheelFilename(f\"Invalid wheel filename (extension must be '.whl'): {filename}\")\n    filename = filename[:-4]\n    dashes = filename.count('-')\n    if dashes not in (4, 5):\n        raise InvalidWheelFilename(f'Invalid wheel filename (wrong number of parts): {filename}')\n    parts = filename.split('-', dashes - 2)\n    name_part = parts[0]\n    if '__' in name_part or re.match('^[\\\\w\\\\d._]*$', name_part, re.UNICODE) is None:\n        raise InvalidWheelFilename(f'Invalid project name: {filename}')\n    name = canonicalize_name(name_part)\n    version = Version(parts[1])\n    if dashes == 5:\n        build_part = parts[2]\n        build_match = _build_tag_regex.match(build_part)\n        if build_match is None:\n            raise InvalidWheelFilename(f\"Invalid build number: {build_part} in '{filename}'\")\n        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))\n    else:\n        build = ()\n    tags = parse_tag(parts[-1])\n    return (name, version, build, tags)",
    "label": true
  },
  {
    "code": "def canonic_package_data(package_data: dict) -> dict:\n    if '*' in package_data:\n        package_data[''] = package_data.pop('*')\n    return package_data",
    "label": true
  },
  {
    "code": "def distribute(n, iterable):\n    if n < 1:\n        raise ValueError('n must be at least 1')\n    children = tee(iterable, n)\n    return [islice(it, index, None, n) for index, it in enumerate(children)]",
    "label": true
  },
  {
    "code": "def test_pickled_adder():\n    padder = pickle.dumps(adder)\n    padd5 = pickle.loads(padder)(x)\n    assert padd5(y) == x + y",
    "label": true
  },
  {
    "code": "def build_editable(wheel_directory, config_settings, metadata_directory=None):\n    backend = _build_backend()\n    try:\n        hook = backend.build_editable\n    except AttributeError:\n        raise HookMissing()\n    else:\n        prebuilt_whl = _find_already_built_wheel(metadata_directory)\n        if prebuilt_whl:\n            shutil.copy2(prebuilt_whl, wheel_directory)\n            return os.path.basename(prebuilt_whl)\n        return hook(wheel_directory, config_settings, metadata_directory)",
    "label": true
  },
  {
    "code": "def _get_pylint_home() -> str:\n    if 'PYLINTHOME' in os.environ:\n        return os.environ['PYLINTHOME']\n    _warn_about_old_home(pathlib.Path(DEFAULT_PYLINT_HOME))\n    return DEFAULT_PYLINT_HOME",
    "label": true
  },
  {
    "code": "def collapse(iterable, base_type=None, levels=None):\n\n    def walk(node, level):\n        if levels is not None and level > levels or isinstance(node, (str, bytes)) or (base_type is not None and isinstance(node, base_type)):\n            yield node\n            return\n        try:\n            tree = iter(node)\n        except TypeError:\n            yield node\n            return\n        else:\n            for child in tree:\n                yield from walk(child, level + 1)\n    yield from walk(iterable, 0)",
    "label": true
  },
  {
    "code": "def load_cdll(name, macos10_16_path):\n    try:\n        if version_info >= (10, 16):\n            path = macos10_16_path\n        else:\n            path = find_library(name)\n        if not path:\n            raise OSError\n        return CDLL(path, use_errno=True)\n    except OSError:\n        raise_from(ImportError('The library %s failed to load' % name), None)",
    "label": true
  },
  {
    "code": "def _get_config_var(name: str, warn: bool=False) -> Union[int, str, None]:\n    value: Union[int, str, None] = sysconfig.get_config_var(name)\n    if value is None and warn:\n        logger.debug(\"Config variable '%s' is unset, Python ABI tag may be incorrect\", name)\n    return value",
    "label": true
  },
  {
    "code": "def _regexp_paths_csv_validator(_: Any, name: str, value: str | list[Pattern[str]]) -> list[Pattern[str]]:\n    if isinstance(value, list):\n        return value\n    patterns = []\n    for val in _csv_validator(_, name, value):\n        patterns.append(re.compile(str(pathlib.PureWindowsPath(val)).replace('\\\\', '\\\\\\\\') + '|' + pathlib.PureWindowsPath(val).as_posix()))\n    return patterns",
    "label": true
  },
  {
    "code": "def _latex_line_begin_tabular(colwidths, colaligns, booktabs=False, longtable=False):\n    alignment = {'left': 'l', 'right': 'r', 'center': 'c', 'decimal': 'r'}\n    tabular_columns_fmt = ''.join([alignment.get(a, 'l') for a in colaligns])\n    return '\\n'.join([('\\\\begin{tabular}{' if not longtable else '\\\\begin{longtable}{') + tabular_columns_fmt + '}', '\\\\toprule' if booktabs else '\\\\hline'])",
    "label": true
  },
  {
    "code": "def do_filesizeformat(value: t.Union[str, float, int], binary: bool=False) -> str:\n    bytes = float(value)\n    base = 1024 if binary else 1000\n    prefixes = ['KiB' if binary else 'kB', 'MiB' if binary else 'MB', 'GiB' if binary else 'GB', 'TiB' if binary else 'TB', 'PiB' if binary else 'PB', 'EiB' if binary else 'EB', 'ZiB' if binary else 'ZB', 'YiB' if binary else 'YB']\n    if bytes == 1:\n        return '1 Byte'\n    elif bytes < base:\n        return f'{int(bytes)} Bytes'\n    else:\n        for i, prefix in enumerate(prefixes):\n            unit = base ** (i + 2)\n            if bytes < unit:\n                return f'{base * bytes / unit:.1f} {prefix}'\n        return f'{base * bytes / unit:.1f} {prefix}'",
    "label": true
  },
  {
    "code": "def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n    formats = [cpu_arch]\n    if cpu_arch == 'x86_64':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat64', 'fat32'])\n    elif cpu_arch == 'i386':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat32', 'fat'])\n    elif cpu_arch == 'ppc64':\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append('fat64')\n    elif cpu_arch == 'ppc':\n        if version > (10, 6):\n            return []\n        formats.extend(['fat32', 'fat'])\n    if cpu_arch in {'arm64', 'x86_64'}:\n        formats.append('universal2')\n    if cpu_arch in {'x86_64', 'i386', 'ppc64', 'ppc', 'intel'}:\n        formats.append('universal')\n    return formats",
    "label": true
  },
  {
    "code": "def with_metaclass(meta, *bases):\n\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})",
    "label": true
  },
  {
    "code": "def parse_num(maybe_num) -> int:\n    try:\n        return int(maybe_num)\n    except ValueError:\n        return -1",
    "label": true
  },
  {
    "code": "def _format_default(default: t.Any) -> t.Any:\n    if isinstance(default, (io.IOBase, LazyFile)) and hasattr(default, 'name'):\n        return default.name\n    return default",
    "label": true
  },
  {
    "code": "def _pad_row(cells, padding):\n    if cells:\n        pad = ' ' * padding\n        padded_cells = [pad + cell + pad for cell in cells]\n        return padded_cells\n    else:\n        return cells",
    "label": true
  },
  {
    "code": "def _validate_header_part(header, header_part, header_validator_index):\n    if isinstance(header_part, str):\n        validator = _HEADER_VALIDATORS_STR[header_validator_index]\n    elif isinstance(header_part, bytes):\n        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]\n    else:\n        raise InvalidHeader(f'Header part ({header_part!r}) from {header} must be of type str or bytes, not {type(header_part)}')\n    if not validator.match(header_part):\n        header_kind = 'name' if header_validator_index == 0 else 'value'\n        raise InvalidHeader(f'Invalid leading whitespace, reserved character(s), or returncharacter(s) in header {header_kind}: {header_part!r}')",
    "label": true
  },
  {
    "code": "def infer_enum(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[bases.Instance]:\n    try:\n        inferred = node.func.infer(context)\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not any((isinstance(item, nodes.ClassDef) and item.qname() == ENUM_QNAME for item in inferred)):\n        raise UseInferenceDefault\n    enum_meta = _extract_single_node(\"\\n    class EnumMeta(object):\\n        'docstring'\\n        def __call__(self, node):\\n            class EnumAttribute(object):\\n                name = ''\\n                value = 0\\n            return EnumAttribute()\\n        def __iter__(self):\\n            class EnumAttribute(object):\\n                name = ''\\n                value = 0\\n            return [EnumAttribute()]\\n        def __reversed__(self):\\n            class EnumAttribute(object):\\n                name = ''\\n                value = 0\\n            return (EnumAttribute, )\\n        def __next__(self):\\n            return next(iter(self))\\n        def __getitem__(self, attr):\\n            class Value(object):\\n                @property\\n                def name(self):\\n                    return ''\\n                @property\\n                def value(self):\\n                    return attr\\n\\n            return Value()\\n        __members__ = ['']\\n    \")\n    class_node = infer_func_form(node, [enum_meta], context=context, enum=True)[0]\n    return iter([class_node.instantiate_class()])",
    "label": true
  },
  {
    "code": "def extract_zipped_paths(path):\n    if os.path.exists(path):\n        return path\n    archive, member = os.path.split(path)\n    while archive and (not os.path.exists(archive)):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            break\n        member = '/'.join([prefix, member])\n    if not zipfile.is_zipfile(archive):\n        return path\n    zip_file = zipfile.ZipFile(archive)\n    if member not in zip_file.namelist():\n        return path\n    tmp = tempfile.gettempdir()\n    extracted_path = os.path.join(tmp, member.split('/')[-1])\n    if not os.path.exists(extracted_path):\n        with atomic_open(extracted_path) as file_handler:\n            file_handler.write(zip_file.read(member))\n    return extracted_path",
    "label": true
  },
  {
    "code": "def with_cached_index_content(fn: ParseLinks) -> ParseLinks:\n\n    @functools.lru_cache(maxsize=None)\n    def wrapper(cacheable_page: CacheablePageContent) -> List[Link]:\n        return list(fn(cacheable_page.page))\n\n    @functools.wraps(fn)\n    def wrapper_wrapper(page: 'IndexContent') -> List[Link]:\n        if page.cache_link_parsing:\n            return wrapper(CacheablePageContent(page))\n        return list(fn(page))\n    return wrapper_wrapper",
    "label": true
  },
  {
    "code": "def native_concat(values: t.Iterable[t.Any]) -> t.Optional[t.Any]:\n    head = list(islice(values, 2))\n    if not head:\n        return None\n    if len(head) == 1:\n        raw = head[0]\n        if not isinstance(raw, str):\n            return raw\n    else:\n        if isinstance(values, GeneratorType):\n            values = chain(head, values)\n        raw = ''.join([str(v) for v in values])\n    try:\n        return literal_eval(parse(raw, mode='eval'))\n    except (ValueError, SyntaxError, MemoryError):\n        return raw",
    "label": true
  },
  {
    "code": "def _mk_tmp(request: FixtureRequest, factory: TempPathFactory) -> Path:\n    name = request.node.name\n    name = re.sub('[\\\\W]', '_', name)\n    MAXVAL = 30\n    name = name[:MAXVAL]\n    return factory.mktemp(name, numbered=True)",
    "label": true
  },
  {
    "code": "def _looks_like_functools_member(node: Attribute | Call, member: str) -> bool:\n    if isinstance(node, Attribute):\n        return node.attrname == member\n    if isinstance(node.func, Name):\n        return node.func.name == member\n    if isinstance(node.func, Attribute):\n        return node.func.attrname == member and isinstance(node.func.expr, Name) and (node.func.expr.name == 'functools')\n    return False",
    "label": true
  },
  {
    "code": "def lookup(node, name) -> list:\n    if isinstance(node, (astroid.List, astroid.Tuple, astroid.Const, astroid.Dict, astroid.Set)):\n        return _builtin_lookup(node, name)\n    if isinstance(node, astroid.Instance):\n        return _lookup_in_mro(node, name)\n    if isinstance(node, astroid.ClassDef):\n        return _class_lookup(node, name)\n    raise AttributeInferenceError(attribute=name, target=node)",
    "label": true
  },
  {
    "code": "def register_builtin_transform(transform, builtin_name) -> None:\n\n    def _transform_wrapper(node, context: InferenceContext | None=None):\n        result = transform(node, context=context)\n        if result:\n            if not result.parent:\n                result.parent = node\n            if result.lineno is None:\n                result.lineno = node.lineno\n            if hasattr(result, 'col_offset') and result.col_offset is None:\n                result.col_offset = node.col_offset\n        return iter([result])\n    AstroidManager().register_transform(nodes.Call, inference_tip(_transform_wrapper), partial(_builtin_filter_predicate, builtin_name=builtin_name))",
    "label": true
  },
  {
    "code": "def _format_args(args, defaults=None, annotations=None, skippable_names: set[str] | None=None) -> str:\n    if skippable_names is None:\n        skippable_names = set()\n    values = []\n    if args is None:\n        return ''\n    if annotations is None:\n        annotations = []\n    if defaults is not None:\n        default_offset = len(args) - len(defaults)\n    packed = itertools.zip_longest(args, annotations)\n    for i, (arg, annotation) in enumerate(packed):\n        if arg.name in skippable_names:\n            continue\n        if isinstance(arg, Tuple):\n            values.append(f'({_format_args(arg.elts)})')\n        else:\n            argname = arg.name\n            default_sep = '='\n            if annotation is not None:\n                argname += ': ' + annotation.as_string()\n                default_sep = ' = '\n            values.append(argname)\n            if defaults is not None and i >= default_offset:\n                if defaults[i - default_offset] is not None:\n                    values[-1] += default_sep + defaults[i - default_offset].as_string()\n    return ', '.join(values)",
    "label": true
  },
  {
    "code": "def hasnew(obj: object) -> bool:\n    new: object = getattr(obj, '__new__', None)\n    if new:\n        return new != object.__new__\n    return False",
    "label": true
  },
  {
    "code": "def identify_sig_or_bom(sequence: bytes) -> Tuple[Optional[str], bytes]:\n    for iana_encoding in ENCODING_MARKS:\n        marks: Union[bytes, List[bytes]] = ENCODING_MARKS[iana_encoding]\n        if isinstance(marks, bytes):\n            marks = [marks]\n        for mark in marks:\n            if sequence.startswith(mark):\n                return (iana_encoding, mark)\n    return (None, b'')",
    "label": true
  },
  {
    "code": "def _read_field_from_msg(msg: Message, field: str) -> Optional[str]:\n    value = msg[field]\n    if value == 'UNKNOWN':\n        return None\n    return value",
    "label": true
  },
  {
    "code": "def show_test_item(item: Item) -> None:\n    tw = item.config.get_terminal_writer()\n    tw.line()\n    tw.write(' ' * 8)\n    tw.write(item.nodeid)\n    used_fixtures = sorted(getattr(item, 'fixturenames', []))\n    if used_fixtures:\n        tw.write(' (fixtures used: {})'.format(', '.join(used_fixtures)))\n    tw.flush()",
    "label": true
  },
  {
    "code": "def as_base_candidate(candidate: Candidate) -> Optional[BaseCandidate]:\n    base_candidate_classes = (AlreadyInstalledCandidate, EditableCandidate, LinkCandidate)\n    if isinstance(candidate, base_candidate_classes):\n        return candidate\n    return None",
    "label": true
  },
  {
    "code": "def _format_as_name_version(dist: BaseDistribution) -> str:\n    dist_version = dist.version\n    if isinstance(dist_version, Version):\n        return f'{dist.raw_name}=={dist_version}'\n    return f'{dist.raw_name}==={dist_version}'",
    "label": true
  },
  {
    "code": "def finder(package):\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result",
    "label": true
  },
  {
    "code": "def find_lexer_class_by_name(_alias):\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name]\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls\n    raise ClassNotFound('no lexer for alias %r found' % _alias)",
    "label": true
  },
  {
    "code": "def stream_decode_response_unicode(iterator, r):\n    if r.encoding is None:\n        yield from iterator\n        return\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv",
    "label": true
  },
  {
    "code": "def remove_suffix(text, suffix):\n    rest, suffix, null = text.partition(suffix)\n    return rest",
    "label": true
  },
  {
    "code": "def test_encode():\n    assert encode('\u00e9', 'latin1') == b'\\xe9'\n    assert encode('\u00e9', 'utf8') == b'\\xc3\\xa9'\n    assert encode('\u00e9', 'utf8') == b'\\xc3\\xa9'\n    assert encode('\u00e9', 'utf-16') == b'\\xe9\\x00'\n    assert encode('\u00e9', 'utf-16le') == b'\\xe9\\x00'\n    assert encode('\u00e9', 'utf-16be') == b'\\x00\\xe9'",
    "label": true
  },
  {
    "code": "def _get_user_dirs_folder(key: str) -> str | None:\n    user_dirs_config_path = os.path.join(Unix().user_config_dir, 'user-dirs.dirs')\n    if os.path.exists(user_dirs_config_path):\n        parser = ConfigParser()\n        with open(user_dirs_config_path) as stream:\n            parser.read_string(f'[top]\\n{stream.read()}')\n        if key not in parser['top']:\n            return None\n        path = parser['top'][key].strip('\"')\n        path = path.replace('$HOME', os.path.expanduser('~'))\n        return path\n    return None",
    "label": true
  },
  {
    "code": "def windows_only(func):\n    if platform.system() != 'Windows':\n        return lambda *args, **kwargs: None\n    return func",
    "label": true
  },
  {
    "code": "def display(segments: Iterable[Segment], text: str) -> None:\n    html = _render_segments(segments)\n    jupyter_renderable = JupyterRenderable(html, text)\n    try:\n        from IPython.display import display as ipython_display\n        ipython_display(jupyter_renderable)\n    except ModuleNotFoundError:\n        pass",
    "label": true
  },
  {
    "code": "def srange(s: str) -> str:\n    _expanded = lambda p: p if not isinstance(p, ParseResults) else ''.join((chr(c) for c in range(ord(p[0]), ord(p[1]) + 1)))\n    try:\n        return ''.join((_expanded(part) for part in _reBracketExpr.parse_string(s).body))\n    except Exception as e:\n        return ''",
    "label": true
  },
  {
    "code": "def _get_break_loop_node(break_node: nodes.Break) -> nodes.For | nodes.While | None:\n    loop_nodes = (nodes.For, nodes.While)\n    parent = break_node.parent\n    while not isinstance(parent, loop_nodes) or break_node in getattr(parent, 'orelse', []):\n        break_node = parent\n        parent = parent.parent\n        if parent is None:\n            break\n    return parent",
    "label": true
  },
  {
    "code": "def format_name(project: str, extras: FrozenSet[str]) -> str:\n    if not extras:\n        return project\n    canonical_extras = sorted((canonicalize_name(e) for e in extras))\n    return '{}[{}]'.format(project, ','.join(canonical_extras))",
    "label": true
  },
  {
    "code": "def usage(status: int=0) -> NoReturn:\n    print('finds copy pasted blocks in a set of files')\n    print()\n    print('Usage: symilar [-d|--duplicates min_duplicated_lines] [-i|--ignore-comments] [--ignore-docstrings] [--ignore-imports] [--ignore-signatures] file1...')\n    sys.exit(status)",
    "label": true
  },
  {
    "code": "def prepare_metadata_for_build_wheel(metadata_directory, config_settings, _allow_fallback):\n    backend = _build_backend()\n    try:\n        hook = backend.prepare_metadata_for_build_wheel\n    except AttributeError:\n        if not _allow_fallback:\n            raise HookMissing()\n    else:\n        return hook(metadata_directory, config_settings)\n    whl_basename = backend.build_wheel(metadata_directory, config_settings)\n    return _get_wheel_metadata_from_wheel(whl_basename, metadata_directory, config_settings)",
    "label": true
  },
  {
    "code": "def _escape_regex_range_chars(s: str) -> str:\n    for c in '\\\\^-[]':\n        s = s.replace(c, _bslash + c)\n    s = s.replace('\\n', '\\\\n')\n    s = s.replace('\\t', '\\\\t')\n    return str(s)",
    "label": true
  },
  {
    "code": "def nth_permutation(iterable, r, index):\n    pool = list(iterable)\n    n = len(pool)\n    if r is None or r == n:\n        r, c = (n, factorial(n))\n    elif not 0 <= r < n:\n        raise ValueError\n    else:\n        c = factorial(n) // factorial(n - r)\n    if index < 0:\n        index += c\n    if not 0 <= index < c:\n        raise IndexError\n    if c == 0:\n        return tuple()\n    result = [0] * r\n    q = index * factorial(n) // c if r < n else index\n    for d in range(1, n + 1):\n        q, i = divmod(q, d)\n        if 0 <= n - d < r:\n            result[n - d] = i\n        if q == 0:\n            break\n    return tuple(map(pool.pop, result))",
    "label": true
  },
  {
    "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n    netaddr, bits = net.split('/')\n    netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n    return ipaddr & netmask == network & netmask",
    "label": true
  },
  {
    "code": "def ask_password(message: str) -> str:\n    _check_no_input(message)\n    return getpass.getpass(message)",
    "label": true
  },
  {
    "code": "def objective(baselexer):\n    _oc_keywords = re.compile('@(?:end|implementation|protocol)')\n    _oc_message = re.compile('\\\\[\\\\s*[a-zA-Z_]\\\\w*\\\\s+(?:[a-zA-Z_]\\\\w*\\\\s*\\\\]|(?:[a-zA-Z_]\\\\w*)?:)')\n\n    class GeneratedObjectiveCVariant(baselexer):\n        \"\"\"\n        Implements Objective-C syntax on top of an existing C family lexer.\n        \"\"\"\n        tokens = {'statements': [('@\"', String, 'string'), ('@(YES|NO)', Number), (\"@'(\\\\\\\\.|\\\\\\\\[0-7]{1,3}|\\\\\\\\x[a-fA-F0-9]{1,2}|[^\\\\\\\\\\\\'\\\\n])'\", String.Char), ('@(\\\\d+\\\\.\\\\d*|\\\\.\\\\d+|\\\\d+)[eE][+-]?\\\\d+[lL]?', Number.Float), ('@(\\\\d+\\\\.\\\\d*|\\\\.\\\\d+|\\\\d+[fF])[fF]?', Number.Float), ('@0x[0-9a-fA-F]+[Ll]?', Number.Hex), ('@0[0-7]+[Ll]?', Number.Oct), ('@\\\\d+[Ll]?', Number.Integer), ('@\\\\(', Literal, 'literal_number'), ('@\\\\[', Literal, 'literal_array'), ('@\\\\{', Literal, 'literal_dictionary'), (words(('@selector', '@private', '@protected', '@public', '@encode', '@synchronized', '@try', '@throw', '@catch', '@finally', '@end', '@property', '@synthesize', '__bridge', '__bridge_transfer', '__autoreleasing', '__block', '__weak', '__strong', 'weak', 'strong', 'copy', 'retain', 'assign', 'unsafe_unretained', 'atomic', 'nonatomic', 'readonly', 'readwrite', 'setter', 'getter', 'typeof', 'in', 'out', 'inout', 'release', 'class', '@dynamic', '@optional', '@required', '@autoreleasepool', '@import'), suffix='\\\\b'), Keyword), (words(('id', 'instancetype', 'Class', 'IMP', 'SEL', 'BOOL', 'IBOutlet', 'IBAction', 'unichar'), suffix='\\\\b'), Keyword.Type), ('@(true|false|YES|NO)\\\\n', Name.Builtin), ('(YES|NO|nil|self|super)\\\\b', Name.Builtin), ('(Boolean|UInt8|SInt8|UInt16|SInt16|UInt32|SInt32)\\\\b', Keyword.Type), ('(TRUE|FALSE)\\\\b', Name.Builtin), ('(@interface|@implementation)(\\\\s+)', bygroups(Keyword, Text), ('#pop', 'oc_classname')), ('(@class|@protocol)(\\\\s+)', bygroups(Keyword, Text), ('#pop', 'oc_forward_classname')), ('@', Punctuation), inherit], 'oc_classname': [('([a-zA-Z$_][\\\\w$]*)(\\\\s*:\\\\s*)([a-zA-Z$_][\\\\w$]*)?(\\\\s*)(\\\\{)', bygroups(Name.Class, Text, Name.Class, Text, Punctuation), ('#pop', 'oc_ivars')), ('([a-zA-Z$_][\\\\w$]*)(\\\\s*:\\\\s*)([a-zA-Z$_][\\\\w$]*)?', bygroups(Name.Class, Text, Name.Class), '#pop'), ('([a-zA-Z$_][\\\\w$]*)(\\\\s*)(\\\\([a-zA-Z$_][\\\\w$]*\\\\))(\\\\s*)(\\\\{)', bygroups(Name.Class, Text, Name.Label, Text, Punctuation), ('#pop', 'oc_ivars')), ('([a-zA-Z$_][\\\\w$]*)(\\\\s*)(\\\\([a-zA-Z$_][\\\\w$]*\\\\))', bygroups(Name.Class, Text, Name.Label), '#pop'), ('([a-zA-Z$_][\\\\w$]*)(\\\\s*)(\\\\{)', bygroups(Name.Class, Text, Punctuation), ('#pop', 'oc_ivars')), ('([a-zA-Z$_][\\\\w$]*)', Name.Class, '#pop')], 'oc_forward_classname': [('([a-zA-Z$_][\\\\w$]*)(\\\\s*,\\\\s*)', bygroups(Name.Class, Text), 'oc_forward_classname'), ('([a-zA-Z$_][\\\\w$]*)(\\\\s*;?)', bygroups(Name.Class, Text), '#pop')], 'oc_ivars': [include('whitespace'), include('statements'), (';', Punctuation), ('\\\\{', Punctuation, '#push'), ('\\\\}', Punctuation, '#pop')], 'root': [('^([-+])(\\\\s*)(\\\\(.*?\\\\))?(\\\\s*)([a-zA-Z$_][\\\\w$]*:?)', bygroups(Punctuation, Text, using(this), Text, Name.Function), 'method'), inherit], 'method': [include('whitespace'), (',', Punctuation), ('\\\\.\\\\.\\\\.', Punctuation), ('(\\\\(.*?\\\\))(\\\\s*)([a-zA-Z$_][\\\\w$]*)', bygroups(using(this), Text, Name.Variable)), ('[a-zA-Z$_][\\\\w$]*:', Name.Function), (';', Punctuation, '#pop'), ('\\\\{', Punctuation, 'function'), default('#pop')], 'literal_number': [('\\\\(', Punctuation, 'literal_number_inner'), ('\\\\)', Literal, '#pop'), include('statement')], 'literal_number_inner': [('\\\\(', Punctuation, '#push'), ('\\\\)', Punctuation, '#pop'), include('statement')], 'literal_array': [('\\\\[', Punctuation, 'literal_array_inner'), ('\\\\]', Literal, '#pop'), include('statement')], 'literal_array_inner': [('\\\\[', Punctuation, '#push'), ('\\\\]', Punctuation, '#pop'), include('statement')], 'literal_dictionary': [('\\\\}', Literal, '#pop'), include('statement')]}\n\n        def analyse_text(text):\n            if _oc_keywords.search(text):\n                return 1.0\n            elif '@\"' in text:\n                return 0.8\n            elif re.search('@[0-9]+', text):\n                return 0.7\n            elif _oc_message.search(text):\n                return 0.8\n            return 0\n\n        def get_tokens_unprocessed(self, text, stack=('root',)):\n            from pygments.lexers._cocoa_builtins import COCOA_INTERFACES, COCOA_PROTOCOLS, COCOA_PRIMITIVES\n            for index, token, value in baselexer.get_tokens_unprocessed(self, text, stack):\n                if token is Name or token is Name.Class:\n                    if value in COCOA_INTERFACES or value in COCOA_PROTOCOLS or value in COCOA_PRIMITIVES:\n                        token = Name.Builtin.Pseudo\n                yield (index, token, value)\n    return GeneratedObjectiveCVariant",
    "label": true
  },
  {
    "code": "def __getattr__(name: str) -> object:\n    if name == 'Instance':\n        warnings.warn(INSTANCE_COLLECTOR, 2)\n        return InstanceDummy\n    raise AttributeError(f'module {__name__} has no attribute {name}')",
    "label": true
  },
  {
    "code": "def newer_group(sources, target, missing='error'):\n    if not os.path.exists(target):\n        return 1\n    from stat import ST_MTIME\n    target_mtime = os.stat(target)[ST_MTIME]\n    for source in sources:\n        if not os.path.exists(source):\n            if missing == 'error':\n                pass\n            elif missing == 'ignore':\n                continue\n            elif missing == 'newer':\n                return 1\n        source_mtime = os.stat(source)[ST_MTIME]\n        if source_mtime > target_mtime:\n            return 1\n    else:\n        return 0",
    "label": true
  },
  {
    "code": "def _append_multiline_row(lines, padded_multiline_cells, padded_widths, colaligns, rowfmt, pad, rowalign=None):\n    colwidths = [w - 2 * pad for w in padded_widths]\n    cells_lines = [c.splitlines() for c in padded_multiline_cells]\n    nlines = max(map(len, cells_lines))\n    cells_lines = [_align_cell_veritically(cl, nlines, w, rowalign) for cl, w in zip(cells_lines, colwidths)]\n    lines_cells = [[cl[i] for cl in cells_lines] for i in range(nlines)]\n    for ln in lines_cells:\n        padded_ln = _pad_row(ln, pad)\n        _append_basic_row(lines, padded_ln, colwidths, colaligns, rowfmt)\n    return lines",
    "label": true
  },
  {
    "code": "def get_win_folder_from_env_vars(csidl_name: str) -> str:\n    result = get_win_folder_if_csidl_name_not_env_var(csidl_name)\n    if result is not None:\n        return result\n    env_var_name = {'CSIDL_APPDATA': 'APPDATA', 'CSIDL_COMMON_APPDATA': 'ALLUSERSPROFILE', 'CSIDL_LOCAL_APPDATA': 'LOCALAPPDATA'}.get(csidl_name)\n    if env_var_name is None:\n        msg = f'Unknown CSIDL name: {csidl_name}'\n        raise ValueError(msg)\n    result = os.environ.get(env_var_name)\n    if result is None:\n        msg = f'Unset environment variable: {env_var_name}'\n        raise ValueError(msg)\n    return result",
    "label": true
  },
  {
    "code": "def load_types(pickleable=True, unpickleable=True):\n    from importlib import reload\n    from . import _objects\n    if pickleable:\n        objects.update(_objects.succeeds)\n    else:\n        [objects.pop(obj, None) for obj in _objects.succeeds]\n    if unpickleable:\n        objects.update(_objects.failures)\n    else:\n        [objects.pop(obj, None) for obj in _objects.failures]\n    objects.update(_objects.registered)\n    del _objects\n    [types.__dict__.pop(obj) for obj in list(types.__dict__.keys()) if obj.find('Type') != -1]\n    reload(types)",
    "label": true
  },
  {
    "code": "def _revert_extension():\n    for type, func in list(StockPickler.dispatch.items()):\n        if func.__module__ == __name__:\n            del StockPickler.dispatch[type]\n            if type in pickle_dispatch_copy:\n                StockPickler.dispatch[type] = pickle_dispatch_copy[type]",
    "label": true
  },
  {
    "code": "def get_configuration_files() -> Dict[Kind, List[str]]:\n    global_config_files = [os.path.join(path, CONFIG_BASENAME) for path in appdirs.site_config_dirs('pip')]\n    site_config_file = os.path.join(sys.prefix, CONFIG_BASENAME)\n    legacy_config_file = os.path.join(os.path.expanduser('~'), 'pip' if WINDOWS else '.pip', CONFIG_BASENAME)\n    new_config_file = os.path.join(appdirs.user_config_dir('pip'), CONFIG_BASENAME)\n    return {kinds.GLOBAL: global_config_files, kinds.SITE: [site_config_file], kinds.USER: [legacy_config_file, new_config_file]}",
    "label": true
  },
  {
    "code": "def find_imports_in_stream(input_stream: TextIO, config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, unique: Union[bool, ImportKey]=False, top_only: bool=False, _seen: Optional[Set[str]]=None, **config_kwargs: Any) -> Iterator[identify.Import]:\n    config = _config(config=config, **config_kwargs)\n    identified_imports = identify.imports(input_stream, config=config, file_path=file_path, top_only=top_only)\n    if not unique:\n        yield from identified_imports\n    seen: Set[str] = set() if _seen is None else _seen\n    for identified_import in identified_imports:\n        if unique in (True, ImportKey.ALIAS):\n            key = identified_import.statement()\n        elif unique == ImportKey.ATTRIBUTE:\n            key = f'{identified_import.module}.{identified_import.attribute}'\n        elif unique == ImportKey.MODULE:\n            key = identified_import.module\n        elif unique == ImportKey.PACKAGE:\n            key = identified_import.module.split('.')[0]\n        if key and key not in seen:\n            seen.add(key)\n            yield identified_import",
    "label": true
  },
  {
    "code": "def raise_for_status(resp: Response) -> None:\n    http_error_msg = ''\n    if isinstance(resp.reason, bytes):\n        try:\n            reason = resp.reason.decode('utf-8')\n        except UnicodeDecodeError:\n            reason = resp.reason.decode('iso-8859-1')\n    else:\n        reason = resp.reason\n    if 400 <= resp.status_code < 500:\n        http_error_msg = f'{resp.status_code} Client Error: {reason} for url: {resp.url}'\n    elif 500 <= resp.status_code < 600:\n        http_error_msg = f'{resp.status_code} Server Error: {reason} for url: {resp.url}'\n    if http_error_msg:\n        raise NetworkConnectionError(http_error_msg, response=resp)",
    "label": true
  },
  {
    "code": "def do_float(value: t.Any, default: float=0.0) -> float:\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return default",
    "label": true
  },
  {
    "code": "def rehash(path: str, blocksize: int=1 << 20) -> Tuple[str, str]:\n    h, length = hash_file(path, blocksize)\n    digest = 'sha256=' + urlsafe_b64encode(h.digest()).decode('latin1').rstrip('=')\n    return (digest, str(length))",
    "label": true
  },
  {
    "code": "def _get_zipimporters() -> Iterator[tuple[str, zipimport.zipimporter]]:\n    for filepath, importer in sys.path_importer_cache.items():\n        if isinstance(importer, zipimport.zipimporter):\n            yield (filepath, importer)",
    "label": true
  },
  {
    "code": "def _get_uid(name):\n    if getpwnam is None or name is None:\n        return None\n    try:\n        result = getpwnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None",
    "label": true
  },
  {
    "code": "def show_vendor_versions() -> None:\n    logger.info('vendored library versions:')\n    vendor_txt_versions = create_vendor_txt_map()\n    with indent_log():\n        show_actual_vendor_versions(vendor_txt_versions)",
    "label": true
  },
  {
    "code": "def difference(iterable, func=sub, *, initial=None):\n    a, b = tee(iterable)\n    try:\n        first = [next(b)]\n    except StopIteration:\n        return iter([])\n    if initial is not None:\n        first = []\n    return chain(first, map(func, b, a))",
    "label": true
  },
  {
    "code": "def convert_path(pathname):\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n    paths = pathname.split('/')\n    while os.curdir in paths:\n        paths.remove(os.curdir)\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)",
    "label": true
  },
  {
    "code": "def infer_dataclass_field_call(node: nodes.Call, ctx: context.InferenceContext | None=None) -> Iterator[InferenceResult]:\n    if not isinstance(node.parent, (nodes.AnnAssign, nodes.Assign)):\n        raise UseInferenceDefault\n    result = _get_field_default(node)\n    if not result:\n        yield Uninferable\n    else:\n        default_type, default = result\n        if default_type == 'default':\n            yield from default.infer(context=ctx)\n        else:\n            new_call = parse(default.as_string()).body[0].value\n            new_call.parent = node.parent\n            yield from new_call.infer(context=ctx)",
    "label": true
  },
  {
    "code": "def _clean_url_path(path: str, is_local_path: bool) -> str:\n    if is_local_path:\n        clean_func = _clean_file_url_path\n    else:\n        clean_func = _clean_url_path_part\n    parts = _reserved_chars_re.split(path)\n    cleaned_parts = []\n    for to_clean, reserved in pairwise(itertools.chain(parts, [''])):\n        cleaned_parts.append(clean_func(to_clean))\n        cleaned_parts.append(reserved.upper())\n    return ''.join(cleaned_parts)",
    "label": true
  },
  {
    "code": "def safe_version(version):\n    try:\n        return str(packaging.version.Version(version))\n    except packaging.version.InvalidVersion:\n        version = version.replace(' ', '.')\n        return re.sub('[^A-Za-z0-9.]+', '-', version)",
    "label": true
  },
  {
    "code": "def _check_const_name(node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_upper_case_with_underscores(name):\n        msg = f'{node_type.capitalize()} name \"{name}\" should be in UPPER_CASE_WITH_UNDERSCORES format. Constants should be all-uppercase words with each word separated by an underscore. A single leading underscore can be used to denote a private constant.'\n        if node_type == 'class constant':\n            msg += \" A double leading underscore invokes Python's name-mangling rules.\"\n        error_msgs.append(msg)\n    return error_msgs",
    "label": true
  },
  {
    "code": "def _get_suffixes():\n    if imp:\n        return [s[0] for s in imp.get_suffixes()]\n    else:\n        return importlib.machinery.EXTENSION_SUFFIXES",
    "label": true
  },
  {
    "code": "def parse_key(src: str, pos: Pos) -> tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: str | None = src[pos]\n        except IndexError:\n            char = None\n        if char != '.':\n            return (pos, key)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def _has_different_parameters(original: list[nodes.AssignName], overridden: list[nodes.AssignName], dummy_parameter_regex: Pattern[str]) -> list[str]:\n    result: list[str] = []\n    zipped = zip_longest(original, overridden)\n    for original_param, overridden_param in zipped:\n        if not overridden_param:\n            return ['Number of parameters ']\n        if not original_param:\n            try:\n                overridden_param.parent.default_value(overridden_param.name)\n                continue\n            except astroid.NoDefault:\n                return ['Number of parameters ']\n        names = [param.name for param in (original_param, overridden_param)]\n        if any((dummy_parameter_regex.match(name) for name in names)):\n            continue\n        if original_param.name != overridden_param.name:\n            result.append(f\"Parameter '{original_param.name}' has been renamed to '{overridden_param.name}' in\")\n    return result",
    "label": true
  },
  {
    "code": "def unpack_file(filename: str, location: str, content_type: Optional[str]=None) -> None:\n    filename = os.path.realpath(filename)\n    if content_type == 'application/zip' or filename.lower().endswith(ZIP_EXTENSIONS) or zipfile.is_zipfile(filename):\n        unzip_file(filename, location, flatten=not filename.endswith('.whl'))\n    elif content_type == 'application/x-gzip' or tarfile.is_tarfile(filename) or filename.lower().endswith(TAR_EXTENSIONS + BZ2_EXTENSIONS + XZ_EXTENSIONS):\n        untar_file(filename, location)\n    else:\n        logger.critical('Cannot unpack file %s (downloaded from %s, content-type: %s); cannot detect archive format', filename, location, content_type)\n        raise InstallationError(f'Cannot determine archive format of {location}')",
    "label": true
  },
  {
    "code": "def is_local(path: str) -> bool:\n    if not running_under_virtualenv():\n        return True\n    return path.startswith(normalize_path(sys.prefix))",
    "label": true
  },
  {
    "code": "def get_entrypoints(dist: BaseDistribution) -> Tuple[Dict[str, str], Dict[str, str]]:\n    console_scripts = {}\n    gui_scripts = {}\n    for entry_point in dist.iter_entry_points():\n        if entry_point.group == 'console_scripts':\n            console_scripts[entry_point.name] = entry_point.value\n        elif entry_point.group == 'gui_scripts':\n            gui_scripts[entry_point.name] = entry_point.value\n    return (console_scripts, gui_scripts)",
    "label": true
  },
  {
    "code": "def resolve_partial(partial: 'EditablePartial[T]') -> T:\n    if isinstance(partial, EditablePartial):\n        partial.args = resolve_partial(partial.args)\n        partial.kwargs = resolve_partial(partial.kwargs)\n        return partial()\n    elif isinstance(partial, list):\n        return [resolve_partial(x) for x in partial]\n    elif isinstance(partial, dict):\n        return {key: resolve_partial(x) for key, x in partial.items()}\n    else:\n        return partial",
    "label": true
  },
  {
    "code": "def product_index(element, *args):\n    index = 0\n    for x, pool in zip_longest(element, args, fillvalue=_marker):\n        if x is _marker or pool is _marker:\n            raise ValueError('element is not a product of args')\n        pool = tuple(pool)\n        index = index * len(pool) + pool.index(x)\n    return index",
    "label": true
  },
  {
    "code": "def _normalize(*values: str, key: str) -> Tuple[str, ...]:\n    if key == 'extra':\n        return tuple((canonicalize_name(v) for v in values))\n    return values",
    "label": true
  },
  {
    "code": "def clear() -> None:\n    if not isatty(sys.stdout):\n        return\n    echo('\\x1b[2J\\x1b[1;1H', nl=False)",
    "label": true
  },
  {
    "code": "def _linux_platforms(is_32bit: bool=_32_BIT_INTERPRETER) -> Iterator[str]:\n    linux = _normalize_string(sysconfig.get_platform())\n    if is_32bit:\n        if linux == 'linux_x86_64':\n            linux = 'linux_i686'\n        elif linux == 'linux_aarch64':\n            linux = 'linux_armv7l'\n    _, arch = linux.split('_', 1)\n    yield from _manylinux.platform_tags(linux, arch)\n    yield from _musllinux.platform_tags(arch)\n    yield linux",
    "label": true
  },
  {
    "code": "def extract_from_urllib3():\n    util.SSLContext = orig_util_SSLContext\n    util.ssl_.SSLContext = orig_util_SSLContext\n    util.HAS_SNI = orig_util_HAS_SNI\n    util.ssl_.HAS_SNI = orig_util_HAS_SNI\n    util.IS_PYOPENSSL = False\n    util.ssl_.IS_PYOPENSSL = False",
    "label": true
  },
  {
    "code": "def citation():\n    print(__doc__[-491:-118])\n    return",
    "label": true
  },
  {
    "code": "def pypy_partial(val):\n    is_pypy = platform.python_implementation() == 'PyPy'\n    return val + is_pypy",
    "label": true
  },
  {
    "code": "def assert_raises(exception, function, *args, **kwargs):\n    try:\n        function(*args, **kwargs)\n    except exception:\n        return\n    else:\n        raise AssertionError('Did not raise %s.' % exception)",
    "label": true
  },
  {
    "code": "def filename_match(filename, patterns, default=True):\n    if not patterns:\n        return default\n    return any((fnmatch(filename, pattern) for pattern in patterns))",
    "label": true
  },
  {
    "code": "def get_args():\n    parser = ArgumentParser()\n    parser.add_argument('url', help='The URL to try and cache')\n    return parser.parse_args()",
    "label": true
  },
  {
    "code": "def unique_everseen(iterable, key=None):\n    seenset = set()\n    seenset_add = seenset.add\n    seenlist = []\n    seenlist_add = seenlist.append\n    use_key = key is not None\n    for element in iterable:\n        k = key(element) if use_key else element\n        try:\n            if k not in seenset:\n                seenset_add(k)\n                yield element\n        except TypeError:\n            if k not in seenlist:\n                seenlist_add(k)\n                yield element",
    "label": true
  },
  {
    "code": "def set_from_last_child(node):\n    last_child = _get_last_child(node)\n    if not last_child:\n        set_without_children(node)\n        return node\n    elif not hasattr(last_child, 'end_lineno'):\n        set_without_children(last_child)\n    if last_child.end_lineno is not None:\n        node.end_lineno = last_child.end_lineno\n    if last_child.end_col_offset is not None:\n        node.end_col_offset = last_child.end_col_offset\n    return node",
    "label": true
  },
  {
    "code": "def webify(color):\n    if color.startswith('calc') or color.startswith('var'):\n        return color\n    else:\n        return '#' + color",
    "label": true
  },
  {
    "code": "def platform_tags(archs: Sequence[str]) -> Iterator[str]:\n    sys_musl = _get_musl_version(sys.executable)\n    if sys_musl is None:\n        return\n    for arch in archs:\n        for minor in range(sys_musl.minor, -1, -1):\n            yield f'musllinux_{sys_musl.major}_{minor}_{arch}'",
    "label": true
  },
  {
    "code": "def interpreter_name() -> str:\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name",
    "label": true
  },
  {
    "code": "def _first_paragraph(doc: str) -> str:\n    paragraph, _, _ = doc.partition('\\n\\n')\n    return paragraph",
    "label": true
  },
  {
    "code": "def compose(*funcs):\n\n    def compose_two(f1, f2):\n        return lambda *args, **kwargs: f1(f2(*args, **kwargs))\n    return functools.reduce(compose_two, funcs)",
    "label": true
  },
  {
    "code": "def _options_enum() -> str:\n    enum = '\\n    class Options(_IntFlag):\\n        OP_ALL = 1\\n        OP_NO_SSLv2 = 2\\n        OP_NO_SSLv3 = 3\\n        OP_NO_TLSv1 = 4\\n        OP_NO_TLSv1_1 = 5\\n        OP_NO_TLSv1_2 = 6\\n        OP_NO_TLSv1_3 = 7\\n        OP_CIPHER_SERVER_PREFERENCE = 8\\n        OP_SINGLE_DH_USE = 9\\n        OP_SINGLE_ECDH_USE = 10\\n        OP_NO_COMPRESSION = 11\\n        OP_NO_TICKET = 12\\n        OP_NO_RENEGOTIATION = 13'\n    if PY38_PLUS:\n        enum += '\\n        OP_ENABLE_MIDDLEBOX_COMPAT = 14'\n    return enum",
    "label": true
  },
  {
    "code": "def consecutive_groups(iterable, ordering=lambda x: x):\n    for k, g in groupby(enumerate(iterable), key=lambda x: x[0] - ordering(x[1])):\n        yield map(itemgetter(1), g)",
    "label": true
  },
  {
    "code": "def _detect_pathlib_path(p):\n    if (3, 4) <= sys.version_info:\n        import pathlib\n        if isinstance(p, pathlib.PurePath):\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def user_config_dir(appname: str, roaming: bool=True) -> str:\n    if sys.platform == 'darwin':\n        return _macos_user_config_dir(appname, roaming)\n    return _appdirs.user_config_dir(appname, appauthor=False, roaming=roaming)",
    "label": true
  },
  {
    "code": "def site_config_dirs(appname: str) -> List[str]:\n    if sys.platform == 'darwin':\n        return [_appdirs.site_data_dir(appname, appauthor=False, multipath=True)]\n    dirval = _appdirs.site_config_dir(appname, appauthor=False, multipath=True)\n    if sys.platform == 'win32':\n        return [dirval]\n    return dirval.split(os.pathsep) + ['/etc']",
    "label": true
  },
  {
    "code": "def resolve_partial(partial: 'EditablePartial[T]') -> T:\n    if isinstance(partial, EditablePartial):\n        partial.args = resolve_partial(partial.args)\n        partial.kwargs = resolve_partial(partial.kwargs)\n        return partial()\n    elif isinstance(partial, list):\n        return [resolve_partial(x) for x in partial]\n    elif isinstance(partial, dict):\n        return {key: resolve_partial(x) for key, x in partial.items()}\n    else:\n        return partial",
    "label": true
  },
  {
    "code": "def _is_class_var(node: nodes.NodeNG) -> bool:\n    if PY39_PLUS:\n        try:\n            inferred = next(node.infer())\n        except (InferenceError, StopIteration):\n            return False\n        return getattr(inferred, 'name', '') == 'ClassVar'\n    return isinstance(node, nodes.Subscript) and (isinstance(node.value, nodes.Name) and node.value.name == 'ClassVar' or (isinstance(node.value, nodes.Attribute) and node.value.attrname == 'ClassVar'))",
    "label": true
  },
  {
    "code": "def parse_list_header(value):\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def test_classes():\n    from io import BytesIO as StringIO\n    y = 'from _io import BytesIO\\n'\n    x = y if IS_PYPY or sys.hexversion >= PY310b else 'from io import BytesIO\\n'\n    s = StringIO()\n    assert likely_import(StringIO) == x\n    assert likely_import(s) == y\n    assert likely_import(Foo) == 'from %s import Foo\\n' % __name__\n    assert likely_import(_foo) == 'from %s import Foo\\n' % __name__",
    "label": true
  },
  {
    "code": "def init_logging() -> None:\n    logging.setLoggerClass(VerboseLogger)\n    logging.addLevelName(VERBOSE, 'VERBOSE')",
    "label": true
  },
  {
    "code": "def _visible_exprs(exprs: Iterable[pyparsing.ParserElement]):\n    non_diagramming_exprs = (pyparsing.ParseElementEnhance, pyparsing.PositionToken, pyparsing.And._ErrorStop)\n    return [e for e in exprs if not (e.customName or e.resultsName or isinstance(e, non_diagramming_exprs))]",
    "label": true
  },
  {
    "code": "def parse_configuration(distribution: 'Distribution', command_options: AllCommandOptions, ignore_option_errors=False) -> Tuple['ConfigMetadataHandler', 'ConfigOptionsHandler']:\n    with expand.EnsurePackagesDiscovered(distribution) as ensure_discovered:\n        options = ConfigOptionsHandler(distribution, command_options, ignore_option_errors, ensure_discovered)\n        options.parse()\n        if not distribution.package_dir:\n            distribution.package_dir = options.package_dir\n        meta = ConfigMetadataHandler(distribution.metadata, command_options, ignore_option_errors, ensure_discovered, distribution.package_dir, distribution.src_root)\n        meta.parse()\n        distribution._referenced_files.update(options._referenced_files, meta._referenced_files)\n    return (meta, options)",
    "label": true
  },
  {
    "code": "def _handle_merge_hash(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    if not parser.values.hashes:\n        parser.values.hashes = {}\n    try:\n        algo, digest = value.split(':', 1)\n    except ValueError:\n        parser.error('Arguments to {} must be a hash name followed by a value, like --hash=sha256:abcde...'.format(opt_str))\n    if algo not in STRONG_HASHES:\n        parser.error('Allowed hash algorithms for {} are {}.'.format(opt_str, ', '.join(STRONG_HASHES)))\n    parser.values.hashes.setdefault(algo, []).append(digest)",
    "label": true
  },
  {
    "code": "def _handle_only_binary(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(value, existing.only_binary, existing.no_binary)",
    "label": true
  },
  {
    "code": "def _cpython_abis(py_version: PythonVersion, warn: bool=False) -> List[str]:\n    py_version = tuple(py_version)\n    abis = []\n    version = _version_nodot(py_version[:2])\n    debug = pymalloc = ucs4 = ''\n    with_debug = _get_config_var('Py_DEBUG', warn)\n    has_refcount = hasattr(sys, 'gettotalrefcount')\n    has_ext = '_d.pyd' in EXTENSION_SUFFIXES\n    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n        debug = 'd'\n    if py_version < (3, 8):\n        with_pymalloc = _get_config_var('WITH_PYMALLOC', warn)\n        if with_pymalloc or with_pymalloc is None:\n            pymalloc = 'm'\n        if py_version < (3, 3):\n            unicode_size = _get_config_var('Py_UNICODE_SIZE', warn)\n            if unicode_size == 4 or (unicode_size is None and sys.maxunicode == 1114111):\n                ucs4 = 'u'\n    elif debug:\n        abis.append(f'cp{version}')\n    abis.insert(0, 'cp{version}{debug}{pymalloc}{ucs4}'.format(version=version, debug=debug, pymalloc=pymalloc, ucs4=ucs4))\n    return abis",
    "label": true
  },
  {
    "code": "def _get_prepared_distribution(req: InstallRequirement, build_tracker: BuildTracker, finder: PackageFinder, build_isolation: bool, check_build_deps: bool) -> BaseDistribution:\n    abstract_dist = make_distribution_for_install_requirement(req)\n    with build_tracker.track(req):\n        abstract_dist.prepare_distribution_metadata(finder, build_isolation, check_build_deps)\n    return abstract_dist.get_metadata_distribution()",
    "label": true
  },
  {
    "code": "def _get_text_stdin(buffer_stream: t.BinaryIO) -> t.TextIO:\n    text_stream = _NonClosingTextIOWrapper(io.BufferedReader(_WindowsConsoleReader(STDIN_HANDLE)), 'utf-16-le', 'strict', line_buffering=True)\n    return t.cast(t.TextIO, ConsoleStream(text_stream, buffer_stream))",
    "label": true
  },
  {
    "code": "def report_messages_by_module_stats(sect: Section, stats: LinterStats, _: LinterStats | None) -> None:\n    module_stats = stats.by_module\n    if len(module_stats) == 1:\n        raise exceptions.EmptyReportError()\n    by_mod: defaultdict[str, dict[str, int | float]] = collections.defaultdict(dict)\n    for m_type in ('fatal', 'error', 'warning', 'refactor', 'convention'):\n        total = stats.get_global_message_count(m_type)\n        for module in module_stats.keys():\n            mod_total = stats.get_module_message_count(module, m_type)\n            percent = 0 if total == 0 else float(mod_total * 100) / total\n            by_mod[module][m_type] = percent\n    sorted_result = []\n    for module, mod_info in by_mod.items():\n        sorted_result.append((mod_info['error'], mod_info['warning'], mod_info['refactor'], mod_info['convention'], module))\n    sorted_result.sort()\n    sorted_result.reverse()\n    lines = ['module', 'error', 'warning', 'refactor', 'convention']\n    for line in sorted_result:\n        if all((entry == 0 for entry in line[:-1])):\n            continue\n        lines.append(line[-1])\n        for val in line[:-1]:\n            lines.append(f'{val:.2f}')\n    if len(lines) == 5:\n        raise exceptions.EmptyReportError()\n    sect.append(Table(children=lines, cols=5, rheaders=1))",
    "label": true
  },
  {
    "code": "def check_if_graphviz_supports_format(output_format: str) -> None:\n    dot_output = subprocess.run(['dot', '-T?'], capture_output=True, check=False, encoding='utf-8')\n    match = re.match(pattern='.*Use one of: (?P<formats>(\\\\S*\\\\s?)+)', string=dot_output.stderr.strip())\n    if not match:\n        print('Unable to determine Graphviz supported output formats. Pyreverse will continue, but subsequent error messages regarding the output format may come from Graphviz directly.')\n        return\n    supported_formats = match.group('formats')\n    if output_format not in supported_formats.split():\n        print(f'Format {output_format} is not supported by Graphviz. It supports: {supported_formats}')\n        sys.exit(32)",
    "label": true
  },
  {
    "code": "def assert_raises(exception, function, *args, **kwargs):\n    try:\n        function(*args, **kwargs)\n    except exception:\n        return\n    else:\n        raise AssertionError('Did not raise %s.' % exception)",
    "label": true
  },
  {
    "code": "def _path_from_filename(filename: str, is_jython: bool=IS_JYTHON) -> str:\n    if not is_jython:\n        return filename\n    head, has_pyclass, _ = filename.partition('$py.class')\n    if has_pyclass:\n        return head + '.py'\n    return filename",
    "label": true
  },
  {
    "code": "def chmod(path, mode):\n    log.debug('changing mode of %s to %o', path, mode)\n    try:\n        _chmod(path, mode)\n    except os.error as e:\n        log.debug('chmod failed: %s', e)",
    "label": true
  },
  {
    "code": "def _implementation():\n    implementation = platform.python_implementation()\n    if implementation == 'CPython':\n        implementation_version = platform.python_version()\n    elif implementation == 'PyPy':\n        implementation_version = '{}.{}.{}'.format(sys.pypy_version_info.major, sys.pypy_version_info.minor, sys.pypy_version_info.micro)\n        if sys.pypy_version_info.releaselevel != 'final':\n            implementation_version = ''.join([implementation_version, sys.pypy_version_info.releaselevel])\n    elif implementation == 'Jython':\n        implementation_version = platform.python_version()\n    elif implementation == 'IronPython':\n        implementation_version = platform.python_version()\n    else:\n        implementation_version = 'Unknown'\n    return {'name': implementation, 'version': implementation_version}",
    "label": true
  },
  {
    "code": "def pytest_sessionfinish(session, exitstatus: Union[int, ExitCode]):\n    tmp_path_factory: TempPathFactory = session.config._tmp_path_factory\n    basetemp = tmp_path_factory._basetemp\n    if basetemp is None:\n        return\n    policy = tmp_path_factory._retention_policy\n    if exitstatus == 0 and policy == 'failed' and (tmp_path_factory._given_basetemp is None):\n        if basetemp.is_dir():\n            rmtree(basetemp, ignore_errors=True)\n    if basetemp.is_dir():\n        cleanup_dead_symlinks(basetemp)",
    "label": true
  },
  {
    "code": "def get_cycles(graph_dict: dict[str, set[str]], vertices: list[str] | None=None) -> Sequence[list[str]]:\n    if not graph_dict:\n        return ()\n    result: list[list[str]] = []\n    if vertices is None:\n        vertices = list(graph_dict.keys())\n    for vertice in vertices:\n        _get_cycles(graph_dict, [], set(), result, vertice)\n    return result",
    "label": true
  },
  {
    "code": "def select_wait_for_socket(sock, read=False, write=False, timeout=None):\n    if not read and (not write):\n        raise RuntimeError('must specify at least one of read=True, write=True')\n    rcheck = []\n    wcheck = []\n    if read:\n        rcheck.append(sock)\n    if write:\n        wcheck.append(sock)\n    fn = partial(select.select, rcheck, wcheck, wcheck)\n    rready, wready, xready = _retry_on_intr(fn, timeout)\n    return bool(rready or wready or xready)",
    "label": true
  },
  {
    "code": "def check_list(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if not isinstance(value, list):\n        raise TypeCheckError('is not a list')\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for i, v in enumerate(samples):\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f'item {i}')\n                raise",
    "label": true
  },
  {
    "code": "def test_collection_function_recursion():\n    g = copy(collection_function_recursion())\n    assert g()['g'] is g",
    "label": true
  },
  {
    "code": "def _ann_node_to_type(node: nodes.Name) -> TypeResult:\n    try:\n        ann_node_type = _node_to_type(node)\n    except SyntaxError:\n        return TypeFailAnnotationInvalid(node)\n    ann_type = _generic_to_annotation(ann_node_type, node)\n    return ann_type",
    "label": true
  },
  {
    "code": "def _is_property_kind(node: nodes.NodeNG, *kinds: str) -> bool:\n    if not isinstance(node, (astroid.UnboundMethod, nodes.FunctionDef)):\n        return False\n    if node.decorators:\n        for decorator in node.decorators.nodes:\n            if isinstance(decorator, nodes.Attribute) and decorator.attrname in kinds:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def _temporary_keychain() -> tuple[SecKeychainRef, str]:\n    random_bytes = os.urandom(40)\n    filename = base64.b16encode(random_bytes[:8]).decode('utf-8')\n    password = base64.b16encode(random_bytes[8:])\n    tempdirectory = tempfile.mkdtemp()\n    keychain_path = os.path.join(tempdirectory, filename).encode('utf-8')\n    keychain = Security.SecKeychainRef()\n    status = Security.SecKeychainCreate(keychain_path, len(password), password, False, None, ctypes.byref(keychain))\n    _assert_no_error(status)\n    return (keychain, tempdirectory)",
    "label": true
  },
  {
    "code": "def _infer_augassign(self: nodes.AugAssign, context: InferenceContext | None=None) -> Generator[InferenceResult | util.BadBinaryOperationMessage, None, None]:\n    context = context or InferenceContext()\n    rhs_context = context.clone()\n    lhs_iter = self.target.infer_lhs(context=context)\n    rhs_iter = self.value.infer(context=rhs_context)\n    for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any((isinstance(value, util.UninferableBase) for value in (rhs, lhs))):\n            yield util.Uninferable\n            return\n        try:\n            yield from _infer_binary_operation(left=lhs, right=rhs, binary_opnode=self, context=context, flow_factory=_get_aug_flow)\n        except _NonDeducibleTypeHierarchy:\n            yield util.Uninferable",
    "label": true
  },
  {
    "code": "def decode(input, fallback_encoding, errors='replace'):\n    fallback_encoding = _get_encoding(fallback_encoding)\n    bom_encoding, input = _detect_bom(input)\n    encoding = bom_encoding or fallback_encoding\n    return (encoding.codec_info.decode(input, errors)[0], encoding)",
    "label": true
  },
  {
    "code": "def test_the_rest():\n    for obj in [Bar, Foo, Foo.bar, _foo.bar]:\n        pyfile = dumpIO_source(obj, alias='_obj')\n        _obj = loadIO_source(pyfile)\n        assert _obj.__name__ == obj.__name__",
    "label": true
  },
  {
    "code": "def distinct_combinations(iterable, r):\n    if r < 0:\n        raise ValueError('r must be non-negative')\n    elif r == 0:\n        yield ()\n        return\n    pool = tuple(iterable)\n    generators = [unique_everseen(enumerate(pool), key=itemgetter(1))]\n    current_combo = [None] * r\n    level = 0\n    while generators:\n        try:\n            cur_idx, p = next(generators[-1])\n        except StopIteration:\n            generators.pop()\n            level -= 1\n            continue\n        current_combo[level] = p\n        if level + 1 == r:\n            yield tuple(current_combo)\n        else:\n            generators.append(unique_everseen(enumerate(pool[cur_idx + 1:], cur_idx + 1), key=itemgetter(1)))\n            level += 1",
    "label": true
  },
  {
    "code": "def readlines(filename):\n    try:\n        with tokenize.open(filename) as f:\n            return f.readlines()\n    except (LookupError, SyntaxError, UnicodeError):\n        with open(filename, encoding='latin-1') as f:\n            return f.readlines()",
    "label": true
  },
  {
    "code": "def has_default_eq(obj: object) -> bool:\n    if hasattr(obj.__eq__, '__code__') and hasattr(obj.__eq__.__code__, 'co_filename'):\n        code_filename = obj.__eq__.__code__.co_filename\n        if isattrs(obj):\n            return 'attrs generated eq' in code_filename\n        return code_filename == '<string>'\n    return True",
    "label": true
  },
  {
    "code": "def get_unicode_from_response(r):\n    warnings.warn('In requests 3.0, get_unicode_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)', DeprecationWarning)\n    tried_encodings = []\n    encoding = get_encoding_from_headers(r.headers)\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n    try:\n        return str(r.content, encoding, errors='replace')\n    except TypeError:\n        return r.content",
    "label": true
  },
  {
    "code": "def partitions(iterable):\n    sequence = list(iterable)\n    n = len(sequence)\n    for i in powerset(range(1, n)):\n        yield [sequence[i:j] for i, j in zip((0,) + i, i + (n,))]",
    "label": true
  },
  {
    "code": "def test_moduledict_where_not_main():\n    try:\n        from . import test_moduledict\n    except ImportError:\n        import test_moduledict\n    name = 'test_moduledict.py'\n    if os.path.exists(name) and os.path.exists(name + 'c'):\n        os.remove(name + 'c')\n    if os.path.exists(name) and hasattr(test_moduledict, '__cached__') and os.path.exists(test_moduledict.__cached__):\n        os.remove(getattr(test_moduledict, '__cached__'))\n    if os.path.exists('__pycache__') and (not os.listdir('__pycache__')):\n        os.removedirs('__pycache__')",
    "label": true
  },
  {
    "code": "def _normalize_host(host, scheme):\n    host = normalize_host(host, scheme)\n    if host.startswith('[') and host.endswith(']'):\n        host = host[1:-1]\n    return host",
    "label": true
  },
  {
    "code": "def _sset_dict(key, ob, state):\n    ob.clear()\n    ob.update(state)",
    "label": true
  },
  {
    "code": "def with_metaclass(meta, *bases):\n\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})",
    "label": true
  },
  {
    "code": "def _semantic_key(s):\n\n    def make_tuple(s, absent):\n        if s is None:\n            result = (absent,)\n        else:\n            parts = s[1:].split('.')\n            result = tuple([p.zfill(8) if p.isdigit() else p for p in parts])\n        return result\n    m = is_semver(s)\n    if not m:\n        raise UnsupportedVersionError(s)\n    groups = m.groups()\n    major, minor, patch = [int(i) for i in groups[:3]]\n    pre, build = (make_tuple(groups[3], '|'), make_tuple(groups[5], '*'))\n    return ((major, minor, patch), pre, build)",
    "label": true
  },
  {
    "code": "def _is_compatible(name: str, arch: str, version: _GLibCVersion) -> bool:\n    sys_glibc = _get_glibc_version()\n    if sys_glibc < version:\n        return False\n    try:\n        import _manylinux\n    except ImportError:\n        return True\n    if hasattr(_manylinux, 'manylinux_compatible'):\n        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n        if result is not None:\n            return bool(result)\n        return True\n    if version == _GLibCVersion(2, 5):\n        if hasattr(_manylinux, 'manylinux1_compatible'):\n            return bool(_manylinux.manylinux1_compatible)\n    if version == _GLibCVersion(2, 12):\n        if hasattr(_manylinux, 'manylinux2010_compatible'):\n            return bool(_manylinux.manylinux2010_compatible)\n    if version == _GLibCVersion(2, 17):\n        if hasattr(_manylinux, 'manylinux2014_compatible'):\n            return bool(_manylinux.manylinux2014_compatible)\n    return True",
    "label": true
  },
  {
    "code": "def merge_cookies(cookiejar, cookies):\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError('You can only merge into CookieJar')\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n    return cookiejar",
    "label": true
  },
  {
    "code": "def _load_ep(ep: 'metadata.EntryPoint') -> Optional[Tuple[str, Type]]:\n    try:\n        return (ep.name, ep.load())\n    except Exception as ex:\n        msg = f'{ex.__class__.__name__} while trying to load entry-point {ep.name}'\n        _logger.warning(f'{msg}: {ex}')\n        return None",
    "label": true
  },
  {
    "code": "def get_lexer_for_filename(_fn, code=None, **options):\n    res = find_lexer_class_for_filename(_fn, code)\n    if not res:\n        raise ClassNotFound('no lexer for filename %r found' % _fn)\n    return res(**options)",
    "label": true
  },
  {
    "code": "def _should_enable_warnings(cmd_line_warn_options: typing.Iterable[str], warn_env_var: typing.Optional[str]) -> bool:\n    enable = bool(warn_env_var)\n    for warn_opt in cmd_line_warn_options:\n        w_action, w_message, w_category, w_module, w_line = (warn_opt + '::::').split(':')[:5]\n        if not w_action.lower().startswith('i') and (not (w_message or w_category or w_module) or w_module == 'pyparsing'):\n            enable = True\n        elif w_action.lower().startswith('i') and w_module in ('pyparsing', ''):\n            enable = False\n    return enable",
    "label": true
  },
  {
    "code": "def _display_annotation(annotation: Any) -> str:\n    if annotation is type(None):\n        return 'None'\n    if hasattr(annotation, '__origin__'):\n        return repr(annotation)\n    elif hasattr(annotation, '__name__'):\n        return annotation.__name__\n    else:\n        return repr(annotation)",
    "label": true
  },
  {
    "code": "def _cmpkey(epoch: int, release: Tuple[int, ...], pre: Optional[Tuple[str, int]], post: Optional[Tuple[str, int]], dev: Optional[Tuple[str, int]], local: Optional[Tuple[SubLocalType]]) -> CmpKey:\n    _release = tuple(reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release)))))\n    if pre is None and post is None and (dev is not None):\n        _pre: PrePostDevType = NegativeInfinity\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n    if post is None:\n        _post: PrePostDevType = NegativeInfinity\n    else:\n        _post = post\n    if dev is None:\n        _dev: PrePostDevType = Infinity\n    else:\n        _dev = dev\n    if local is None:\n        _local: LocalType = NegativeInfinity\n    else:\n        _local = tuple(((i, '') if isinstance(i, int) else (NegativeInfinity, i) for i in local))\n    return (epoch, _release, _pre, _post, _dev, _local)",
    "label": true
  },
  {
    "code": "def libc_ver() -> Tuple[str, str]:\n    glibc_version = glibc_version_string()\n    if glibc_version is None:\n        return ('', '')\n    else:\n        return ('glibc', glibc_version)",
    "label": true
  },
  {
    "code": "def _default_success_debug_action(instring: str, startloc: int, endloc: int, expr: 'ParserElement', toks: ParseResults, cache_hit: bool=False):\n    cache_hit_str = '*' if cache_hit else ''\n    print(f'{cache_hit_str}Matched {expr} -> {toks.as_list()}')",
    "label": true
  },
  {
    "code": "def _expand_allowed_platforms(platforms: Optional[List[str]]) -> Optional[List[str]]:\n    if not platforms:\n        return None\n    seen = set()\n    result = []\n    for p in platforms:\n        if p in seen:\n            continue\n        additions = [c for c in _get_custom_platforms(p) if c not in seen]\n        seen.update(additions)\n        result.extend(additions)\n    return result",
    "label": true
  },
  {
    "code": "def dist_factory(path_item, entry, only):\n    lower = entry.lower()\n    is_egg_info = lower.endswith('.egg-info')\n    is_dist_info = lower.endswith('.dist-info') and os.path.isdir(os.path.join(path_item, entry))\n    is_meta = is_egg_info or is_dist_info\n    return distributions_from_metadata if is_meta else find_distributions if not only and _is_egg_path(entry) else resolve_egg_link if not only and lower.endswith('.egg-link') else NoDists()",
    "label": true
  },
  {
    "code": "def _translate_ch_to_exc(ch: str) -> t.Optional[BaseException]:\n    if ch == '\\x03':\n        raise KeyboardInterrupt()\n    if ch == '\\x04' and (not WIN):\n        raise EOFError()\n    if ch == '\\x1a' and WIN:\n        raise EOFError()\n    return None",
    "label": true
  },
  {
    "code": "def iglob(path_glob):\n    if _CHECK_RECURSIVE_GLOB.search(path_glob):\n        msg = 'invalid glob %r: recursive glob \"**\" must be used alone'\n        raise ValueError(msg % path_glob)\n    if _CHECK_MISMATCH_SET.search(path_glob):\n        msg = \"invalid glob %r: mismatching set marker '{' or '}'\"\n        raise ValueError(msg % path_glob)\n    return _iglob(path_glob)",
    "label": true
  },
  {
    "code": "def _mac_arch(arch: str, is_32bit: bool=_32_BIT_INTERPRETER) -> str:\n    if not is_32bit:\n        return arch\n    if arch.startswith('ppc'):\n        return 'ppc'\n    return 'i386'",
    "label": true
  },
  {
    "code": "def register_plugins(linter: PyLinter, directory: str) -> None:\n    imported = {}\n    for filename in os.listdir(directory):\n        base, extension = os.path.splitext(filename)\n        if base in imported or base == '__pycache__':\n            continue\n        if extension in PY_EXTS and base != '__init__' or (not extension and os.path.isdir(os.path.join(directory, base)) and (not filename.startswith('.'))):\n            try:\n                module = modutils.load_module_from_file(os.path.join(directory, filename))\n            except ValueError:\n                continue\n            except ImportError as exc:\n                print(f'Problem importing module {filename}: {exc}', file=sys.stderr)\n            else:\n                if hasattr(module, 'register'):\n                    module.register(linter)\n                    imported[base] = 1",
    "label": true
  },
  {
    "code": "def get_purelib() -> str:\n    new = _sysconfig.get_purelib()\n    if _USE_SYSCONFIG:\n        return new\n    old = _distutils.get_purelib()\n    if _looks_like_deb_system_dist_packages(old):\n        return old\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key='purelib'):\n        _log_context()\n    return old",
    "label": true
  },
  {
    "code": "def _get_previous_entrypoints(dist: 'Distribution') -> Dict[str, list]:\n    ignore = ('console_scripts', 'gui_scripts')\n    value = getattr(dist, 'entry_points', None) or {}\n    return {k: v for k, v in value.items() if k not in ignore}",
    "label": true
  },
  {
    "code": "def parse_build_tag(build_tag: str) -> str:\n    if not build_tag[0].isdigit():\n        raise ArgumentTypeError('build tag must begin with a digit')\n    elif '-' in build_tag:\n        raise ArgumentTypeError(\"invalid character ('-') in build tag\")\n    return build_tag",
    "label": true
  },
  {
    "code": "def chop_cells(text: str, max_size: int, position: int=0) -> List[str]:\n    _get_character_cell_size = get_character_cell_size\n    characters = [(character, _get_character_cell_size(character)) for character in text]\n    total_size = position\n    lines: List[List[str]] = [[]]\n    append = lines[-1].append\n    for character, size in reversed(characters):\n        if total_size + size > max_size:\n            lines.append([character])\n            append = lines[-1].append\n            total_size = size\n        else:\n            total_size += size\n            append(character)\n    return [''.join(line) for line in lines]",
    "label": true
  },
  {
    "code": "def _map_positions_to_result(line: _StrLike, search_dict: dict[_StrLike, _BadChar], new_line: _StrLike, byte_str_length: int=1) -> dict[int, _BadChar]:\n    result: dict[int, _BadChar] = {}\n    for search_for, char in search_dict.items():\n        if search_for not in line:\n            continue\n        if char.unescaped == '\\r' and line.endswith(new_line):\n            ignore_pos = len(line) - 2 * byte_str_length\n        else:\n            ignore_pos = None\n        start = 0\n        pos = line.find(search_for, start)\n        while pos > 0:\n            if pos != ignore_pos:\n                col = int(pos / byte_str_length)\n                result[col] = char\n            start = pos + 1\n            pos = line.find(search_for, start)\n    return result",
    "label": true
  },
  {
    "code": "def _line_length(line: _StrLike, codec: str) -> int:\n    if isinstance(line, bytes):\n        decoded = _remove_bom(line, codec).decode(codec, 'replace')\n    else:\n        decoded = line\n    stripped = decoded.rstrip('\\n')\n    if stripped != decoded:\n        stripped = stripped.rstrip('\\r')\n    return len(stripped)",
    "label": true
  },
  {
    "code": "def _absolute_root(path: _Path) -> str:\n    path_ = Path(path)\n    parent = path_.parent\n    if path_.exists():\n        return str(path_.resolve())\n    else:\n        return str(parent.resolve() / path_.name)",
    "label": true
  },
  {
    "code": "def _find_egg_info(directory: str) -> str:\n    filenames = [f for f in os.listdir(directory) if f.endswith('.egg-info')]\n    if not filenames:\n        raise InstallationError(f'No .egg-info directory found in {directory}')\n    if len(filenames) > 1:\n        raise InstallationError('More than one .egg-info directory found in {}'.format(directory))\n    return os.path.join(directory, filenames[0])",
    "label": true
  },
  {
    "code": "def _prepare(install_requires: Dict[str, Requirement], extras_require: Mapping[str, Dict[str, Requirement]]) -> Tuple[List[str], Dict[str, List[str]]]:\n    extras = _convert_extras_requirements(extras_require)\n    return _move_install_requirements_markers(install_requires, extras)",
    "label": true
  },
  {
    "code": "def pytest_unconfigure(config: Config) -> None:\n    xml = config.stash.get(xml_key, None)\n    if xml:\n        del config.stash[xml_key]\n        config.pluginmanager.unregister(xml)",
    "label": true
  },
  {
    "code": "def _load_spec(spec: ModuleSpec, module_name: str) -> ModuleType:\n    name = getattr(spec, '__name__', module_name)\n    if name in sys.modules:\n        return sys.modules[name]\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[name] = module\n    spec.loader.exec_module(module)\n    return module",
    "label": true
  },
  {
    "code": "def _check_marker(marker):\n    if not marker:\n        return\n    m = Marker(marker)\n    m.evaluate()",
    "label": true
  },
  {
    "code": "def make_headers(keep_alive=None, accept_encoding=None, user_agent=None, basic_auth=None, proxy_basic_auth=None, disable_cache=None):\n    headers = {}\n    if accept_encoding:\n        if isinstance(accept_encoding, str):\n            pass\n        elif isinstance(accept_encoding, list):\n            accept_encoding = ','.join(accept_encoding)\n        else:\n            accept_encoding = ACCEPT_ENCODING\n        headers['accept-encoding'] = accept_encoding\n    if user_agent:\n        headers['user-agent'] = user_agent\n    if keep_alive:\n        headers['connection'] = 'keep-alive'\n    if basic_auth:\n        headers['authorization'] = 'Basic ' + b64encode(b(basic_auth)).decode('utf-8')\n    if proxy_basic_auth:\n        headers['proxy-authorization'] = 'Basic ' + b64encode(b(proxy_basic_auth)).decode('utf-8')\n    if disable_cache:\n        headers['cache-control'] = 'no-cache'\n    return headers",
    "label": true
  },
  {
    "code": "def _getauto():\n    var = (('BufAdd', 'BufAdd'), ('BufCreate', 'BufCreate'), ('BufDelete', 'BufDelete'), ('BufEnter', 'BufEnter'), ('BufFilePost', 'BufFilePost'), ('BufFilePre', 'BufFilePre'), ('BufHidden', 'BufHidden'), ('BufLeave', 'BufLeave'), ('BufNew', 'BufNew'), ('BufNewFile', 'BufNewFile'), ('BufRead', 'BufRead'), ('BufReadCmd', 'BufReadCmd'), ('BufReadPost', 'BufReadPost'), ('BufReadPre', 'BufReadPre'), ('BufUnload', 'BufUnload'), ('BufWinEnter', 'BufWinEnter'), ('BufWinLeave', 'BufWinLeave'), ('BufWipeout', 'BufWipeout'), ('BufWrite', 'BufWrite'), ('BufWriteCmd', 'BufWriteCmd'), ('BufWritePost', 'BufWritePost'), ('BufWritePre', 'BufWritePre'), ('Cmd', 'Cmd'), ('CmdwinEnter', 'CmdwinEnter'), ('CmdwinLeave', 'CmdwinLeave'), ('ColorScheme', 'ColorScheme'), ('CompleteDone', 'CompleteDone'), ('CursorHold', 'CursorHold'), ('CursorHoldI', 'CursorHoldI'), ('CursorMoved', 'CursorMoved'), ('CursorMovedI', 'CursorMovedI'), ('EncodingChanged', 'EncodingChanged'), ('FileAppendCmd', 'FileAppendCmd'), ('FileAppendPost', 'FileAppendPost'), ('FileAppendPre', 'FileAppendPre'), ('FileChangedRO', 'FileChangedRO'), ('FileChangedShell', 'FileChangedShell'), ('FileChangedShellPost', 'FileChangedShellPost'), ('FileEncoding', 'FileEncoding'), ('FileReadCmd', 'FileReadCmd'), ('FileReadPost', 'FileReadPost'), ('FileReadPre', 'FileReadPre'), ('FileType', 'FileType'), ('FileWriteCmd', 'FileWriteCmd'), ('FileWritePost', 'FileWritePost'), ('FileWritePre', 'FileWritePre'), ('FilterReadPost', 'FilterReadPost'), ('FilterReadPre', 'FilterReadPre'), ('FilterWritePost', 'FilterWritePost'), ('FilterWritePre', 'FilterWritePre'), ('FocusGained', 'FocusGained'), ('FocusLost', 'FocusLost'), ('FuncUndefined', 'FuncUndefined'), ('GUIEnter', 'GUIEnter'), ('GUIFailed', 'GUIFailed'), ('InsertChange', 'InsertChange'), ('InsertCharPre', 'InsertCharPre'), ('InsertEnter', 'InsertEnter'), ('InsertLeave', 'InsertLeave'), ('MenuPopup', 'MenuPopup'), ('QuickFixCmdPost', 'QuickFixCmdPost'), ('QuickFixCmdPre', 'QuickFixCmdPre'), ('QuitPre', 'QuitPre'), ('RemoteReply', 'RemoteReply'), ('SessionLoadPost', 'SessionLoadPost'), ('ShellCmdPost', 'ShellCmdPost'), ('ShellFilterPost', 'ShellFilterPost'), ('SourceCmd', 'SourceCmd'), ('SourcePre', 'SourcePre'), ('SpellFileMissing', 'SpellFileMissing'), ('StdinReadPost', 'StdinReadPost'), ('StdinReadPre', 'StdinReadPre'), ('SwapExists', 'SwapExists'), ('Syntax', 'Syntax'), ('TabEnter', 'TabEnter'), ('TabLeave', 'TabLeave'), ('TermChanged', 'TermChanged'), ('TermResponse', 'TermResponse'), ('TextChanged', 'TextChanged'), ('TextChangedI', 'TextChangedI'), ('User', 'User'), ('UserGettingBored', 'UserGettingBored'), ('VimEnter', 'VimEnter'), ('VimLeave', 'VimLeave'), ('VimLeavePre', 'VimLeavePre'), ('VimResized', 'VimResized'), ('WinEnter', 'WinEnter'), ('WinLeave', 'WinLeave'), ('event', 'event'))\n    return var",
    "label": true
  },
  {
    "code": "def make_setuptools_shim_args(setup_py_path: str, global_options: Optional[Sequence[str]]=None, no_user_config: bool=False, unbuffered_output: bool=False) -> List[str]:\n    args = [sys.executable]\n    if unbuffered_output:\n        args += ['-u']\n    args += ['-c', _SETUPTOOLS_SHIM.format(setup_py_path)]\n    if global_options:\n        args += global_options\n    if no_user_config:\n        args += ['--no-user-cfg']\n    return args",
    "label": true
  },
  {
    "code": "def safe_version(version):\n    version = version.replace(' ', '.')\n    return re.sub('[^A-Za-z0-9.]+', '-', version)",
    "label": true
  },
  {
    "code": "def decode(string: Any, encodings: list[str] | None=None):\n    if not isinstance(string, bytes):\n        return string\n    encodings = encodings or ['utf-8', 'latin1', 'ascii']\n    for encoding in encodings:\n        with contextlib.suppress(UnicodeEncodeError, UnicodeDecodeError):\n            return string.decode(encoding)\n    return string.decode(encodings[0], errors='ignore')",
    "label": true
  },
  {
    "code": "def version_f(args):\n    from .. import __version__\n    print('wheel %s' % __version__)",
    "label": true
  },
  {
    "code": "def get_filetype_from_buffer(buf, max_lines=5):\n    lines = buf.splitlines()\n    for l in lines[-1:-max_lines - 1:-1]:\n        ret = get_filetype_from_line(l)\n        if ret:\n            return ret\n    for i in range(max_lines, -1, -1):\n        if i < len(lines):\n            ret = get_filetype_from_line(lines[i])\n            if ret:\n                return ret\n    return None",
    "label": true
  },
  {
    "code": "def _iter_built(infos: Iterator[IndexCandidateInfo]) -> Iterator[Candidate]:\n    versions_found: Set[_BaseVersion] = set()\n    for version, func in infos:\n        if version in versions_found:\n            continue\n        candidate = func()\n        if candidate is None:\n            continue\n        yield candidate\n        versions_found.add(version)",
    "label": true
  },
  {
    "code": "def _parse_marker_op(tokenizer: Tokenizer) -> Op:\n    if tokenizer.check('IN'):\n        tokenizer.read()\n        return Op('in')\n    elif tokenizer.check('NOT'):\n        tokenizer.read()\n        tokenizer.expect('WS', expected=\"whitespace after 'not'\")\n        tokenizer.expect('IN', expected=\"'in' after 'not'\")\n        return Op('not in')\n    elif tokenizer.check('OP'):\n        return Op(tokenizer.read().text)\n    else:\n        return tokenizer.raise_syntax_error('Expected marker operator, one of <=, <, !=, ==, >=, >, ~=, ===, in, not in')",
    "label": true
  },
  {
    "code": "def infer_getattr(node, context: InferenceContext | None=None):\n    obj, attr = _infer_getattr_args(node, context)\n    if isinstance(obj, util.UninferableBase) or isinstance(attr, util.UninferableBase) or (not hasattr(obj, 'igetattr')):\n        return util.Uninferable\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n    raise UseInferenceDefault",
    "label": true
  },
  {
    "code": "def _number_node_helper(tree: HuffmanTree, count: int) -> int:\n    if not tree.left.is_leaf():\n        count = _number_node_helper(tree.left, count)\n    if not tree.right.is_leaf():\n        count = _number_node_helper(tree.right, count)\n    tree.number = count\n    return count + 1",
    "label": true
  },
  {
    "code": "def make_headers(keep_alive: bool | None=None, accept_encoding: bool | list[str] | str | None=None, user_agent: str | None=None, basic_auth: str | None=None, proxy_basic_auth: str | None=None, disable_cache: bool | None=None) -> dict[str, str]:\n    headers: dict[str, str] = {}\n    if accept_encoding:\n        if isinstance(accept_encoding, str):\n            pass\n        elif isinstance(accept_encoding, list):\n            accept_encoding = ','.join(accept_encoding)\n        else:\n            accept_encoding = ACCEPT_ENCODING\n        headers['accept-encoding'] = accept_encoding\n    if user_agent:\n        headers['user-agent'] = user_agent\n    if keep_alive:\n        headers['connection'] = 'keep-alive'\n    if basic_auth:\n        headers['authorization'] = f\"Basic {b64encode(basic_auth.encode('latin-1')).decode()}\"\n    if proxy_basic_auth:\n        headers['proxy-authorization'] = f\"Basic {b64encode(proxy_basic_auth.encode('latin-1')).decode()}\"\n    if disable_cache:\n        headers['cache-control'] = 'no-cache'\n    return headers",
    "label": true
  },
  {
    "code": "def parse_wininst_info(wininfo_name, egginfo_name):\n    egginfo = None\n    if egginfo_name:\n        egginfo = egg_info_re.search(egginfo_name)\n        if not egginfo:\n            raise ValueError(f'Egg info filename {egginfo_name} is not valid')\n    w_name, sep, rest = wininfo_name.partition('-')\n    if not sep:\n        raise ValueError(f'Installer filename {wininfo_name} is not valid')\n    rest = rest[:-4]\n    rest2, sep, w_pyver = rest.rpartition('-')\n    if sep and w_pyver.startswith('py'):\n        rest = rest2\n        w_pyver = w_pyver.replace('.', '')\n    else:\n        w_pyver = 'py2.py3'\n    w_ver, sep, w_arch = rest.rpartition('.')\n    if not sep:\n        raise ValueError(f'Installer filename {wininfo_name} is not valid')\n    if egginfo:\n        w_name = egginfo.group('name')\n        w_ver = egginfo.group('ver')\n    return {'name': w_name, 'ver': w_ver, 'arch': w_arch, 'pyver': w_pyver}",
    "label": true
  },
  {
    "code": "def get_choice_opt(options, optname, allowed, default=None, normcase=False):\n    string = options.get(optname, default)\n    if normcase:\n        string = string.lower()\n    if string not in allowed:\n        raise OptionError('Value for option %s must be one of %s' % (optname, ', '.join(map(str, allowed))))\n    return string",
    "label": true
  },
  {
    "code": "def _worth_extracting(element: pyparsing.ParserElement) -> bool:\n    children = element.recurse()\n    return any((child.recurse() for child in children))",
    "label": true
  },
  {
    "code": "def make_vcs_requirement_url(repo_url: str, rev: str, project_name: str, subdir: Optional[str]=None) -> str:\n    egg_project_name = project_name.replace('-', '_')\n    req = f'{repo_url}@{rev}#egg={egg_project_name}'\n    if subdir:\n        req += f'&subdirectory={subdir}'\n    return req",
    "label": true
  },
  {
    "code": "def get_contextlib_with_statements(node: nodes.NodeNG) -> Iterator[nodes.With]:\n    for with_node in node.node_ancestors():\n        if isinstance(with_node, nodes.With):\n            yield with_node",
    "label": true
  },
  {
    "code": "def enum(*sequential: Any, **named: Any) -> Type[Any]:\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    reverse = {value: key for key, value in enums.items()}\n    enums['reverse_mapping'] = reverse\n    return type('Enum', (), enums)",
    "label": true
  },
  {
    "code": "def test_attr():\n    import attr\n\n    @attr.s\n    class A:\n        a = attr.ib()\n    v = A(1)\n    assert dill.copy(v) == v",
    "label": true
  },
  {
    "code": "def _get_custom_platforms(arch: str) -> List[str]:\n    arch_prefix, arch_sep, arch_suffix = arch.partition('_')\n    if arch.startswith('macosx'):\n        arches = _mac_platforms(arch)\n    elif arch_prefix in ['manylinux2014', 'manylinux2010']:\n        arches = _custom_manylinux_platforms(arch)\n    else:\n        arches = [arch]\n    return arches",
    "label": true
  },
  {
    "code": "def build_huffman_tree(freq_dict: dict[int, int]) -> HuffmanTree:\n    copy = list(freq_dict.items())\n    if len(copy) == 1:\n        return HuffmanTree(None, HuffmanTree(copy[0][0]), HuffmanTree((copy[0][0] + 1) % 256))\n    copy.sort(key=_two)\n    copy.reverse()\n    while len(copy) > 1:\n        item1 = copy.pop(-1)\n        if isinstance(item1[0], HuffmanTree):\n            tree1 = item1[0]\n        else:\n            tree1 = HuffmanTree(item1[0])\n        item2 = copy.pop(-1)\n        if isinstance(item2[0], HuffmanTree):\n            tree2 = item2[0]\n        else:\n            tree2 = HuffmanTree(item2[0])\n        big_tree = HuffmanTree(None, tree1, tree2)\n        value = item1[1] + item2[1]\n        index = _binary_search_tuple2(copy, value)\n        copy.insert(index, (big_tree, value))\n    return copy[0][0]",
    "label": true
  },
  {
    "code": "def extract_by_key(d, keys):\n    if isinstance(keys, string_types):\n        keys = keys.split()\n    result = {}\n    for key in keys:\n        if key in d:\n            result[key] = d[key]\n    return result",
    "label": true
  },
  {
    "code": "def retry_call(func, cleanup=lambda: None, retries=0, trap=()):\n    attempts = itertools.count() if retries == float('inf') else range(retries)\n    for attempt in attempts:\n        try:\n            return func()\n        except trap:\n            cleanup()\n    return func()",
    "label": true
  },
  {
    "code": "def infer_typing_namedtuple_class(class_node, context: InferenceContext | None=None):\n    annassigns_fields = [annassign.target.name for annassign in class_node.body if isinstance(annassign, nodes.AnnAssign)]\n    code = dedent('\\n    from collections import namedtuple\\n    namedtuple({typename!r}, {fields!r})\\n    ').format(typename=class_node.name, fields=','.join(annassigns_fields))\n    node = extract_node(code)\n    try:\n        generated_class_node = next(infer_named_tuple(node, context))\n    except StopIteration as e:\n        raise InferenceError(node=node, context=context) from e\n    for method in class_node.mymethods():\n        generated_class_node.locals[method.name] = [method]\n    for body_node in class_node.body:\n        if isinstance(body_node, nodes.Assign):\n            for target in body_node.targets:\n                attr = target.name\n                generated_class_node.locals[attr] = class_node.locals[attr]\n        elif isinstance(body_node, nodes.ClassDef):\n            generated_class_node.locals[body_node.name] = [body_node]\n    return iter((generated_class_node,))",
    "label": true
  },
  {
    "code": "def format_header_param_rfc2231(name, value):\n    if isinstance(value, six.binary_type):\n        value = value.decode('utf-8')\n    if not any((ch in value for ch in '\"\\\\\\r\\n')):\n        result = u'%s=\"%s\"' % (name, value)\n        try:\n            result.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n        else:\n            return result\n    if six.PY2:\n        value = value.encode('utf-8')\n    value = email.utils.encode_rfc2231(value, 'utf-8')\n    value = '%s*=%s' % (name, value)\n    if six.PY2:\n        value = value.decode('utf-8')\n    return value",
    "label": true
  },
  {
    "code": "def cleanup_candidates(root: Path, prefix: str, keep: int) -> Iterator[Path]:\n    max_existing = max(map(parse_num, find_suffixes(root, prefix)), default=-1)\n    max_delete = max_existing - keep\n    paths = find_prefixed(root, prefix)\n    paths, paths2 = itertools.tee(paths)\n    numbers = map(parse_num, extract_suffixes(paths2, prefix))\n    for path, number in zip(paths, numbers):\n        if number <= max_delete:\n            yield path",
    "label": true
  },
  {
    "code": "def get_default_options() -> list[str]:\n    options = []\n    home = os.environ.get('HOME', '')\n    if home:\n        rcfile = os.path.join(home, RCFILE)\n        try:\n            with open(rcfile, encoding='utf-8') as file_handle:\n                options = file_handle.read().split()\n        except OSError:\n            pass\n    return options",
    "label": true
  },
  {
    "code": "def _has_default_namedtuple_repr(obj: object) -> bool:\n    obj_file = None\n    try:\n        obj_file = inspect.getfile(obj.__repr__)\n    except (OSError, TypeError):\n        pass\n    default_repr_file = inspect.getfile(_dummy_namedtuple.__repr__)\n    return obj_file == default_repr_file",
    "label": true
  },
  {
    "code": "def _format_boolop(explanations: Iterable[str], is_or: bool) -> str:\n    explanation = '(' + (is_or and ' or ' or ' and ').join(explanations) + ')'\n    return explanation.replace('%', '%%')",
    "label": true
  },
  {
    "code": "def get_argument_from_call(call_node: nodes.Call, position: int | None=None, keyword: str | None=None) -> nodes.Name:\n    if position is None and keyword is None:\n        raise ValueError('Must specify at least one of: position or keyword.')\n    if position is not None:\n        try:\n            return call_node.args[position]\n        except IndexError:\n            pass\n    if keyword and call_node.keywords:\n        for arg in call_node.keywords:\n            if arg.arg == keyword:\n                return arg.value\n    raise NoSuchArgumentError",
    "label": true
  },
  {
    "code": "def _get_data_from_buffer(obj):\n    view = memoryview(obj)\n    if view.itemsize != 1:\n        raise ValueError('cannot unpack from multi-byte object')\n    return view",
    "label": true
  },
  {
    "code": "def product_index(element, *args):\n    index = 0\n    for x, pool in zip_longest(element, args, fillvalue=_marker):\n        if x is _marker or pool is _marker:\n            raise ValueError('element is not a product of args')\n        pool = tuple(pool)\n        index = index * len(pool) + pool.index(x)\n    return index",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('general')\n    group._addoption('-k', action='store', dest='keyword', default='', metavar='EXPRESSION', help=\"Only run tests which match the given substring expression. An expression is a Python evaluatable expression where all names are substring-matched against test names and their parent classes. Example: -k 'test_method or test_other' matches all test functions and classes whose name contains 'test_method' or 'test_other', while -k 'not test_method' matches those that don't contain 'test_method' in their names. -k 'not test_method and not test_other' will eliminate the matches. Additionally keywords are matched to classes and functions containing extra names in their 'extra_keyword_matches' set, as well as functions which have names assigned directly to them. The matching is case-insensitive.\")\n    group._addoption('-m', action='store', dest='markexpr', default='', metavar='MARKEXPR', help=\"Only run tests matching given mark expression. For example: -m 'mark1 and not mark2'.\")\n    group.addoption('--markers', action='store_true', help='show markers (builtin, plugin and per-project ones).')\n    parser.addini('markers', 'Markers for test functions', 'linelist')\n    parser.addini(EMPTY_PARAMETERSET_OPTION, 'Default marker for empty parametersets')",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    import pdb\n    if config.getvalue('trace'):\n        config.pluginmanager.register(PdbTrace(), 'pdbtrace')\n    if config.getvalue('usepdb'):\n        config.pluginmanager.register(PdbInvoke(), 'pdbinvoke')\n    pytestPDB._saved.append((pdb.set_trace, pytestPDB._pluginmanager, pytestPDB._config))\n    pdb.set_trace = pytestPDB.set_trace\n    pytestPDB._pluginmanager = config.pluginmanager\n    pytestPDB._config = config\n\n    def fin() -> None:\n        pdb.set_trace, pytestPDB._pluginmanager, pytestPDB._config = pytestPDB._saved.pop()\n    config.add_cleanup(fin)",
    "label": true
  },
  {
    "code": "def _generate(mod: str='', auto_open: bool=False, visitor_options: Optional[Dict[str, Any]]=None) -> None:\n    abs_path = _get_valid_file_path(mod)\n    if abs_path is None:\n        return\n    file_name = os.path.splitext(os.path.basename(abs_path))[0]\n    module = AstroidBuilder().file_build(abs_path)\n    visitor = CFGVisitor(options=visitor_options)\n    module.accept(visitor)\n    _display(visitor.cfgs, file_name, auto_open=auto_open)",
    "label": true
  },
  {
    "code": "def _ancestry(path):\n    path = path.rstrip(posixpath.sep)\n    while path and path != posixpath.sep:\n        yield path\n        path, tail = posixpath.split(path)",
    "label": true
  },
  {
    "code": "def __getattr__(name: str) -> object:\n    if name == 'Instance':\n        from _pytest.python import Instance\n        return Instance\n    raise AttributeError(f'module {__name__} has no attribute {name}')",
    "label": true
  },
  {
    "code": "def test_data_not_none():\n    FooS = dill.copy(Foo)\n    assert FooS.data.fget is not None\n    assert FooS.data.fset is not None\n    assert FooS.data.fdel is None",
    "label": true
  },
  {
    "code": "def proxy_alias(alias_name, node_type):\n    proxy = type(alias_name, (lazy_object_proxy.Proxy,), {'__class__': object.__dict__['__class__'], '__instancecheck__': _instancecheck})\n    return proxy(lambda: node_type)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('terminal reporting')\n    group._addoption('--pastebin', metavar='mode', action='store', dest='pastebin', default=None, choices=['failed', 'all'], help='Send failed|all info to bpaste.net pastebin service')",
    "label": true
  },
  {
    "code": "def get_build_platform():\n    from sysconfig import get_platform\n    plat = get_platform()\n    if sys.platform == 'darwin' and (not plat.startswith('macosx-')):\n        try:\n            version = _macos_vers()\n            machine = os.uname()[4].replace(' ', '_')\n            return 'macosx-%d.%d-%s' % (int(version[0]), int(version[1]), _macos_arch(machine))\n        except ValueError:\n            pass\n    return plat",
    "label": true
  },
  {
    "code": "def resolve_color_default(color: t.Optional[bool]=None) -> t.Optional[bool]:\n    if color is not None:\n        return color\n    ctx = get_current_context(silent=True)\n    if ctx is not None:\n        return ctx.color\n    return None",
    "label": true
  },
  {
    "code": "def resolve_reference(reference: str) -> Any:\n    modulename, varname = reference.partition(':')[::2]\n    if not modulename or not varname:\n        raise ValueError(f'{reference!r} is not a module:varname reference')\n    obj = import_module(modulename)\n    for attr in varname.split('.'):\n        obj = getattr(obj, attr)\n    return obj",
    "label": true
  },
  {
    "code": "def windowed(seq, n, fillvalue=None, step=1):\n    if n < 0:\n        raise ValueError('n must be >= 0')\n    if n == 0:\n        yield tuple()\n        return\n    if step < 1:\n        raise ValueError('step must be >= 1')\n    window = deque(maxlen=n)\n    i = n\n    for _ in map(window.append, seq):\n        i -= 1\n        if not i:\n            i = step\n            yield tuple(window)\n    size = len(window)\n    if size == 0:\n        return\n    elif size < n:\n        yield tuple(chain(window, repeat(fillvalue, n - size)))\n    elif 0 < i < min(step, n):\n        window += (fillvalue,) * i\n        yield tuple(window)",
    "label": true
  },
  {
    "code": "def get_int_opt(options, optname, default=None):\n    string = options.get(optname, default)\n    try:\n        return int(string)\n    except TypeError:\n        raise OptionError('Invalid type %r for option %s; you must give an integer value' % (string, optname))\n    except ValueError:\n        raise OptionError('Invalid value %r for option %s; you must give an integer value' % (string, optname))",
    "label": true
  },
  {
    "code": "def check_nfc(label: str) -> None:\n    if unicodedata.normalize('NFC', label) != label:\n        raise IDNAError('Label must be in Normalization Form C')",
    "label": true
  },
  {
    "code": "def process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))",
    "label": true
  },
  {
    "code": "def _get_with_identifier(mapping: Mapping[str, V], identifier: str, default: D) -> Union[D, V]:\n    if identifier in mapping:\n        return mapping[identifier]\n    name, open_bracket, _ = identifier.partition('[')\n    if open_bracket and name in mapping:\n        return mapping[name]\n    return default",
    "label": true
  },
  {
    "code": "def check_initial_combiner(label: str) -> bool:\n    if unicodedata.category(label[0])[0] == 'M':\n        raise IDNAError('Label begins with an illegal combining character')\n    return True",
    "label": true
  },
  {
    "code": "def report() -> None:\n    console = Console()\n    inspect(console)\n    features = get_windows_console_features()\n    inspect(features)\n    env_names = ('TERM', 'COLORTERM', 'CLICOLOR', 'NO_COLOR', 'TERM_PROGRAM', 'COLUMNS', 'LINES', 'JUPYTER_COLUMNS', 'JUPYTER_LINES', 'JPY_PARENT_PID', 'VSCODE_VERBOSE_LOGGING')\n    env = {name: os.getenv(name) for name in env_names}\n    console.print(Panel.fit(Pretty(env), title='[b]Environment Variables'))\n    console.print(f'platform=\"{platform.system()}\"')",
    "label": true
  },
  {
    "code": "def _pathlib_compat(path):\n    try:\n        return path.__fspath__()\n    except AttributeError:\n        return str(path)",
    "label": true
  },
  {
    "code": "def _handle_no_use_pep517(option: Option, opt: str, value: str, parser: OptionParser) -> None:\n    if value is not None:\n        msg = 'A value was passed for --no-use-pep517,\\n        probably using either the PIP_NO_USE_PEP517 environment variable\\n        or the \"no-use-pep517\" config file option. Use an appropriate value\\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\\n        config file option instead.\\n        '\n        raise_option_error(parser, option=option, msg=msg)\n    packages = ('setuptools', 'wheel')\n    if not all((importlib.util.find_spec(package) for package in packages)):\n        msg = f\"It is not possible to use --no-use-pep517 without {' and '.join(packages)} installed.\"\n        raise_option_error(parser, option=option, msg=msg)\n    parser.values.use_pep517 = False",
    "label": true
  },
  {
    "code": "def _is_attribute_property(name: str, klass: nodes.ClassDef) -> bool:\n    try:\n        attributes = klass.getattr(name)\n    except astroid.NotFoundError:\n        return False\n    property_name = 'builtins.property'\n    for attr in attributes:\n        if isinstance(attr, util.UninferableBase):\n            continue\n        try:\n            inferred = next(attr.infer())\n        except astroid.InferenceError:\n            continue\n        if isinstance(inferred, nodes.FunctionDef) and decorated_with_property(inferred):\n            return True\n        if inferred.pytype() != property_name:\n            continue\n        cls = node_frame_class(inferred)\n        if cls == klass.declared_metaclass():\n            continue\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def _handle_ns(packageName, path_item):\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n    try:\n        spec = importer.find_spec(packageName)\n    except AttributeError:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            loader = importer.find_module(packageName)\n    else:\n        loader = spec.loader if spec else None\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = types.ModuleType(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module, '__path__'):\n        raise TypeError('Not a package:', packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        importlib.import_module(packageName)\n        _rebuild_mod_path(path, packageName, module)\n    return subpath",
    "label": true
  },
  {
    "code": "def _validate_header_part(header, header_part, header_validator_index):\n    if isinstance(header_part, str):\n        validator = _HEADER_VALIDATORS_STR[header_validator_index]\n    elif isinstance(header_part, bytes):\n        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]\n    else:\n        raise InvalidHeader(f'Header part ({header_part!r}) from {header} must be of type str or bytes, not {type(header_part)}')\n    if not validator.match(header_part):\n        header_kind = 'name' if header_validator_index == 0 else 'value'\n        raise InvalidHeader(f'Invalid leading whitespace, reserved character(s), or returncharacter(s) in header {header_kind}: {header_part!r}')",
    "label": true
  },
  {
    "code": "def wcswidth(s: str) -> int:\n    width = 0\n    for c in unicodedata.normalize('NFC', s):\n        wc = wcwidth(c)\n        if wc < 0:\n            return -1\n        width += wc\n    return width",
    "label": true
  },
  {
    "code": "def check_requirements(dist, attr, value):\n    try:\n        list(_reqs.parse(value))\n        if isinstance(value, (dict, set)):\n            raise TypeError('Unordered types are not allowed')\n    except (TypeError, ValueError) as error:\n        tmpl = '{attr!r} must be a string or list of strings containing valid project/version requirement specifiers; {error}'\n        raise DistutilsSetupError(tmpl.format(attr=attr, error=error)) from error",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e275(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    keyword = source_lines[line - 1][:col].split()[-1]\n    keyword_idx = source_lines[line - 1].index(keyword)\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(keyword_idx, col), LineType.ERROR, source_lines[line - 1] + '  # INSERT SPACE AFTER KEYWORD')\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def get_formatter_for_filename(fn, **options):\n    fn = basename(fn)\n    for modname, name, _, filenames, _ in FORMATTERS.values():\n        for filename in filenames:\n            if _fn_matches(fn, filename):\n                if name not in _formatter_cache:\n                    _load_formatters(modname)\n                return _formatter_cache[name](**options)\n    for cls in find_plugin_formatters():\n        for filename in cls.filenames:\n            if _fn_matches(fn, filename):\n                return cls(**options)\n    raise ClassNotFound('no formatter found for file name %r' % fn)",
    "label": true
  },
  {
    "code": "def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n    formats = [cpu_arch]\n    if cpu_arch == 'x86_64':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat64', 'fat32'])\n    elif cpu_arch == 'i386':\n        if version < (10, 4):\n            return []\n        formats.extend(['intel', 'fat32', 'fat'])\n    elif cpu_arch == 'ppc64':\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append('fat64')\n    elif cpu_arch == 'ppc':\n        if version > (10, 6):\n            return []\n        formats.extend(['fat32', 'fat'])\n    if cpu_arch in {'arm64', 'x86_64'}:\n        formats.append('universal2')\n    if cpu_arch in {'x86_64', 'i386', 'ppc64', 'ppc', 'intel'}:\n        formats.append('universal')\n    return formats",
    "label": true
  },
  {
    "code": "def test_method_decorator():\n\n    class A(object):\n\n        @classmethod\n        def test(cls):\n            pass\n    a = A()\n    res = dill.dumps(a)\n    new_obj = dill.loads(res)\n    new_obj.__class__.test()",
    "label": true
  },
  {
    "code": "def _is_redundant(index_node: Union[nodes.AssignName, nodes.Name], loop_node: Union[nodes.For, nodes.Comprehension]) -> bool:\n    _, assignments = index_node.lookup(index_node.name)\n    if not assignments:\n        return False\n    elif isinstance(index_node, nodes.AssignName):\n        return assignments[0] != loop_node.target\n    else:\n        return assignments[0] != loop_node.target or _is_load_subscript(index_node, loop_node)",
    "label": true
  },
  {
    "code": "def expr(s: Scanner) -> ast.expr:\n    ret = and_expr(s)\n    while s.accept(TokenType.OR):\n        rhs = and_expr(s)\n        ret = ast.BoolOp(ast.Or(), [ret, rhs])\n    return ret",
    "label": true
  },
  {
    "code": "def assert_header_parsing(headers):\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError('expected httplib.Message, got {0}.'.format(type(headers)))\n    defects = getattr(headers, 'defects', None)\n    get_payload = getattr(headers, 'get_payload', None)\n    unparsed_data = None\n    if get_payload:\n        if not headers.is_multipart():\n            payload = get_payload()\n            if isinstance(payload, (bytes, str)):\n                unparsed_data = payload\n    if defects:\n        defects = [defect for defect in defects if not isinstance(defect, (StartBoundaryNotFoundDefect, MultipartInvariantViolationDefect))]\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)",
    "label": true
  },
  {
    "code": "def create_urllib3_context(ssl_version=None, cert_reqs=None, options=None, ciphers=None):\n    if not ssl_version or ssl_version == PROTOCOL_TLS:\n        ssl_version = PROTOCOL_TLS_CLIENT\n    context = SSLContext(ssl_version)\n    context.set_ciphers(ciphers or DEFAULT_CIPHERS)\n    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs\n    if options is None:\n        options = 0\n        options |= OP_NO_SSLv2\n        options |= OP_NO_SSLv3\n        options |= OP_NO_COMPRESSION\n        options |= OP_NO_TICKET\n    context.options |= options\n    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(context, 'post_handshake_auth', None) is not None:\n        context.post_handshake_auth = True\n\n    def disable_check_hostname():\n        if getattr(context, 'check_hostname', None) is not None:\n            context.check_hostname = False\n    if cert_reqs == ssl.CERT_REQUIRED:\n        context.verify_mode = cert_reqs\n        disable_check_hostname()\n    else:\n        disable_check_hostname()\n        context.verify_mode = cert_reqs\n    if hasattr(context, 'keylog_filename'):\n        sslkeylogfile = os.environ.get('SSLKEYLOGFILE')\n        if sslkeylogfile:\n            context.keylog_filename = sslkeylogfile\n    return context",
    "label": true
  },
  {
    "code": "def remove_move(name):\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError('no such move, %r' % (name,))",
    "label": true
  },
  {
    "code": "def _read_file(filepath: Union[bytes, _Path]) -> str:\n    with io.open(filepath, encoding='utf-8') as f:\n        return f.read()",
    "label": true
  },
  {
    "code": "def returns_list_of_Ts(func: callable, args: list, tp: type):\n    result = type_check_simple(func, args, list)\n    if not result[0]:\n        return (False, result[1])\n    msg = type_error_message(func.__name__, 'list of {}s'.format(tp.__name__), result[1])\n    for item in result[1]:\n        if not isinstance(item, tp):\n            return (False, msg)\n    return (True, result[1])",
    "label": true
  },
  {
    "code": "def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:\n    tokenizer.consume('WS')\n    if tokenizer.check('LEFT_PARENTHESIS', peek=True):\n        with tokenizer.enclosing_tokens('LEFT_PARENTHESIS', 'RIGHT_PARENTHESIS', around='marker expression'):\n            tokenizer.consume('WS')\n            marker: MarkerAtom = _parse_marker(tokenizer)\n            tokenizer.consume('WS')\n    else:\n        marker = _parse_marker_item(tokenizer)\n    tokenizer.consume('WS')\n    return marker",
    "label": true
  },
  {
    "code": "def rehabilitate_bridge(bridges: list[list], bridge_ids: list[int], new_year: str, is_major: bool) -> None:\n    for i in bridge_ids:\n        bridge = find_bridge_by_id(bridges, i)\n        if is_major:\n            bridge[COLUMN_LAST_MAJOR_REHAB] = new_year\n        else:\n            bridge[COLUMN_LAST_MINOR_REHAB] = new_year",
    "label": true
  },
  {
    "code": "def unquote_unreserved(uri):\n    parts = uri.split('%')\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]\n            else:\n                parts[i] = f'%{parts[i]}'\n        else:\n            parts[i] = f'%{parts[i]}'\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def function_with_unassigned_variable():\n    if False:\n        value = None\n    return lambda: value",
    "label": true
  },
  {
    "code": "def compatible_platforms(provided, required):\n    if provided is None or required is None or provided == required:\n        return True\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n        if not provMac:\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = '%s.%s' % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= '10.3' or (dversion == 8 and macosversion >= '10.4'):\n                    return True\n            return False\n        if provMac.group(1) != reqMac.group(1) or provMac.group(3) != reqMac.group(3):\n            return False\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def adjacent(predicate, iterable, distance=1):\n    if distance < 0:\n        raise ValueError('distance must be at least 0')\n    i1, i2 = tee(iterable)\n    padding = [False] * distance\n    selected = chain(padding, map(predicate, i1), padding)\n    adjacent_to_selected = map(any, windowed(selected, 2 * distance + 1))\n    return zip(adjacent_to_selected, i2)",
    "label": true
  },
  {
    "code": "def getpluginversioninfo(config: Config) -> List[str]:\n    lines = []\n    plugininfo = config.pluginmanager.list_plugin_distinfo()\n    if plugininfo:\n        lines.append('setuptools registered plugins:')\n        for plugin, dist in plugininfo:\n            loc = getattr(plugin, '__file__', repr(plugin))\n            content = f'{dist.project_name}-{dist.version} at {loc}'\n            lines.append('  ' + content)\n    return lines",
    "label": true
  },
  {
    "code": "def detect(byte_str: Union[bytes, bytearray], should_rename_legacy: bool=False) -> ResultDict:\n    if not isinstance(byte_str, bytearray):\n        if not isinstance(byte_str, bytes):\n            raise TypeError(f'Expected object of type bytes or bytearray, got: {type(byte_str)}')\n        byte_str = bytearray(byte_str)\n    detector = UniversalDetector(should_rename_legacy=should_rename_legacy)\n    detector.feed(byte_str)\n    return detector.close()",
    "label": true
  },
  {
    "code": "def find_assigned_names_recursive(target: nodes.AssignName | nodes.BaseContainer) -> Iterator[str]:\n    if isinstance(target, nodes.AssignName):\n        if target.name is not None:\n            yield target.name\n    elif isinstance(target, nodes.BaseContainer):\n        for elt in target.elts:\n            yield from find_assigned_names_recursive(elt)",
    "label": true
  },
  {
    "code": "def regex_opt(strings, prefix='', suffix=''):\n    strings = sorted(strings)\n    return prefix + regex_opt_inner(strings, '(') + suffix",
    "label": true
  },
  {
    "code": "def _rlistdir(dirname):\n    if not dirname:\n        if isinstance(dirname, bytes):\n            dirname = os.curdir.encode('ASCII')\n        else:\n            dirname = os.curdir\n    try:\n        names = os.listdir(dirname)\n    except os.error:\n        return\n    for x in names:\n        yield x\n        path = os.path.join(dirname, x) if dirname else x\n        for y in _rlistdir(path):\n            yield os.path.join(x, y)",
    "label": true
  },
  {
    "code": "def run_symilar(argv: Sequence[str] | None=None) -> NoReturn:\n    from pylint.checkers.similar import Run as SimilarRun\n    SimilarRun(argv or sys.argv[1:])",
    "label": true
  },
  {
    "code": "def thread_exception_runtest_hook() -> Generator[None, None, None]:\n    with catch_threading_exception() as cm:\n        yield\n        if cm.args:\n            thread_name = '<unknown>' if cm.args.thread is None else cm.args.thread.name\n            msg = f'Exception in thread {thread_name}\\n\\n'\n            msg += ''.join(traceback.format_exception(cm.args.exc_type, cm.args.exc_value, cm.args.exc_traceback))\n            warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e223(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip('\\t'))\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _parse_arguments_into_strings(pos_only_store: dict[str, tuple[str | None, str | None]], kw_only_store: dict[str, tuple[str | None, str | None]]) -> tuple[str, str]:\n    pos_only, kw_only = ('', '')\n    for pos_arg, data in pos_only_store.items():\n        pos_only += pos_arg\n        if data[0]:\n            pos_only += ': ' + data[0]\n        if data[1]:\n            pos_only += ' = ' + data[1]\n        pos_only += ', '\n    for kw_arg, data in kw_only_store.items():\n        kw_only += kw_arg\n        if data[0]:\n            kw_only += ': ' + data[0]\n        if data[1]:\n            kw_only += ' = ' + data[1]\n        kw_only += ', '\n    return (pos_only, kw_only)",
    "label": true
  },
  {
    "code": "def test_encode():\n    assert encode('\u00e9', 'latin1') == b'\\xe9'\n    assert encode('\u00e9', 'utf8') == b'\\xc3\\xa9'\n    assert encode('\u00e9', 'utf8') == b'\\xc3\\xa9'\n    assert encode('\u00e9', 'utf-16') == b'\\xe9\\x00'\n    assert encode('\u00e9', 'utf-16le') == b'\\xe9\\x00'\n    assert encode('\u00e9', 'utf-16be') == b'\\x00\\xe9'",
    "label": true
  },
  {
    "code": "def parse_requirements(filename: str, session: PipSession, finder: Optional['PackageFinder']=None, options: Optional[optparse.Values]=None, constraint: bool=False) -> Generator[ParsedRequirement, None, None]:\n    line_parser = get_line_parser(finder)\n    parser = RequirementsFileParser(session, line_parser)\n    for parsed_line in parser.parse(filename, constraint):\n        parsed_req = handle_line(parsed_line, options=options, finder=finder, session=session)\n        if parsed_req is not None:\n            yield parsed_req",
    "label": true
  },
  {
    "code": "def chunked(iterable, n, strict=False):\n    iterator = iter(partial(take, n, iter(iterable)), [])\n    if strict:\n\n        def ret():\n            for chunk in iterator:\n                if len(chunk) != n:\n                    raise ValueError('iterable is not divisible by n.')\n                yield chunk\n        return iter(ret())\n    else:\n        return iterator",
    "label": true
  },
  {
    "code": "def _no_context_variadic_positional(node: nodes.Call, scope: nodes.Lambda) -> bool:\n    variadics = node.starargs + node.kwargs\n    return _no_context_variadic(node, scope.args.vararg, nodes.Starred, variadics)",
    "label": true
  },
  {
    "code": "def _parse_version_many(tokenizer: Tokenizer) -> str:\n    parsed_specifiers = ''\n    while tokenizer.check('SPECIFIER'):\n        span_start = tokenizer.position\n        parsed_specifiers += tokenizer.read().text\n        if tokenizer.check('VERSION_PREFIX_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('.* suffix can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position + 1)\n        if tokenizer.check('VERSION_LOCAL_LABEL_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('Local version label can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position)\n        tokenizer.consume('WS')\n        if not tokenizer.check('COMMA'):\n            break\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def join_continuation(lines):\n    lines = iter(lines)\n    for item in lines:\n        while item.endswith('\\\\'):\n            try:\n                item = item[:-2].strip() + next(lines)\n            except StopIteration:\n                return\n        yield item",
    "label": true
  },
  {
    "code": "def side_effect(func, iterable, chunk_size=None, before=None, after=None):\n    try:\n        if before is not None:\n            before()\n        if chunk_size is None:\n            for item in iterable:\n                func(item)\n                yield item\n        else:\n            for chunk in chunked(iterable, chunk_size):\n                func(chunk)\n                yield from chunk\n    finally:\n        if after is not None:\n            after()",
    "label": true
  },
  {
    "code": "def test_invalid_label():\n    assert_raises(LookupError, decode, b'\\xef\\xbb\\xbf\\xc3\\xa9', 'invalid')\n    assert_raises(LookupError, encode, '\u00e9', 'invalid')\n    assert_raises(LookupError, iter_decode, [], 'invalid')\n    assert_raises(LookupError, iter_encode, [], 'invalid')\n    assert_raises(LookupError, IncrementalDecoder, 'invalid')\n    assert_raises(LookupError, IncrementalEncoder, 'invalid')",
    "label": true
  },
  {
    "code": "def format_size(bytes: float) -> str:\n    if bytes > 1000 * 1000:\n        return '{:.1f} MB'.format(bytes / 1000.0 / 1000)\n    elif bytes > 10 * 1000:\n        return '{} kB'.format(int(bytes / 1000))\n    elif bytes > 1000:\n        return '{:.1f} kB'.format(bytes / 1000.0)\n    else:\n        return '{} bytes'.format(int(bytes))",
    "label": true
  },
  {
    "code": "def partitions(iterable):\n    sequence = list(iterable)\n    n = len(sequence)\n    for i in powerset(range(1, n)):\n        yield [sequence[i:j] for i, j in zip((0,) + i, i + (n,))]",
    "label": true
  },
  {
    "code": "def _padboth(width, s):\n    fmt = '{0:^%ds}' % width\n    return fmt.format(s)",
    "label": true
  },
  {
    "code": "def _get_cache_dir(req: InstallRequirement, wheel_cache: WheelCache) -> str:\n    cache_available = bool(wheel_cache.cache_dir)\n    assert req.link\n    if cache_available and _should_cache(req):\n        cache_dir = wheel_cache.get_path_for_link(req.link)\n    else:\n        cache_dir = wheel_cache.get_ephem_path_for_link(req.link)\n    return cache_dir",
    "label": true
  },
  {
    "code": "def rich_repr(cls: Optional[Type[T]]=None, *, angular: bool=False) -> Union[Type[T], Callable[[Type[T]], Type[T]]]:\n    if cls is None:\n        return auto(angular=angular)\n    else:\n        return auto(cls)",
    "label": true
  },
  {
    "code": "def url_to_file_path(url, filecache):\n    key = CacheController.cache_url(url)\n    return filecache._fn(key)",
    "label": true
  },
  {
    "code": "def _modpath_from_file(filename: str, is_namespace: bool, path: list[str]) -> list[str]:\n\n    def _is_package_cb(inner_path: str, parts: list[str]) -> bool:\n        return modutils.check_modpath_has_init(inner_path, parts) or is_namespace\n    return modutils.modpath_from_file_with_callback(filename, path=path, is_package_cb=_is_package_cb)",
    "label": true
  },
  {
    "code": "def pass_obj(f: 't.Callable[te.Concatenate[t.Any, P], R]') -> 't.Callable[P, R]':\n\n    def new_func(*args: 'P.args', **kwargs: 'P.kwargs') -> 'R':\n        return f(get_current_context().obj, *args, **kwargs)\n    return update_wrapper(new_func, f)",
    "label": true
  },
  {
    "code": "def write_arg(cmd, basename, filename, force=False):\n    argname = os.path.splitext(basename)[0]\n    value = getattr(cmd.distribution, argname, None)\n    if value is not None:\n        value = '\\n'.join(value) + '\\n'\n    cmd.write_or_delete_file(argname, filename, value, force)",
    "label": true
  },
  {
    "code": "def check_literal(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n\n    def get_literal_args(literal_args: tuple[Any, ...]) -> tuple[Any, ...]:\n        retval: list[Any] = []\n        for arg in literal_args:\n            if _is_literal_type(get_origin(arg)):\n                retval.extend(get_literal_args(arg.__args__))\n            elif arg is None or isinstance(arg, (int, str, bytes, bool, Enum)):\n                retval.append(arg)\n            else:\n                raise TypeError(f'Illegal literal value: {arg}')\n        return tuple(retval)\n    final_args = tuple(get_literal_args(args))\n    try:\n        index = final_args.index(value)\n    except ValueError:\n        pass\n    else:\n        if type(final_args[index]) is type(value):\n            return\n    formatted_args = ', '.join((repr(arg) for arg in final_args))\n    raise TypeCheckError(f'is not any of ({formatted_args})') from None",
    "label": true
  },
  {
    "code": "def _ipy_display_hook(value: Any, console: Optional['Console']=None, overflow: 'OverflowMethod'='ignore', crop: bool=False, indent_guides: bool=False, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> Union[str, None]:\n    from .console import ConsoleRenderable\n    if _safe_isinstance(value, JupyterRenderable) or value is None:\n        return None\n    console = console or get_console()\n    with console.capture() as capture:\n        if _safe_isinstance(value, ConsoleRenderable):\n            console.line()\n        console.print(value if _safe_isinstance(value, RichRenderable) else Pretty(value, overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all, margin=12), crop=crop, new_line_start=True, end='')\n    return capture.get().rstrip('\\n')",
    "label": true
  },
  {
    "code": "def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n    if len(py_version) > 1:\n        yield f'py{_version_nodot(py_version[:2])}'\n    yield f'py{py_version[0]}'\n    if len(py_version) > 1:\n        for minor in range(py_version[1] - 1, -1, -1):\n            yield f'py{_version_nodot((py_version[0], minor))}'",
    "label": true
  },
  {
    "code": "def random_permutation(iterable, r=None):\n    pool = tuple(iterable)\n    r = len(pool) if r is None else r\n    return tuple(sample(pool, r))",
    "label": true
  },
  {
    "code": "def renames(old: str, new: str) -> None:\n    head, tail = os.path.split(new)\n    if head and tail and (not os.path.exists(head)):\n        os.makedirs(head)\n    shutil.move(old, new)\n    head, tail = os.path.split(old)\n    if head and tail:\n        try:\n            os.removedirs(head)\n        except OSError:\n            pass",
    "label": true
  },
  {
    "code": "def validate(config: dict, filepath: _Path) -> bool:\n    from . import _validate_pyproject as validator\n    trove_classifier = validator.FORMAT_FUNCTIONS.get('trove-classifier')\n    if hasattr(trove_classifier, '_disable_download'):\n        trove_classifier._disable_download()\n    try:\n        return validator.validate(config)\n    except validator.ValidationError as ex:\n        summary = f'configuration error: {ex.summary}'\n        if ex.name.strip('`') != 'project':\n            _logger.debug(summary)\n            _logger.debug(ex.details)\n        error = f'invalid pyproject.toml config: {ex.name}.'\n        raise ValueError(f'{error}\\n{summary}') from None",
    "label": true
  },
  {
    "code": "def make_install_req_from_editable(link: Link, template: InstallRequirement) -> InstallRequirement:\n    assert template.editable, 'template not editable'\n    ireq = install_req_from_editable(link.url, user_supplied=template.user_supplied, comes_from=template.comes_from, use_pep517=template.use_pep517, isolated=template.isolated, constraint=template.constraint, permit_editable_wheels=template.permit_editable_wheels, global_options=template.global_options, hash_options=template.hash_options, config_settings=template.config_settings)\n    ireq.extras = template.extras\n    return ireq",
    "label": true
  },
  {
    "code": "def _to_str(s, encoding='utf8', errors='ignore'):\n    if isinstance(s, bytes):\n        return s.decode(encoding=encoding, errors=errors)\n    return str(s)",
    "label": true
  },
  {
    "code": "def install_given_reqs(requirements: List[InstallRequirement], global_options: Sequence[str], root: Optional[str], home: Optional[str], prefix: Optional[str], warn_script_location: bool, use_user_site: bool, pycompile: bool) -> List[InstallationResult]:\n    to_install = collections.OrderedDict(_validate_requirements(requirements))\n    if to_install:\n        logger.info('Installing collected packages: %s', ', '.join(to_install.keys()))\n    installed = []\n    with indent_log():\n        for req_name, requirement in to_install.items():\n            if requirement.should_reinstall:\n                logger.info('Attempting uninstall: %s', req_name)\n                with indent_log():\n                    uninstalled_pathset = requirement.uninstall(auto_confirm=True)\n            else:\n                uninstalled_pathset = None\n            try:\n                requirement.install(global_options, root=root, home=home, prefix=prefix, warn_script_location=warn_script_location, use_user_site=use_user_site, pycompile=pycompile)\n            except Exception:\n                if uninstalled_pathset and (not requirement.install_succeeded):\n                    uninstalled_pathset.rollback()\n                raise\n            else:\n                if uninstalled_pathset and requirement.install_succeeded:\n                    uninstalled_pathset.commit()\n            installed.append(InstallationResult(req_name))\n    return installed",
    "label": true
  },
  {
    "code": "def _raise_wrapfail(wrap_controller: Generator[None, Result[ResultType], None] | Generator[None, object, object], msg: str) -> NoReturn:\n    co = wrap_controller.gi_code\n    raise RuntimeError('wrap_controller at %r %s:%d %s' % (co.co_name, co.co_filename, co.co_firstlineno, msg))",
    "label": true
  },
  {
    "code": "def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n    if len(py_version) > 1:\n        yield f'py{_version_nodot(py_version[:2])}'\n    yield f'py{py_version[0]}'\n    if len(py_version) > 1:\n        for minor in range(py_version[1] - 1, -1, -1):\n            yield f'py{_version_nodot((py_version[0], minor))}'",
    "label": true
  },
  {
    "code": "def _get_windows_console_stream(f: t.TextIO, encoding: t.Optional[str], errors: t.Optional[str]) -> t.Optional[t.TextIO]:\n    if get_buffer is not None and encoding in {'utf-16-le', None} and (errors in {'strict', None}) and _is_console(f):\n        func = _stream_factories.get(f.fileno())\n        if func is not None:\n            b = getattr(f, 'buffer', None)\n            if b is None:\n                return None\n            return func(b)",
    "label": true
  },
  {
    "code": "def is_protocol_class(cls: nodes.NodeNG) -> bool:\n    if not isinstance(cls, nodes.ClassDef):\n        return False\n    if cls.qname() in TYPING_PROTOCOLS:\n        return True\n    for base in cls.bases:\n        try:\n            for inf_base in base.infer():\n                if inf_base.qname() in TYPING_PROTOCOLS:\n                    return True\n        except astroid.InferenceError:\n            continue\n    return False",
    "label": true
  },
  {
    "code": "def _eval_scope_callable(scope_callable: 'Callable[[str, Config], _ScopeName]', fixture_name: str, config: Config) -> '_ScopeName':\n    try:\n        result = scope_callable(fixture_name=fixture_name, config=config)\n    except Exception as e:\n        raise TypeError(\"Error evaluating {} while defining fixture '{}'.\\nExpected a function with the signature (*, fixture_name, config)\".format(scope_callable, fixture_name)) from e\n    if not isinstance(result, str):\n        fail(\"Expected {} to return a 'str' while defining fixture '{}', but it returned:\\n{!r}\".format(scope_callable, fixture_name, result), pytrace=False)\n    return result",
    "label": true
  },
  {
    "code": "def _macos_user_config_dir(appname: str, roaming: bool=True) -> str:\n    path = _appdirs.user_data_dir(appname, appauthor=False, roaming=roaming)\n    if os.path.isdir(path):\n        return path\n    linux_like_path = '~/.config/'\n    if appname:\n        linux_like_path = os.path.join(linux_like_path, appname)\n    return os.path.expanduser(linux_like_path)",
    "label": true
  },
  {
    "code": "def in_venv():\n    if hasattr(sys, 'real_prefix'):\n        result = True\n    else:\n        result = sys.prefix != getattr(sys, 'base_prefix', sys.prefix)\n    return result",
    "label": true
  },
  {
    "code": "def _non_empty_string_validator(opt: Any, _: str, value: str) -> str:\n    if not value:\n        msg = \"indent string can't be empty.\"\n        raise optparse.OptionValueError(msg)\n    return utils._unquote(value)",
    "label": true
  },
  {
    "code": "def loads(str, ignore=None, **kwds):\n    file = StringIO(str)\n    return load(file, ignore, **kwds)",
    "label": true
  },
  {
    "code": "def numpy_core_numerictypes_transform():\n    generic_src = \"\\n    class generic(object):\\n        def __init__(self, value):\\n            self.T = np.ndarray([0, 0])\\n            self.base = None\\n            self.data = None\\n            self.dtype = None\\n            self.flags = None\\n            # Should be a numpy.flatiter instance but not available for now\\n            # Putting an array instead so that iteration and indexing are authorized\\n            self.flat = np.ndarray([0, 0])\\n            self.imag = None\\n            self.itemsize = None\\n            self.nbytes = None\\n            self.ndim = None\\n            self.real = None\\n            self.size = None\\n            self.strides = None\\n\\n        def all(self): return uninferable\\n        def any(self): return uninferable\\n        def argmax(self): return uninferable\\n        def argmin(self): return uninferable\\n        def argsort(self): return uninferable\\n        def astype(self, dtype, order='K', casting='unsafe', subok=True, copy=True): return np.ndarray([0, 0])\\n        def base(self): return uninferable\\n        def byteswap(self): return uninferable\\n        def choose(self): return uninferable\\n        def clip(self): return uninferable\\n        def compress(self): return uninferable\\n        def conj(self): return uninferable\\n        def conjugate(self): return uninferable\\n        def copy(self): return uninferable\\n        def cumprod(self): return uninferable\\n        def cumsum(self): return uninferable\\n        def data(self): return uninferable\\n        def diagonal(self): return uninferable\\n        def dtype(self): return uninferable\\n        def dump(self): return uninferable\\n        def dumps(self): return uninferable\\n        def fill(self): return uninferable\\n        def flags(self): return uninferable\\n        def flat(self): return uninferable\\n        def flatten(self): return uninferable\\n        def getfield(self): return uninferable\\n        def imag(self): return uninferable\\n        def item(self): return uninferable\\n        def itemset(self): return uninferable\\n        def itemsize(self): return uninferable\\n        def max(self): return uninferable\\n        def mean(self): return uninferable\\n        def min(self): return uninferable\\n        def nbytes(self): return uninferable\\n        def ndim(self): return uninferable\\n        def newbyteorder(self): return uninferable\\n        def nonzero(self): return uninferable\\n        def prod(self): return uninferable\\n        def ptp(self): return uninferable\\n        def put(self): return uninferable\\n        def ravel(self): return uninferable\\n        def real(self): return uninferable\\n        def repeat(self): return uninferable\\n        def reshape(self): return uninferable\\n        def resize(self): return uninferable\\n        def round(self): return uninferable\\n        def searchsorted(self): return uninferable\\n        def setfield(self): return uninferable\\n        def setflags(self): return uninferable\\n        def shape(self): return uninferable\\n        def size(self): return uninferable\\n        def sort(self): return uninferable\\n        def squeeze(self): return uninferable\\n        def std(self): return uninferable\\n        def strides(self): return uninferable\\n        def sum(self): return uninferable\\n        def swapaxes(self): return uninferable\\n        def take(self): return uninferable\\n        def tobytes(self): return uninferable\\n        def tofile(self): return uninferable\\n        def tolist(self): return uninferable\\n        def tostring(self): return uninferable\\n        def trace(self): return uninferable\\n        def transpose(self): return uninferable\\n        def var(self): return uninferable\\n        def view(self): return uninferable\\n        \"\n    if numpy_supports_type_hints():\n        generic_src += '\\n        @classmethod\\n        def __class_getitem__(cls, value):\\n            return cls\\n        '\n    return parse(generic_src + \"\\n    class dtype(object):\\n        def __init__(self, obj, align=False, copy=False):\\n            self.alignment = None\\n            self.base = None\\n            self.byteorder = None\\n            self.char = None\\n            self.descr = None\\n            self.fields = None\\n            self.flags = None\\n            self.hasobject = None\\n            self.isalignedstruct = None\\n            self.isbuiltin = None\\n            self.isnative = None\\n            self.itemsize = None\\n            self.kind = None\\n            self.metadata = None\\n            self.name = None\\n            self.names = None\\n            self.num = None\\n            self.shape = None\\n            self.str = None\\n            self.subdtype = None\\n            self.type = None\\n\\n        def newbyteorder(self, new_order='S'): return uninferable\\n        def __neg__(self): return uninferable\\n\\n    class busdaycalendar(object):\\n        def __init__(self, weekmask='1111100', holidays=None):\\n            self.holidays = None\\n            self.weekmask = None\\n\\n    class flexible(generic): pass\\n    class bool_(generic): pass\\n    class number(generic):\\n        def __neg__(self): return uninferable\\n    class datetime64(generic):\\n        def __init__(self, nb, unit=None): pass\\n\\n\\n    class void(flexible):\\n        def __init__(self, *args, **kwargs):\\n            self.base = None\\n            self.dtype = None\\n            self.flags = None\\n        def getfield(self): return uninferable\\n        def setfield(self): return uninferable\\n\\n\\n    class character(flexible): pass\\n\\n\\n    class integer(number):\\n        def __init__(self, value):\\n           self.denominator = None\\n           self.numerator = None\\n\\n\\n    class inexact(number): pass\\n\\n\\n    class str_(str, character):\\n        def maketrans(self, x, y=None, z=None): return uninferable\\n\\n\\n    class bytes_(bytes, character):\\n        def fromhex(self, string): return uninferable\\n        def maketrans(self, frm, to): return uninferable\\n\\n\\n    class signedinteger(integer): pass\\n\\n\\n    class unsignedinteger(integer): pass\\n\\n\\n    class complexfloating(inexact): pass\\n\\n\\n    class floating(inexact): pass\\n\\n\\n    class float64(floating, float):\\n        def fromhex(self, string): return uninferable\\n\\n\\n    class uint64(unsignedinteger): pass\\n    class complex64(complexfloating): pass\\n    class int16(signedinteger): pass\\n    class float96(floating): pass\\n    class int8(signedinteger): pass\\n    class uint32(unsignedinteger): pass\\n    class uint8(unsignedinteger): pass\\n    class _typedict(dict): pass\\n    class complex192(complexfloating): pass\\n    class timedelta64(signedinteger):\\n        def __init__(self, nb, unit=None): pass\\n    class int32(signedinteger): pass\\n    class uint16(unsignedinteger): pass\\n    class float32(floating): pass\\n    class complex128(complexfloating, complex): pass\\n    class float16(floating): pass\\n    class int64(signedinteger): pass\\n\\n    buffer_type = memoryview\\n    bool8 = bool_\\n    byte = int8\\n    bytes0 = bytes_\\n    cdouble = complex128\\n    cfloat = complex128\\n    clongdouble = complex192\\n    clongfloat = complex192\\n    complex_ = complex128\\n    csingle = complex64\\n    double = float64\\n    float_ = float64\\n    half = float16\\n    int0 = int32\\n    int_ = int32\\n    intc = int32\\n    intp = int32\\n    long = int32\\n    longcomplex = complex192\\n    longdouble = float96\\n    longfloat = float96\\n    longlong = int64\\n    object0 = object_\\n    object_ = object_\\n    short = int16\\n    single = float32\\n    singlecomplex = complex64\\n    str0 = str_\\n    string_ = bytes_\\n    ubyte = uint8\\n    uint = uint32\\n    uint0 = uint32\\n    uintc = uint32\\n    uintp = uint32\\n    ulonglong = uint64\\n    unicode = str_\\n    unicode_ = str_\\n    ushort = uint16\\n    void0 = void\\n    \")",
    "label": true
  },
  {
    "code": "def _no_slots_copy(dct):\n    dict_copy = dict(dct)\n    if '__slots__' in dict_copy:\n        for slot in dict_copy['__slots__']:\n            dict_copy.pop(slot, None)\n    return dict_copy",
    "label": true
  },
  {
    "code": "def generate(node: nodes.Template, environment: 'Environment', name: t.Optional[str], filename: t.Optional[str], stream: t.Optional[t.TextIO]=None, defer_init: bool=False, optimized: bool=True) -> t.Optional[str]:\n    if not isinstance(node, nodes.Template):\n        raise TypeError(\"Can't compile non template nodes\")\n    generator = environment.code_generator_class(environment, name, filename, stream, defer_init, optimized)\n    generator.visit(node)\n    if stream is None:\n        return generator.stream.getvalue()\n    return None",
    "label": true
  },
  {
    "code": "def _get_python_inc_nt(prefix, spec_prefix, plat_specific):\n    if python_build:\n        return os.path.join(prefix, 'include') + os.path.pathsep + os.path.join(prefix, 'PC')\n    return os.path.join(prefix, 'include')",
    "label": true
  },
  {
    "code": "def _csv_open(fn, mode, **kwargs):\n    if sys.version_info[0] < 3:\n        mode += 'b'\n    else:\n        kwargs['newline'] = ''\n        kwargs['encoding'] = 'utf-8'\n    return open(fn, mode, **kwargs)",
    "label": true
  },
  {
    "code": "def normalize_mark_list(mark_list: Iterable[Union[Mark, MarkDecorator]]) -> Iterable[Mark]:\n    for mark in mark_list:\n        mark_obj = getattr(mark, 'mark', mark)\n        if not isinstance(mark_obj, Mark):\n            raise TypeError(f'got {repr(mark_obj)} instead of Mark')\n        yield mark_obj",
    "label": true
  },
  {
    "code": "def find_imports_in_paths(paths: Iterator[Union[str, Path]], config: Config=DEFAULT_CONFIG, file_path: Optional[Path]=None, unique: Union[bool, ImportKey]=False, top_only: bool=False, **config_kwargs: Any) -> Iterator[identify.Import]:\n    config = _config(config=config, **config_kwargs)\n    seen: Optional[Set[str]] = set() if unique else None\n    yield from chain(*(find_imports_in_file(file_name, unique=unique, config=config, top_only=top_only, _seen=seen) for file_name in files.find(map(str, paths), config, [], [])))",
    "label": true
  },
  {
    "code": "def is_subclass_of(child: nodes.ClassDef, parent: nodes.ClassDef) -> bool:\n    if not all((isinstance(node, nodes.ClassDef) for node in (child, parent))):\n        return False\n    for ancestor in child.ancestors():\n        try:\n            if astroid.helpers.is_subtype(ancestor, parent):\n                return True\n        except astroid.exceptions._NonDeducibleTypeHierarchy:\n            continue\n    return False",
    "label": true
  },
  {
    "code": "def convolve(signal, kernel):\n    kernel = tuple(kernel)[::-1]\n    n = len(kernel)\n    window = deque([0], maxlen=n) * n\n    for x in chain(signal, repeat(0, n - 1)):\n        window.append(x)\n        yield sum(map(operator.mul, kernel, window))",
    "label": true
  },
  {
    "code": "def local_open(url):\n    scheme, server, path, param, query, frag = urllib.parse.urlparse(url)\n    filename = urllib.request.url2pathname(path)\n    if os.path.isfile(filename):\n        return urllib.request.urlopen(url)\n    elif path.endswith('/') and os.path.isdir(filename):\n        files = []\n        for f in os.listdir(filename):\n            filepath = os.path.join(filename, f)\n            if f == 'index.html':\n                with open(filepath, 'r') as fp:\n                    body = fp.read()\n                break\n            elif os.path.isdir(filepath):\n                f += '/'\n            files.append('<a href=\"{name}\">{name}</a>'.format(name=f))\n        else:\n            tmpl = '<html><head><title>{url}</title></head><body>{files}</body></html>'\n            body = tmpl.format(url=url, files='\\n'.join(files))\n        status, message = (200, 'OK')\n    else:\n        status, message, body = (404, 'Path not found', 'Not found')\n    headers = {'content-type': 'text/html'}\n    body_stream = io.StringIO(body)\n    return urllib.error.HTTPError(url, status, message, headers, body_stream)",
    "label": true
  },
  {
    "code": "def allexcept(*args):\n    newcats = cats[:]\n    for arg in args:\n        newcats.remove(arg)\n    return ''.join((globals()[cat] for cat in newcats))",
    "label": true
  },
  {
    "code": "def get_output(command: List[str]) -> str:\n    result = subprocess.run(command, stdout=subprocess.PIPE, check=True)\n    return result.stdout.decode()",
    "label": true
  },
  {
    "code": "def _similar_names(owner: SuccessfulInferenceResult, attrname: str | None, distance_threshold: int, max_choices: int) -> list[str]:\n    possible_names: list[tuple[str, int]] = []\n    names = _node_names(owner)\n    for name in names:\n        if name == attrname:\n            continue\n        distance = _string_distance(attrname or '', name)\n        if distance <= distance_threshold:\n            possible_names.append((name, distance))\n    picked = [name for name, _ in heapq.nsmallest(max_choices, possible_names, key=operator.itemgetter(1))]\n    return sorted(picked)",
    "label": true
  },
  {
    "code": "def pytest_unconfigure(config: Config) -> None:\n    if pastebinfile_key in config.stash:\n        pastebinfile = config.stash[pastebinfile_key]\n        pastebinfile.seek(0)\n        sessionlog = pastebinfile.read()\n        pastebinfile.close()\n        del config.stash[pastebinfile_key]\n        tr = config.pluginmanager.getplugin('terminalreporter')\n        del tr._tw.__dict__['write']\n        tr.write_sep('=', 'Sending information to Paste Service')\n        pastebinurl = create_new_paste(sessionlog)\n        tr.write_line('pastebin session-log: %s\\n' % pastebinurl)",
    "label": true
  },
  {
    "code": "def _create_typemap():\n    import types\n    d = dict(list(__builtin__.__dict__.items()) + list(types.__dict__.items())).items()\n    for key, value in d:\n        if getattr(value, '__module__', None) == 'builtins' and type(value) is type:\n            yield (key, value)\n    return",
    "label": true
  },
  {
    "code": "def pass_none(func):\n\n    @functools.wraps(func)\n    def wrapper(param, *args, **kwargs):\n        if param is not None:\n            return func(param, *args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def get_provider(moduleOrReq):\n    if isinstance(moduleOrReq, Requirement):\n        return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    try:\n        module = sys.modules[moduleOrReq]\n    except KeyError:\n        __import__(moduleOrReq)\n        module = sys.modules[moduleOrReq]\n    loader = getattr(module, '__loader__', None)\n    return _find_adapter(_provider_factories, loader)(module)",
    "label": true
  },
  {
    "code": "def build_netloc(host: str, port: Optional[int]) -> str:\n    if port is None:\n        return host\n    if ':' in host:\n        host = f'[{host}]'\n    return f'{host}:{port}'",
    "label": true
  },
  {
    "code": "def make_input_stream(input: t.Optional[t.Union[str, bytes, t.IO[t.Any]]], charset: str) -> t.BinaryIO:\n    if hasattr(input, 'read'):\n        rv = _find_binary_reader(t.cast(t.IO[t.Any], input))\n        if rv is not None:\n            return rv\n        raise TypeError('Could not find binary reader for input stream.')\n    if input is None:\n        input = b''\n    elif isinstance(input, str):\n        input = input.encode(charset)\n    return io.BytesIO(input)",
    "label": true
  },
  {
    "code": "def class_callable(init: Callable) -> Callable:\n    c = init.copy_with(tuple([*init.__args__[1:-1], init.__args__[0]]))\n    c.__polymorphic_tvars__ = init.__polymorphic_tvars__.copy()\n    return c",
    "label": true
  },
  {
    "code": "def _clone_node_with_lineno(node, parent, lineno):\n    if isinstance(node, EvaluatedObject):\n        node = node.original\n    cls = node.__class__\n    other_fields = node._other_fields\n    _astroid_fields = node._astroid_fields\n    init_params = {'lineno': lineno, 'col_offset': node.col_offset, 'parent': parent}\n    postinit_params = {param: getattr(node, param) for param in _astroid_fields}\n    if other_fields:\n        init_params.update({param: getattr(node, param) for param in other_fields})\n    new_node = cls(**init_params)\n    if hasattr(node, 'postinit') and _astroid_fields:\n        new_node.postinit(**postinit_params)\n    return new_node",
    "label": true
  },
  {
    "code": "def _is_linux_i686(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.I386)",
    "label": true
  },
  {
    "code": "def _diff_text(left: str, right: str, verbose: int=0) -> List[str]:\n    from difflib import ndiff\n    explanation: List[str] = []\n    if verbose < 1:\n        i = 0\n        for i in range(min(len(left), len(right))):\n            if left[i] != right[i]:\n                break\n        if i > 42:\n            i -= 10\n            explanation = ['Skipping %s identical leading characters in diff, use -v to show' % i]\n            left = left[i:]\n            right = right[i:]\n        if len(left) == len(right):\n            for i in range(len(left)):\n                if left[-i] != right[-i]:\n                    break\n            if i > 42:\n                i -= 10\n                explanation += ['Skipping {} identical trailing characters in diff, use -v to show'.format(i)]\n                left = left[:-i]\n                right = right[:-i]\n    keepends = True\n    if left.isspace() or right.isspace():\n        left = repr(str(left))\n        right = repr(str(right))\n        explanation += ['Strings contain only whitespace, escaping them using repr()']\n    explanation += [line.strip('\\n') for line in ndiff(right.splitlines(keepends), left.splitlines(keepends))]\n    return explanation",
    "label": true
  },
  {
    "code": "def egg_link_path_from_sys_path(raw_name: str) -> Optional[str]:\n    egg_link_name = _egg_link_name(raw_name)\n    for path_item in sys.path:\n        egg_link = os.path.join(path_item, egg_link_name)\n        if os.path.isfile(egg_link):\n            return egg_link\n    return None",
    "label": true
  },
  {
    "code": "def _object_type(node: SuccessfulInferenceResult, context: InferenceContext | None=None) -> Generator[InferenceResult | None, None, None]:\n    astroid_manager = manager.AstroidManager()\n    builtins = astroid_manager.builtins_module\n    context = context or InferenceContext()\n    for inferred in node.infer(context=context):\n        if isinstance(inferred, scoped_nodes.ClassDef):\n            if inferred.newstyle:\n                metaclass = inferred.metaclass(context=context)\n                if metaclass:\n                    yield metaclass\n                    continue\n            yield builtins.getattr('type')[0]\n        elif isinstance(inferred, (scoped_nodes.Lambda, bases.UnboundMethod)):\n            yield _function_type(inferred, builtins)\n        elif isinstance(inferred, scoped_nodes.Module):\n            yield _build_proxy_class('module', builtins)\n        elif isinstance(inferred, nodes.Unknown):\n            raise InferenceError\n        elif isinstance(inferred, util.UninferableBase):\n            yield inferred\n        elif isinstance(inferred, (bases.Proxy, nodes.Slice, objects.Super)):\n            yield inferred._proxied\n        else:\n            raise AssertionError(f\"We don't handle {type(inferred)} currently\")",
    "label": true
  },
  {
    "code": "def _loop_exits_early(loop: nodes.For | nodes.While) -> bool:\n    loop_nodes = (nodes.For, nodes.While)\n    definition_nodes = (nodes.FunctionDef, nodes.ClassDef)\n    inner_loop_nodes: list[nodes.For | nodes.While] = [_node for _node in loop.nodes_of_class(loop_nodes, skip_klass=definition_nodes) if _node != loop]\n    return any((_node for _node in loop.nodes_of_class(nodes.Break, skip_klass=definition_nodes) if _get_break_loop_node(_node) not in inner_loop_nodes))",
    "label": true
  },
  {
    "code": "def distribute(n, iterable):\n    if n < 1:\n        raise ValueError('n must be at least 1')\n    children = tee(iterable, n)\n    return [islice(it, index, None, n) for index, it in enumerate(children)]",
    "label": true
  },
  {
    "code": "def generate_editable_metadata(build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str) -> str:\n    metadata_tmpdir = TempDirectory(kind='modern-metadata', globally_managed=True)\n    metadata_dir = metadata_tmpdir.path\n    with build_env:\n        runner = runner_with_spinner_message('Preparing editable metadata (pyproject.toml)')\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_editable(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n    return os.path.join(metadata_dir, distinfo_dir)",
    "label": true
  },
  {
    "code": "def _glibc_version_string_ctypes() -> Optional[str]:\n    try:\n        import ctypes\n    except ImportError:\n        return None\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        return None\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    if not isinstance(version_str, str):\n        version_str = version_str.decode('ascii')\n    return version_str",
    "label": true
  },
  {
    "code": "def _class_or_tuple_to_container(node, context: InferenceContext | None=None):\n    try:\n        node_infer = next(node.infer(context=context))\n    except StopIteration as e:\n        raise InferenceError(node=node, context=context) from e\n    if isinstance(node_infer, nodes.Tuple):\n        try:\n            class_container = [next(node.infer(context=context)) for node in node_infer.elts]\n        except StopIteration as e:\n            raise InferenceError(node=node, context=context) from e\n        class_container = [klass_node for klass_node in class_container if klass_node is not None]\n    else:\n        class_container = [node_infer]\n    return class_container",
    "label": true
  },
  {
    "code": "def packages_distributions() -> Mapping[str, List[str]]:\n    pkg_to_dist = collections.defaultdict(list)\n    for dist in distributions():\n        for pkg in _top_level_declared(dist) or _top_level_inferred(dist):\n            pkg_to_dist[pkg].append(dist.metadata['Name'])\n    return dict(pkg_to_dist)",
    "label": true
  },
  {
    "code": "def load_ipython_extension(ip: Any) -> None:\n    from pip._vendor.rich.pretty import install\n    from pip._vendor.rich.traceback import install as tr_install\n    install()\n    tr_install()",
    "label": true
  },
  {
    "code": "def _weak_function_proxy_callback(ref, proxy, callback):\n    if proxy._self_expired:\n        return\n    proxy._self_expired = True\n    if callback is not None:\n        callback(proxy)",
    "label": true
  },
  {
    "code": "def load_group(value, group):\n    lines = yield_lines(value)\n    text = f'[{group}]\\n' + '\\n'.join(lines)\n    return metadata.EntryPoints._from_text(text)",
    "label": true
  },
  {
    "code": "def with_iter(context_manager):\n    with context_manager as iterable:\n        yield from iterable",
    "label": true
  },
  {
    "code": "def _parse_version_many(tokenizer: Tokenizer) -> str:\n    parsed_specifiers = ''\n    while tokenizer.check('SPECIFIER'):\n        span_start = tokenizer.position\n        parsed_specifiers += tokenizer.read().text\n        if tokenizer.check('VERSION_PREFIX_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('.* suffix can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position + 1)\n        if tokenizer.check('VERSION_LOCAL_LABEL_TRAIL', peek=True):\n            tokenizer.raise_syntax_error('Local version label can only be used with `==` or `!=` operators', span_start=span_start, span_end=tokenizer.position)\n        tokenizer.consume('WS')\n        if not tokenizer.check('COMMA'):\n            break\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def build_iter_view(matches):\n    if callable(matches):\n        return _FactoryIterableView(matches)\n    if not isinstance(matches, collections_abc.Sequence):\n        matches = list(matches)\n    return _SequenceIterableView(matches)",
    "label": true
  },
  {
    "code": "def _mkstemp(*args, **kw):\n    old_open = os.open\n    try:\n        os.open = os_open\n        return tempfile.mkstemp(*args, **kw)\n    finally:\n        os.open = old_open",
    "label": true
  },
  {
    "code": "def get_requires_for_build_editable(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_editable\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)",
    "label": true
  },
  {
    "code": "def _apply_tool_table(dist: 'Distribution', config: dict, filename: _Path):\n    tool_table = config.get('tool', {}).get('setuptools', {})\n    if not tool_table:\n        return\n    for field, value in tool_table.items():\n        norm_key = json_compatible_key(field)\n        if norm_key in TOOL_TABLE_DEPRECATIONS:\n            suggestion, kwargs = TOOL_TABLE_DEPRECATIONS[norm_key]\n            msg = f'The parameter `{norm_key}` is deprecated, {suggestion}'\n            SetuptoolsDeprecationWarning.emit('Deprecated config', msg, **kwargs)\n        norm_key = TOOL_TABLE_RENAMES.get(norm_key, norm_key)\n        _set_config(dist, norm_key, value)\n    _copy_command_options(config, dist, filename)",
    "label": true
  },
  {
    "code": "def get_metadata_version(self):\n    mv = getattr(self, 'metadata_version', None)\n    if mv is None:\n        mv = Version('2.1')\n        self.metadata_version = mv\n    return mv",
    "label": true
  },
  {
    "code": "def get_seq(obj, cache={str: False, frozenset: False, list: True, set: True, dict: True, tuple: True, type: False, types.ModuleType: False, types.FunctionType: False, types.BuiltinFunctionType: False}):\n    try:\n        o_type = obj.__class__\n    except AttributeError:\n        o_type = type(obj)\n    hsattr = hasattr\n    if o_type in cache:\n        if cache[o_type]:\n            if hsattr(obj, 'copy'):\n                return obj.copy()\n            return obj\n    elif HAS_NUMPY and o_type in (numpy.ndarray, numpy.ma.core.MaskedConstant):\n        if obj.shape and obj.size:\n            return obj\n        else:\n            return []\n    elif hsattr(obj, '__contains__') and hsattr(obj, '__iter__') and hsattr(obj, '__len__') and hsattr(o_type, '__contains__') and hsattr(o_type, '__iter__') and hsattr(o_type, '__len__'):\n        cache[o_type] = True\n        if hsattr(obj, 'copy'):\n            return obj.copy()\n        return obj\n    else:\n        cache[o_type] = False\n        return None",
    "label": true
  },
  {
    "code": "def _get_legacy_hook_marks(method: Any, hook_type: str, opt_names: Tuple[str, ...]) -> Dict[str, bool]:\n    if TYPE_CHECKING:\n        assert inspect.isroutine(method)\n    known_marks: set[str] = {m.name for m in getattr(method, 'pytestmark', [])}\n    must_warn: list[str] = []\n    opts: dict[str, bool] = {}\n    for opt_name in opt_names:\n        opt_attr = getattr(method, opt_name, AttributeError)\n        if opt_attr is not AttributeError:\n            must_warn.append(f'{opt_name}={opt_attr}')\n            opts[opt_name] = True\n        elif opt_name in known_marks:\n            must_warn.append(f'{opt_name}=True')\n            opts[opt_name] = True\n        else:\n            opts[opt_name] = False\n    if must_warn:\n        hook_opts = ', '.join(must_warn)\n        message = _pytest.deprecated.HOOK_LEGACY_MARKING.format(type=hook_type, fullname=method.__qualname__, hook_opts=hook_opts)\n        warn_explicit_for(cast(FunctionType, method), message)\n    return opts",
    "label": true
  },
  {
    "code": "def pformat(obj: t.Any) -> str:\n    from pprint import pformat\n    return pformat(obj)",
    "label": true
  },
  {
    "code": "def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\n    iter_values = iter(values)\n    try:\n        previous_value = next(iter_values)\n    except StopIteration:\n        return\n    for value in iter_values:\n        yield (False, previous_value)\n        previous_value = value\n    yield (True, previous_value)",
    "label": true
  },
  {
    "code": "def _is_linux_armhf(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.Arm) and (f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5) and (f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD)",
    "label": true
  },
  {
    "code": "def read_json(path):\n    with open(path, encoding='utf-8') as f:\n        return json.load(f)",
    "label": true
  },
  {
    "code": "def render_missing_space_in_doctest(msg, _node, source_lines=None):\n    line = msg.line\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(None, None), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def create_vendor_txt_map() -> Dict[str, str]:\n    with importlib.resources.open_text('pip._vendor', 'vendor.txt') as f:\n        lines = [line.strip().split(' ', 1)[0] for line in f.readlines() if '==' in line]\n    return dict((line.split('==', 1) for line in lines))",
    "label": true
  },
  {
    "code": "def glob_relative(patterns: Iterable[str], root_dir: Optional[_Path]=None) -> List[str]:\n    glob_characters = {'*', '?', '[', ']', '{', '}'}\n    expanded_values = []\n    root_dir = root_dir or os.getcwd()\n    for value in patterns:\n        if any((char in value for char in glob_characters)):\n            glob_path = os.path.abspath(os.path.join(root_dir, value))\n            expanded_values.extend(sorted((os.path.relpath(path, root_dir).replace(os.sep, '/') for path in iglob(glob_path, recursive=True))))\n        else:\n            path = os.path.relpath(value, root_dir).replace(os.sep, '/')\n            expanded_values.append(path)\n    return expanded_values",
    "label": true
  },
  {
    "code": "def get_wrapping_class(node):\n    klass = node.frame(future=True)\n    while klass is not None and (not isinstance(klass, ClassDef)):\n        if klass.parent is None:\n            klass = None\n        else:\n            klass = klass.parent.frame(future=True)\n    return klass",
    "label": true
  },
  {
    "code": "def except_(*exceptions, replace=None, use=None):\n\n    def decorate(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except exceptions:\n                try:\n                    return eval(use)\n                except TypeError:\n                    return replace\n        return wrapper\n    return decorate",
    "label": true
  },
  {
    "code": "def wheel_version(wheel_data: Message) -> Tuple[int, ...]:\n    version_text = wheel_data['Wheel-Version']\n    if version_text is None:\n        raise UnsupportedWheel('WHEEL is missing Wheel-Version')\n    version = version_text.strip()\n    try:\n        return tuple(map(int, version.split('.')))\n    except ValueError:\n        raise UnsupportedWheel(f'invalid Wheel-Version: {version!r}')",
    "label": true
  },
  {
    "code": "def get_csv_rows_for_installed(old_csv_rows: List[List[str]], installed: Dict[RecordPath, RecordPath], changed: Set[RecordPath], generated: List[str], lib_dir: str) -> List[InstalledCSVRow]:\n    installed_rows: List[InstalledCSVRow] = []\n    for row in old_csv_rows:\n        if len(row) > 3:\n            logger.warning('RECORD line has more than three elements: %s', row)\n        old_record_path = cast('RecordPath', row[0])\n        new_record_path = installed.pop(old_record_path, old_record_path)\n        if new_record_path in changed:\n            digest, length = rehash(_record_to_fs_path(new_record_path, lib_dir))\n        else:\n            digest = row[1] if len(row) > 1 else ''\n            length = row[2] if len(row) > 2 else ''\n        installed_rows.append((new_record_path, digest, length))\n    for f in generated:\n        path = _fs_to_record_path(f, lib_dir)\n        digest, length = rehash(f)\n        installed_rows.append((path, digest, length))\n    for installed_record_path in installed.values():\n        installed_rows.append((installed_record_path, '', ''))\n    return installed_rows",
    "label": true
  },
  {
    "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    if session_setting is None:\n        return request_setting\n    if request_setting is None:\n        return session_setting\n    if not (isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)):\n        return request_setting\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n    none_keys = [k for k, v in merged_setting.items() if v is None]\n    for key in none_keys:\n        del merged_setting[key]\n    return merged_setting",
    "label": true
  },
  {
    "code": "def _find_project_config() -> Iterator[Path]:\n    if Path('__init__.py').is_file():\n        curdir = Path(os.getcwd()).resolve()\n        while (curdir / '__init__.py').is_file():\n            curdir = curdir.parent\n            for rc_name in RC_NAMES:\n                rc_path = curdir / rc_name\n                if rc_path.is_file():\n                    yield rc_path.resolve()",
    "label": true
  },
  {
    "code": "def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n    if tokenizer.check('VARIABLE'):\n        return process_env_var(tokenizer.read().text.replace('.', '_'))\n    elif tokenizer.check('QUOTED_STRING'):\n        return process_python_str(tokenizer.read().text)\n    else:\n        tokenizer.raise_syntax_error(message='Expected a marker variable or quoted string')",
    "label": true
  },
  {
    "code": "def _align_header(header, alignment, width, visible_width, is_multiline=False, width_fn=None):\n    if is_multiline:\n        header_lines = re.split(_multiline_codes, header)\n        padded_lines = [_align_header(h, alignment, width, width_fn(h)) for h in header_lines]\n        return '\\n'.join(padded_lines)\n    ninvisible = len(header) - visible_width\n    width += ninvisible\n    if alignment == 'left':\n        return _padright(width, header)\n    elif alignment == 'center':\n        return _padboth(width, header)\n    elif not alignment:\n        return f'{header}'\n    else:\n        return _padleft(width, header)",
    "label": true
  },
  {
    "code": "def minmax(iterable_or_value, *others, key=None, default=_marker):\n    iterable = (iterable_or_value, *others) if others else iterable_or_value\n    it = iter(iterable)\n    try:\n        lo = hi = next(it)\n    except StopIteration as e:\n        if default is _marker:\n            raise ValueError('`minmax()` argument is an empty iterable. Provide a `default` value to suppress this error.') from e\n        return default\n    if key is None:\n        for x, y in zip_longest(it, it, fillvalue=lo):\n            if y < x:\n                x, y = (y, x)\n            if x < lo:\n                lo = x\n            if hi < y:\n                hi = y\n    else:\n        lo_key = hi_key = key(lo)\n        for x, y in zip_longest(it, it, fillvalue=lo):\n            x_key, y_key = (key(x), key(y))\n            if y_key < x_key:\n                x, y, x_key, y_key = (y, x, y_key, x_key)\n            if x_key < lo_key:\n                lo, lo_key = (x, x_key)\n            if hi_key < y_key:\n                hi, hi_key = (y, y_key)\n    return (lo, hi)",
    "label": true
  },
  {
    "code": "def build_huffman_tree(freq_dict: dict[int, int]) -> HuffmanTree:\n    copy = list(freq_dict.items())\n    if len(copy) == 1:\n        return HuffmanTree(None, HuffmanTree(copy[0][0]), HuffmanTree((copy[0][0] + 1) % 256))\n    copy.sort(key=_two)\n    copy.reverse()\n    while len(copy) > 1:\n        item1 = copy.pop(-1)\n        if isinstance(item1[0], HuffmanTree):\n            tree1 = item1[0]\n        else:\n            tree1 = HuffmanTree(item1[0])\n        item2 = copy.pop(-1)\n        if isinstance(item2[0], HuffmanTree):\n            tree2 = item2[0]\n        else:\n            tree2 = HuffmanTree(item2[0])\n        big_tree = HuffmanTree(None, tree1, tree2)\n        value = item1[1] + item2[1]\n        index = _binary_search_tuple2(copy, value)\n        copy.insert(index, (big_tree, value))\n    return copy[0][0]",
    "label": true
  },
  {
    "code": "def get_legacy_build_wheel_path(names: List[str], temp_dir: str, name: str, command_args: List[str], command_output: str) -> Optional[str]:\n    names = sorted(names)\n    if not names:\n        msg = 'Legacy build of wheel for {!r} created no files.\\n'.format(name)\n        msg += format_command_result(command_args, command_output)\n        logger.warning(msg)\n        return None\n    if len(names) > 1:\n        msg = 'Legacy build of wheel for {!r} created more than one file.\\nFilenames (choosing first): {}\\n'.format(name, names)\n        msg += format_command_result(command_args, command_output)\n        logger.warning(msg)\n    return os.path.join(temp_dir, names[0])",
    "label": true
  },
  {
    "code": "def unzip_file(filename: str, location: str, flatten: bool=True) -> None:\n    ensure_dir(location)\n    zipfp = open(filename, 'rb')\n    try:\n        zip = zipfile.ZipFile(zipfp, allowZip64=True)\n        leading = has_leading_dir(zip.namelist()) and flatten\n        for info in zip.infolist():\n            name = info.filename\n            fn = name\n            if leading:\n                fn = split_leading_dir(name)[1]\n            fn = os.path.join(location, fn)\n            dir = os.path.dirname(fn)\n            if not is_within_directory(location, fn):\n                message = 'The zip file ({}) has a file ({}) trying to install outside target directory ({})'\n                raise InstallationError(message.format(filename, fn, location))\n            if fn.endswith('/') or fn.endswith('\\\\'):\n                ensure_dir(fn)\n            else:\n                ensure_dir(dir)\n                fp = zip.open(name)\n                try:\n                    with open(fn, 'wb') as destfp:\n                        shutil.copyfileobj(fp, destfp)\n                finally:\n                    fp.close()\n                    if zip_item_is_executable(info):\n                        set_extracted_file_to_default_mode_plus_executable(fn)\n    finally:\n        zipfp.close()",
    "label": true
  },
  {
    "code": "def get_vendor_version_from_module(module_name: str) -> Optional[str]:\n    module = get_module_from_module_name(module_name)\n    version = getattr(module, '__version__', None)\n    if not version:\n        assert module.__file__ is not None\n        env = get_environment([os.path.dirname(module.__file__)])\n        dist = env.get_distribution(module_name)\n        if dist:\n            version = str(dist.version)\n    return version",
    "label": true
  },
  {
    "code": "def test_string_concatenation(long_lst: list) -> None:\n    for method in [_forloop, _joinstr]:\n        start = time.time()\n        method(long_lst)\n        print(f'It took {time.time() - start} to concatenate a list of {len(long_lst)} length using {method}.')",
    "label": true
  },
  {
    "code": "def write_file(filename, contents):\n    contents = '\\n'.join(contents)\n    contents = contents.encode('utf-8')\n    with open(filename, 'wb') as f:\n        f.write(contents)",
    "label": true
  },
  {
    "code": "def test_circular_reference():\n    assert copy(obj4())\n    obj4_copy = dill.loads(dill.dumps(obj4()))\n    assert type(obj4_copy) is type(obj4_copy).__init__.__closure__[0].cell_contents\n    assert type(obj4_copy.b) is type(obj4_copy.b).__init__.__closure__[0].cell_contents",
    "label": true
  },
  {
    "code": "def walk_egg(egg_dir):\n    walker = sorted_walk(egg_dir)\n    base, dirs, files = next(walker)\n    if 'EGG-INFO' in dirs:\n        dirs.remove('EGG-INFO')\n    yield (base, dirs, files)\n    for bdf in walker:\n        yield bdf",
    "label": true
  },
  {
    "code": "def _nullpager(stream: t.TextIO, generator: t.Iterable[str], color: t.Optional[bool]) -> None:\n    for text in generator:\n        if not color:\n            text = strip_ansi(text)\n        stream.write(text)",
    "label": true
  },
  {
    "code": "def get_eval_context(node: 'Node', ctx: t.Optional[EvalContext]) -> EvalContext:\n    if ctx is None:\n        if node.environment is None:\n            raise RuntimeError('if no eval context is passed, the node must have an attached environment.')\n        return EvalContext(node.environment)\n    return ctx",
    "label": true
  },
  {
    "code": "def parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n    if filename.endswith('.tar.gz'):\n        file_stem = filename[:-len('.tar.gz')]\n    elif filename.endswith('.zip'):\n        file_stem = filename[:-len('.zip')]\n    else:\n        raise InvalidSdistFilename(f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'): {filename}\")\n    name_part, sep, version_part = file_stem.rpartition('-')\n    if not sep:\n        raise InvalidSdistFilename(f'Invalid sdist filename: {filename}')\n    name = canonicalize_name(name_part)\n    version = Version(version_part)\n    return (name, version)",
    "label": true
  },
  {
    "code": "def string_to_tokentype(s):\n    if isinstance(s, _TokenType):\n        return s\n    if not s:\n        return Token\n    node = Token\n    for item in s.split('.'):\n        node = getattr(node, item)\n    return node",
    "label": true
  },
  {
    "code": "def canonicalize_name(name: str) -> NormalizedName:\n    value = _canonicalize_regex.sub('-', name).lower()\n    return cast(NormalizedName, value)",
    "label": true
  },
  {
    "code": "def check_invalid_constraint_type(req: InstallRequirement) -> str:\n    problem = ''\n    if not req.name:\n        problem = 'Unnamed requirements are not allowed as constraints'\n    elif req.editable:\n        problem = 'Editable requirements are not allowed as constraints'\n    elif req.extras:\n        problem = 'Constraints cannot have extras'\n    if problem:\n        deprecated(reason='Constraints are only allowed to take the form of a package name and a version specifier. Other forms were originally permitted as an accident of the implementation, but were undocumented. The new implementation of the resolver no longer supports these forms.', replacement='replacing the constraint with a requirement', gone_in=None, issue=8210)\n    return problem",
    "label": true
  },
  {
    "code": "def pass_none(func):\n\n    @functools.wraps(func)\n    def wrapper(param, *args, **kwargs):\n        if param is not None:\n            return func(param, *args, **kwargs)\n    return wrapper",
    "label": true
  },
  {
    "code": "def _find_frame_imports(name: str, frame: nodes.LocalsDictNodeNG) -> bool:\n    if name in _flattened_scope_names(frame.nodes_of_class(nodes.Global)):\n        return False\n    imports = frame.nodes_of_class((nodes.Import, nodes.ImportFrom))\n    for import_node in imports:\n        for import_name, import_alias in import_node.names:\n            if import_alias:\n                if import_alias == name:\n                    return True\n            elif import_name and import_name == name:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def factor(n):\n    isqrt = getattr(math, 'isqrt', lambda x: int(math.sqrt(x)))\n    for prime in sieve(isqrt(n) + 1):\n        while True:\n            quotient, remainder = divmod(n, prime)\n            if remainder:\n                break\n            yield prime\n            n = quotient\n            if n == 1:\n                return\n    if n >= 2:\n        yield n",
    "label": true
  },
  {
    "code": "def _worth_extracting(element: pyparsing.ParserElement) -> bool:\n    children = element.recurse()\n    return any((child.recurse() for child in children))",
    "label": true
  },
  {
    "code": "def test_one_arg_functions():\n    for obj in [g, h, squared]:\n        pyfile = dumpIO_source(obj, alias='_obj')\n        _obj = loadIO_source(pyfile)\n        assert _obj(4) == obj(4)",
    "label": true
  },
  {
    "code": "def _deprecation_notice(fn: Fn) -> Fn:\n\n    @wraps(fn)\n    def _wrapper(*args, **kwargs):\n        SetuptoolsDeprecationWarning.emit('Deprecated API usage.', f'\\n            As setuptools moves its configuration towards `pyproject.toml`,\\n            `{__name__}.{fn.__name__}` became deprecated.\\n\\n            For the time being, you can use the `{setupcfg.__name__}` module\\n            to access a backward compatible API, but this module is provisional\\n            and might be removed in the future.\\n\\n            To read project metadata, consider using\\n            ``build.util.project_wheel_metadata`` (https://pypi.org/project/build/).\\n            For simple scenarios, you can also try parsing the file directly\\n            with the help of ``configparser``.\\n            ')\n        return fn(*args, **kwargs)\n    return cast(Fn, _wrapper)",
    "label": true
  },
  {
    "code": "def _create_cfstring_array(lst):\n    cf_arr = None\n    try:\n        cf_arr = CoreFoundation.CFArrayCreateMutable(CoreFoundation.kCFAllocatorDefault, 0, ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks))\n        if not cf_arr:\n            raise MemoryError('Unable to allocate memory!')\n        for item in lst:\n            cf_str = _cfstr(item)\n            if not cf_str:\n                raise MemoryError('Unable to allocate memory!')\n            try:\n                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)\n            finally:\n                CoreFoundation.CFRelease(cf_str)\n    except BaseException as e:\n        if cf_arr:\n            CoreFoundation.CFRelease(cf_arr)\n        raise ssl.SSLError('Unable to allocate array: %s' % (e,))\n    return cf_arr",
    "label": true
  },
  {
    "code": "def query_vcvarsall(version, arch='x86'):\n    vcvarsall = find_vcvarsall(version)\n    interesting = {'include', 'lib', 'libpath', 'path'}\n    result = {}\n    if vcvarsall is None:\n        raise DistutilsPlatformError('Unable to find vcvarsall.bat')\n    log.debug(\"Calling 'vcvarsall.bat %s' (version=%s)\", arch, version)\n    popen = subprocess.Popen('\"{}\" {} & set'.format(vcvarsall, arch), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    try:\n        stdout, stderr = popen.communicate()\n        if popen.wait() != 0:\n            raise DistutilsPlatformError(stderr.decode('mbcs'))\n        stdout = stdout.decode('mbcs')\n        for line in stdout.split('\\n'):\n            line = Reg.convert_mbcs(line)\n            if '=' not in line:\n                continue\n            line = line.strip()\n            key, value = line.split('=', 1)\n            key = key.lower()\n            if key in interesting:\n                if value.endswith(os.pathsep):\n                    value = value[:-1]\n                result[key] = removeDuplicates(value)\n    finally:\n        popen.stdout.close()\n        popen.stderr.close()\n    if len(result) != len(interesting):\n        raise ValueError(str(list(result.keys())))\n    return result",
    "label": true
  },
  {
    "code": "def _pipe_segment_with_colons(align, colwidth):\n    w = colwidth\n    if align in ['right', 'decimal']:\n        return '-' * (w - 1) + ':'\n    elif align == 'center':\n        return ':' + '-' * (w - 2) + ':'\n    elif align == 'left':\n        return ':' + '-' * (w - 1)\n    else:\n        return '-' * w",
    "label": true
  },
  {
    "code": "def make_distribution_for_install_requirement(install_req: InstallRequirement) -> AbstractDistribution:\n    if install_req.editable:\n        return SourceDistribution(install_req)\n    if install_req.is_wheel:\n        return WheelDistribution(install_req)\n    return SourceDistribution(install_req)",
    "label": true
  },
  {
    "code": "def _aug_op(instance: InferenceResult, opnode: nodes.AugAssign, op: str, other: InferenceResult, context: InferenceContext, reverse: bool=False) -> functools.partial[Generator[InferenceResult, None, None]]:\n    method_name = protocols.AUGMENTED_OP_METHOD[op]\n    return functools.partial(_invoke_binop_inference, instance=instance, op=op, opnode=opnode, other=other, context=context, method_name=method_name)",
    "label": true
  },
  {
    "code": "def parse_basic_str_escape(src: str, pos: Pos, *, multiline: bool=False) -> tuple[Pos, str]:\n    escape_id = src[pos:pos + 2]\n    pos += 2\n    if multiline and escape_id in {'\\\\ ', '\\\\\\t', '\\\\\\n'}:\n        if escape_id != '\\\\\\n':\n            pos = skip_chars(src, pos, TOML_WS)\n            try:\n                char = src[pos]\n            except IndexError:\n                return (pos, '')\n            if char != '\\n':\n                raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\")\n            pos += 1\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        return (pos, '')\n    if escape_id == '\\\\u':\n        return parse_hex_char(src, pos, 4)\n    if escape_id == '\\\\U':\n        return parse_hex_char(src, pos, 8)\n    try:\n        return (pos, BASIC_STR_ESCAPE_REPLACEMENTS[escape_id])\n    except KeyError:\n        raise suffixed_err(src, pos, \"Unescaped '\\\\' in a string\") from None",
    "label": true
  },
  {
    "code": "def newer(source, target):\n    if not os.path.exists(source):\n        raise DistutilsFileError(\"file '%s' does not exist\" % os.path.abspath(source))\n    if not os.path.exists(target):\n        return 1\n    from stat import ST_MTIME\n    mtime1 = os.stat(source)[ST_MTIME]\n    mtime2 = os.stat(target)[ST_MTIME]\n    return mtime1 > mtime2",
    "label": true
  },
  {
    "code": "def reinit():\n    if wrapped_stdout is not None:\n        sys.stdout = wrapped_stdout\n    if wrapped_stderr is not None:\n        sys.stderr = wrapped_stderr",
    "label": true
  },
  {
    "code": "def check_header_validity(header):\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)",
    "label": true
  },
  {
    "code": "def attach_const_node(node, name: str, value) -> None:\n    if name not in node.special_attributes:\n        _attach_local_node(node, nodes.const_factory(value), name)",
    "label": true
  },
  {
    "code": "def _read_field_unescaped_from_msg(msg: Message, field: str) -> Optional[str]:\n    value = _read_field_from_msg(msg, field)\n    if value is None:\n        return value\n    return rfc822_unescape(value)",
    "label": true
  },
  {
    "code": "def rfc822_escape(header):\n    lines = header.split('\\n')\n    sep = '\\n' + 8 * ' '\n    return sep.join(lines)",
    "label": true
  },
  {
    "code": "def _is_arg_name(s, index, node):\n    if not node.arg:\n        return False\n    return s[index:index + len(node.arg)] == node.arg",
    "label": true
  },
  {
    "code": "def decode(s: Union[str, bytes, bytearray], strict: bool=False, uts46: bool=False, std3_rules: bool=False) -> str:\n    try:\n        if isinstance(s, (bytes, bytearray)):\n            s = s.decode('ascii')\n    except UnicodeDecodeError:\n        raise IDNAError('Invalid ASCII in A-label')\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split('.')\n    if not labels or labels == ['']:\n        raise IDNAError('Empty domain')\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError('Empty label')\n    if trailing_dot:\n        result.append('')\n    return '.'.join(result)",
    "label": true
  },
  {
    "code": "def _check_generic(cls, parameters, elen=_marker):\n    if not elen:\n        raise TypeError(f'{cls} is not a generic class')\n    if elen is _marker:\n        if not hasattr(cls, '__parameters__') or not cls.__parameters__:\n            raise TypeError(f'{cls} is not a generic class')\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, '__parameters__'):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum((isinstance(p, TypeVarTuple) for p in parameters))\n            if num_tv_tuples > 0 and alen >= elen - num_tv_tuples:\n                return\n        raise TypeError(f\"Too {('many' if alen > elen else 'few')} parameters for {cls}; actual {alen}, expected {elen}\")",
    "label": true
  },
  {
    "code": "def _add_rcfile_default_pylintrc(args: list[str]) -> list[str]:\n    if not any(('--rcfile' in arg for arg in args)):\n        args.insert(0, f'--rcfile={PYLINTRC}')\n    return args",
    "label": true
  },
  {
    "code": "def getfslineno(obj: object) -> Tuple[Union[str, Path], int]:\n    obj = get_real_func(obj)\n    if hasattr(obj, 'place_as'):\n        obj = obj.place_as\n    try:\n        code = Code.from_function(obj)\n    except TypeError:\n        try:\n            fn = inspect.getsourcefile(obj) or inspect.getfile(obj)\n        except TypeError:\n            return ('', -1)\n        fspath = fn and absolutepath(fn) or ''\n        lineno = -1\n        if fspath:\n            try:\n                _, lineno = findsource(obj)\n            except OSError:\n                pass\n        return (fspath, lineno)\n    return (code.path, code.firstlineno)",
    "label": true
  },
  {
    "code": "def split_auth_netloc_from_url(url: str) -> Tuple[str, str, Tuple[Optional[str], Optional[str]]]:\n    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)\n    return (url_without_auth, netloc, auth)",
    "label": true
  },
  {
    "code": "def _compute_tags(original_tags: Iterable[str], new_tags: str | None) -> set[str]:\n    if new_tags is None:\n        return set(original_tags)\n    if new_tags.startswith('+'):\n        return {*original_tags, *new_tags[1:].split('.')}\n    if new_tags.startswith('-'):\n        return set(original_tags) - set(new_tags[1:].split('.'))\n    return set(new_tags.split('.'))",
    "label": true
  },
  {
    "code": "def method_cache(method: CallableT, cache_wrapper: Callable[[CallableT], CallableT]=functools.lru_cache()) -> CallableT:\n\n    def wrapper(self: object, *args: object, **kwargs: object) -> object:\n        bound_method: CallableT = types.MethodType(method, self)\n        cached_method = cache_wrapper(bound_method)\n        setattr(self, method.__name__, cached_method)\n        return cached_method(*args, **kwargs)\n    wrapper.cache_clear = lambda: None\n    return _special_method_cache(method, cache_wrapper) or wrapper",
    "label": true
  },
  {
    "code": "def has_tls() -> bool:\n    try:\n        import _ssl\n        return True\n    except ImportError:\n        pass\n    from pip._vendor.urllib3.util import IS_PYOPENSSL\n    return IS_PYOPENSSL",
    "label": true
  },
  {
    "code": "def arg_matches_format_type(arg_type: SuccessfulInferenceResult, format_type: str) -> bool:\n    if format_type in 'sr':\n        return True\n    if isinstance(arg_type, astroid.Instance):\n        arg_type = arg_type.pytype()\n        if arg_type == 'builtins.str':\n            return format_type == 'c'\n        if arg_type == 'builtins.float':\n            return format_type in 'deEfFgGn%'\n        if arg_type == 'builtins.int':\n            return True\n        return False\n    return True",
    "label": true
  },
  {
    "code": "def make_safe_parse_float(parse_float: ParseFloat) -> ParseFloat:\n    if parse_float is float:\n        return float\n\n    def safe_parse_float(float_str: str) -> Any:\n        float_value = parse_float(float_str)\n        if isinstance(float_value, (dict, list)):\n            raise ValueError('parse_float must not return dicts or lists')\n        return float_value\n    return safe_parse_float",
    "label": true
  },
  {
    "code": "def _get_elf_header() -> Optional[_ELFFileHeader]:\n    try:\n        with open(sys.executable, 'rb') as f:\n            elf_header = _ELFFileHeader(f)\n    except (OSError, TypeError, _ELFFileHeader._InvalidELFFileHeader):\n        return None\n    return elf_header",
    "label": true
  },
  {
    "code": "def path_to_cache_dir(path):\n    d, p = os.path.splitdrive(os.path.abspath(path))\n    if d:\n        d = d.replace(':', '---')\n    p = p.replace(os.sep, '--')\n    return d + p + '.cache'",
    "label": true
  },
  {
    "code": "def validate_https___docs_python_org_3_install(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, dict):\n        raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must be object', value=data, name='' + (name_prefix or 'data') + '', definition={'$schema': 'http://json-schema.org/draft-07/schema', '$id': 'https://docs.python.org/3/install/', 'title': '``tool.distutils`` table', '$$description': ['Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` scripts via `distutils configuration files', '<https://docs.python.org/3/install/#distutils-configuration-files>`_.', '``tool.distutils`` subtables could be used with the same purpose', '(NOT CURRENTLY IMPLEMENTED).'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if 'global' in data_keys:\n            data_keys.remove('global')\n            data__global = data['global']\n            if not isinstance(data__global, dict):\n                raise JsonSchemaValueException('' + (name_prefix or 'data') + '.global must be object', value=data__global, name='' + (name_prefix or 'data') + '.global', definition={'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}, rule='type')\n        for data_key, data_val in data.items():\n            if REGEX_PATTERNS['.+'].search(data_key):\n                if data_key in data_keys:\n                    data_keys.remove(data_key)\n                if not isinstance(data_val, dict):\n                    raise JsonSchemaValueException('' + (name_prefix or 'data') + '.{data_key}'.format(**locals()) + ' must be object', value=data_val, name='' + (name_prefix or 'data') + '.{data_key}'.format(**locals()) + '', definition={'type': 'object'}, rule='type')\n    return data",
    "label": true
  },
  {
    "code": "def optimizeconst(f: F) -> F:\n\n    def new_func(self: 'CodeGenerator', node: nodes.Expr, frame: 'Frame', **kwargs: t.Any) -> t.Any:\n        if self.optimizer is not None and (not frame.eval_ctx.volatile):\n            new_node = self.optimizer.visit(node, frame.eval_ctx)\n            if new_node != node:\n                return self.visit(new_node, frame)\n        return f(self, node, frame, **kwargs)\n    return update_wrapper(t.cast(F, new_func), f)",
    "label": true
  },
  {
    "code": "def _check_module_name(_node_type: str, name: str) -> List[str]:\n    error_msgs = []\n    if not _is_in_snake_case(name):\n        error_msgs.append(f'Module name \"{name}\" should be in snake_case format. Modules should be all-lowercase names, with each name separated by underscores.')\n    return error_msgs",
    "label": true
  },
  {
    "code": "def get_lexer_by_name(_alias, **options):\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls(**options)\n    raise ClassNotFound('no lexer for alias %r found' % _alias)",
    "label": true
  },
  {
    "code": "def _normalize_cached(filename, _cache={}):\n    try:\n        return _cache[filename]\n    except KeyError:\n        _cache[filename] = result = normalize_path(filename)\n        return result",
    "label": true
  },
  {
    "code": "def _wrap_text_to_colwidths(list_of_lists, colwidths, numparses=True):\n    numparses = _expand_iterable(numparses, len(list_of_lists[0]), True)\n    result = []\n    for row in list_of_lists:\n        new_row = []\n        for cell, width, numparse in zip(row, colwidths, numparses):\n            if _isnumber(cell) and numparse:\n                new_row.append(cell)\n                continue\n            if width is not None:\n                wrapper = _CustomTextWrap(width=width)\n                casted_cell = str(cell) if _isnumber(cell) else _type(cell, numparse)(cell)\n                wrapped = wrapper.wrap(casted_cell)\n                new_row.append('\\n'.join(wrapped))\n            else:\n                new_row.append(cell)\n        result.append(new_row)\n    return result",
    "label": true
  },
  {
    "code": "def _check_cryptography(cryptography_version):\n    try:\n        cryptography_version = list(map(int, cryptography_version.split('.')))\n    except ValueError:\n        return\n    if cryptography_version < [1, 3, 4]:\n        warning = 'Old version of cryptography ({}) may cause slowdown.'.format(cryptography_version)\n        warnings.warn(warning, RequestsDependencyWarning)",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))",
    "label": true
  },
  {
    "code": "def _default_exception_debug_action(instring: str, loc: int, expr: 'ParserElement', exc: Exception, cache_hit: bool=False):\n    cache_hit_str = '*' if cache_hit else ''\n    print(f'{cache_hit_str}Match {expr} failed, {type(exc).__name__} raised: {exc}')",
    "label": true
  },
  {
    "code": "def prepare_metadata_for_build_editable(metadata_directory, config_settings, _allow_fallback):\n    backend = _build_backend()\n    try:\n        hook = backend.prepare_metadata_for_build_editable\n    except AttributeError:\n        if not _allow_fallback:\n            raise HookMissing()\n        try:\n            build_hook = backend.build_editable\n        except AttributeError:\n            raise HookMissing(hook_name='build_editable')\n        else:\n            whl_basename = build_hook(metadata_directory, config_settings)\n            return _get_wheel_metadata_from_wheel(whl_basename, metadata_directory, config_settings)\n    else:\n        return hook(metadata_directory, config_settings)",
    "label": true
  },
  {
    "code": "def once(func):\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if not hasattr(wrapper, 'saved_result'):\n            wrapper.saved_result = func(*args, **kwargs)\n        return wrapper.saved_result\n    wrapper.reset = lambda: vars(wrapper).__delitem__('saved_result')\n    return wrapper",
    "label": true
  },
  {
    "code": "def run_pylint(argv: Sequence[str] | None=None) -> None:\n    from pylint.lint import Run as PylintRun\n    try:\n        PylintRun(argv or sys.argv[1:])\n    except KeyboardInterrupt:\n        sys.exit(1)",
    "label": true
  },
  {
    "code": "def address_in_network(ip, net):\n    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n    netaddr, bits = net.split('/')\n    netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n    return ipaddr & netmask == network & netmask",
    "label": true
  },
  {
    "code": "def platform_tags() -> Iterator[str]:\n    if platform.system() == 'Darwin':\n        return mac_platforms()\n    elif platform.system() == 'Linux':\n        return _linux_platforms()\n    else:\n        return _generic_platforms()",
    "label": true
  },
  {
    "code": "def dump_source(object, **kwds):\n    from .source import importable, getname\n    import tempfile\n    kwds.setdefault('delete', True)\n    kwds.pop('suffix', '')\n    alias = kwds.pop('alias', '')\n    name = str(alias) or getname(object)\n    name = '\\n#NAME: %s\\n' % name\n    file = tempfile.NamedTemporaryFile(suffix='.py', **kwds)\n    file.write(b(''.join([importable(object, alias=alias), name])))\n    file.flush()\n    return file",
    "label": true
  },
  {
    "code": "def print_yielded(func):\n    print_all = functools.partial(map, print)\n    print_results = compose(more_itertools.consume, print_all, func)\n    return functools.wraps(func)(print_results)",
    "label": true
  },
  {
    "code": "def read_chunks(file: BinaryIO, size: int=io.DEFAULT_BUFFER_SIZE) -> Generator[bytes, None, None]:\n    while True:\n        chunk = file.read(size)\n        if not chunk:\n            break\n        yield chunk",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e201(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    curr_idx = col + len(source_lines[line - 1][col:]) - len(source_lines[line - 1][col:].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _load_formatters(module_name):\n    mod = __import__(module_name, None, None, ['__all__'])\n    for formatter_name in mod.__all__:\n        cls = getattr(mod, formatter_name)\n        _formatter_cache[cls.name] = cls",
    "label": true
  },
  {
    "code": "def render_message(msg, node, source_lines):\n    renderer = CUSTOM_MESSAGES.get(msg.symbol, render_generic)\n    yield from renderer(msg, node, source_lines)",
    "label": true
  },
  {
    "code": "def iter_encode(input, encoding=UTF8, errors='strict'):\n    encode = IncrementalEncoder(encoding, errors).encode\n    return _iter_encode_generator(input, encode)",
    "label": true
  },
  {
    "code": "def batched(iterable, n):\n    if hexversion >= 51118240:\n        warnings.warn('batched will be removed in a future version of more-itertools. Use the standard library itertools.batched function instead', DeprecationWarning)\n    it = iter(iterable)\n    while True:\n        batch = list(islice(it, n))\n        if not batch:\n            break\n        yield batch",
    "label": true
  },
  {
    "code": "def wrap(s):\n    paragraphs = s.splitlines()\n    wrapped = ('\\n'.join(textwrap.wrap(para)) for para in paragraphs)\n    return '\\n\\n'.join(wrapped)",
    "label": true
  },
  {
    "code": "def egg_link_path_from_location(raw_name: str) -> Optional[str]:\n    sites: List[str] = []\n    if running_under_virtualenv():\n        sites.append(site_packages)\n        if not virtualenv_no_global() and user_site:\n            sites.append(user_site)\n    else:\n        if user_site:\n            sites.append(user_site)\n        sites.append(site_packages)\n    egg_link_name = _egg_link_name(raw_name)\n    for site in sites:\n        egglink = os.path.join(site, egg_link_name)\n        if os.path.isfile(egglink):\n            return egglink\n    return None",
    "label": true
  },
  {
    "code": "def _rich_progress_bar(iterable: Iterable[bytes], *, bar_type: str, size: int) -> Generator[bytes, None, None]:\n    assert bar_type == 'on', 'This should only be used in the default mode.'\n    if not size:\n        total = float('inf')\n        columns: Tuple[ProgressColumn, ...] = (TextColumn('[progress.description]{task.description}'), SpinnerColumn('line', speed=1.5), FileSizeColumn(), TransferSpeedColumn(), TimeElapsedColumn())\n    else:\n        total = size\n        columns = (TextColumn('[progress.description]{task.description}'), BarColumn(), DownloadColumn(), TransferSpeedColumn(), TextColumn('eta'), TimeRemainingColumn())\n    progress = Progress(*columns, refresh_per_second=30)\n    task_id = progress.add_task(' ' * (get_indentation() + 2), total=total)\n    with progress:\n        for chunk in iterable:\n            yield chunk\n            progress.update(task_id, advance=len(chunk))",
    "label": true
  },
  {
    "code": "def split_filename(filename, project_name=None):\n    result = None\n    pyver = None\n    filename = unquote(filename).replace(' ', '-')\n    m = PYTHON_VERSION.search(filename)\n    if m:\n        pyver = m.group(1)\n        filename = filename[:m.start()]\n    if project_name and len(filename) > len(project_name) + 1:\n        m = re.match(re.escape(project_name) + '\\\\b', filename)\n        if m:\n            n = m.end()\n            result = (filename[:n], filename[n + 1:], pyver)\n    if result is None:\n        m = PROJECT_NAME_AND_VERSION.match(filename)\n        if m:\n            result = (m.group(1), m.group(3), pyver)\n    return result",
    "label": true
  },
  {
    "code": "def test_two_arg_functions():\n    for obj in [add]:\n        pyfile = dumpIO_source(obj, alias='_obj')\n        _obj = loadIO_source(pyfile)\n        assert _obj(4, 2) == obj(4, 2)",
    "label": true
  },
  {
    "code": "def get_console() -> 'Console':\n    global _console\n    if _console is None:\n        from .console import Console\n        _console = Console()\n    return _console",
    "label": true
  },
  {
    "code": "def iterate(func, start):\n    while True:\n        yield start\n        start = func(start)",
    "label": true
  },
  {
    "code": "def write_pkg_info(cmd, basename, filename):\n    log.info('writing %s', filename)\n    if not cmd.dry_run:\n        metadata = cmd.distribution.metadata\n        metadata.version, oldver = (cmd.egg_version, metadata.version)\n        metadata.name, oldname = (cmd.egg_name, metadata.name)\n        try:\n            metadata.write_pkg_info(cmd.egg_info)\n        finally:\n            metadata.name, metadata.version = (oldname, oldver)\n        safe = getattr(cmd.distribution, 'zip_safe', None)\n        bdist_egg.write_safety_flag(cmd.egg_info, safe)",
    "label": true
  },
  {
    "code": "def collect_one_node(collector: Collector) -> CollectReport:\n    ihook = collector.ihook\n    ihook.pytest_collectstart(collector=collector)\n    rep: CollectReport = ihook.pytest_make_collect_report(collector=collector)\n    call = rep.__dict__.pop('call', None)\n    if call and check_interactive_exception(call, rep):\n        ihook.pytest_exception_interact(node=collector, call=call, report=rep)\n    return rep",
    "label": true
  },
  {
    "code": "def detect(byte_str: bytes, should_rename_legacy: bool=False, **kwargs: Any) -> Dict[str, Optional[Union[str, float]]]:\n    if len(kwargs):\n        warn(f\"charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()\")\n    if not isinstance(byte_str, (bytearray, bytes)):\n        raise TypeError('Expected object of type bytes or bytearray, got: {0}'.format(type(byte_str)))\n    if isinstance(byte_str, bytearray):\n        byte_str = bytes(byte_str)\n    r = from_bytes(byte_str).best()\n    encoding = r.encoding if r is not None else None\n    language = r.language if r is not None and r.language != 'Unknown' else ''\n    confidence = 1.0 - r.chaos if r is not None else None\n    if r is not None and encoding == 'utf_8' and r.bom:\n        encoding += '_sig'\n    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:\n        encoding = CHARDET_CORRESPONDENCE[encoding]\n    return {'encoding': encoding, 'language': language, 'confidence': confidence}",
    "label": true
  },
  {
    "code": "def license():\n    print(__license__)\n    return",
    "label": true
  },
  {
    "code": "def default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {'implementation_name': implementation_name, 'implementation_version': iver, 'os_name': os.name, 'platform_machine': platform.machine(), 'platform_release': platform.release(), 'platform_system': platform.system(), 'platform_version': platform.version(), 'python_full_version': platform.python_version(), 'platform_python_implementation': platform.python_implementation(), 'python_version': '.'.join(platform.python_version_tuple()[:2]), 'sys_platform': sys.platform}",
    "label": true
  },
  {
    "code": "def collate(*iterables, **kwargs):\n    warnings.warn('collate is no longer part of more_itertools, use heapq.merge', DeprecationWarning)\n    return merge(*iterables, **kwargs)",
    "label": true
  },
  {
    "code": "def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None, socket_options=None):\n    host, port = address\n    if host.startswith('['):\n        host = host.strip('[]')\n    err = None\n    family = allowed_gai_family()\n    try:\n        host.encode('idna')\n    except UnicodeError:\n        return six.raise_from(LocationParseError(u\"'%s', label empty or too long\" % host), None)\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket.socket(af, socktype, proto)\n            _set_socket_options(sock, socket_options)\n            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n        except socket.error as e:\n            err = e\n            if sock is not None:\n                sock.close()\n                sock = None\n    if err is not None:\n        raise err\n    raise socket.error('getaddrinfo returns an empty list')",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e122_and_e127_and_e131(msg, _node, source_lines=None):\n    line = msg.line\n    curr_line_start_index = len(source_lines[line - 1]) - len(source_lines[line - 1].lstrip())\n    end_index = curr_line_start_index if curr_line_start_index > 0 else len(source_lines[line - 1])\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(0, end_index), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def _zip_equal(*iterables):\n    try:\n        first_size = len(iterables[0])\n        for i, it in enumerate(iterables[1:], 1):\n            size = len(it)\n            if size != first_size:\n                break\n        else:\n            return zip(*iterables)\n        raise UnequalIterablesError(details=(first_size, i, size))\n    except TypeError:\n        return _zip_equal_generator(iterables)",
    "label": true
  },
  {
    "code": "def _get_ttype_class(ttype):\n    fname = STANDARD_TYPES.get(ttype)\n    if fname:\n        return fname\n    aname = ''\n    while fname is None:\n        aname = '-' + ttype[-1] + aname\n        ttype = ttype.parent\n        fname = STANDARD_TYPES.get(ttype)\n    return fname + aname",
    "label": true
  },
  {
    "code": "def parse_literal_str(src: str, pos: Pos) -> tuple[Pos, str]:\n    pos += 1\n    start_pos = pos\n    pos = skip_until(src, pos, \"'\", error_on=ILLEGAL_LITERAL_STR_CHARS, error_on_eof=True)\n    return (pos + 1, src[start_pos:pos])",
    "label": true
  },
  {
    "code": "def just_fix_windows_console():\n    global fixed_windows_console\n    if sys.platform != 'win32':\n        return\n    if fixed_windows_console:\n        return\n    if wrapped_stdout is not None or wrapped_stderr is not None:\n        return\n    new_stdout = AnsiToWin32(sys.stdout, convert=None, strip=None, autoreset=False)\n    if new_stdout.convert:\n        sys.stdout = new_stdout\n    new_stderr = AnsiToWin32(sys.stderr, convert=None, strip=None, autoreset=False)\n    if new_stderr.convert:\n        sys.stderr = new_stderr\n    fixed_windows_console = True",
    "label": true
  },
  {
    "code": "def parse_key_part(src: str, pos: Pos) -> Tuple[Pos, str]:\n    try:\n        char: Optional[str] = src[pos]\n    except IndexError:\n        char = None\n    if char in BARE_KEY_CHARS:\n        start_pos = pos\n        pos = skip_chars(src, pos, BARE_KEY_CHARS)\n        return (pos, src[start_pos:pos])\n    if char == \"'\":\n        return parse_literal_str(src, pos)\n    if char == '\"':\n        return parse_one_line_basic_str(src, pos)\n    raise suffixed_err(src, pos, 'Invalid initial character for a key part')",
    "label": true
  },
  {
    "code": "def _get_module(obj: Any) -> ModuleType:\n    module_name = obj.__module__\n    module = sys.modules[module_name]\n    if module_name != '__main__' or not RENAME_MAIN_TO_PYDEV_UMD or _PYDEV_UMD_NAME not in sys.modules:\n        return module\n    if isinstance(obj, (FunctionType, type)):\n        name = obj.__name__\n    else:\n        return module\n    if name in vars(module):\n        return module\n    else:\n        return sys.modules[_PYDEV_UMD_NAME]",
    "label": true
  },
  {
    "code": "def _ssl_wrap_socket_impl(sock: socket.socket, ssl_context: ssl.SSLContext, tls_in_tls: bool, server_hostname: str | None=None) -> ssl.SSLSocket | SSLTransportType:\n    if tls_in_tls:\n        if not SSLTransport:\n            raise ProxySchemeUnsupported(\"TLS in TLS requires support for the 'ssl' module\")\n        SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n        return SSLTransport(sock, ssl_context, server_hostname)\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)",
    "label": true
  },
  {
    "code": "def skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:\n    try:\n        while src[pos] in chars:\n            pos += 1\n    except IndexError:\n        pass\n    return pos",
    "label": true
  },
  {
    "code": "def _report_unserialization_failure(type_name: str, report_class: Type[BaseReport], reportdict) -> NoReturn:\n    url = 'https://github.com/pytest-dev/pytest/issues'\n    stream = StringIO()\n    pprint('-' * 100, stream=stream)\n    pprint('INTERNALERROR: Unknown entry type returned: %s' % type_name, stream=stream)\n    pprint('report_name: %s' % report_class, stream=stream)\n    pprint(reportdict, stream=stream)\n    pprint('Please report this bug at %s' % url, stream=stream)\n    pprint('-' * 100, stream=stream)\n    raise RuntimeError(stream.getvalue())",
    "label": true
  },
  {
    "code": "def _find_adapter(registry, ob):\n    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))\n    for t in types:\n        if t in registry:\n            return registry[t]",
    "label": true
  },
  {
    "code": "def get_encodings_from_content(content):\n    warnings.warn('In requests 3.0, get_encodings_from_content will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)', DeprecationWarning)\n    charset_re = re.compile('<meta.*?charset=[\"\\\\\\']*(.+?)[\"\\\\\\'>]', flags=re.I)\n    pragma_re = re.compile('<meta.*?content=[\"\\\\\\']*;?charset=(.+?)[\"\\\\\\'>]', flags=re.I)\n    xml_re = re.compile('^<\\\\?xml.*?encoding=[\"\\\\\\']*(.+?)[\"\\\\\\'>]')\n    return charset_re.findall(content) + pragma_re.findall(content) + xml_re.findall(content)",
    "label": true
  },
  {
    "code": "def from_key_val_list(value):\n    if value is None:\n        return None\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError('cannot encode objects that are not 2-tuples')\n    return OrderedDict(value)",
    "label": true
  },
  {
    "code": "def convert_extras(extras: Optional[str]) -> Set[str]:\n    if not extras:\n        return set()\n    return get_requirement('placeholder' + extras.lower()).extras",
    "label": true
  },
  {
    "code": "def _infer_caller():\n\n    def is_this_file(frame_info):\n        return frame_info.filename == __file__\n\n    def is_wrapper(frame_info):\n        return frame_info.function == 'wrapper'\n    not_this_file = itertools.filterfalse(is_this_file, inspect.stack())\n    callers = itertools.filterfalse(is_wrapper, not_this_file)\n    return next(callers).frame",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    config.stash[old_mark_config_key] = MARK_GEN._config\n    MARK_GEN._config = config\n    empty_parameterset = config.getini(EMPTY_PARAMETERSET_OPTION)\n    if empty_parameterset not in ('skip', 'xfail', 'fail_at_collect', None, ''):\n        raise UsageError('{!s} must be one of skip, xfail or fail_at_collect but it is {!r}'.format(EMPTY_PARAMETERSET_OPTION, empty_parameterset))",
    "label": true
  },
  {
    "code": "def _ensure_api_header(response: Response) -> None:\n    content_type = response.headers.get('Content-Type', 'Unknown')\n    content_type_l = content_type.lower()\n    if content_type_l.startswith(('text/html', 'application/vnd.pypi.simple.v1+html', 'application/vnd.pypi.simple.v1+json')):\n        return\n    raise _NotAPIContent(content_type, response.request.method)",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def rewind_body(prepared_request):\n    body_seek = getattr(prepared_request.body, 'seek', None)\n    if body_seek is not None and isinstance(prepared_request._body_position, integer_types):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError('An error occurred when rewinding request body for redirect.')\n    else:\n        raise UnrewindableBodyError('Unable to rewind request body for redirect.')",
    "label": true
  },
  {
    "code": "def _bypass_ensure_directory(path):\n    if not WRITE_SUPPORT:\n        raise IOError('\"os.mkdir\" not supported on this platform.')\n    dirname, filename = split(path)\n    if dirname and filename and (not isdir(dirname)):\n        _bypass_ensure_directory(dirname)\n        try:\n            mkdir(dirname, 493)\n        except FileExistsError:\n            pass",
    "label": true
  },
  {
    "code": "def ascii_escaped(val: bytes | str) -> str:\n    if isinstance(val, bytes):\n        ret = _bytes_to_ascii(val)\n    else:\n        ret = val.encode('unicode_escape').decode('ascii')\n    return _translate_non_printable(ret)",
    "label": true
  },
  {
    "code": "def _align_column_choose_padfn(strings, alignment, has_invisible):\n    if alignment == 'right':\n        if not PRESERVE_WHITESPACE:\n            strings = [s.strip() for s in strings]\n        padfn = _padleft\n    elif alignment == 'center':\n        if not PRESERVE_WHITESPACE:\n            strings = [s.strip() for s in strings]\n        padfn = _padboth\n    elif alignment == 'decimal':\n        if has_invisible:\n            decimals = [_afterpoint(_strip_ansi(s)) for s in strings]\n        else:\n            decimals = [_afterpoint(s) for s in strings]\n        maxdecimals = max(decimals)\n        strings = [s + (maxdecimals - decs) * ' ' for s, decs in zip(strings, decimals)]\n        padfn = _padleft\n    elif not alignment:\n        padfn = _padnone\n    else:\n        if not PRESERVE_WHITESPACE:\n            strings = [s.strip() for s in strings]\n        padfn = _padright\n    return (strings, padfn)",
    "label": true
  },
  {
    "code": "def cpython_tags(python_version: Optional[PythonVersion]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    interpreter = f'cp{_version_nodot(python_version[:2])}'\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    for explicit_abi in ('abi3', 'none'):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    if _abi3_applies(python_version):\n        yield from (Tag(interpreter, 'abi3', platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, 'none', platform_) for platform_ in platforms)\n    if _abi3_applies(python_version):\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = 'cp{version}'.format(version=_version_nodot((python_version[0], minor_version)))\n                yield Tag(interpreter, 'abi3', platform_)",
    "label": true
  },
  {
    "code": "def _check_path(path: Path, fspath: LEGACY_PATH) -> None:\n    if Path(fspath) != path:\n        raise ValueError(f'Path({fspath!r}) != {path!r}\\nif both path and fspath are given they need to be equal')",
    "label": true
  },
  {
    "code": "def pytest_configure(config: Config) -> None:\n    packages_option = config.getoption('typeguard_packages')\n    if packages_option:\n        if packages_option == ':all:':\n            packages: list[str] | None = None\n        else:\n            packages = [pkg.strip() for pkg in packages_option.split(',')]\n            already_imported_packages = sorted((package for package in packages if package in sys.modules))\n            if already_imported_packages:\n                warnings.warn(f\"typeguard cannot check these packages because they are already imported: {', '.join(already_imported_packages)}\", InstrumentationWarning, stacklevel=1)\n        install_import_hook(packages=packages)\n    debug_option = config.getoption('typeguard_debug_instrumentation')\n    if debug_option:\n        global_config.debug_instrumentation = True\n    fail_callback_option = config.getoption('typeguard_typecheck_fail_callback')\n    if fail_callback_option:\n        callback = resolve_reference(fail_callback_option)\n        if not callable(callback):\n            raise TypeError(f'{fail_callback_option} ({qualified_name(callback.__class__)}) is not a callable')\n        global_config.typecheck_fail_callback = callback\n    forward_ref_policy_option = config.getoption('typeguard_forward_ref_policy')\n    if forward_ref_policy_option:\n        forward_ref_policy = ForwardRefPolicy.__members__[forward_ref_policy_option]\n        global_config.forward_ref_policy = forward_ref_policy\n    collection_check_strategy_option = config.getoption('typeguard_collection_check_strategy')\n    if collection_check_strategy_option:\n        collection_check_strategy = CollectionCheckStrategy.__members__[collection_check_strategy_option]\n        global_config.collection_check_strategy = collection_check_strategy",
    "label": true
  },
  {
    "code": "def create_dict_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot declare {key} twice')\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.get_or_create_nest(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']', pos):\n        raise suffixed_err(src, pos, \"Expected ']' at the end of a table declaration\")\n    return (pos + 1, key)",
    "label": true
  },
  {
    "code": "def test_labels():\n    assert lookup('utf-8').name == 'utf-8'\n    assert lookup('Utf-8').name == 'utf-8'\n    assert lookup('UTF-8').name == 'utf-8'\n    assert lookup('utf8').name == 'utf-8'\n    assert lookup('utf8').name == 'utf-8'\n    assert lookup('utf8 ').name == 'utf-8'\n    assert lookup(' \\r\\nutf8\\t').name == 'utf-8'\n    assert lookup('u8') is None\n    assert lookup('utf-8\\xa0') is None\n    assert lookup('US-ASCII').name == 'windows-1252'\n    assert lookup('iso-8859-1').name == 'windows-1252'\n    assert lookup('latin1').name == 'windows-1252'\n    assert lookup('LATIN1').name == 'windows-1252'\n    assert lookup('latin-1') is None\n    assert lookup('LAT\u0130N1') is None",
    "label": true
  },
  {
    "code": "def _ensure_api_header(response: Response) -> None:\n    content_type = response.headers.get('Content-Type', 'Unknown')\n    content_type_l = content_type.lower()\n    if content_type_l.startswith(('text/html', 'application/vnd.pypi.simple.v1+html', 'application/vnd.pypi.simple.v1+json')):\n        return\n    raise _NotAPIContent(content_type, response.request.method)",
    "label": true
  },
  {
    "code": "def get_config_h_filename():\n    if python_build:\n        if os.name == 'nt':\n            inc_dir = os.path.join(_sys_home or project_base, 'PC')\n        else:\n            inc_dir = _sys_home or project_base\n        return os.path.join(inc_dir, 'pyconfig.h')\n    else:\n        return sysconfig.get_config_h_filename()",
    "label": true
  },
  {
    "code": "def build_parser() -> optparse.OptionParser:\n    parser = optparse.OptionParser(add_help_option=False)\n    option_factories = SUPPORTED_OPTIONS + SUPPORTED_OPTIONS_REQ\n    for option_factory in option_factories:\n        option = option_factory()\n        parser.add_option(option)\n\n    def parser_exit(self: Any, msg: str) -> 'NoReturn':\n        raise OptionParsingError(msg)\n    parser.exit = parser_exit\n    return parser",
    "label": true
  },
  {
    "code": "def _functools_partial_inference(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[objects.PartialFunction]:\n    call = arguments.CallSite.from_call(node, context=context)\n    number_of_positional = len(call.positional_arguments)\n    if number_of_positional < 1:\n        raise UseInferenceDefault('functools.partial takes at least one argument')\n    if number_of_positional == 1 and (not call.keyword_arguments):\n        raise UseInferenceDefault('functools.partial needs at least to have some filled arguments')\n    partial_function = call.positional_arguments[0]\n    try:\n        inferred_wrapped_function = next(partial_function.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if isinstance(inferred_wrapped_function, UninferableBase):\n        raise UseInferenceDefault('Cannot infer the wrapped function')\n    if not isinstance(inferred_wrapped_function, FunctionDef):\n        raise UseInferenceDefault('The wrapped function is not a function')\n    if not inferred_wrapped_function.args:\n        function_parameters = []\n    else:\n        function_parameters = chain(inferred_wrapped_function.args.args or (), inferred_wrapped_function.args.posonlyargs or (), inferred_wrapped_function.args.kwonlyargs or ())\n    parameter_names = {param.name for param in function_parameters if isinstance(param, AssignName)}\n    if set(call.keyword_arguments) - parameter_names:\n        raise UseInferenceDefault('wrapped function received unknown parameters')\n    partial_function = objects.PartialFunction(call, name=inferred_wrapped_function.name, lineno=inferred_wrapped_function.lineno, col_offset=inferred_wrapped_function.col_offset, parent=node.parent)\n    partial_function.postinit(args=inferred_wrapped_function.args, body=inferred_wrapped_function.body, decorators=inferred_wrapped_function.decorators, returns=inferred_wrapped_function.returns, type_comment_returns=inferred_wrapped_function.type_comment_returns, type_comment_args=inferred_wrapped_function.type_comment_args, doc_node=inferred_wrapped_function.doc_node)\n    return iter((partial_function,))",
    "label": true
  },
  {
    "code": "def _append_basic_row(lines, padded_cells, colwidths, colaligns, rowfmt, rowalign=None):\n    lines.append(_build_row(padded_cells, colwidths, colaligns, rowfmt))\n    return lines",
    "label": true
  },
  {
    "code": "def parse_multiline_str(src: str, pos: Pos, *, literal: bool) -> Tuple[Pos, str]:\n    pos += 3\n    if src.startswith('\\n', pos):\n        pos += 1\n    if literal:\n        delim = \"'\"\n        end_pos = skip_until(src, pos, \"'''\", error_on=ILLEGAL_MULTILINE_LITERAL_STR_CHARS, error_on_eof=True)\n        result = src[pos:end_pos]\n        pos = end_pos + 3\n    else:\n        delim = '\"'\n        pos, result = parse_basic_str(src, pos, multiline=True)\n    if not src.startswith(delim, pos):\n        return (pos, result)\n    pos += 1\n    if not src.startswith(delim, pos):\n        return (pos, result + delim)\n    pos += 1\n    return (pos, result + delim * 2)",
    "label": true
  },
  {
    "code": "def decode_entity(match):\n    what = match.group(0)\n    return html.unescape(what)",
    "label": true
  },
  {
    "code": "def safe_listdir(path):\n    try:\n        return os.listdir(path)\n    except (PermissionError, NotADirectoryError):\n        pass\n    except OSError as e:\n        if e.errno not in (errno.ENOTDIR, errno.EACCES, errno.ENOENT):\n            raise\n    return ()",
    "label": true
  },
  {
    "code": "def set_file_position(body, pos):\n    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except (IOError, OSError):\n            pos = _FAILEDTELL\n    return pos",
    "label": true
  },
  {
    "code": "def _glibc_version_string_confstr() -> Optional[str]:\n    try:\n        version_string = os.confstr('CS_GNU_LIBC_VERSION')\n        assert version_string is not None\n        _, version = version_string.split()\n    except (AssertionError, AttributeError, OSError, ValueError):\n        return None\n    return version",
    "label": true
  },
  {
    "code": "def get_module_and_frameid(node: nodes.NodeNG) -> tuple[str, str]:\n    frame = node.frame(future=True)\n    module, obj = ('', [])\n    while frame:\n        if isinstance(frame, Module):\n            module = frame.name\n        else:\n            obj.append(getattr(frame, 'name', '<lambda>'))\n        try:\n            frame = frame.parent.frame(future=True)\n        except AttributeError:\n            break\n    obj.reverse()\n    return (module, '.'.join(obj))",
    "label": true
  },
  {
    "code": "def find_plugin_styles():\n    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):\n        yield (entrypoint.name, entrypoint.load())",
    "label": true
  },
  {
    "code": "def has_starred_node_recursive(node: nodes.For | nodes.Comprehension | nodes.Set) -> Iterator[bool]:\n    if isinstance(node, nodes.Starred):\n        yield True\n    elif isinstance(node, nodes.Set):\n        for elt in node.elts:\n            yield from has_starred_node_recursive(elt)\n    elif isinstance(node, (nodes.For, nodes.Comprehension)):\n        for elt in node.iter.elts:\n            yield from has_starred_node_recursive(elt)",
    "label": true
  },
  {
    "code": "def _call_validator(opttype: str, optdict: Any, option: str, value: Any) -> Any:\n    if opttype not in VALIDATORS:\n        raise TypeError(f'Unsupported type \"{opttype}\"')\n    try:\n        return VALIDATORS[opttype](optdict, option, value)\n    except TypeError:\n        try:\n            return VALIDATORS[opttype](value)\n        except Exception as e:\n            raise optparse.OptionValueError(f'{option} value ({value!r}) should be of type {opttype}') from e",
    "label": true
  },
  {
    "code": "def interpreter_version(*, warn: bool=False) -> str:\n    version = _get_config_var('py_version_nodot', warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version",
    "label": true
  },
  {
    "code": "def _verify_one(req: InstallRequirement, wheel_path: str) -> None:\n    canonical_name = canonicalize_name(req.name or '')\n    w = Wheel(os.path.basename(wheel_path))\n    if canonicalize_name(w.name) != canonical_name:\n        raise InvalidWheelFilename('Wheel has unexpected file name: expected {!r}, got {!r}'.format(canonical_name, w.name))\n    dist = get_wheel_distribution(FilesystemWheel(wheel_path), canonical_name)\n    dist_verstr = str(dist.version)\n    if canonicalize_version(dist_verstr) != canonicalize_version(w.version):\n        raise InvalidWheelFilename('Wheel has unexpected file name: expected {!r}, got {!r}'.format(dist_verstr, w.version))\n    metadata_version_value = dist.metadata_version\n    if metadata_version_value is None:\n        raise UnsupportedWheel('Missing Metadata-Version')\n    try:\n        metadata_version = Version(metadata_version_value)\n    except InvalidVersion:\n        msg = f'Invalid Metadata-Version: {metadata_version_value}'\n        raise UnsupportedWheel(msg)\n    if metadata_version >= Version('1.2') and (not isinstance(dist.version, Version)):\n        raise UnsupportedWheel('Metadata 1.2 mandates PEP 440 version, but {!r} is not'.format(dist_verstr))",
    "label": true
  },
  {
    "code": "def _default_start_debug_action(instring: str, loc: int, expr: 'ParserElement', cache_hit: bool=False):\n    cache_hit_str = '*' if cache_hit else ''\n    print(f\"{cache_hit_str}Match {expr} at loc {loc}({lineno(loc, instring)},{col(loc, instring)})\\n  {line(loc, instring)}\\n  {' ' * (col(loc, instring) - 1)}^\")",
    "label": true
  },
  {
    "code": "def rlocate(iterable, pred=bool, window_size=None):\n    if window_size is None:\n        try:\n            len_iter = len(iterable)\n            return (len_iter - i - 1 for i in locate(reversed(iterable), pred))\n        except TypeError:\n            pass\n    return reversed(list(locate(iterable, pred, window_size)))",
    "label": true
  },
  {
    "code": "def parse_basic_str_escape(src: str, pos: Pos, *, multiline: bool=False) -> Tuple[Pos, str]:\n    escape_id = src[pos:pos + 2]\n    pos += 2\n    if multiline and escape_id in {'\\\\ ', '\\\\\\t', '\\\\\\n'}:\n        if escape_id != '\\\\\\n':\n            pos = skip_chars(src, pos, TOML_WS)\n            try:\n                char = src[pos]\n            except IndexError:\n                return (pos, '')\n            if char != '\\n':\n                raise suffixed_err(src, pos, 'Unescaped \"\\\\\" in a string')\n            pos += 1\n        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)\n        return (pos, '')\n    if escape_id == '\\\\u':\n        return parse_hex_char(src, pos, 4)\n    if escape_id == '\\\\U':\n        return parse_hex_char(src, pos, 8)\n    try:\n        return (pos, BASIC_STR_ESCAPE_REPLACEMENTS[escape_id])\n    except KeyError:\n        if len(escape_id) != 2:\n            raise suffixed_err(src, pos, 'Unterminated string')\n        raise suffixed_err(src, pos, 'Unescaped \"\\\\\" in a string')",
    "label": true
  },
  {
    "code": "def _create_namedtuple(name, fieldnames, modulename, defaults=None):\n    class_ = _import_module(modulename + '.' + name, safe=True)\n    if class_ is not None:\n        return class_\n    import collections\n    t = collections.namedtuple(name, fieldnames, defaults=defaults, module=modulename)\n    return t",
    "label": true
  },
  {
    "code": "def op_to_dunder_unary(op: str) -> str:\n    if op == '-':\n        return '__neg__'\n    elif op == '+':\n        return '__pos__'\n    elif op == '~':\n        return '__invert__'\n    else:\n        return op",
    "label": true
  },
  {
    "code": "def write_json(obj, path, **kwargs):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(obj, f, **kwargs)",
    "label": true
  },
  {
    "code": "def get_project_data(name):\n    url = '%s/%s/project.json' % (name[0].upper(), name)\n    url = urljoin(_external_data_base_url, url)\n    result = _get_external_data(url)\n    return result",
    "label": true
  },
  {
    "code": "def infer_hasattr(node, context: InferenceContext | None=None):\n    try:\n        obj, attr = _infer_getattr_args(node, context)\n        if isinstance(obj, util.UninferableBase) or isinstance(attr, util.UninferableBase) or (not hasattr(obj, 'getattr')):\n            return util.Uninferable\n        obj.getattr(attr, context=context)\n    except UseInferenceDefault:\n        return util.Uninferable\n    except AttributeInferenceError:\n        return nodes.Const(False)\n    return nodes.Const(True)",
    "label": true
  },
  {
    "code": "def _is_literal(o):\n    if not isinstance(o, string_types) or not o:\n        return False\n    return o[0] in '\\'\"'",
    "label": true
  },
  {
    "code": "def check_hyphen_ok(label: str) -> bool:\n    if label[2:4] == '--':\n        raise IDNAError('Label has disallowed hyphens in 3rd and 4th position')\n    if label[0] == '-' or label[-1] == '-':\n        raise IDNAError('Label must not start or end with a hyphen')\n    return True",
    "label": true
  },
  {
    "code": "def display(segments: Iterable[Segment], text: str) -> None:\n    html = _render_segments(segments)\n    jupyter_renderable = JupyterRenderable(html, text)\n    try:\n        from IPython.display import display as ipython_display\n        ipython_display(jupyter_renderable)\n    except ModuleNotFoundError:\n        pass",
    "label": true
  },
  {
    "code": "def use_diff(on=True):\n    global _use_diff, diff\n    _use_diff = on\n    if _use_diff and diff is None:\n        try:\n            from . import diff as d\n        except ImportError:\n            import diff as d\n        diff = d",
    "label": true
  },
  {
    "code": "def _always_object(classes):\n    if object not in classes:\n        return classes + (object,)\n    return classes",
    "label": true
  },
  {
    "code": "def get_win_folder_from_env_vars(csidl_name: str) -> str:\n    result = get_win_folder_if_csidl_name_not_env_var(csidl_name)\n    if result is not None:\n        return result\n    env_var_name = {'CSIDL_APPDATA': 'APPDATA', 'CSIDL_COMMON_APPDATA': 'ALLUSERSPROFILE', 'CSIDL_LOCAL_APPDATA': 'LOCALAPPDATA'}.get(csidl_name)\n    if env_var_name is None:\n        msg = f'Unknown CSIDL name: {csidl_name}'\n        raise ValueError(msg)\n    result = os.environ.get(env_var_name)\n    if result is None:\n        msg = f'Unset environment variable: {env_var_name}'\n        raise ValueError(msg)\n    return result",
    "label": true
  },
  {
    "code": "def run_script(dist_spec, script_name):\n    ns = sys._getframe(1).f_globals\n    name = ns['__name__']\n    ns.clear()\n    ns['__name__'] = name\n    require(dist_spec)[0].run_script(script_name, ns)",
    "label": true
  },
  {
    "code": "def is_within_directory(directory: str, target: str) -> bool:\n    abs_directory = os.path.abspath(directory)\n    abs_target = os.path.abspath(target)\n    prefix = os.path.commonprefix([abs_directory, abs_target])\n    return prefix == abs_directory",
    "label": true
  },
  {
    "code": "def _get_binop_contexts(context, left, right):\n    for arg in (right, left):\n        new_context = context.clone()\n        new_context.callcontext = CallContext(args=[arg])\n        new_context.boundnode = None\n        yield new_context",
    "label": true
  },
  {
    "code": "def _split_explanation(explanation: str) -> List[str]:\n    raw_lines = (explanation or '').split('\\n')\n    lines = [raw_lines[0]]\n    for values in raw_lines[1:]:\n        if values and values[0] in ['{', '}', '~', '>']:\n            lines.append(values)\n        else:\n            lines[-1] += '\\\\n' + values\n    return lines",
    "label": true
  },
  {
    "code": "def _check_dist_requires_python(dist: BaseDistribution, version_info: Tuple[int, int, int], ignore_requires_python: bool=False) -> None:\n    try:\n        requires_python = str(dist.requires_python)\n    except FileNotFoundError as e:\n        raise NoneMetadataError(dist, str(e))\n    try:\n        is_compatible = check_requires_python(requires_python, version_info=version_info)\n    except specifiers.InvalidSpecifier as exc:\n        logger.warning('Package %r has an invalid Requires-Python: %s', dist.raw_name, exc)\n        return\n    if is_compatible:\n        return\n    version = '.'.join(map(str, version_info))\n    if ignore_requires_python:\n        logger.debug('Ignoring failed Requires-Python check for package %r: %s not in %r', dist.raw_name, version, requires_python)\n        return\n    raise UnsupportedPythonVersion('Package {!r} requires a different Python: {} not in {!r}'.format(dist.raw_name, version, requires_python))",
    "label": true
  },
  {
    "code": "def validate_https___packaging_python_org_en_latest_specifications_declaring_project_metadata___definitions_author(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, dict):\n        raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must be object', value=data, name='' + (name_prefix or 'data') + '', definition={'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if 'name' in data_keys:\n            data_keys.remove('name')\n            data__name = data['name']\n            if not isinstance(data__name, str):\n                raise JsonSchemaValueException('' + (name_prefix or 'data') + '.name must be string', value=data__name, name='' + (name_prefix or 'data') + '.name', definition={'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, rule='type')\n        if 'email' in data_keys:\n            data_keys.remove('email')\n            data__email = data['email']\n            if not isinstance(data__email, str):\n                raise JsonSchemaValueException('' + (name_prefix or 'data') + '.email must be string', value=data__email, name='' + (name_prefix or 'data') + '.email', definition={'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}, rule='type')\n            if isinstance(data__email, str):\n                if not REGEX_PATTERNS['idn-email_re_pattern'].match(data__email):\n                    raise JsonSchemaValueException('' + (name_prefix or 'data') + '.email must be idn-email', value=data__email, name='' + (name_prefix or 'data') + '.email', definition={'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}, rule='format')\n        if data_keys:\n            raise JsonSchemaValueException('' + (name_prefix or 'data') + ' must not contain ' + str(data_keys) + ' properties', value=data, name='' + (name_prefix or 'data') + '', definition={'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, rule='additionalProperties')\n    return data",
    "label": true
  },
  {
    "code": "def _iter_built_with_inserted(installed: Candidate, infos: Iterator[IndexCandidateInfo]) -> Iterator[Candidate]:\n    versions_found: Set[_BaseVersion] = set()\n    for version, func in infos:\n        if version in versions_found:\n            continue\n        if installed.version >= version:\n            yield installed\n            versions_found.add(installed.version)\n        candidate = func()\n        if candidate is None:\n            continue\n        yield candidate\n        versions_found.add(version)\n    if installed.version not in versions_found:\n        yield installed",
    "label": true
  },
  {
    "code": "def _get_user_dirs_folder(key: str) -> str | None:\n    user_dirs_config_path = Path(Unix().user_config_dir) / 'user-dirs.dirs'\n    if user_dirs_config_path.exists():\n        parser = ConfigParser()\n        with user_dirs_config_path.open() as stream:\n            parser.read_string(f'[top]\\n{stream.read()}')\n        if key not in parser['top']:\n            return None\n        path = parser['top'][key].strip('\"')\n        return path.replace('$HOME', os.path.expanduser('~'))\n    return None",
    "label": true
  },
  {
    "code": "def _dnsname_to_stdlib(name: str) -> str | None:\n\n    def idna_encode(name: str) -> bytes | None:\n        \"\"\"\n        Borrowed wholesale from the Python Cryptography Project. It turns out\n        that we can't just safely call `idna.encode`: it can explode for\n        wildcard names. This avoids that problem.\n        \"\"\"\n        import idna\n        try:\n            for prefix in ['*.', '.']:\n                if name.startswith(prefix):\n                    name = name[len(prefix):]\n                    return prefix.encode('ascii') + idna.encode(name)\n            return idna.encode(name)\n        except idna.core.IDNAError:\n            return None\n    if ':' in name:\n        return name\n    encoded_name = idna_encode(name)\n    if encoded_name is None:\n        return None\n    return encoded_name.decode('utf-8')",
    "label": true
  },
  {
    "code": "def _pairwise(iterable):\n    a, b = tee(iterable)\n    next(b, None)\n    yield from zip(a, b)",
    "label": true
  },
  {
    "code": "def inspect_bridge(bridges: list[list], bridge_id: int, inspect_date: str, inspect_bci: float) -> None:\n    bridge = find_bridge_by_id(bridges, bridge_id)\n    bridge[COLUMN_LAST_INSPECTED] = inspect_date\n    bridge[COLUMN_BCI][INDEX_BCI_YEARS].insert(0, inspect_date[-4:])\n    bridge[COLUMN_BCI][INDEX_BCI_SCORES].insert(0, inspect_bci)",
    "label": true
  },
  {
    "code": "def _macos_user_config_dir(appname: str, roaming: bool=True) -> str:\n    path = _appdirs.user_data_dir(appname, appauthor=False, roaming=roaming)\n    if os.path.isdir(path):\n        return path\n    linux_like_path = '~/.config/'\n    if appname:\n        linux_like_path = os.path.join(linux_like_path, appname)\n    return os.path.expanduser(linux_like_path)",
    "label": true
  },
  {
    "code": "def run_pyta(filename: str, config_file: str) -> None:\n    import json\n    error_message = '\\nCould not run PythonTA correctly.\\nPlease make sure you have run the setup.py provided on Quercus: that should install PythonTA for you.\\nPlease attend office hours if you require assistance in running PythonTA.'\n    try:\n        import python_ta\n        with open(config_file) as cf:\n            config_dict = json.loads(cf.read())\n            config_dict['output-format'] = 'python_ta.reporters.PlainReporter'\n        python_ta.check_all(filename, config=config_dict)\n    except:\n        print(error_message)",
    "label": true
  },
  {
    "code": "def _get_maxsize_for_saferepr(config: Optional[Config]) -> Optional[int]:\n    verbosity = config.getoption('verbose') if config is not None else 0\n    if verbosity >= 2:\n        return None\n    if verbosity >= 1:\n        return DEFAULT_REPR_MAX_SIZE * 10\n    return DEFAULT_REPR_MAX_SIZE",
    "label": true
  },
  {
    "code": "def split_sections(s):\n    section = None\n    content = []\n    for line in yield_lines(s):\n        if line.startswith('['):\n            if line.endswith(']'):\n                if section or content:\n                    yield (section, content)\n                section = line[1:-1].strip()\n                content = []\n            else:\n                raise ValueError('Invalid section heading', line)\n        else:\n            content.append(line)\n    yield (section, content)",
    "label": true
  },
  {
    "code": "def uts46_remap(domain: str, std3_rules: bool=True, transitional: bool=False) -> str:\n    from .uts46data import uts46data\n    output = ''\n    for pos, char in enumerate(domain):\n        code_point = ord(char)\n        try:\n            uts46row = uts46data[code_point if code_point < 256 else bisect.bisect_left(uts46data, (code_point, 'Z')) - 1]\n            status = uts46row[1]\n            replacement = None\n            if len(uts46row) == 3:\n                replacement = uts46row[2]\n            if status == 'V' or (status == 'D' and (not transitional)) or (status == '3' and (not std3_rules) and (replacement is None)):\n                output += char\n            elif replacement is not None and (status == 'M' or (status == '3' and (not std3_rules)) or (status == 'D' and transitional)):\n                output += replacement\n            elif status != 'I':\n                raise IndexError()\n        except IndexError:\n            raise InvalidCodepoint('Codepoint {} not allowed at position {} in {}'.format(_unot(code_point), pos + 1, repr(domain)))\n    return unicodedata.normalize('NFC', output)",
    "label": true
  },
  {
    "code": "def object_build_function(node: nodes.Module | nodes.ClassDef, member: _FunctionTypes, localname: str) -> None:\n    args, posonlyargs, defaults, kwonlyargs, kwonly_defaults = _get_args_info_from_callable(member)\n    func = build_function(getattr(member, '__name__', None) or localname, args, posonlyargs, defaults, member.__doc__, kwonlyargs=kwonlyargs, kwonlydefaults=kwonly_defaults)\n    node.add_local_node(func, localname)",
    "label": true
  },
  {
    "code": "def method_cache(method, cache_wrapper=None):\n    cache_wrapper = cache_wrapper or functools.lru_cache()\n\n    def wrapper(self, *args, **kwargs):\n        bound_method = types.MethodType(method, self)\n        cached_method = cache_wrapper(bound_method)\n        setattr(self, method.__name__, cached_method)\n        return cached_method(*args, **kwargs)\n    wrapper.cache_clear = lambda: None\n    return wrapper",
    "label": true
  },
  {
    "code": "def _parse_extras(tokenizer: Tokenizer) -> List[str]:\n    if not tokenizer.check('LEFT_BRACKET', peek=True):\n        return []\n    with tokenizer.enclosing_tokens('LEFT_BRACKET', 'RIGHT_BRACKET', around='extras'):\n        tokenizer.consume('WS')\n        extras = _parse_extras_list(tokenizer)\n        tokenizer.consume('WS')\n    return extras",
    "label": true
  },
  {
    "code": "def _resolve_assignment_parts(parts, assign_path, context):\n    assign_path = assign_path[:]\n    index = assign_path.pop(0)\n    for part in parts:\n        assigned = None\n        if isinstance(part, nodes.Dict):\n            try:\n                assigned, _ = part.items[index]\n            except IndexError:\n                return\n        elif hasattr(part, 'getitem'):\n            index_node = nodes.Const(index)\n            try:\n                assigned = part.getitem(index_node, context)\n            except (AstroidTypeError, AstroidIndexError):\n                return\n        if not assigned:\n            return\n        if not assign_path:\n            yield assigned\n        elif isinstance(assigned, util.UninferableBase):\n            return\n        else:\n            try:\n                yield from _resolve_assignment_parts(assigned.infer(context), assign_path, context)\n            except InferenceError:\n                return",
    "label": true
  },
  {
    "code": "def _object_type_is_subclass(obj_type, class_or_seq, context: InferenceContext | None=None):\n    if not isinstance(class_or_seq, (tuple, list)):\n        class_seq = (class_or_seq,)\n    else:\n        class_seq = class_or_seq\n    if isinstance(obj_type, util.UninferableBase):\n        return util.Uninferable\n    class_seq = [item if not isinstance(item, bases.Instance) else util.Uninferable for item in class_seq]\n    for klass in class_seq:\n        if isinstance(klass, util.UninferableBase):\n            raise AstroidTypeError('arg 2 must be a type or tuple of types')\n        for obj_subclass in obj_type.mro():\n            if obj_subclass == klass:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def _get_ttype_class(ttype):\n    fname = STANDARD_TYPES.get(ttype)\n    if fname:\n        return fname\n    aname = ''\n    while fname is None:\n        aname = '-' + ttype[-1] + aname\n        ttype = ttype.parent\n        fname = STANDARD_TYPES.get(ttype)\n    return fname + aname",
    "label": true
  },
  {
    "code": "def prepare_select_or_reject(context: 'Context', args: t.Tuple, kwargs: t.Dict[str, t.Any], modfunc: t.Callable[[t.Any], t.Any], lookup_attr: bool) -> t.Callable[[t.Any], t.Any]:\n    if lookup_attr:\n        try:\n            attr = args[0]\n        except LookupError:\n            raise FilterArgumentError('Missing parameter for attribute name') from None\n        transfunc = make_attrgetter(context.environment, attr)\n        off = 1\n    else:\n        off = 0\n\n        def transfunc(x: V) -> V:\n            return x\n    try:\n        name = args[off]\n        args = args[1 + off:]\n\n        def func(item: t.Any) -> t.Any:\n            return context.environment.call_test(name, item, args, kwargs)\n    except LookupError:\n        func = bool\n    return lambda item: modfunc(func(transfunc(item)))",
    "label": true
  },
  {
    "code": "def get_fun_with_strftime2():\n    import datetime\n    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
    "label": true
  },
  {
    "code": "def numpy_core_umath_transform():\n    ufunc_optional_keyword_arguments = \"out=None, where=True, casting='same_kind', order='K', dtype=None, subok=True\"\n    return parse('\\n    class FakeUfunc:\\n        def __init__(self):\\n            self.__doc__ = str()\\n            self.__name__ = str()\\n            self.nin = 0\\n            self.nout = 0\\n            self.nargs = 0\\n            self.ntypes = 0\\n            self.types = None\\n            self.identity = None\\n            self.signature = None\\n\\n        @classmethod\\n        def reduce(cls, a, axis=None, dtype=None, out=None):\\n            return numpy.ndarray([0, 0])\\n\\n        @classmethod\\n        def accumulate(cls, array, axis=None, dtype=None, out=None):\\n            return numpy.ndarray([0, 0])\\n\\n        @classmethod\\n        def reduceat(cls, a, indices, axis=None, dtype=None, out=None):\\n            return numpy.ndarray([0, 0])\\n\\n        @classmethod\\n        def outer(cls, A, B, **kwargs):\\n            return numpy.ndarray([0, 0])\\n\\n        @classmethod\\n        def at(cls, a, indices, b=None):\\n            return numpy.ndarray([0, 0])\\n\\n    class FakeUfuncOneArg(FakeUfunc):\\n        def __call__(self, x, {opt_args:s}):\\n            return numpy.ndarray([0, 0])\\n\\n    class FakeUfuncOneArgBis(FakeUfunc):\\n        def __call__(self, x, {opt_args:s}):\\n            return numpy.ndarray([0, 0]), numpy.ndarray([0, 0])\\n\\n    class FakeUfuncTwoArgs(FakeUfunc):\\n        def __call__(self, x1, x2, {opt_args:s}):\\n            return numpy.ndarray([0, 0])\\n\\n    # Constants\\n    e = 2.718281828459045\\n    euler_gamma = 0.5772156649015329\\n\\n    # One arg functions with optional kwargs\\n    arccos = FakeUfuncOneArg()\\n    arccosh = FakeUfuncOneArg()\\n    arcsin = FakeUfuncOneArg()\\n    arcsinh = FakeUfuncOneArg()\\n    arctan = FakeUfuncOneArg()\\n    arctanh = FakeUfuncOneArg()\\n    cbrt = FakeUfuncOneArg()\\n    conj = FakeUfuncOneArg()\\n    conjugate = FakeUfuncOneArg()\\n    cosh = FakeUfuncOneArg()\\n    deg2rad = FakeUfuncOneArg()\\n    degrees = FakeUfuncOneArg()\\n    exp2 = FakeUfuncOneArg()\\n    expm1 = FakeUfuncOneArg()\\n    fabs = FakeUfuncOneArg()\\n    frexp = FakeUfuncOneArgBis()\\n    isfinite = FakeUfuncOneArg()\\n    isinf = FakeUfuncOneArg()\\n    log = FakeUfuncOneArg()\\n    log1p = FakeUfuncOneArg()\\n    log2 = FakeUfuncOneArg()\\n    logical_not = FakeUfuncOneArg()\\n    modf = FakeUfuncOneArgBis()\\n    negative = FakeUfuncOneArg()\\n    positive = FakeUfuncOneArg()\\n    rad2deg = FakeUfuncOneArg()\\n    radians = FakeUfuncOneArg()\\n    reciprocal = FakeUfuncOneArg()\\n    rint = FakeUfuncOneArg()\\n    sign = FakeUfuncOneArg()\\n    signbit = FakeUfuncOneArg()\\n    sinh = FakeUfuncOneArg()\\n    spacing = FakeUfuncOneArg()\\n    square = FakeUfuncOneArg()\\n    tan = FakeUfuncOneArg()\\n    tanh = FakeUfuncOneArg()\\n    trunc = FakeUfuncOneArg()\\n\\n    # Two args functions with optional kwargs\\n    add = FakeUfuncTwoArgs()\\n    bitwise_and = FakeUfuncTwoArgs()\\n    bitwise_or = FakeUfuncTwoArgs()\\n    bitwise_xor = FakeUfuncTwoArgs()\\n    copysign = FakeUfuncTwoArgs()\\n    divide = FakeUfuncTwoArgs()\\n    divmod = FakeUfuncTwoArgs()\\n    equal = FakeUfuncTwoArgs()\\n    float_power = FakeUfuncTwoArgs()\\n    floor_divide = FakeUfuncTwoArgs()\\n    fmax = FakeUfuncTwoArgs()\\n    fmin = FakeUfuncTwoArgs()\\n    fmod = FakeUfuncTwoArgs()\\n    greater = FakeUfuncTwoArgs()\\n    gcd = FakeUfuncTwoArgs()\\n    hypot = FakeUfuncTwoArgs()\\n    heaviside = FakeUfuncTwoArgs()\\n    lcm = FakeUfuncTwoArgs()\\n    ldexp = FakeUfuncTwoArgs()\\n    left_shift = FakeUfuncTwoArgs()\\n    less = FakeUfuncTwoArgs()\\n    logaddexp = FakeUfuncTwoArgs()\\n    logaddexp2 = FakeUfuncTwoArgs()\\n    logical_and = FakeUfuncTwoArgs()\\n    logical_or = FakeUfuncTwoArgs()\\n    logical_xor = FakeUfuncTwoArgs()\\n    maximum = FakeUfuncTwoArgs()\\n    minimum = FakeUfuncTwoArgs()\\n    multiply = FakeUfuncTwoArgs()\\n    nextafter = FakeUfuncTwoArgs()\\n    not_equal = FakeUfuncTwoArgs()\\n    power = FakeUfuncTwoArgs()\\n    remainder = FakeUfuncTwoArgs()\\n    right_shift = FakeUfuncTwoArgs()\\n    subtract = FakeUfuncTwoArgs()\\n    true_divide = FakeUfuncTwoArgs()\\n    '.format(opt_args=ufunc_optional_keyword_arguments))",
    "label": true
  },
  {
    "code": "def check_specifier(dist, attr, value):\n    try:\n        SpecifierSet(value)\n    except (InvalidSpecifier, AttributeError) as error:\n        tmpl = '{attr!r} must be a string containing valid version specifiers; {error}'\n        raise DistutilsSetupError(tmpl.format(attr=attr, error=error)) from error",
    "label": true
  },
  {
    "code": "def generic_tags(interpreter: Optional[str]=None, abis: Optional[Iterable[str]]=None, platforms: Optional[Iterable[str]]=None, *, warn: bool=False) -> Iterator[Tag]:\n    if not interpreter:\n        interp_name = interpreter_name()\n        interp_version = interpreter_version(warn=warn)\n        interpreter = ''.join([interp_name, interp_version])\n    if abis is None:\n        abis = _generic_abi()\n    else:\n        abis = list(abis)\n    platforms = list(platforms or platform_tags())\n    if 'none' not in abis:\n        abis.append('none')\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)",
    "label": true
  },
  {
    "code": "def hash_lineset(lineset: LineSet, min_common_lines: int=DEFAULT_MIN_SIMILARITY_LINE) -> tuple[HashToIndex_T, IndexToLines_T]:\n    hash2index = defaultdict(list)\n    index2lines = {}\n    lines = tuple((x.text for x in lineset.stripped_lines))\n    shifted_lines = [iter(lines[i:]) for i in range(min_common_lines)]\n    for i, *succ_lines in enumerate(zip(*shifted_lines)):\n        start_linenumber = LineNumber(lineset.stripped_lines[i].line_number)\n        try:\n            end_linenumber = lineset.stripped_lines[i + min_common_lines].line_number\n        except IndexError:\n            end_linenumber = LineNumber(lineset.stripped_lines[-1].line_number + 1)\n        index = Index(i)\n        index2lines[index] = SuccessiveLinesLimits(start=start_linenumber, end=end_linenumber)\n        l_c = LinesChunk(lineset.name, index, *succ_lines)\n        hash2index[l_c].append(index)\n    return (hash2index, index2lines)",
    "label": true
  },
  {
    "code": "def only(iterable, default=None, too_long=None):\n    it = iter(iterable)\n    first_value = next(it, default)\n    try:\n        second_value = next(it)\n    except StopIteration:\n        pass\n    else:\n        msg = 'Expected exactly one item in iterable, but got {!r}, {!r}, and perhaps more.'.format(first_value, second_value)\n        raise too_long or ValueError(msg)\n    return first_value",
    "label": true
  },
  {
    "code": "def format_alias(name, aliases):\n    source, command = aliases[name]\n    if source == config_file('global'):\n        source = '--global-config '\n    elif source == config_file('user'):\n        source = '--user-config '\n    elif source == config_file('local'):\n        source = ''\n    else:\n        source = '--filename=%r' % source\n    return source + name + ' ' + command",
    "label": true
  },
  {
    "code": "def measure_renderables(console: 'Console', options: 'ConsoleOptions', renderables: Sequence['RenderableType']) -> 'Measurement':\n    if not renderables:\n        return Measurement(0, 0)\n    get_measurement = Measurement.get\n    measurements = [get_measurement(console, options, renderable) for renderable in renderables]\n    measured_width = Measurement(max(measurements, key=itemgetter(0)).minimum, max(measurements, key=itemgetter(1)).maximum)\n    return measured_width",
    "label": true
  },
  {
    "code": "def _call_aside(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f",
    "label": true
  },
  {
    "code": "def _forloop(lst: list) -> None:\n    bits = ''\n    for byte in lst:\n        bits += byte_to_bits(byte)",
    "label": true
  },
  {
    "code": "def counted_array(expr: ParserElement, int_expr: typing.Optional[ParserElement]=None, *, intExpr: typing.Optional[ParserElement]=None) -> ParserElement:\n    intExpr = intExpr or int_expr\n    array_expr = Forward()\n\n    def count_field_parse_action(s, l, t):\n        nonlocal array_expr\n        n = t[0]\n        array_expr <<= expr * n if n else Empty()\n        del t[:]\n    if intExpr is None:\n        intExpr = Word(nums).set_parse_action(lambda t: int(t[0]))\n    else:\n        intExpr = intExpr.copy()\n    intExpr.set_name('arrayLen')\n    intExpr.add_parse_action(count_field_parse_action, call_during_try=True)\n    return (intExpr + array_expr).set_name('(len) ' + str(expr) + '...')",
    "label": true
  },
  {
    "code": "def partition(pred, iterable):\n    if pred is None:\n        pred = bool\n    evaluations = ((pred(x), x) for x in iterable)\n    t1, t2 = tee(evaluations)\n    return ((x for cond, x in t1 if not cond), (x for cond, x in t2 if cond))",
    "label": true
  },
  {
    "code": "def TWO_PLOT(x, y1, y2, xerr, y1err, y2err, title, xlabel, y1label, y2label):\n    plt.figure()\n    plt.errorbar(x, y1, yerr=y1err, xerr=xerr, fmt='.', capsize=2, label=y1label)\n    plt.errorbar(x, y2, yerr=y2err, xerr=xerr, fmt='.', capsize=2, label=y2label)\n    plt.xlabel(xlabel)\n    plt.ylabel(y1label)\n    plt.ylabel(y2label)\n    plt.title(title)\n    plt.legend()\n    plt.show()",
    "label": true
  },
  {
    "code": "def _parse_filters(f_strs):\n    filters = []\n    if not f_strs:\n        return filters\n    for f_str in f_strs:\n        if ':' in f_str:\n            fname, fopts = f_str.split(':', 1)\n            filters.append((fname, _parse_options([fopts])))\n        else:\n            filters.append((f_str, {}))\n    return filters",
    "label": true
  },
  {
    "code": "def _guess_content_type(file: str) -> Optional[str]:\n    _, ext = os.path.splitext(file.lower())\n    if not ext:\n        return None\n    if ext in _CONTENT_TYPES:\n        return _CONTENT_TYPES[ext]\n    valid = ', '.join((f'{k} ({v})' for k, v in _CONTENT_TYPES.items()))\n    msg = f'only the following file extensions are recognized: {valid}.'\n    raise ValueError(f'Undefined content type for {file}, {msg}')",
    "label": true
  },
  {
    "code": "def zip_equal(*iterables):\n    if hexversion >= 50987174:\n        warnings.warn('zip_equal will be removed in a future version of more-itertools. Use the builtin zip function with strict=True instead.', DeprecationWarning)\n    return _zip_equal(*iterables)",
    "label": true
  },
  {
    "code": "def copy(obj, *args, **kwds):\n    ignore = kwds.pop('ignore', Unpickler.settings['ignore'])\n    return loads(dumps(obj, *args, **kwds), ignore=ignore)",
    "label": true
  },
  {
    "code": "def compatible_platforms(provided, required):\n    if provided is None or required is None or provided == required:\n        return True\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n        if not provMac:\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = '%s.%s' % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= '10.3' or (dversion == 8 and macosversion >= '10.4'):\n                    return True\n            return False\n        if provMac.group(1) != reqMac.group(1) or provMac.group(3) != reqMac.group(3):\n            return False\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def _get_url_from_path(path: str, name: str) -> Optional[str]:\n    if _looks_like_path(name) and os.path.isdir(path):\n        if is_installable_dir(path):\n            return path_to_url(path)\n        raise InstallationError(f\"Directory {name!r} is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\")\n    if not is_archive_file(path):\n        return None\n    if os.path.isfile(path):\n        return path_to_url(path)\n    urlreq_parts = name.split('@', 1)\n    if len(urlreq_parts) >= 2 and (not _looks_like_path(urlreq_parts[0])):\n        return None\n    logger.warning('Requirement %r looks like a filename, but the file does not exist', name)\n    return path_to_url(path)",
    "label": true
  },
  {
    "code": "def bytes_to_nodes(buf: bytes) -> list[ReadNode]:\n    lst = []\n    for i in range(0, len(buf), 4):\n        l_type = buf[i]\n        l_data = buf[i + 1]\n        r_type = buf[i + 2]\n        r_data = buf[i + 3]\n        lst.append(ReadNode(l_type, l_data, r_type, r_data))\n    return lst",
    "label": true
  },
  {
    "code": "def pytest_cmdline_main(config: Config) -> Optional[Union[int, ExitCode]]:\n    if config.option.cacheshow and (not config.option.help):\n        from _pytest.main import wrap_session\n        return wrap_session(config, cacheshow)\n    return None",
    "label": true
  },
  {
    "code": "def _get_custom_interpreter(implementation: Optional[str]=None, version: Optional[str]=None) -> str:\n    if implementation is None:\n        implementation = interpreter_name()\n    if version is None:\n        version = interpreter_version()\n    return f'{implementation}{version}'",
    "label": true
  },
  {
    "code": "def should_strip_ansi(stream: t.Optional[t.IO[t.Any]]=None, color: t.Optional[bool]=None) -> bool:\n    if color is None:\n        if stream is None:\n            stream = sys.stdin\n        return not isatty(stream) and (not _is_jupyter_kernel_output(stream))\n    return not color",
    "label": true
  },
  {
    "code": "def create_main_parser() -> ConfigOptionParser:\n    parser = ConfigOptionParser(usage='\\n%prog <command> [options]', add_help_option=False, formatter=UpdatingDefaultsHelpFormatter(), name='global', prog=get_prog())\n    parser.disable_interspersed_args()\n    parser.version = get_pip_version()\n    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)\n    parser.add_option_group(gen_opts)\n    parser.main = True\n    description = [''] + [f'{name:27} {command_info.summary}' for name, command_info in commands_dict.items()]\n    parser.description = '\\n'.join(description)\n    return parser",
    "label": true
  },
  {
    "code": "def join_lines(lines_enum: ReqFileLines) -> ReqFileLines:\n    primary_line_number = None\n    new_line: List[str] = []\n    for line_number, line in lines_enum:\n        if not line.endswith('\\\\') or COMMENT_RE.match(line):\n            if COMMENT_RE.match(line):\n                line = ' ' + line\n            if new_line:\n                new_line.append(line)\n                assert primary_line_number is not None\n                yield (primary_line_number, ''.join(new_line))\n                new_line = []\n            else:\n                yield (line_number, line)\n        else:\n            if not new_line:\n                primary_line_number = line_number\n            new_line.append(line.strip('\\\\'))\n    if new_line:\n        assert primary_line_number is not None\n        yield (primary_line_number, ''.join(new_line))",
    "label": true
  },
  {
    "code": "def _parse_marker_op(tokenizer: Tokenizer) -> Op:\n    if tokenizer.check('IN'):\n        tokenizer.read()\n        return Op('in')\n    elif tokenizer.check('NOT'):\n        tokenizer.read()\n        tokenizer.expect('WS', expected=\"whitespace after 'not'\")\n        tokenizer.expect('IN', expected=\"'in' after 'not'\")\n        return Op('not in')\n    elif tokenizer.check('OP'):\n        return Op(tokenizer.read().text)\n    else:\n        return tokenizer.raise_syntax_error('Expected marker operator, one of <=, <, !=, ==, >=, >, ~=, ===, in, not in')",
    "label": true
  },
  {
    "code": "def outdent(code, spaces=None, all=True):\n    indent = indentsize(code)\n    if spaces is None or spaces > indent or spaces < 0:\n        spaces = indent\n    if not all:\n        return code[spaces:]\n    return '\\n'.join(_outdent(code.split('\\n'), spaces=spaces, all=all))",
    "label": true
  },
  {
    "code": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    tags = set()\n    interpreters, abis, platforms = tag.split('-')\n    for interpreter in interpreters.split('.'):\n        for abi in abis.split('.'):\n            for platform_ in platforms.split('.'):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)",
    "label": true
  },
  {
    "code": "def create_dict_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:\n    pos += 1\n    pos = skip_chars(src, pos, TOML_WS)\n    pos, key = parse_key(src, pos)\n    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot declare {key} twice')\n    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)\n    try:\n        out.data.get_or_create_nest(key)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if not src.startswith(']', pos):\n        raise suffixed_err(src, pos, \"Expected ']' at the end of a table declaration\")\n    return (pos + 1, key)",
    "label": true
  },
  {
    "code": "def install_import_hook(packages: Iterable[str] | None=None, *, cls: type[TypeguardFinder]=TypeguardFinder) -> ImportHookManager:\n    if packages is None:\n        target_packages: list[str] | None = None\n    elif isinstance(packages, str):\n        target_packages = [packages]\n    else:\n        target_packages = list(packages)\n    for finder in sys.meta_path:\n        if isclass(finder) and finder.__name__ == 'PathFinder' and hasattr(finder, 'find_spec'):\n            break\n    else:\n        raise RuntimeError('Cannot find a PathFinder in sys.meta_path')\n    hook = cls(target_packages, finder)\n    sys.meta_path.insert(0, hook)\n    return ImportHookManager(hook)",
    "label": true
  },
  {
    "code": "def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n    if len(py_version) > 1:\n        yield f'py{_version_nodot(py_version[:2])}'\n    yield f'py{py_version[0]}'\n    if len(py_version) > 1:\n        for minor in range(py_version[1] - 1, -1, -1):\n            yield f'py{_version_nodot((py_version[0], minor))}'",
    "label": true
  },
  {
    "code": "def _build_result(state):\n    mapping = state.mapping\n    all_keys = {id(v): k for k, v in mapping.items()}\n    all_keys[id(None)] = None\n    graph = DirectedGraph()\n    graph.add(None)\n    connected = {None}\n    for key, criterion in state.criteria.items():\n        if not _has_route_to_root(state.criteria, key, all_keys, connected):\n            continue\n        if key not in graph:\n            graph.add(key)\n        for p in criterion.iter_parent():\n            try:\n                pkey = all_keys[id(p)]\n            except KeyError:\n                continue\n            if pkey not in graph:\n                graph.add(pkey)\n            graph.connect(pkey, key)\n    return Result(mapping={k: v for k, v in mapping.items() if k in connected}, graph=graph, criteria=state.criteria)",
    "label": true
  },
  {
    "code": "def choose_boundary():\n    boundary = binascii.hexlify(os.urandom(16))\n    if not six.PY2:\n        boundary = boundary.decode('ascii')\n    return boundary",
    "label": true
  },
  {
    "code": "def save_results(results: LinterStats, base: str | Path, pylint_home: str | Path=PYLINT_HOME) -> None:\n    base = Path(base)\n    pylint_home = Path(pylint_home)\n    try:\n        pylint_home.mkdir(parents=True, exist_ok=True)\n    except OSError:\n        print(f'Unable to create directory {pylint_home}', file=sys.stderr)\n    data_file = _get_pdata_path(base, 1)\n    try:\n        with open(data_file, 'wb') as stream:\n            pickle.dump(results, stream)\n    except OSError as ex:\n        print(f'Unable to create file {data_file}: {ex}', file=sys.stderr)",
    "label": true
  },
  {
    "code": "def timsort(lst: list) -> None:\n    runs = find_runs(lst)\n    while len(runs) > 1:\n        slice2 = runs.pop()\n        slice1 = runs.pop()\n        _merge(lst, slice1[0], slice1[1], slice2[1])\n        runs.append((slice1[0], slice2[1]))",
    "label": true
  },
  {
    "code": "def is_string_sequence(seq):\n    result = True\n    i = None\n    for i, s in enumerate(seq):\n        if not isinstance(s, string_types):\n            result = False\n            break\n    assert i is not None\n    return result",
    "label": true
  },
  {
    "code": "def _get_system_sitepackages() -> Set[str]:\n    if hasattr(site, 'getsitepackages'):\n        system_sites = site.getsitepackages()\n    else:\n        system_sites = [get_purelib(), get_platlib()]\n    return {os.path.normcase(path) for path in system_sites}",
    "label": true
  },
  {
    "code": "def _infer_binop(self: nodes.BinOp, context: InferenceContext | None=None) -> Generator[InferenceResult | util.BadBinaryOperationMessage, None, None]:\n    left = self.left\n    right = self.right\n    context = context or InferenceContext()\n    lhs_context = copy_context(context)\n    rhs_context = copy_context(context)\n    lhs_iter = left.infer(context=lhs_context)\n    rhs_iter = right.infer(context=rhs_context)\n    for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any((isinstance(value, util.UninferableBase) for value in (rhs, lhs))):\n            yield util.Uninferable\n            return\n        try:\n            yield from _infer_binary_operation(lhs, rhs, self, context, _get_binop_flow)\n        except _NonDeducibleTypeHierarchy:\n            yield util.Uninferable",
    "label": true
  },
  {
    "code": "def _unique(fn: Callable[..., Generator[Any, None, None]]) -> Callable[..., Generator[Any, None, None]]:\n\n    @functools.wraps(fn)\n    def unique(*args: Any, **kw: Any) -> Generator[Any, None, None]:\n        seen: Set[Any] = set()\n        for item in fn(*args, **kw):\n            if item not in seen:\n                seen.add(item)\n                yield item\n    return unique",
    "label": true
  },
  {
    "code": "def is_installable_dir(path: str) -> bool:\n    if not os.path.isdir(path):\n        return False\n    if os.path.isfile(os.path.join(path, 'pyproject.toml')):\n        return True\n    if os.path.isfile(os.path.join(path, 'setup.py')):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def unicode_is_ascii(u_string):\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode('ascii')\n        return True\n    except UnicodeEncodeError:\n        return False",
    "label": true
  },
  {
    "code": "def load_results(base: str | Path, pylint_home: str | Path=PYLINT_HOME) -> LinterStats | None:\n    base = Path(base)\n    pylint_home = Path(pylint_home)\n    data_file = _get_pdata_path(base, 1, pylint_home)\n    if not data_file.exists():\n        return None\n    try:\n        with open(data_file, 'rb') as stream:\n            data = pickle.load(stream)\n            if not isinstance(data, LinterStats):\n                warnings.warn(f\"You're using an old pylint cache with invalid data following an upgrade, please delete '{data_file}'.\", UserWarning)\n                raise TypeError\n            return data\n    except Exception:\n        return None",
    "label": true
  },
  {
    "code": "def get_file_type(*args, **kwargs):\n    open = kwargs.pop('open', __builtin__.open)\n    f = open(os.devnull, *args, **kwargs)\n    t = type(f)\n    f.close()\n    return t",
    "label": true
  },
  {
    "code": "def _lookup_style(style):\n    if isinstance(style, str):\n        return get_style_by_name(style)\n    return style",
    "label": true
  },
  {
    "code": "def get_file_url(link: Link, download_dir: Optional[str]=None, hashes: Optional[Hashes]=None) -> File:\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link.file_path\n    if hashes:\n        hashes.check_against_path(from_path)\n    return File(from_path, None)",
    "label": true
  },
  {
    "code": "def make_default_short_help(help: str, max_length: int=45) -> str:\n    paragraph_end = help.find('\\n\\n')\n    if paragraph_end != -1:\n        help = help[:paragraph_end]\n    words = help.split()\n    if not words:\n        return ''\n    if words[0] == '\\x08':\n        words = words[1:]\n    total_length = 0\n    last_index = len(words) - 1\n    for i, word in enumerate(words):\n        total_length += len(word) + (i > 0)\n        if total_length > max_length:\n            break\n        if word[-1] == '.':\n            return ' '.join(words[:i + 1])\n        if total_length == max_length and i != last_index:\n            break\n    else:\n        return ' '.join(words)\n    total_length += len('...')\n    while i > 0:\n        total_length -= len(words[i]) + (i > 0)\n        if total_length <= max_length:\n            break\n        i -= 1\n    return ' '.join(words[:i]) + '...'",
    "label": true
  },
  {
    "code": "def cp_similarity(iana_name_a: str, iana_name_b: str) -> float:\n    if is_multi_byte_encoding(iana_name_a) or is_multi_byte_encoding(iana_name_b):\n        return 0.0\n    decoder_a = importlib.import_module('encodings.{}'.format(iana_name_a)).IncrementalDecoder\n    decoder_b = importlib.import_module('encodings.{}'.format(iana_name_b)).IncrementalDecoder\n    id_a: IncrementalDecoder = decoder_a(errors='ignore')\n    id_b: IncrementalDecoder = decoder_b(errors='ignore')\n    character_match_count: int = 0\n    for i in range(255):\n        to_be_decoded: bytes = bytes([i])\n        if id_a.decode(to_be_decoded) == id_b.decode(to_be_decoded):\n            character_match_count += 1\n    return character_match_count / 254",
    "label": true
  },
  {
    "code": "def _parse_marker(tokenizer: Tokenizer) -> MarkerList:\n    expression = [_parse_marker_atom(tokenizer)]\n    while tokenizer.check('BOOLOP'):\n        token = tokenizer.read()\n        expr_right = _parse_marker_atom(tokenizer)\n        expression.extend((token.text, expr_right))\n    return expression",
    "label": true
  },
  {
    "code": "def get_filetype_from_buffer(buf, max_lines=5):\n    lines = buf.splitlines()\n    for l in lines[-1:-max_lines - 1:-1]:\n        ret = get_filetype_from_line(l)\n        if ret:\n            return ret\n    for i in range(max_lines, -1, -1):\n        if i < len(lines):\n            ret = get_filetype_from_line(lines[i])\n            if ret:\n                return ret\n    return None",
    "label": true
  },
  {
    "code": "def make_install_req_from_editable(link: Link, template: InstallRequirement) -> InstallRequirement:\n    assert template.editable, 'template not editable'\n    ireq = install_req_from_editable(link.url, user_supplied=template.user_supplied, comes_from=template.comes_from, use_pep517=template.use_pep517, isolated=template.isolated, constraint=template.constraint, permit_editable_wheels=template.permit_editable_wheels, global_options=template.global_options, hash_options=template.hash_options, config_settings=template.config_settings)\n    ireq.extras = template.extras\n    return ireq",
    "label": true
  },
  {
    "code": "def prepare_metadata_for_build_editable(metadata_directory, config_settings, _allow_fallback):\n    backend = _build_backend()\n    try:\n        hook = backend.prepare_metadata_for_build_editable\n    except AttributeError:\n        if not _allow_fallback:\n            raise HookMissing()\n        try:\n            build_hook = backend.build_editable\n        except AttributeError:\n            raise HookMissing(hook_name='build_editable')\n        else:\n            whl_basename = build_hook(metadata_directory, config_settings)\n            return _get_wheel_metadata_from_wheel(whl_basename, metadata_directory, config_settings)\n    else:\n        return hook(metadata_directory, config_settings)",
    "label": true
  },
  {
    "code": "def _builtin_lookup(node, name) -> list:\n    values = node.locals.get(name, [])\n    if not values:\n        raise AttributeInferenceError(attribute=name, target=node)\n    return values",
    "label": true
  },
  {
    "code": "def _parse_local_version(local: Optional[str]) -> Optional[LocalType]:\n    if local is not None:\n        return tuple((part.lower() if not part.isdigit() else int(part) for part in _local_version_separators.split(local)))\n    return None",
    "label": true
  },
  {
    "code": "def _c3_merge(sequences, cls, context):\n    result = []\n    while True:\n        sequences = [s for s in sequences if s]\n        if not sequences:\n            return result\n        for s1 in sequences:\n            candidate = s1[0]\n            for s2 in sequences:\n                if candidate in s2[1:]:\n                    candidate = None\n                    break\n            else:\n                break\n        if not candidate:\n            raise InconsistentMroError(message='Cannot create a consistent method resolution order for MROs {mros} of class {cls!r}.', mros=sequences, cls=cls, context=context)\n        result.append(candidate)\n        for seq in sequences:\n            if seq[0] == candidate:\n                del seq[0]\n    return None",
    "label": true
  },
  {
    "code": "def _expand_allowed_platforms(platforms: Optional[List[str]]) -> Optional[List[str]]:\n    if not platforms:\n        return None\n    seen = set()\n    result = []\n    for p in platforms:\n        if p in seen:\n            continue\n        additions = [c for c in _get_custom_platforms(p) if c not in seen]\n        seen.update(additions)\n        result.extend(additions)\n    return result",
    "label": true
  },
  {
    "code": "def _download_classifiers() -> str:\n    import ssl\n    from email.message import Message\n    from urllib.request import urlopen\n    url = 'https://pypi.org/pypi?:action=list_classifiers'\n    context = ssl.create_default_context()\n    with urlopen(url, context=context) as response:\n        headers = Message()\n        headers['content_type'] = response.getheader('content-type', 'text/plain')\n        return response.read().decode(headers.get_param('charset', 'utf-8'))",
    "label": true
  },
  {
    "code": "def get_cookie_header(jar, request):\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get('Cookie')",
    "label": true
  },
  {
    "code": "def map_reduce(iterable, keyfunc, valuefunc=None, reducefunc=None):\n    valuefunc = (lambda x: x) if valuefunc is None else valuefunc\n    ret = defaultdict(list)\n    for item in iterable:\n        key = keyfunc(item)\n        value = valuefunc(item)\n        ret[key].append(value)\n    if reducefunc is not None:\n        for key, value_list in ret.items():\n            ret[key] = reducefunc(value_list)\n    ret.default_factory = None\n    return ret",
    "label": true
  },
  {
    "code": "def resolve_proxies(request, proxies, trust_env=True):\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get('no_proxy')\n    new_proxies = proxies.copy()\n    if trust_env and (not should_bypass_proxies(url, no_proxy=no_proxy)):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n        proxy = environ_proxies.get(scheme, environ_proxies.get('all'))\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies",
    "label": true
  },
  {
    "code": "def _collect_tvars(type: type) -> List[type]:\n    if isinstance(type, TypeVar):\n        return [type]\n    elif isinstance(type, _GenericAlias) and type.__args__:\n        return sum([_collect_tvars(arg) for arg in type.__args__], [])\n    else:\n        return []",
    "label": true
  },
  {
    "code": "def fixture(fixture_function: Optional[FixtureFunction]=None, *, scope: 'Union[_ScopeName, Callable[[str, Config], _ScopeName]]'='function', params: Optional[Iterable[object]]=None, autouse: bool=False, ids: Optional[Union[Sequence[Optional[object]], Callable[[Any], Optional[object]]]]=None, name: Optional[str]=None) -> Union[FixtureFunctionMarker, FixtureFunction]:\n    fixture_marker = FixtureFunctionMarker(scope=scope, params=tuple(params) if params is not None else None, autouse=autouse, ids=None if ids is None else ids if callable(ids) else tuple(ids), name=name, _ispytest=True)\n    if fixture_function:\n        return fixture_marker(fixture_function)\n    return fixture_marker",
    "label": true
  },
  {
    "code": "def get_file_content(url: str, session: PipSession) -> Tuple[str, str]:\n    scheme = get_url_scheme(url)\n    if scheme in ['http', 'https', 'file']:\n        resp = session.get(url)\n        raise_for_status(resp)\n        return (resp.url, resp.text)\n    try:\n        with open(url, 'rb') as f:\n            content = auto_decode(f.read())\n    except OSError as exc:\n        raise InstallationError(f'Could not open requirements file: {exc}')\n    return (url, content)",
    "label": true
  },
  {
    "code": "def supported_hashes(hashes: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:\n    if hashes is None:\n        return None\n    hashes = {n: v for n, v in hashes.items() if n in _SUPPORTED_HASHES}\n    if not hashes:\n        return None\n    return hashes",
    "label": true
  },
  {
    "code": "def _mac_arch(arch: str, is_32bit: bool=_32_BIT_INTERPRETER) -> str:\n    if not is_32bit:\n        return arch\n    if arch.startswith('ppc'):\n        return 'ppc'\n    return 'i386'",
    "label": true
  },
  {
    "code": "def normalize_version_info(py_version_info: Tuple[int, ...]) -> Tuple[int, int, int]:\n    if len(py_version_info) < 3:\n        py_version_info += (3 - len(py_version_info)) * (0,)\n    elif len(py_version_info) > 3:\n        py_version_info = py_version_info[:3]\n    return cast('VersionInfo', py_version_info)",
    "label": true
  },
  {
    "code": "def looks_like_xml(text):\n    if xml_decl_re.match(text):\n        return True\n    key = hash(text)\n    try:\n        return _looks_like_xml_cache[key]\n    except KeyError:\n        m = doctype_lookup_re.search(text)\n        if m is not None:\n            return True\n        rv = tag_re.search(text[:1000]) is not None\n        _looks_like_xml_cache[key] = rv\n        return rv",
    "label": true
  },
  {
    "code": "def match_to_localtime(match: 're.Match') -> time:\n    hour_str, minute_str, sec_str, micros_str = match.groups()\n    micros = int(micros_str.ljust(6, '0')) if micros_str else 0\n    return time(int(hour_str), int(minute_str), int(sec_str), micros)",
    "label": true
  },
  {
    "code": "def _check_nested_type(func: callable, args: list, tp: type):\n    success, result = _check(func, args, list)\n    if not success:\n        return (False, result)\n    msg = _type_error_message(func, 'list of {}s'.format(tp.__name__), result)\n    for item in result:\n        if not isinstance(item, tp):\n            return (False, msg)\n    return (True, result)",
    "label": true
  },
  {
    "code": "def after_log(logger: 'logging.Logger', log_level: int, sec_format: str='%0.3f') -> typing.Callable[['RetryCallState'], None]:\n\n    def log_it(retry_state: 'RetryCallState') -> None:\n        if retry_state.fn is None:\n            fn_name = '<unknown>'\n        else:\n            fn_name = _utils.get_callback_name(retry_state.fn)\n        logger.log(log_level, f\"Finished call to '{fn_name}' after {sec_format % retry_state.seconds_since_start}(s), this was the {_utils.to_ordinal(retry_state.attempt_number)} time calling it.\")\n    return log_it",
    "label": true
  },
  {
    "code": "def _rich_progress_bar(iterable: Iterable[bytes], *, bar_type: str, size: int) -> Generator[bytes, None, None]:\n    assert bar_type == 'on', 'This should only be used in the default mode.'\n    if not size:\n        total = float('inf')\n        columns: Tuple[ProgressColumn, ...] = (TextColumn('[progress.description]{task.description}'), SpinnerColumn('line', speed=1.5), FileSizeColumn(), TransferSpeedColumn(), TimeElapsedColumn())\n    else:\n        total = size\n        columns = (TextColumn('[progress.description]{task.description}'), BarColumn(), DownloadColumn(), TransferSpeedColumn(), TextColumn('eta'), TimeRemainingColumn())\n    progress = Progress(*columns, refresh_per_second=30)\n    task_id = progress.add_task(' ' * (get_indentation() + 2), total=total)\n    with progress:\n        for chunk in iterable:\n            yield chunk\n            progress.update(task_id, advance=len(chunk))",
    "label": true
  },
  {
    "code": "def interpreter_name() -> str:\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name",
    "label": true
  },
  {
    "code": "def install(console: Optional['Console']=None, overflow: 'OverflowMethod'='ignore', crop: bool=False, indent_guides: bool=False, max_length: Optional[int]=None, max_string: Optional[int]=None, max_depth: Optional[int]=None, expand_all: bool=False) -> None:\n    from pip._vendor.rich import get_console\n    console = console or get_console()\n    assert console is not None\n\n    def display_hook(value: Any) -> None:\n        \"\"\"Replacement sys.displayhook which prettifies objects with Rich.\"\"\"\n        if value is not None:\n            assert console is not None\n            builtins._ = None\n            console.print(value if _safe_isinstance(value, RichRenderable) else Pretty(value, overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all), crop=crop)\n            builtins._ = value\n    if 'get_ipython' in globals():\n        ip = get_ipython()\n        from IPython.core.formatters import BaseFormatter\n\n        class RichFormatter(BaseFormatter):\n            pprint: bool = True\n\n            def __call__(self, value: Any) -> Any:\n                if self.pprint:\n                    return _ipy_display_hook(value, console=get_console(), overflow=overflow, indent_guides=indent_guides, max_length=max_length, max_string=max_string, max_depth=max_depth, expand_all=expand_all)\n                else:\n                    return repr(value)\n        rich_formatter = RichFormatter()\n        ip.display_formatter.formatters['text/plain'] = rich_formatter\n    else:\n        sys.displayhook = display_hook",
    "label": true
  },
  {
    "code": "def test_deleted():\n    global sin\n    from dill import dumps, loads\n    from math import sin, pi\n\n    def sinc(x):\n        return sin(x) / x\n    settings['recurse'] = True\n    _sinc = dumps(sinc)\n    sin = globals().pop('sin')\n    sin = 1\n    del sin\n    sinc_ = loads(_sinc)\n    res = sinc_(1)\n    from math import sin\n    assert sinc(1) == res",
    "label": true
  },
  {
    "code": "def print_results(hits: List['TransformedHit'], name_column_width: Optional[int]=None, terminal_width: Optional[int]=None) -> None:\n    if not hits:\n        return\n    if name_column_width is None:\n        name_column_width = max([len(hit['name']) + len(highest_version(hit.get('versions', ['-']))) for hit in hits]) + 4\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary'] or ''\n        latest = highest_version(hit.get('versions', ['-']))\n        if terminal_width is not None:\n            target_width = terminal_width - name_column_width - 5\n            if target_width > 10:\n                summary_lines = textwrap.wrap(summary, target_width)\n                summary = ('\\n' + ' ' * (name_column_width + 3)).join(summary_lines)\n        name_latest = f'{name} ({latest})'\n        line = f'{name_latest:{name_column_width}} - {summary}'\n        try:\n            write_output(line)\n            print_dist_installation_info(name, latest)\n        except UnicodeEncodeError:\n            pass",
    "label": true
  },
  {
    "code": "def _rewrite_test(fn: Path, config: Config) -> Tuple[os.stat_result, types.CodeType]:\n    stat = os.stat(fn)\n    source = fn.read_bytes()\n    strfn = str(fn)\n    tree = ast.parse(source, filename=strfn)\n    rewrite_asserts(tree, source, strfn, config)\n    co = compile(tree, strfn, 'exec', dont_inherit=True)\n    return (stat, co)",
    "label": true
  },
  {
    "code": "def _parse_marker(tokenizer: Tokenizer) -> MarkerList:\n    expression = [_parse_marker_atom(tokenizer)]\n    while tokenizer.check('BOOLOP'):\n        token = tokenizer.read()\n        expr_right = _parse_marker_atom(tokenizer)\n        expression.extend((token.text, expr_right))\n    return expression",
    "label": true
  },
  {
    "code": "def _parse_version_many(tokenizer: Tokenizer) -> str:\n    parsed_specifiers = ''\n    while tokenizer.check('SPECIFIER'):\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume('WS')\n        if not tokenizer.check('COMMA'):\n            break\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume('WS')\n    return parsed_specifiers",
    "label": true
  },
  {
    "code": "def decode(input, fallback_encoding, errors='replace'):\n    fallback_encoding = _get_encoding(fallback_encoding)\n    bom_encoding, input = _detect_bom(input)\n    encoding = bom_encoding or fallback_encoding\n    return (encoding.codec_info.decode(input, errors)[0], encoding)",
    "label": true
  },
  {
    "code": "def _bool(val):\n    try:\n        return bool(val)\n    except ValueError:\n        return False",
    "label": true
  },
  {
    "code": "def copy_context(context: InferenceContext | None) -> InferenceContext:\n    if context is not None:\n        return context.clone()\n    return InferenceContext()",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e116(msg, _node, source_lines=None):\n    line = msg.line\n    curr_idx = len(source_lines[line - 1]) - len(source_lines[line - 1].lstrip())\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(0, curr_idx), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def raise_check(func, **kwds):\n    try:\n        with capture('stdout') as out:\n            check(func, **kwds)\n    except Exception:\n        e = sys.exc_info()[1]\n        raise AssertionError(str(e))\n    else:\n        assert 'Traceback' not in out.getvalue()\n    finally:\n        out.close()",
    "label": true
  },
  {
    "code": "def warn_legacy_versions_and_specifiers(package_set: PackageSet) -> None:\n    for project_name, package_details in package_set.items():\n        if isinstance(package_details.version, LegacyVersion):\n            deprecated(reason=f'{project_name} {package_details.version} has a non-standard version number.', replacement=f'to upgrade to a newer version of {project_name} or contact the author to suggest that they release a version with a conforming version number', issue=12063, gone_in='23.3')\n        for dep in package_details.dependencies:\n            if any((isinstance(spec, LegacySpecifier) for spec in dep.specifier)):\n                deprecated(reason=f'{project_name} {package_details.version} has a non-standard dependency specifier {dep}.', replacement=f'to upgrade to a newer version of {project_name} or contact the author to suggest that they release a version with a conforming dependency specifiers', issue=12063, gone_in='23.3')",
    "label": true
  },
  {
    "code": "def _sigmasks_enum() -> str:\n    if sys.platform != 'win32':\n        return '\\n    import enum\\n    class Sigmasks(enum.IntEnum):\\n        SIG_BLOCK   = enum.auto()\\n        SIG_UNBLOCK = enum.auto()\\n        SIG_SETMASK = enum.auto()\\n        '\n    return ''",
    "label": true
  },
  {
    "code": "def _is_in_pascal_case(name: str) -> bool:\n    pattern = '(_?[A-Z][a-zA-Z0-9]*)$'\n    return re.match(pattern, name) is not None",
    "label": true
  },
  {
    "code": "def sliced(seq, n, strict=False):\n    iterator = takewhile(len, (seq[i:i + n] for i in count(0, n)))\n    if strict:\n\n        def ret():\n            for _slice in iterator:\n                if len(_slice) != n:\n                    raise ValueError('seq is not divisible by n.')\n                yield _slice\n        return iter(ret())\n    else:\n        return iterator",
    "label": true
  },
  {
    "code": "def project_from_files(files: list[str], func_wrapper: _WrapperFuncT=_astroid_wrapper, project_name: str='no name', black_list: tuple[str, ...]=constants.DEFAULT_IGNORE_LIST) -> Project:\n    astroid_manager = astroid.MANAGER\n    project = Project(project_name)\n    for something in files:\n        if not os.path.exists(something):\n            fpath = astroid.modutils.file_from_modpath(something.split('.'))\n        elif os.path.isdir(something):\n            fpath = os.path.join(something, '__init__.py')\n        else:\n            fpath = something\n        ast = func_wrapper(astroid_manager.ast_from_file, fpath)\n        if ast is None:\n            continue\n        project.path = project.path or ast.file\n        project.add_module(ast)\n        base_name = ast.name\n        if ast.package and something.find('__init__') == -1:\n            for fpath in astroid.modutils.get_module_files(os.path.dirname(ast.file), black_list):\n                ast = func_wrapper(astroid_manager.ast_from_file, fpath)\n                if ast is None or ast.name == base_name:\n                    continue\n                project.add_module(ast)\n    return project",
    "label": true
  },
  {
    "code": "def _get_decoder(mode):\n    if ',' in mode:\n        return MultiDecoder(mode)\n    if mode == 'gzip':\n        return GzipDecoder()\n    if brotli is not None and mode == 'br':\n        return BrotliDecoder()\n    return DeflateDecoder()",
    "label": true
  },
  {
    "code": "def get_platform():\n    if os.name == 'nt':\n        TARGET_TO_PLAT = {'x86': 'win32', 'x64': 'win-amd64', 'arm': 'win-arm32', 'arm64': 'win-arm64'}\n        target = os.environ.get('VSCMD_ARG_TGT_ARCH')\n        return TARGET_TO_PLAT.get(target) or get_host_platform()\n    return get_host_platform()",
    "label": true
  },
  {
    "code": "def get_filter_by_name(filtername, **options):\n    cls = find_filter_class(filtername)\n    if cls:\n        return cls(**options)\n    else:\n        raise ClassNotFound('filter %r not found' % filtername)",
    "label": true
  },
  {
    "code": "def _supports_protocol(value: nodes.NodeNG, protocol_callback: Callable[[nodes.NodeNG], bool]) -> bool:\n    if isinstance(value, nodes.ClassDef):\n        if not has_known_bases(value):\n            return True\n        meta = value.metaclass()\n        if meta is not None:\n            if protocol_callback(meta):\n                return True\n    if isinstance(value, astroid.BaseInstance):\n        if not has_known_bases(value):\n            return True\n        if value.has_dynamic_getattr():\n            return True\n        if protocol_callback(value):\n            return True\n    if isinstance(value, nodes.ComprehensionScope):\n        return True\n    if isinstance(value, astroid.bases.Proxy) and isinstance(value._proxied, astroid.BaseInstance) and has_known_bases(value._proxied):\n        value = value._proxied\n        return protocol_callback(value)\n    return False",
    "label": true
  },
  {
    "code": "def _set_parent_ns(packageName):\n    parts = packageName.split('.')\n    name = parts.pop()\n    if parts:\n        parent = '.'.join(parts)\n        setattr(sys.modules[parent], name, sys.modules[packageName])",
    "label": true
  },
  {
    "code": "def all_unique(iterable, key=None):\n    seenset = set()\n    seenset_add = seenset.add\n    seenlist = []\n    seenlist_add = seenlist.append\n    for element in map(key, iterable) if key else iterable:\n        try:\n            if element in seenset:\n                return False\n            seenset_add(element)\n        except TypeError:\n            if element in seenlist:\n                return False\n            seenlist_add(element)\n    return True",
    "label": true
  },
  {
    "code": "def _split_multiple_exc_types(target: str) -> list[str]:\n    delimiters = '(\\\\s*,(?:\\\\s*or\\\\s)?\\\\s*|\\\\s+or\\\\s+)'\n    return re.split(delimiters, target)",
    "label": true
  },
  {
    "code": "def _ordered_dict_mock():\n    base_ordered_dict_class = '\\n    class OrderedDict(dict):\\n        def __reversed__(self): return self[::-1]\\n        def move_to_end(self, key, last=False): pass'\n    if PY39_PLUS:\n        base_ordered_dict_class += '\\n        @classmethod\\n        def __class_getitem__(cls, item): return cls'\n    return base_ordered_dict_class",
    "label": true
  },
  {
    "code": "def get_contextlib_suppressors(node: nodes.NodeNG, exception: type[Exception] | str=Exception) -> Iterator[nodes.With]:\n    for with_node in get_contextlib_with_statements(node):\n        for item, _ in with_node.items:\n            if isinstance(item, nodes.Call):\n                inferred = safe_infer(item.func)\n                if isinstance(inferred, nodes.ClassDef) and inferred.qname() == 'contextlib.suppress':\n                    if _suppresses_exception(item, exception):\n                        yield with_node",
    "label": true
  },
  {
    "code": "def build_iter_view(matches):\n    if callable(matches):\n        return _FactoryIterableView(matches)\n    if not isinstance(matches, collections_abc.Sequence):\n        matches = list(matches)\n    return _SequenceIterableView(matches)",
    "label": true
  },
  {
    "code": "def format_header_param_rfc2231(name: str, value: _TYPE_FIELD_VALUE) -> str:\n    import warnings\n    warnings.warn(\"'format_header_param_rfc2231' is deprecated and will be removed in urllib3 v2.1.0. This is not valid for multipart/form-data header parameters.\", DeprecationWarning, stacklevel=2)\n    if isinstance(value, bytes):\n        value = value.decode('utf-8')\n    if not any((ch in value for ch in '\"\\\\\\r\\n')):\n        result = f'{name}=\"{value}\"'\n        try:\n            result.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n        else:\n            return result\n    value = email.utils.encode_rfc2231(value, 'utf-8')\n    value = f'{name}*={value}'\n    return value",
    "label": true
  },
  {
    "code": "def make_numbered_dir_with_cleanup(root: Path, prefix: str, keep: int, lock_timeout: float, mode: int) -> Path:\n    e = None\n    for i in range(10):\n        try:\n            p = make_numbered_dir(root, prefix, mode)\n            if keep != 0:\n                lock_path = create_cleanup_lock(p)\n                register_cleanup_lock_removal(lock_path)\n        except Exception as exc:\n            e = exc\n        else:\n            consider_lock_dead_if_created_before = p.stat().st_mtime - lock_timeout\n            atexit.register(cleanup_numbered_dir, root, prefix, keep, consider_lock_dead_if_created_before)\n            return p\n    assert e is not None\n    raise e",
    "label": true
  },
  {
    "code": "def profile_len(list_class: type, size: int) -> float:\n    my_list = list_class([0] * size)\n    return timeit('len(my_list)', number=1, globals=locals())",
    "label": true
  },
  {
    "code": "def get_dirs_from_args(args: Iterable[str]) -> List[Path]:\n\n    def is_option(x: str) -> bool:\n        return x.startswith('-')\n\n    def get_file_part_from_node_id(x: str) -> str:\n        return x.split('::')[0]\n\n    def get_dir_from_path(path: Path) -> Path:\n        if path.is_dir():\n            return path\n        return path.parent\n    possible_paths = (absolutepath(get_file_part_from_node_id(arg)) for arg in args if not is_option(arg))\n    return [get_dir_from_path(path) for path in possible_paths if safe_exists(path)]",
    "label": true
  },
  {
    "code": "def _iter_all_modules(package: Union[str, types.ModuleType], prefix: str='') -> Iterator[str]:\n    import os\n    import pkgutil\n    if isinstance(package, str):\n        path = package\n    else:\n        package_path = package.__path__\n        path, prefix = (package_path[0], package.__name__ + '.')\n    for _, name, is_package in pkgutil.iter_modules([path]):\n        if is_package:\n            for m in _iter_all_modules(os.path.join(path, name), prefix=name + '.'):\n                yield (prefix + m)\n        else:\n            yield (prefix + name)",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('general')\n    group.addoption('--runxfail', action='store_true', dest='runxfail', default=False, help='Report the results of xfail tests as if they were not marked')\n    parser.addini('xfail_strict', 'Default for the strict parameter of xfail markers when not given explicitly (default: False)', default=False, type='bool')",
    "label": true
  },
  {
    "code": "def wrap_spec(package):\n    from . import _adapters\n    return _adapters.SpecLoaderAdapter(package.__spec__, TraversableResourcesLoader)",
    "label": true
  },
  {
    "code": "def test_namedtuple():\n    assert Z is dill.loads(dill.dumps(Z))\n    assert Zi == dill.loads(dill.dumps(Zi))\n    assert X is dill.loads(dill.dumps(X))\n    assert Xi == dill.loads(dill.dumps(Xi))\n    assert Defaults is dill.loads(dill.dumps(Defaults))\n    assert Defaultsi == dill.loads(dill.dumps(Defaultsi))\n    assert Bad is not dill.loads(dill.dumps(Bad))\n    assert Bad._fields == dill.loads(dill.dumps(Bad))._fields\n    assert tuple(Badi) == tuple(dill.loads(dill.dumps(Badi)))\n\n    class A:\n\n        class B(namedtuple('C', ['one', 'two'])):\n            \"\"\"docstring\"\"\"\n        B.__module__ = 'testing'\n    a = A()\n    assert dill.copy(a)\n    assert dill.copy(A.B).__name__ == 'B'\n    assert dill.copy(A.B).__qualname__.endswith('.<locals>.A.B')\n    assert dill.copy(A.B).__doc__ == 'docstring'\n    assert dill.copy(A.B).__module__ == 'testing'\n    from typing import NamedTuple\n\n    def A():\n\n        class B(NamedTuple):\n            x: int\n        return B\n    assert type(dill.copy(A()(8))).__qualname__ == type(A()(8)).__qualname__",
    "label": true
  },
  {
    "code": "def _src_path(name: str, config: Config, src_paths: Optional[Iterable[Path]]=None, prefix: Tuple[str, ...]=()) -> Optional[Tuple[str, str]]:\n    if src_paths is None:\n        src_paths = config.src_paths\n    root_module_name, *nested_module = name.split('.', 1)\n    new_prefix = prefix + (root_module_name,)\n    namespace = '.'.join(new_prefix)\n    for src_path in src_paths:\n        module_path = (src_path / root_module_name).resolve()\n        if not prefix and (not module_path.is_dir()) and (src_path.name == root_module_name):\n            module_path = src_path.resolve()\n        if nested_module and (namespace in config.namespace_packages or (config.auto_identify_namespace_packages and _is_namespace_package(module_path, config.supported_extensions))):\n            return _src_path(nested_module[0], config, (module_path,), new_prefix)\n        if _is_module(module_path) or _is_package(module_path) or _src_path_is_module(src_path, root_module_name):\n            return (sections.FIRSTPARTY, f'Found in one of the configured src_paths: {src_path}.')\n    return None",
    "label": true
  },
  {
    "code": "def one(iterable, too_short=None, too_long=None):\n    it = iter(iterable)\n    try:\n        first_value = next(it)\n    except StopIteration as e:\n        raise too_short or ValueError('too few items in iterable (expected 1)') from e\n    try:\n        second_value = next(it)\n    except StopIteration:\n        pass\n    else:\n        msg = 'Expected exactly one item in iterable, but got {!r}, {!r}, and perhaps more.'.format(first_value, second_value)\n        raise too_long or ValueError(msg)\n    return first_value",
    "label": true
  },
  {
    "code": "def render_pep8_errors_e124(msg, _node, source_lines=None):\n    line = msg.line\n    res = re.search('column (\\\\d+)', msg.msg)\n    col = int(res.group().split()[-1])\n    yield from render_context(line - 2, line, source_lines)\n    yield (line, slice(col, col + 1), LineType.ERROR, source_lines[line - 1])\n    yield from render_context(line + 1, line + 3, source_lines)",
    "label": true
  },
  {
    "code": "def hash_file(path: str, blocksize: int=1 << 20) -> Tuple[Any, int]:\n    h = hashlib.sha256()\n    length = 0\n    with open(path, 'rb') as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    return (h, length)",
    "label": true
  },
  {
    "code": "def get_user_id() -> int | None:\n    if sys.platform in ('win32', 'emscripten'):\n        return None\n    uid = os.getuid()\n    return uid if uid != -1 else None",
    "label": true
  },
  {
    "code": "def pytest_report_to_serializable(report: Union[CollectReport, TestReport]) -> Optional[Dict[str, Any]]:\n    if isinstance(report, (TestReport, CollectReport)):\n        data = report._to_json()\n        data['$report_type'] = report.__class__.__name__\n        return data\n    return None",
    "label": true
  },
  {
    "code": "def _get_previous_field_default(node: nodes.ClassDef, name: str) -> nodes.NodeNG | None:\n    for base in reversed(node.mro()):\n        if not base.is_dataclass:\n            continue\n        if name in base.locals:\n            for assign in base.locals[name]:\n                if isinstance(assign.parent, nodes.AnnAssign) and assign.parent.value and isinstance(assign.parent.value, nodes.Call) and _looks_like_dataclass_field_call(assign.parent.value):\n                    default = _get_field_default(assign.parent.value)\n                    if default:\n                        return default[1]\n    return None",
    "label": true
  },
  {
    "code": "def internalcode(f: F) -> F:\n    internal_code.add(f.__code__)\n    return f",
    "label": true
  },
  {
    "code": "def _get_protocol_attrs(cls):\n    attrs = set()\n    for base in cls.__mro__[:-1]:\n        if base.__name__ in {'Protocol', 'Generic'}:\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in (*base.__dict__, *annotations):\n            if not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS:\n                attrs.add(attr)\n    return attrs",
    "label": true
  },
  {
    "code": "def parse(version: str) -> Union['LegacyVersion', 'Version']:\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)",
    "label": true
  },
  {
    "code": "def _nest_path(parent: _Path, path: _Path) -> str:\n    path = parent if path in {'.', ''} else os.path.join(parent, path)\n    return os.path.normpath(path)",
    "label": true
  },
  {
    "code": "def read_pkg_file(self, file):\n    msg = message_from_file(file)\n    self.metadata_version = Version(msg['metadata-version'])\n    self.name = _read_field_from_msg(msg, 'name')\n    self.version = _read_field_from_msg(msg, 'version')\n    self.description = _read_field_from_msg(msg, 'summary')\n    self.author = _read_field_from_msg(msg, 'author')\n    self.maintainer = None\n    self.author_email = _read_field_from_msg(msg, 'author-email')\n    self.maintainer_email = None\n    self.url = _read_field_from_msg(msg, 'home-page')\n    self.download_url = _read_field_from_msg(msg, 'download-url')\n    self.license = _read_field_unescaped_from_msg(msg, 'license')\n    self.long_description = _read_field_unescaped_from_msg(msg, 'description')\n    if self.long_description is None and self.metadata_version >= Version('2.1'):\n        self.long_description = _read_payload_from_msg(msg)\n    self.description = _read_field_from_msg(msg, 'summary')\n    if 'keywords' in msg:\n        self.keywords = _read_field_from_msg(msg, 'keywords').split(',')\n    self.platforms = _read_list_from_msg(msg, 'platform')\n    self.classifiers = _read_list_from_msg(msg, 'classifier')\n    if self.metadata_version == Version('1.1'):\n        self.requires = _read_list_from_msg(msg, 'requires')\n        self.provides = _read_list_from_msg(msg, 'provides')\n        self.obsoletes = _read_list_from_msg(msg, 'obsoletes')\n    else:\n        self.requires = None\n        self.provides = None\n        self.obsoletes = None\n    self.license_files = _read_list_from_msg(msg, 'license-file')",
    "label": true
  },
  {
    "code": "def with_metaclass(meta, *bases):\n\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})",
    "label": true
  },
  {
    "code": "def import_statement(import_start: str, from_imports: List[str], comments: Sequence[str]=(), line_separator: str='\\n', config: Config=DEFAULT_CONFIG, multi_line_output: Optional[Modes]=None, explode: bool=False) -> str:\n    if explode:\n        formatter = vertical_hanging_indent\n        line_length = 1\n        include_trailing_comma = True\n    else:\n        formatter = formatter_from_string((multi_line_output or config.multi_line_output).name)\n        line_length = config.wrap_length or config.line_length\n        include_trailing_comma = config.include_trailing_comma\n    dynamic_indent = ' ' * (len(import_start) + 1)\n    indent = config.indent\n    statement = formatter(statement=import_start, imports=copy.copy(from_imports), white_space=dynamic_indent, indent=indent, line_length=line_length, comments=comments, line_separator=line_separator, comment_prefix=config.comment_prefix, include_trailing_comma=include_trailing_comma, remove_comments=config.ignore_comments)\n    if config.balanced_wrapping:\n        lines = statement.split(line_separator)\n        line_count = len(lines)\n        if len(lines) > 1:\n            minimum_length = min((len(line) for line in lines[:-1]))\n        else:\n            minimum_length = 0\n        new_import_statement = statement\n        while len(lines[-1]) < minimum_length and len(lines) == line_count and (line_length > 10):\n            statement = new_import_statement\n            line_length -= 1\n            new_import_statement = formatter(statement=import_start, imports=copy.copy(from_imports), white_space=dynamic_indent, indent=indent, line_length=line_length, comments=comments, line_separator=line_separator, comment_prefix=config.comment_prefix, include_trailing_comma=include_trailing_comma, remove_comments=config.ignore_comments)\n            lines = new_import_statement.split(line_separator)\n    if statement.count(line_separator) == 0:\n        return _wrap_line(statement, line_separator, config)\n    return statement",
    "label": true
  },
  {
    "code": "def to_railroad(element: pyparsing.ParserElement, diagram_kwargs: typing.Optional[dict]=None, vertical: int=3, show_results_names: bool=False, show_groups: bool=False) -> List[NamedDiagram]:\n    lookup = ConverterState(diagram_kwargs=diagram_kwargs or {})\n    _to_diagram_element(element, lookup=lookup, parent=None, vertical=vertical, show_results_names=show_results_names, show_groups=show_groups)\n    root_id = id(element)\n    if root_id in lookup:\n        if not element.customName:\n            lookup[root_id].name = ''\n        lookup[root_id].mark_for_extraction(root_id, lookup, force=True)\n    diags = list(lookup.diagrams.values())\n    if len(diags) > 1:\n        seen = set()\n        deduped_diags = []\n        for d in diags:\n            if d.name == '...':\n                continue\n            if d.name is not None and d.name not in seen:\n                seen.add(d.name)\n                deduped_diags.append(d)\n        resolved = [resolve_partial(partial) for partial in deduped_diags]\n    else:\n        resolved = [resolve_partial(partial) for partial in diags]\n    return sorted(resolved, key=lambda diag: diag.index)",
    "label": true
  },
  {
    "code": "def find(paths: Iterable[str], config: Config, skipped: List[str], broken: List[str]) -> Iterator[str]:\n    visited_dirs: Set[Path] = set()\n    for path in paths:\n        if os.path.isdir(path):\n            for dirpath, dirnames, filenames in os.walk(path, topdown=True, followlinks=config.follow_links):\n                base_path = Path(dirpath)\n                for dirname in list(dirnames):\n                    full_path = base_path / dirname\n                    resolved_path = full_path.resolve()\n                    if config.is_skipped(full_path):\n                        skipped.append(dirname)\n                        dirnames.remove(dirname)\n                    elif resolved_path in visited_dirs:\n                        dirnames.remove(dirname)\n                    visited_dirs.add(resolved_path)\n                for filename in filenames:\n                    filepath = os.path.join(dirpath, filename)\n                    if config.is_supported_filetype(filepath):\n                        if config.is_skipped(Path(os.path.abspath(filepath))):\n                            skipped.append(filename)\n                        else:\n                            yield filepath\n        elif not os.path.exists(path):\n            broken.append(path)\n        else:\n            yield path",
    "label": true
  },
  {
    "code": "def type_check_simple(func: callable, args: list, expected: Union[type, tuple]) -> tuple[bool, object]:\n    try:\n        args_copy = deepcopy(args)\n        returned = func(*args_copy)\n    except Exception as exn:\n        return (False, error_message(func, args, exn))\n    if isinstance(returned, expected):\n        return (True, returned)\n    return (False, type_error_message(func.__name__, expected.__name__, returned))",
    "label": true
  },
  {
    "code": "def check_compatibility(version: Tuple[int, ...], name: str) -> None:\n    if version[0] > VERSION_COMPATIBLE[0]:\n        raise UnsupportedWheel(\"{}'s Wheel-Version ({}) is not compatible with this version of pip\".format(name, '.'.join(map(str, version))))\n    elif version > VERSION_COMPATIBLE:\n        logger.warning('Installing from a newer Wheel-Version (%s)', '.'.join(map(str, version)))",
    "label": true
  },
  {
    "code": "def _empty_dir(dir_: _P) -> _P:\n    shutil.rmtree(dir_, ignore_errors=True)\n    os.makedirs(dir_)\n    return dir_",
    "label": true
  },
  {
    "code": "def _parse_glibc_version(version_str: str) -> Tuple[int, int]:\n    m = re.match('(?P<major>[0-9]+)\\\\.(?P<minor>[0-9]+)', version_str)\n    if not m:\n        warnings.warn('Expected glibc version with 2 components major.minor, got: %s' % version_str, RuntimeWarning)\n        return (-1, -1)\n    return (int(m.group('major')), int(m.group('minor')))",
    "label": true
  },
  {
    "code": "def multi_substitution(*substitutions):\n    substitutions = itertools.starmap(substitution, substitutions)\n    substitutions = reversed(tuple(substitutions))\n    return compose(*substitutions)",
    "label": true
  },
  {
    "code": "def _validate_dependencies_met():\n    from cryptography.x509.extensions import Extensions\n    if getattr(Extensions, 'get_extension_for_class', None) is None:\n        raise ImportError(\"'cryptography' module missing required functionality.  Try upgrading to v1.3.4 or newer.\")\n    from OpenSSL.crypto import X509\n    x509 = X509()\n    if getattr(x509, '_x509', None) is None:\n        raise ImportError(\"'pyOpenSSL' module missing required functionality. Try upgrading to v0.14 or newer.\")",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def _signature_from_call(call: nodes.Call) -> _CallSignature:\n    kws = {}\n    args = []\n    starred_kws = []\n    starred_args = []\n    for keyword in call.keywords or []:\n        arg, value = (keyword.arg, keyword.value)\n        if arg is None and isinstance(value, nodes.Name):\n            starred_kws.append(value.name)\n        elif isinstance(value, nodes.Name):\n            kws[arg] = value.name\n        else:\n            kws[arg] = None\n    for arg in call.args:\n        if isinstance(arg, nodes.Starred) and isinstance(arg.value, nodes.Name):\n            starred_args.append(arg.value.name)\n        elif isinstance(arg, nodes.Name):\n            args.append(arg.name)\n        else:\n            args.append(None)\n    return _CallSignature(args, kws, starred_args, starred_kws)",
    "label": true
  },
  {
    "code": "def has_safe_repr(value: t.Any) -> bool:\n    if value is None or value is NotImplemented or value is Ellipsis:\n        return True\n    if type(value) in {bool, int, float, complex, range, str, Markup}:\n        return True\n    if type(value) in {tuple, list, set, frozenset}:\n        return all((has_safe_repr(v) for v in value))\n    if type(value) is dict:\n        return all((has_safe_repr(k) and has_safe_repr(v) for k, v in value.items()))\n    return False",
    "label": true
  },
  {
    "code": "def _get_previous_gui_scripts(dist: 'Distribution') -> Optional[list]:\n    value = getattr(dist, 'entry_points', None) or {}\n    return value.get('gui_scripts')",
    "label": true
  },
  {
    "code": "def infer_dataclass_attribute(node: nodes.Unknown, ctx: context.InferenceContext | None=None) -> Iterator[InferenceResult]:\n    assign = node.parent\n    if not isinstance(assign, nodes.AnnAssign):\n        yield Uninferable\n        return\n    annotation, value = (assign.annotation, assign.value)\n    if value is not None:\n        yield from value.infer(context=ctx)\n    if annotation is not None:\n        yield from _infer_instance_from_annotation(annotation, ctx=ctx)\n    else:\n        yield Uninferable",
    "label": true
  },
  {
    "code": "def parse_key(src: str, pos: Pos) -> tuple[Pos, Key]:\n    pos, key_part = parse_key_part(src, pos)\n    key: Key = (key_part,)\n    pos = skip_chars(src, pos, TOML_WS)\n    while True:\n        try:\n            char: str | None = src[pos]\n        except IndexError:\n            char = None\n        if char != '.':\n            return (pos, key)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)\n        pos, key_part = parse_key_part(src, pos)\n        key += (key_part,)\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def fixup_namespace_packages(path_item, parent=None):\n    _imp.acquire_lock()\n    try:\n        for package in _namespace_packages.get(parent, ()):\n            subpath = _handle_ns(package, path_item)\n            if subpath:\n                fixup_namespace_packages(subpath, package)\n    finally:\n        _imp.release_lock()",
    "label": true
  },
  {
    "code": "def __hook__():\n    global NumpyArrayType, NumpyDType, NumpyUfuncType\n    from numpy import ufunc as NumpyUfuncType\n    from numpy import ndarray as NumpyArrayType\n    from numpy import dtype as NumpyDType\n    return True",
    "label": true
  },
  {
    "code": "def divide(n, iterable):\n    if n < 1:\n        raise ValueError('n must be at least 1')\n    try:\n        iterable[:0]\n    except TypeError:\n        seq = tuple(iterable)\n    else:\n        seq = iterable\n    q, r = divmod(len(seq), n)\n    ret = []\n    stop = 0\n    for i in range(1, n + 1):\n        start = stop\n        stop += q + 1 if i <= r else q\n        ret.append(iter(seq[start:stop]))\n    return ret",
    "label": true
  },
  {
    "code": "def _script_names(bin_dir: str, script_name: str, is_gui: bool) -> Generator[str, None, None]:\n    exe_name = os.path.join(bin_dir, script_name)\n    yield exe_name\n    if not WINDOWS:\n        return\n    yield f'{exe_name}.exe'\n    yield f'{exe_name}.exe.manifest'\n    if is_gui:\n        yield f'{exe_name}-script.pyw'\n    else:\n        yield f'{exe_name}-script.py'",
    "label": true
  },
  {
    "code": "def get_distribution(dist):\n    if isinstance(dist, str):\n        dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement):\n        dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError('Expected string, Requirement, or Distribution', dist)\n    return dist",
    "label": true
  },
  {
    "code": "def _break_around_binary_operators(tokens):\n    line_break = False\n    unary_context = True\n    previous_token_type = None\n    previous_text = None\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            continue\n        if ('\\n' in text or '\\r' in text) and token_type != tokenize.STRING:\n            line_break = True\n        else:\n            yield (token_type, text, previous_token_type, previous_text, line_break, unary_context, start)\n            unary_context = text in '([{,;'\n            line_break = False\n            previous_token_type = token_type\n            previous_text = text",
    "label": true
  },
  {
    "code": "def get_all_formatters():\n    for info in FORMATTERS.values():\n        if info[1] not in _formatter_cache:\n            _load_formatters(info[0])\n        yield _formatter_cache[info[1]]\n    for _, formatter in find_plugin_formatters():\n        yield formatter",
    "label": true
  },
  {
    "code": "def check_mapping(value: Any, origin_type: Any, args: tuple[Any, ...], memo: TypeCheckMemo) -> None:\n    if origin_type is Dict or origin_type is dict:\n        if not isinstance(value, dict):\n            raise TypeCheckError('is not a dict')\n    if origin_type is MutableMapping or origin_type is collections.abc.MutableMapping:\n        if not isinstance(value, collections.abc.MutableMapping):\n            raise TypeCheckError('is not a mutable mapping')\n    elif not isinstance(value, collections.abc.Mapping):\n        raise TypeCheckError('is not a mapping')\n    if args:\n        key_type, value_type = args\n        if key_type is not Any or value_type is not Any:\n            samples = memo.config.collection_check_strategy.iterate_samples(value.items())\n            for k, v in samples:\n                try:\n                    check_type_internal(k, key_type, memo)\n                except TypeCheckError as exc:\n                    exc.append_path_element(f'key {k!r}')\n                    raise\n                try:\n                    check_type_internal(v, value_type, memo)\n                except TypeCheckError as exc:\n                    exc.append_path_element(f'value of key {k!r}')\n                    raise",
    "label": true
  },
  {
    "code": "def get_python_lib(plat_specific=0, standard_lib=0, prefix=None):\n    if IS_PYPY and sys.version_info < (3, 8):\n        if prefix is None:\n            prefix = PREFIX\n        if standard_lib:\n            return os.path.join(prefix, 'lib-python', sys.version[0])\n        return os.path.join(prefix, 'site-packages')\n    early_prefix = prefix\n    if prefix is None:\n        if standard_lib:\n            prefix = plat_specific and BASE_EXEC_PREFIX or BASE_PREFIX\n        else:\n            prefix = plat_specific and EXEC_PREFIX or PREFIX\n    if os.name == 'posix':\n        if plat_specific or standard_lib:\n            libdir = getattr(sys, 'platlibdir', 'lib')\n        else:\n            libdir = 'lib'\n        implementation = 'pypy' if IS_PYPY else 'python'\n        libpython = os.path.join(prefix, libdir, implementation + get_python_version())\n        return _posix_lib(standard_lib, libpython, early_prefix, prefix)\n    elif os.name == 'nt':\n        if standard_lib:\n            return os.path.join(prefix, 'Lib')\n        else:\n            return os.path.join(prefix, 'Lib', 'site-packages')\n    else:\n        raise DistutilsPlatformError(\"I don't know where Python installs its library on platform '%s'\" % os.name)",
    "label": true
  },
  {
    "code": "def warns(expected_warning: Union[Type[Warning], Tuple[Type[Warning], ...]]=Warning, *args: Any, match: Optional[Union[str, Pattern[str]]]=None, **kwargs: Any) -> Union['WarningsChecker', Any]:\n    __tracebackhide__ = True\n    if not args:\n        if kwargs:\n            argnames = ', '.join(sorted(kwargs))\n            raise TypeError(f'Unexpected keyword arguments passed to pytest.warns: {argnames}\\nUse context-manager form instead?')\n        return WarningsChecker(expected_warning, match_expr=match, _ispytest=True)\n    else:\n        func = args[0]\n        if not callable(func):\n            raise TypeError(f'{func!r} object (type: {type(func)}) must be callable')\n        with WarningsChecker(expected_warning, _ispytest=True):\n            return func(*args[1:], **kwargs)",
    "label": true
  },
  {
    "code": "def decoding_stream(stream: BufferedReader | BytesIO, encoding: str, errors: Literal['strict']='strict') -> codecs.StreamReader:\n    try:\n        reader_cls = codecs.getreader(encoding or sys.getdefaultencoding())\n    except LookupError:\n        reader_cls = codecs.getreader(sys.getdefaultencoding())\n    return reader_cls(stream, errors)",
    "label": true
  },
  {
    "code": "def _normalize_host(host, scheme):\n    if host:\n        if isinstance(host, six.binary_type):\n            host = six.ensure_str(host)\n        if scheme in NORMALIZABLE_SCHEMES:\n            is_ipv6 = IPV6_ADDRZ_RE.match(host)\n            if is_ipv6:\n                match = ZONE_ID_RE.search(host)\n                if match:\n                    start, end = match.span(1)\n                    zone_id = host[start:end]\n                    if zone_id.startswith('%25') and zone_id != '%25':\n                        zone_id = zone_id[3:]\n                    else:\n                        zone_id = zone_id[1:]\n                    zone_id = '%' + _encode_invalid_chars(zone_id, UNRESERVED_CHARS)\n                    return host[:start].lower() + zone_id + host[end:]\n                else:\n                    return host.lower()\n            elif not IPV4_RE.match(host):\n                return six.ensure_str(b'.'.join([_idna_encode(label) for label in host.split('.')]))\n    return host",
    "label": true
  },
  {
    "code": "def _visit(block: CFGBlock, graph: graphviz.Digraph, visited: Set[int], end: CFGBlock) -> None:\n    node_id = f'{graph.name}_{block.id}'\n    if node_id in visited:\n        return\n    label = '\\n'.join([s.as_string() for s in block.statements]) + '\\n'\n    label = label.replace('\\\\', '\\\\\\\\')\n    label = label.replace('\\n', '\\\\l')\n    fill_color = 'grey93' if not block.reachable else 'white'\n    fill_color = 'black' if block == end else fill_color\n    graph.node(node_id, label=label, fillcolor=fill_color, style='filled')\n    visited.add(node_id)\n    for edge in block.successors:\n        if edge.label is not None:\n            graph.edge(node_id, f'{graph.name}_{edge.target.id}', str(edge.label))\n        else:\n            graph.edge(node_id, f'{graph.name}_{edge.target.id}')\n        _visit(edge.target, graph, visited, end)",
    "label": true
  },
  {
    "code": "def filter_alt_coherence_matches(results: CoherenceMatches) -> CoherenceMatches:\n    index_results: Dict[str, List[float]] = dict()\n    for result in results:\n        language, ratio = result\n        no_em_name: str = language.replace('\u2014', '')\n        if no_em_name not in index_results:\n            index_results[no_em_name] = []\n        index_results[no_em_name].append(ratio)\n    if any((len(index_results[e]) > 1 for e in index_results)):\n        filtered_results: CoherenceMatches = []\n        for language in index_results:\n            filtered_results.append((language, max(index_results[language])))\n        return filtered_results\n    return results",
    "label": true
  },
  {
    "code": "def gray_product(*iterables):\n    all_iterables = tuple((tuple(x) for x in iterables))\n    iterable_count = len(all_iterables)\n    for iterable in all_iterables:\n        if len(iterable) < 2:\n            raise ValueError('each iterable must have two or more items')\n    a = [0] * iterable_count\n    f = list(range(iterable_count + 1))\n    o = [1] * iterable_count\n    while True:\n        yield tuple((all_iterables[i][a[i]] for i in range(iterable_count)))\n        j = f[0]\n        f[0] = 0\n        if j == iterable_count:\n            break\n        a[j] = a[j] + o[j]\n        if a[j] == 0 or a[j] == len(all_iterables[j]) - 1:\n            o[j] = -o[j]\n            f[j] = f[j + 1]\n            f[j + 1] = j + 1",
    "label": true
  },
  {
    "code": "def infer_named_tuple(node: nodes.Call, context: InferenceContext | None=None) -> Iterator[nodes.ClassDef]:\n    tuple_base_name: list[nodes.NodeNG] = [nodes.Name(name='tuple', parent=node.root())]\n    class_node, name, attributes = infer_func_form(node, tuple_base_name, context=context)\n    call_site = arguments.CallSite.from_call(node, context=context)\n    node = extract_node('import collections; collections.namedtuple')\n    try:\n        func = next(node.infer())\n    except StopIteration as e:\n        raise InferenceError(node=node) from e\n    try:\n        rename = next(call_site.infer_argument(func, 'rename', context)).bool_value()\n    except (InferenceError, StopIteration):\n        rename = False\n    try:\n        attributes = _check_namedtuple_attributes(name, attributes, rename)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault('TypeError: ' + str(exc)) from exc\n    except AstroidValueError as exc:\n        raise UseInferenceDefault('ValueError: ' + str(exc)) from exc\n    replace_args = ', '.join((f'{arg}=None' for arg in attributes))\n    field_def = \"    {name} = property(lambda self: self[{index:d}], doc='Alias for field number {index:d}')\"\n    field_defs = '\\n'.join((field_def.format(name=name, index=index) for index, name in enumerate(attributes)))\n    fake = AstroidBuilder(AstroidManager()).string_build(f'\\nclass {name}(tuple):\\n    __slots__ = ()\\n    _fields = {attributes!r}\\n    def _asdict(self):\\n        return self.__dict__\\n    @classmethod\\n    def _make(cls, iterable, new=tuple.__new__, len=len):\\n        return new(cls, iterable)\\n    def _replace(self, {replace_args}):\\n        return self\\n    def __getnewargs__(self):\\n        return tuple(self)\\n{field_defs}\\n    ')\n    class_node.locals['_asdict'] = fake.body[0].locals['_asdict']\n    class_node.locals['_make'] = fake.body[0].locals['_make']\n    class_node.locals['_replace'] = fake.body[0].locals['_replace']\n    class_node.locals['_fields'] = fake.body[0].locals['_fields']\n    for attr in attributes:\n        class_node.locals[attr] = fake.body[0].locals[attr]\n    return iter([class_node])",
    "label": true
  },
  {
    "code": "def _wipe_internal_state_for_tests():\n    global orig_stdout, orig_stderr\n    orig_stdout = None\n    orig_stderr = None\n    global wrapped_stdout, wrapped_stderr\n    wrapped_stdout = None\n    wrapped_stderr = None\n    global atexit_done\n    atexit_done = False\n    global fixed_windows_console\n    fixed_windows_console = False\n    try:\n        atexit.unregister(reset_all)\n    except AttributeError:\n        pass",
    "label": true
  },
  {
    "code": "def _isnumber(string):\n    if not _isconvertible(float, string):\n        return False\n    elif isinstance(string, (str, bytes)) and (math.isinf(float(string)) or math.isnan(float(string))):\n        return string.lower() in ['inf', '-inf', 'nan']\n    return True",
    "label": true
  },
  {
    "code": "def _getattr(objclass, name, repr_str):\n    try:\n        attr = repr_str.split(\"'\")[3]\n        return eval(attr + '.__dict__[\"' + name + '\"]')\n    except Exception:\n        try:\n            attr = objclass.__dict__\n            if type(attr) is DictProxyType:\n                attr = attr[name]\n            else:\n                attr = getattr(objclass, name)\n        except (AttributeError, KeyError):\n            attr = getattr(objclass, name)\n        return attr",
    "label": true
  },
  {
    "code": "def _inject_headers(name, scheme):\n    fallback = _load_scheme(_pypy_hack(name))\n    scheme.setdefault('headers', fallback['headers'])\n    return scheme",
    "label": true
  },
  {
    "code": "def _get_multicapture(method: '_CaptureMethod') -> MultiCapture[str]:\n    if method == 'fd':\n        return MultiCapture(in_=FDCapture(0), out=FDCapture(1), err=FDCapture(2))\n    elif method == 'sys':\n        return MultiCapture(in_=SysCapture(0), out=SysCapture(1), err=SysCapture(2))\n    elif method == 'no':\n        return MultiCapture(in_=None, out=None, err=None)\n    elif method == 'tee-sys':\n        return MultiCapture(in_=None, out=SysCapture(1, tee=True), err=SysCapture(2, tee=True))\n    raise ValueError(f'unknown capturing method: {method!r}')",
    "label": true
  },
  {
    "code": "def _test_writable_dir_win(path: str) -> bool:\n    basename = 'accesstest_deleteme_fishfingers_custard_'\n    alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789'\n    for _ in range(10):\n        name = basename + ''.join((random.choice(alphabet) for _ in range(6)))\n        file = os.path.join(path, name)\n        try:\n            fd = os.open(file, os.O_RDWR | os.O_CREAT | os.O_EXCL)\n        except FileExistsError:\n            pass\n        except PermissionError:\n            return False\n        else:\n            os.close(fd)\n            os.unlink(file)\n            return True\n    raise OSError('Unexpected condition testing for writable directory')",
    "label": true
  },
  {
    "code": "def get_module_from_module_name(module_name: str) -> ModuleType:\n    module_name = module_name.lower().replace('-', '_')\n    if module_name == 'setuptools':\n        module_name = 'pkg_resources'\n    __import__(f'pip._vendor.{module_name}', globals(), locals(), level=0)\n    return getattr(pip._vendor, module_name)",
    "label": true
  },
  {
    "code": "def _should_cache(req: InstallRequirement) -> Optional[bool]:\n    if req.editable or not req.source_dir:\n        return False\n    if req.link and req.link.is_vcs:\n        assert not req.editable\n        assert req.source_dir\n        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)\n        assert vcs_backend\n        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):\n            return True\n        return False\n    assert req.link\n    base, ext = req.link.splitext()\n    if _contains_egg_info(base):\n        return True\n    return False",
    "label": true
  },
  {
    "code": "def is_sorted(iterable, key=None, reverse=False, strict=False):\n    compare = (le if reverse else ge) if strict else lt if reverse else gt\n    it = iterable if key is None else map(key, iterable)\n    return not any(starmap(compare, pairwise(it)))",
    "label": true
  },
  {
    "code": "def sorted_walk(dir):\n    for base, dirs, files in os.walk(dir):\n        dirs.sort()\n        files.sort()\n        yield (base, dirs, files)",
    "label": true
  },
  {
    "code": "def make_pass_decorator(object_type: t.Type[T], ensure: bool=False) -> t.Callable[['t.Callable[te.Concatenate[T, P], R]'], 't.Callable[P, R]']:\n\n    def decorator(f: 't.Callable[te.Concatenate[T, P], R]') -> 't.Callable[P, R]':\n\n        def new_func(*args: 'P.args', **kwargs: 'P.kwargs') -> 'R':\n            ctx = get_current_context()\n            obj: t.Optional[T]\n            if ensure:\n                obj = ctx.ensure_object(object_type)\n            else:\n                obj = ctx.find_object(object_type)\n            if obj is None:\n                raise RuntimeError(f'Managed to invoke callback without a context object of type {object_type.__name__!r} existing.')\n            return ctx.invoke(f, obj, *args, **kwargs)\n        return update_wrapper(new_func, f)\n    return decorator",
    "label": true
  },
  {
    "code": "def _make_peekable(stream):\n    import io\n    if hasattr(stream, 'peek'):\n        return stream\n    if not (hasattr(stream, 'tell') and hasattr(stream, 'seek')):\n        try:\n            return io.BufferedReader(stream)\n        except Exception:\n            pass\n    return _PeekableReader(stream)",
    "label": true
  },
  {
    "code": "def check_compatibility(version: Tuple[int, ...], name: str) -> None:\n    if version[0] > VERSION_COMPATIBLE[0]:\n        raise UnsupportedWheel(\"{}'s Wheel-Version ({}) is not compatible with this version of pip\".format(name, '.'.join(map(str, version))))\n    elif version > VERSION_COMPATIBLE:\n        logger.warning('Installing from a newer Wheel-Version (%s)', '.'.join(map(str, version)))",
    "label": true
  },
  {
    "code": "def _include_extra(req: str, extra: str, condition: str) -> Requirement:\n    r = Requirement(req)\n    parts = (f'({r.marker})' if r.marker else None, f'({condition})' if condition else None, f'extra == {extra!r}' if extra else None)\n    r.marker = Marker(' and '.join((x for x in parts if x)))\n    return r",
    "label": true
  },
  {
    "code": "def get_executable():\n    result = sys.executable\n    if not isinstance(result, text_type):\n        result = fsdecode(result)\n    return result",
    "label": true
  },
  {
    "code": "def _load_lexers(module_name):\n    mod = __import__(module_name, None, None, ['__all__'])\n    for lexer_name in mod.__all__:\n        cls = getattr(mod, lexer_name)\n        _lexer_cache[cls.name] = cls",
    "label": true
  },
  {
    "code": "def zip_offset(*iterables, offsets, longest=False, fillvalue=None):\n    if len(iterables) != len(offsets):\n        raise ValueError(\"Number of iterables and offsets didn't match\")\n    staggered = []\n    for it, n in zip(iterables, offsets):\n        if n < 0:\n            staggered.append(chain(repeat(fillvalue, -n), it))\n        elif n > 0:\n            staggered.append(islice(it, n, None))\n        else:\n            staggered.append(it)\n    if longest:\n        return zip_longest(*staggered, fillvalue=fillvalue)\n    return zip(*staggered)",
    "label": true
  },
  {
    "code": "def _parse_ini_config(path: Path) -> iniconfig.IniConfig:\n    try:\n        return iniconfig.IniConfig(str(path))\n    except iniconfig.ParseError as exc:\n        raise UsageError(str(exc)) from exc",
    "label": true
  },
  {
    "code": "def _isnumber_with_thousands_separator(string):\n    try:\n        string = string.decode()\n    except (UnicodeDecodeError, AttributeError):\n        pass\n    return bool(re.match(_float_with_thousands_separators, string))",
    "label": true
  },
  {
    "code": "def pytest_ignore_collect(collection_path: Path, config: Config) -> Optional[bool]:\n    ignore_paths = config._getconftest_pathlist('collect_ignore', path=collection_path.parent, rootpath=config.rootpath)\n    ignore_paths = ignore_paths or []\n    excludeopt = config.getoption('ignore')\n    if excludeopt:\n        ignore_paths.extend((absolutepath(x) for x in excludeopt))\n    if collection_path in ignore_paths:\n        return True\n    ignore_globs = config._getconftest_pathlist('collect_ignore_glob', path=collection_path.parent, rootpath=config.rootpath)\n    ignore_globs = ignore_globs or []\n    excludeglobopt = config.getoption('ignore_glob')\n    if excludeglobopt:\n        ignore_globs.extend((absolutepath(x) for x in excludeglobopt))\n    if any((fnmatch.fnmatch(str(collection_path), str(glob)) for glob in ignore_globs)):\n        return True\n    allow_in_venv = config.getoption('collect_in_virtualenv')\n    if not allow_in_venv and _in_venv(collection_path):\n        return True\n    if collection_path.is_dir():\n        norecursepatterns = config.getini('norecursedirs')\n        if any((fnmatch_ex(pat, collection_path) for pat in norecursepatterns)):\n            return True\n    return None",
    "label": true
  },
  {
    "code": "def _get_with_identifier(mapping: Mapping[str, V], identifier: str, default: D) -> Union[D, V]:\n    if identifier in mapping:\n        return mapping[identifier]\n    name, open_bracket, _ = identifier.partition('[')\n    if open_bracket and name in mapping:\n        return mapping[name]\n    return default",
    "label": true
  },
  {
    "code": "def process_event_history(log: dict[str, list[dict]], customer_list: list[Customer]) -> None:\n    billing_date = datetime.datetime.strptime(log['events'][0]['time'], '%Y-%m-%d %H:%M:%S')\n    billing_month = billing_date.month\n    new_month(customer_list, billing_date.month, billing_date.year)\n    for event_data in log['events']:\n        if event_data['type'] == 'sms':\n            continue\n        src_nm = event_data['src_number']\n        dst_nm = event_data['dst_number']\n        date = datetime.datetime.strptime(event_data['time'], '%Y-%m-%d %H:%M:%S')\n        if date.month != billing_month:\n            billing_month = date.month\n            new_month(customer_list, date.month, date.year)\n        duration = event_data['duration']\n        src_loc = event_data['src_loc']\n        dst_loc = event_data['dst_loc']\n        call = Call(src_nm, dst_nm, date, duration, src_loc, dst_loc)\n        find_customer_by_number(src_nm, customer_list).make_call(call)\n        find_customer_by_number(dst_nm, customer_list).receive_call(call)",
    "label": true
  },
  {
    "code": "def prepare_crash_report(ex: Exception, filepath: str, crash_file_path: str) -> Path:\n    issue_template_path = (Path(PYLINT_HOME) / datetime.now().strftime(str(crash_file_path))).resolve()\n    with open(filepath, encoding='utf8') as f:\n        file_content = f.read()\n    template = ''\n    if not issue_template_path.exists():\n        template = 'First, please verify that the bug is not already filled:\\nhttps://github.com/pylint-dev/pylint/issues/\\n\\nThen create a new issue:\\nhttps://github.com/pylint-dev/pylint/issues/new?labels=Crash \ud83d\udca5%2CNeeds triage \ud83d\udce5\\n\\n\\n'\n    template += f'\\nIssue title:\\nCrash ``{ex}`` (if possible, be more specific about what made pylint crash)\\nContent:\\nWhen parsing the following file:\\n\\n<!--\\n If sharing the code is not an option, please state so,\\n but providing only the stacktrace would still be helpful.\\n -->\\n\\n```python\\n{file_content}\\n```\\n\\npylint crashed with a ``{ex.__class__.__name__}`` and with the following stacktrace:\\n```\\n'\n    template += traceback.format_exc()\n    template += '```\\n'\n    try:\n        with open(issue_template_path, 'a', encoding='utf8') as f:\n            f.write(template)\n    except Exception as exc:\n        print(f\"Can't write the issue template for the crash in {issue_template_path} because of: '{exc}'\\nHere's the content anyway:\\n{template}.\", file=sys.stderr)\n    return issue_template_path",
    "label": true
  },
  {
    "code": "def get_legacy_build_wheel_path(names: List[str], temp_dir: str, name: str, command_args: List[str], command_output: str) -> Optional[str]:\n    names = sorted(names)\n    if not names:\n        msg = 'Legacy build of wheel for {!r} created no files.\\n'.format(name)\n        msg += format_command_result(command_args, command_output)\n        logger.warning(msg)\n        return None\n    if len(names) > 1:\n        msg = 'Legacy build of wheel for {!r} created more than one file.\\nFilenames (choosing first): {}\\n'.format(name, names)\n        msg += format_command_result(command_args, command_output)\n        logger.warning(msg)\n    return os.path.join(temp_dir, names[0])",
    "label": true
  },
  {
    "code": "def _pick_get_win_folder() -> Callable[[str], str]:\n    if hasattr(ctypes, 'windll'):\n        return get_win_folder_via_ctypes\n    try:\n        import winreg\n    except ImportError:\n        return get_win_folder_from_env_vars\n    else:\n        return get_win_folder_from_registry",
    "label": true
  },
  {
    "code": "def split_before(iterable, pred, maxsplit=-1):\n    if maxsplit == 0:\n        yield list(iterable)\n        return\n    buf = []\n    it = iter(iterable)\n    for item in it:\n        if pred(item) and buf:\n            yield buf\n            if maxsplit == 1:\n                yield ([item] + list(it))\n                return\n            buf = []\n            maxsplit -= 1\n        buf.append(item)\n    if buf:\n        yield buf",
    "label": true
  },
  {
    "code": "def has_leading_dir(paths: Iterable[str]) -> bool:\n    common_prefix = None\n    for path in paths:\n        prefix, rest = split_leading_dir(path)\n        if not prefix:\n            return False\n        elif common_prefix is None:\n            common_prefix = prefix\n        elif prefix != common_prefix:\n            return False\n    return True",
    "label": true
  },
  {
    "code": "def _get_python_type_of_node(node: nodes.NodeNG) -> str | None:\n    pytype: Callable[[], str] | None = getattr(node, 'pytype', None)\n    if callable(pytype):\n        return pytype()\n    return None",
    "label": true
  },
  {
    "code": "def _write_contents(target, source):\n    child = target.joinpath(source.name)\n    if source.is_dir():\n        child.mkdir()\n        for item in source.iterdir():\n            _write_contents(child, item)\n    else:\n        child.write_bytes(source.read_bytes())\n    return child",
    "label": true
  },
  {
    "code": "def traverse_node(node: ast.AST) -> Iterator[ast.AST]:\n    yield node\n    for child in ast.iter_child_nodes(node):\n        yield from traverse_node(child)",
    "label": true
  },
  {
    "code": "def _restore_modules(unpickler, main_module):\n    try:\n        for modname, name in main_module.__dict__.pop('__dill_imported'):\n            main_module.__dict__[name] = unpickler.find_class(modname, name)\n        for modname, objname, name in main_module.__dict__.pop('__dill_imported_as'):\n            main_module.__dict__[name] = unpickler.find_class(modname, objname)\n        for modname, name in main_module.__dict__.pop('__dill_imported_top_level'):\n            main_module.__dict__[name] = __import__(modname)\n    except KeyError:\n        pass",
    "label": true
  },
  {
    "code": "def _windowsconsoleio_workaround(stream: TextIO) -> None:\n    if not sys.platform.startswith('win32') or hasattr(sys, 'pypy_version_info'):\n        return\n    if not hasattr(stream, 'buffer'):\n        return\n    buffered = hasattr(stream.buffer, 'raw')\n    raw_stdout = stream.buffer.raw if buffered else stream.buffer\n    if not isinstance(raw_stdout, io._WindowsConsoleIO):\n        return\n\n    def _reopen_stdio(f, mode):\n        if not buffered and mode[0] == 'w':\n            buffering = 0\n        else:\n            buffering = -1\n        return io.TextIOWrapper(open(os.dup(f.fileno()), mode, buffering), f.encoding, f.errors, f.newlines, f.line_buffering)\n    sys.stdin = _reopen_stdio(sys.stdin, 'rb')\n    sys.stdout = _reopen_stdio(sys.stdout, 'wb')\n    sys.stderr = _reopen_stdio(sys.stderr, 'wb')",
    "label": true
  },
  {
    "code": "def guess_lexer_for_filename(_fn, _text, **options):\n    fn = basename(_fn)\n    primary = {}\n    matching_lexers = set()\n    for lexer in _iter_lexerclasses():\n        for filename in lexer.filenames:\n            if _fn_matches(fn, filename):\n                matching_lexers.add(lexer)\n                primary[lexer] = True\n        for filename in lexer.alias_filenames:\n            if _fn_matches(fn, filename):\n                matching_lexers.add(lexer)\n                primary[lexer] = False\n    if not matching_lexers:\n        raise ClassNotFound('no lexer for filename %r found' % fn)\n    if len(matching_lexers) == 1:\n        return matching_lexers.pop()(**options)\n    result = []\n    for lexer in matching_lexers:\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        result.append((rv, lexer))\n\n    def type_sort(t):\n        return (t[0], primary[t[1]], t[1].priority, t[1].__name__)\n    result.sort(key=type_sort)\n    return result[-1][1](**options)",
    "label": true
  },
  {
    "code": "def _assert_local(filepath: _Path, root_dir: str):\n    if Path(os.path.abspath(root_dir)) not in Path(os.path.abspath(filepath)).parents:\n        msg = f'Cannot access {filepath!r} (or anything outside {root_dir!r})'\n        raise DistutilsOptionError(msg)\n    return True",
    "label": true
  },
  {
    "code": "def decompress_file(in_file: str, out_file: str) -> None:\n    with open(in_file, 'rb') as f:\n        num_nodes = f.read(1)[0]\n        buf = f.read(num_nodes * 4)\n        node_lst = bytes_to_nodes(buf)\n        tree = generate_tree_general(node_lst, num_nodes - 1)\n        size = bytes_to_int(f.read(4))\n        with open(out_file, 'wb') as g:\n            text = f.read()\n            g.write(decompress_bytes(tree, text, size))",
    "label": true
  },
  {
    "code": "def _handle_no_binary(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(value, existing.no_binary, existing.only_binary)",
    "label": true
  },
  {
    "code": "def _is_linux_armhf(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return f is not None and f.capacity == EIClass.C32 and (f.encoding == EIData.Lsb) and (f.machine == EMachine.Arm) and (f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5) and (f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD)",
    "label": true
  },
  {
    "code": "def iter_decode(input, fallback_encoding, errors='replace'):\n    decoder = IncrementalDecoder(fallback_encoding, errors)\n    generator = _iter_decode_generator(input, decoder)\n    encoding = next(generator)\n    return (generator, encoding)",
    "label": true
  },
  {
    "code": "def multi_substitution(*substitutions):\n    substitutions = itertools.starmap(substitution, substitutions)\n    substitutions = reversed(tuple(substitutions))\n    return compose(*substitutions)",
    "label": true
  },
  {
    "code": "def warn_explicit_for(method: FunctionType, message: PytestWarning) -> None:\n    lineno = method.__code__.co_firstlineno\n    filename = inspect.getfile(method)\n    module = method.__module__\n    mod_globals = method.__globals__\n    try:\n        warnings.warn_explicit(message, type(message), filename=filename, module=module, registry=mod_globals.setdefault('__warningregistry__', {}), lineno=lineno)\n    except Warning as w:\n        raise type(w)(f'{w}\\n at {filename}:{lineno}') from None",
    "label": true
  },
  {
    "code": "def load_file(filepath: _Path) -> dict:\n    from setuptools.extern import tomli\n    with open(filepath, 'rb') as file:\n        return tomli.load(file)",
    "label": true
  },
  {
    "code": "def original_text_for(expr: ParserElement, as_string: bool=True, *, asString: bool=True) -> ParserElement:\n    asString = asString and as_string\n    locMarker = Empty().set_parse_action(lambda s, loc, t: loc)\n    endlocMarker = locMarker.copy()\n    endlocMarker.callPreparse = False\n    matchExpr = locMarker('_original_start') + expr + endlocMarker('_original_end')\n    if asString:\n        extractText = lambda s, l, t: s[t._original_start:t._original_end]\n    else:\n\n        def extractText(s, l, t):\n            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]\n    matchExpr.set_parse_action(extractText)\n    matchExpr.ignoreExprs = expr.ignoreExprs\n    matchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)\n    return matchExpr",
    "label": true
  },
  {
    "code": "def ensure_deletable(path: Path, consider_lock_dead_if_created_before: float) -> bool:\n    if path.is_symlink():\n        return False\n    lock = get_lock_path(path)\n    try:\n        if not lock.is_file():\n            return True\n    except OSError:\n        return False\n    try:\n        lock_time = lock.stat().st_mtime\n    except Exception:\n        return False\n    else:\n        if lock_time < consider_lock_dead_if_created_before:\n            with contextlib.suppress(OSError):\n                lock.unlink()\n                return True\n        return False",
    "label": true
  },
  {
    "code": "def _as_list(value: str) -> List[str]:\n    if isinstance(value, list):\n        return [item.strip() for item in value]\n    filtered = [item.strip() for item in value.replace('\\n', ',').split(',') if item.strip()]\n    return filtered",
    "label": true
  },
  {
    "code": "def _infer_sequence_helper(node: _BaseContainerT, context: InferenceContext | None=None) -> list[SuccessfulInferenceResult]:\n    values = []\n    for elt in node.elts:\n        if isinstance(elt, nodes.Starred):\n            starred = helpers.safe_infer(elt.value, context)\n            if not starred:\n                raise InferenceError(node=node, context=context)\n            if not hasattr(starred, 'elts'):\n                raise InferenceError(node=node, context=context)\n            values.extend(_infer_sequence_helper(starred))\n        elif isinstance(elt, nodes.NamedExpr):\n            value = helpers.safe_infer(elt.value, context)\n            if not value:\n                raise InferenceError(node=node, context=context)\n            values.append(value)\n        else:\n            values.append(elt)\n    return values",
    "label": true
  },
  {
    "code": "def _msvc14_get_vc_env(plat_spec):\n    if 'DISTUTILS_USE_SDK' in environ:\n        return {key.lower(): value for key, value in environ.items()}\n    vcvarsall, vcruntime = _msvc14_find_vcvarsall(plat_spec)\n    if not vcvarsall:\n        raise distutils.errors.DistutilsPlatformError('Unable to find vcvarsall.bat')\n    try:\n        out = subprocess.check_output('cmd /u /c \"{}\" {} && set'.format(vcvarsall, plat_spec), stderr=subprocess.STDOUT).decode('utf-16le', errors='replace')\n    except subprocess.CalledProcessError as exc:\n        raise distutils.errors.DistutilsPlatformError('Error executing {}'.format(exc.cmd)) from exc\n    env = {key.lower(): value for key, _, value in (line.partition('=') for line in out.splitlines()) if key and value}\n    if vcruntime:\n        env['py_vcruntime_redist'] = vcruntime\n    return env",
    "label": true
  },
  {
    "code": "def _find_config_in_home_or_environment() -> Iterator[Path]:\n    if 'PYLINTRC' in os.environ and Path(os.environ['PYLINTRC']).exists():\n        if Path(os.environ['PYLINTRC']).is_file():\n            yield Path(os.environ['PYLINTRC']).resolve()\n    else:\n        try:\n            user_home = Path.home()\n        except RuntimeError:\n            user_home = None\n        if user_home is not None and str(user_home) not in ('~', '/root'):\n            home_rc = user_home / '.pylintrc'\n            if home_rc.is_file():\n                yield home_rc.resolve()\n            home_rc = user_home / '.config' / 'pylintrc'\n            if home_rc.is_file():\n                yield home_rc.resolve()",
    "label": true
  },
  {
    "code": "def _get_encoding_from_headers(headers: ResponseHeaders) -> Optional[str]:\n    if headers and 'Content-Type' in headers:\n        m = email.message.Message()\n        m['content-type'] = headers['Content-Type']\n        charset = m.get_param('charset')\n        if charset:\n            return str(charset)\n    return None",
    "label": true
  },
  {
    "code": "def make_option_group(group: Dict[str, Any], parser: ConfigOptionParser) -> OptionGroup:\n    option_group = OptionGroup(parser, group['name'])\n    for option in group['options']:\n        option_group.add_option(option())\n    return option_group",
    "label": true
  },
  {
    "code": "def group(fit: bool=True) -> Callable[..., Callable[..., Group]]:\n\n    def decorator(method: Callable[..., Iterable[RenderableType]]) -> Callable[..., Group]:\n        \"\"\"Convert a method that returns an iterable of renderables in to a Group.\"\"\"\n\n        @wraps(method)\n        def _replace(*args: Any, **kwargs: Any) -> Group:\n            renderables = method(*args, **kwargs)\n            return Group(*renderables, fit=fit)\n        return _replace\n    return decorator",
    "label": true
  },
  {
    "code": "def build_wheel(wheel_directory, config_settings, metadata_directory=None):\n    prebuilt_whl = _find_already_built_wheel(metadata_directory)\n    if prebuilt_whl:\n        shutil.copy2(prebuilt_whl, wheel_directory)\n        return os.path.basename(prebuilt_whl)\n    return _build_backend().build_wheel(wheel_directory, config_settings, metadata_directory)",
    "label": true
  },
  {
    "code": "def type_check_full(func: callable, args: list, checker_function: callable) -> tuple[bool, object]:\n    try:\n        args_copy = deepcopy(args)\n        returned = func(*args_copy)\n    except Exception as exn:\n        return (False, error_message(func, args, exn))\n    return checker_function(returned)",
    "label": true
  },
  {
    "code": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s",
    "label": true
  },
  {
    "code": "def ask_path_exists(message: str, options: Iterable[str]) -> str:\n    for action in os.environ.get('PIP_EXISTS_ACTION', '').split():\n        if action in options:\n            return action\n    return ask(message, options)",
    "label": true
  },
  {
    "code": "def _extract_version_from_fragment(fragment: str, canonical_name: str) -> Optional[str]:\n    try:\n        version_start = _find_name_version_sep(fragment, canonical_name) + 1\n    except ValueError:\n        return None\n    version = fragment[version_start:]\n    if not version:\n        return None\n    return version",
    "label": true
  },
  {
    "code": "def join_continuation(lines):\n    lines = iter(lines)\n    for item in lines:\n        while item.endswith('\\\\'):\n            try:\n                item = item[:-2].strip() + next(lines)\n            except StopIteration:\n                return\n        yield item",
    "label": true
  },
  {
    "code": "def _get_argument_suggestions(arg: Any, annotation: type) -> str:\n    try:\n        if isinstance(arg, type) and issubclass(arg, annotation):\n            return 'Did you mean {cls}(...) instead of {cls}?'.format(cls=arg.__name__)\n    except TypeError:\n        pass\n    return ''",
    "label": true
  },
  {
    "code": "def _create_stringi(value, position, closed):\n    f = StringIO(value)\n    if closed:\n        f.close()\n    else:\n        f.seek(position)\n    return f",
    "label": true
  },
  {
    "code": "def get_int_opt(options, optname, default=None):\n    string = options.get(optname, default)\n    try:\n        return int(string)\n    except TypeError:\n        raise OptionError('Invalid type %r for option %s; you must give an integer value' % (string, optname))\n    except ValueError:\n        raise OptionError('Invalid value %r for option %s; you must give an integer value' % (string, optname))",
    "label": true
  },
  {
    "code": "def _regex_transformer(value: str) -> Pattern[str]:\n    try:\n        return re.compile(value)\n    except re.error as e:\n        msg = f'Error in provided regular expression: {value} beginning at index {e.pos}: {e.msg}'\n        raise argparse.ArgumentTypeError(msg) from e",
    "label": true
  },
  {
    "code": "def _warn_if_mismatch(old: pathlib.Path, new: pathlib.Path, *, key: str) -> bool:\n    if old == new:\n        return False\n    _warn_mismatched(old, new, key=key)\n    return True",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    parser.addoption('--lsof', action='store_true', dest='lsof', default=False, help='Run FD checks if lsof is available')\n    parser.addoption('--runpytest', default='inprocess', dest='runpytest', choices=('inprocess', 'subprocess'), help=\"Run pytest sub runs in tests using an 'inprocess' or 'subprocess' (python -m main) method\")\n    parser.addini('pytester_example_dir', help='Directory to take the pytester example files from')",
    "label": true
  },
  {
    "code": "def vendored(modulename):\n    vendored_name = '{0}.{1}'.format(__name__, modulename)\n    try:\n        __import__(modulename, globals(), locals(), level=0)\n    except ImportError:\n        pass\n    else:\n        sys.modules[vendored_name] = sys.modules[modulename]\n        base, head = vendored_name.rsplit('.', 1)\n        setattr(sys.modules[base], head, sys.modules[modulename])",
    "label": true
  },
  {
    "code": "def result_invoke(action):\n\n    def wrap(func):\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            result = func(*args, **kwargs)\n            action(result)\n            return result\n        return wrapper\n    return wrap",
    "label": true
  },
  {
    "code": "def test_numpy():\n    try:\n        from numpy import array\n        x = array([1, 2, 3])\n        assert getimportable(x) == 'from numpy import array\\narray([1, 2, 3])\\n'\n        assert getimportable(array) == 'from %s import array\\n' % array.__module__\n        assert getimportable(x, byname=False) == 'from numpy import array\\narray([1, 2, 3])\\n'\n        assert getimportable(array, byname=False) == 'from %s import array\\n' % array.__module__\n    except ImportError:\n        pass",
    "label": true
  },
  {
    "code": "def guess_content_type(filename, default='application/octet-stream'):\n    if filename:\n        return mimetypes.guess_type(filename)[0] or default\n    return default",
    "label": true
  },
  {
    "code": "def read_values(base, key):\n    try:\n        handle = RegOpenKeyEx(base, key)\n    except RegError:\n        return None\n    d = {}\n    i = 0\n    while True:\n        try:\n            name, value, type = RegEnumValue(handle, i)\n        except RegError:\n            break\n        name = name.lower()\n        d[convert_mbcs(name)] = convert_mbcs(value)\n        i += 1\n    return d",
    "label": true
  },
  {
    "code": "def _get_glibc_version():\n    import platform\n    ver = platform.libc_ver()\n    result = []\n    if ver[0] == 'glibc':\n        for s in ver[1].split('.'):\n            result.append(int(s) if s.isdigit() else 0)\n        result = tuple(result)\n    return result",
    "label": true
  },
  {
    "code": "def _get_config_var(name: str, warn: bool=False) -> Union[int, str, None]:\n    value: Union[int, str, None] = sysconfig.get_config_var(name)\n    if value is None and warn:\n        logger.debug(\"Config variable '%s' is unset, Python ABI tag may be incorrect\", name)\n    return value",
    "label": true
  },
  {
    "code": "def find_lexer_class(name):\n    if name in _lexer_cache:\n        return _lexer_cache[name]\n    for module_name, lname, aliases, _, _ in LEXERS.values():\n        if name == lname:\n            _load_lexers(module_name)\n            return _lexer_cache[name]\n    for cls in find_plugin_lexers():\n        if cls.name == name:\n            return cls",
    "label": true
  },
  {
    "code": "def _escape_regex_range_chars(s: str) -> str:\n    for c in '\\\\^-[]':\n        s = s.replace(c, _bslash + c)\n    s = s.replace('\\n', '\\\\n')\n    s = s.replace('\\t', '\\\\t')\n    return str(s)",
    "label": true
  },
  {
    "code": "def process_options(arglist=None, parse_argv=False, config_file=None, parser=None, verbose=None):\n    if not parser:\n        parser = get_parser()\n    if not parser.has_option('--config'):\n        group = parser.add_option_group('Configuration', description='The project options are read from the [%s] section of the tox.ini file or the setup.cfg file located in any parent folder of the path(s) being processed.  Allowed options are: %s.' % (parser.prog, ', '.join(parser.config_options)))\n        group.add_option('--config', metavar='path', default=config_file, help='user config file location')\n    if not arglist and (not parse_argv):\n        arglist = []\n    options, args = parser.parse_args(arglist)\n    options.reporter = None\n    if verbose is not None:\n        options.verbose = verbose\n    if parse_argv and (not args):\n        if options.diff or any((os.path.exists(name) for name in PROJECT_CONFIG)):\n            args = ['.']\n        else:\n            parser.error('input not specified')\n    options = read_config(options, args, arglist, parser)\n    options.reporter = parse_argv and options.quiet == 1 and FileReport\n    options.filename = _parse_multi_options(options.filename)\n    options.exclude = normalize_paths(options.exclude)\n    options.select = _parse_multi_options(options.select)\n    options.ignore = _parse_multi_options(options.ignore)\n    if options.diff:\n        options.reporter = DiffReport\n        stdin = stdin_get_value()\n        options.selected_lines = parse_udiff(stdin, options.filename, args[0])\n        args = sorted(options.selected_lines)\n    return (options, args)",
    "label": true
  },
  {
    "code": "def test_all_labels():\n    for label in LABELS:\n        assert decode(b'', label) == ('', lookup(label))\n        assert encode('', label) == b''\n        for repeat in [0, 1, 12]:\n            output, _ = iter_decode([b''] * repeat, label)\n            assert list(output) == []\n            assert list(iter_encode([''] * repeat, label)) == []\n        decoder = IncrementalDecoder(label)\n        assert decoder.decode(b'') == ''\n        assert decoder.decode(b'', final=True) == ''\n        encoder = IncrementalEncoder(label)\n        assert encoder.encode('') == b''\n        assert encoder.encode('', final=True) == b''\n    for name in set(LABELS.values()):\n        assert lookup(name).name == name",
    "label": true
  },
  {
    "code": "def before_sleep_log(logger: 'logging.Logger', log_level: int, exc_info: bool=False) -> typing.Callable[['RetryCallState'], None]:\n\n    def log_it(retry_state: 'RetryCallState') -> None:\n        local_exc_info: BaseException | bool | None\n        if retry_state.outcome is None:\n            raise RuntimeError('log_it() called before outcome was set')\n        if retry_state.next_action is None:\n            raise RuntimeError('log_it() called before next_action was set')\n        if retry_state.outcome.failed:\n            ex = retry_state.outcome.exception()\n            verb, value = ('raised', f'{ex.__class__.__name__}: {ex}')\n            if exc_info:\n                local_exc_info = retry_state.outcome.exception()\n            else:\n                local_exc_info = False\n        else:\n            verb, value = ('returned', retry_state.outcome.result())\n            local_exc_info = False\n        if retry_state.fn is None:\n            fn_name = '<unknown>'\n        else:\n            fn_name = _utils.get_callback_name(retry_state.fn)\n        logger.log(log_level, f'Retrying {fn_name} in {retry_state.next_action.sleep} seconds as it {verb} {value}.', exc_info=local_exc_info)\n    return log_it",
    "label": true
  },
  {
    "code": "def extract_suffixes(iter: Iterable[PurePath], prefix: str) -> Iterator[str]:\n    p_len = len(prefix)\n    for p in iter:\n        yield p.name[p_len:]",
    "label": true
  },
  {
    "code": "def rewrite_traceback_stack(source: t.Optional[str]=None) -> BaseException:\n    _, exc_value, tb = sys.exc_info()\n    exc_value = t.cast(BaseException, exc_value)\n    tb = t.cast(TracebackType, tb)\n    if isinstance(exc_value, TemplateSyntaxError) and (not exc_value.translated):\n        exc_value.translated = True\n        exc_value.source = source\n        exc_value.with_traceback(None)\n        tb = fake_traceback(exc_value, None, exc_value.filename or '<unknown>', exc_value.lineno)\n    else:\n        tb = tb.tb_next\n    stack = []\n    while tb is not None:\n        if tb.tb_frame.f_code in internal_code:\n            tb = tb.tb_next\n            continue\n        template = tb.tb_frame.f_globals.get('__jinja_template__')\n        if template is not None:\n            lineno = template.get_corresponding_lineno(tb.tb_lineno)\n            fake_tb = fake_traceback(exc_value, tb, template.filename, lineno)\n            stack.append(fake_tb)\n        else:\n            stack.append(tb)\n        tb = tb.tb_next\n    tb_next = None\n    for tb in reversed(stack):\n        tb.tb_next = tb_next\n        tb_next = tb\n    return exc_value.with_traceback(tb_next)",
    "label": true
  },
  {
    "code": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    tags = set()\n    interpreters, abis, platforms = tag.split('-')\n    for interpreter in interpreters.split('.'):\n        for abi in abis.split('.'):\n            for platform_ in platforms.split('.'):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)",
    "label": true
  },
  {
    "code": "def from_package(package: types.ModuleType):\n    spec = wrap_spec(package)\n    reader = spec.loader.get_resource_reader(spec.name)\n    return reader.files()",
    "label": true
  },
  {
    "code": "def resolve_collection_argument(invocation_path: Path, arg: str, *, as_pypath: bool=False) -> Tuple[Path, List[str]]:\n    base, squacket, rest = str(arg).partition('[')\n    strpath, *parts = base.split('::')\n    if parts:\n        parts[-1] = f'{parts[-1]}{squacket}{rest}'\n    if as_pypath:\n        strpath = search_pypath(strpath)\n    fspath = invocation_path / strpath\n    fspath = absolutepath(fspath)\n    if not safe_exists(fspath):\n        msg = 'module or package not found: {arg} (missing __init__.py?)' if as_pypath else 'file or directory not found: {arg}'\n        raise UsageError(msg.format(arg=arg))\n    if parts and fspath.is_dir():\n        msg = 'package argument cannot contain :: selection parts: {arg}' if as_pypath else 'directory argument cannot contain :: selection parts: {arg}'\n        raise UsageError(msg.format(arg=arg))\n    return (fspath, parts)",
    "label": true
  },
  {
    "code": "def infer_str(node, context: InferenceContext | None=None):\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault('TypeError: str() must take no keyword arguments')\n    try:\n        return nodes.Const('')\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc",
    "label": true
  },
  {
    "code": "def parse_tag(tag: str) -> FrozenSet[Tag]:\n    tags = set()\n    interpreters, abis, platforms = tag.split('-')\n    for interpreter in interpreters.split('.'):\n        for abi in abis.split('.'):\n            for platform_ in platforms.split('.'):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)",
    "label": true
  },
  {
    "code": "def _find_name_version_sep(fragment: str, canonical_name: str) -> int:\n    for i, c in enumerate(fragment):\n        if c != '-':\n            continue\n        if canonicalize_name(fragment[:i]) == canonical_name:\n            return i\n    raise ValueError(f'{fragment} does not match {canonical_name}')",
    "label": true
  },
  {
    "code": "def ask_input(message: str) -> str:\n    _check_no_input(message)\n    return input(message)",
    "label": true
  },
  {
    "code": "def _safe_segment(segment):\n    segment = re.sub('[^A-Za-z0-9.]+', '-', segment)\n    segment = re.sub('-[^A-Za-z0-9]+', '-', segment)\n    return re.sub('\\\\.[^A-Za-z0-9]+', '.', segment).strip('.-')",
    "label": true
  },
  {
    "code": "def generate_tree_postorder(node_lst: list[ReadNode], root_index: int) -> HuffmanTree:\n    trees = []\n    for i in range(root_index + 1):\n        node = node_lst[i]\n        tree = _generate_tree_from_node(node)\n        if node.r_type == 1:\n            tree.right = trees.pop(-1)\n        if node.l_type == 1:\n            tree.left = trees.pop(-1)\n        trees.append(tree)\n    return trees[-1]",
    "label": true
  },
  {
    "code": "def decide_user_install(use_user_site: Optional[bool], prefix_path: Optional[str]=None, target_dir: Optional[str]=None, root_path: Optional[str]=None, isolated_mode: bool=False) -> bool:\n    if use_user_site is not None and (not use_user_site):\n        logger.debug('Non-user install by explicit request')\n        return False\n    if use_user_site:\n        if prefix_path:\n            raise CommandError(\"Can not combine '--user' and '--prefix' as they imply different installation locations\")\n        if virtualenv_no_global():\n            raise InstallationError(\"Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\")\n        logger.debug('User install by explicit request')\n        return True\n    assert use_user_site is None\n    if prefix_path or target_dir:\n        logger.debug('Non-user install due to --prefix or --target option')\n        return False\n    if not site.ENABLE_USER_SITE:\n        logger.debug('Non-user install because user site-packages disabled')\n        return False\n    if site_packages_writable(root=root_path, isolated=isolated_mode):\n        logger.debug('Non-user install because site-packages writeable')\n        return False\n    logger.info('Defaulting to user installation because normal site-packages is not writeable')\n    return True",
    "label": true
  },
  {
    "code": "def warn_if_run_as_root() -> None:\n    if running_under_virtualenv():\n        return\n    if not hasattr(os, 'getuid'):\n        return\n    if sys.platform == 'win32' or sys.platform == 'cygwin':\n        return\n    if os.getuid() != 0:\n        return\n    logger.warning(\"Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\")",
    "label": true
  },
  {
    "code": "def _wrapper(args: Optional[List[str]]=None) -> int:\n    sys.stderr.write(\"WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\\n\")\n    return main(args)",
    "label": true
  },
  {
    "code": "def make_setuptools_develop_args(setup_py_path: str, *, global_options: Sequence[str], no_user_config: bool, prefix: Optional[str], home: Optional[str], use_user_site: bool) -> List[str]:\n    assert not (use_user_site and prefix)\n    args = make_setuptools_shim_args(setup_py_path, global_options=global_options, no_user_config=no_user_config)\n    args += ['develop', '--no-deps']\n    if prefix:\n        args += ['--prefix', prefix]\n    if home is not None:\n        args += ['--install-dir', home]\n    if use_user_site:\n        args += ['--user', '--prefix=']\n    return args",
    "label": true
  },
  {
    "code": "def _detect_bom(input):\n    if input.startswith(b'\\xff\\xfe'):\n        return (_UTF16LE, input[2:])\n    if input.startswith(b'\\xfe\\xff'):\n        return (_UTF16BE, input[2:])\n    if input.startswith(b'\\xef\\xbb\\xbf'):\n        return (UTF8, input[3:])\n    return (None, input)",
    "label": true
  },
  {
    "code": "def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n    for mark in item.iter_markers(name='xfail'):\n        run = mark.kwargs.get('run', True)\n        strict = mark.kwargs.get('strict', item.config.getini('xfail_strict'))\n        raises = mark.kwargs.get('raises', None)\n        if 'condition' not in mark.kwargs:\n            conditions = mark.args\n        else:\n            conditions = (mark.kwargs['condition'],)\n        if not conditions:\n            reason = mark.kwargs.get('reason', '')\n            return Xfail(reason, run, strict, raises)\n        for condition in conditions:\n            result, reason = evaluate_condition(item, mark, condition)\n            if result:\n                return Xfail(reason, run, strict, raises)\n    return None",
    "label": true
  },
  {
    "code": "def date(raw: str) -> Date:\n    value = parse_rfc3339(raw)\n    if not isinstance(value, _datetime.date):\n        raise ValueError('date() only accepts date strings.')\n    return item(value)",
    "label": true
  },
  {
    "code": "def replace(iterable, pred, substitutes, count=None, window_size=1):\n    if window_size < 1:\n        raise ValueError('window_size must be at least 1')\n    substitutes = tuple(substitutes)\n    it = chain(iterable, [_marker] * (window_size - 1))\n    windows = windowed(it, window_size)\n    n = 0\n    for w in windows:\n        if pred(*w):\n            if count is None or n < count:\n                n += 1\n                yield from substitutes\n                consume(windows, window_size - 1)\n                continue\n        if w and w[0] is not _marker:\n            yield w[0]",
    "label": true
  },
  {
    "code": "def test_metaclass():\n\n    class metaclass_with_new(type):\n\n        def __new__(mcls, name, bases, ns, **kwds):\n            cls = super().__new__(mcls, name, bases, ns, **kwds)\n            assert mcls is not None\n            assert cls.method(mcls)\n            return cls\n\n        def method(cls, mcls):\n            return isinstance(cls, mcls)\n    l = locals()\n    exec('class subclass_with_new(metaclass=metaclass_with_new):\\n        def __new__(cls):\\n            self = super().__new__(cls)\\n            return self', None, l)\n    subclass_with_new = l['subclass_with_new']\n    assert dill.copy(subclass_with_new())",
    "label": true
  },
  {
    "code": "def socket_timeout(timeout=15):\n\n    def _socket_timeout(func):\n\n        def _socket_timeout(*args, **kwargs):\n            old_timeout = socket.getdefaulttimeout()\n            socket.setdefaulttimeout(timeout)\n            try:\n                return func(*args, **kwargs)\n            finally:\n                socket.setdefaulttimeout(old_timeout)\n        return _socket_timeout\n    return _socket_timeout",
    "label": true
  },
  {
    "code": "def when_imported(name):\n\n    def register(hook):\n        register_post_import_hook(hook, name)\n        return hook\n    return register",
    "label": true
  },
  {
    "code": "def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:\n    tuple_ = _encode_range(int_, 0)\n    pos = bisect.bisect_left(ranges, tuple_)\n    if pos > 0:\n        left, right = _decode_range(ranges[pos - 1])\n        if left <= int_ < right:\n            return True\n    if pos < len(ranges):\n        left, _ = _decode_range(ranges[pos])\n        if left == int_:\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _find_spec_with_path(search_path: Sequence[str], modname: str, module_parts: list[str], processed: list[str], submodule_path: Sequence[str] | None) -> tuple[Finder | _MetaPathFinder, ModuleSpec]:\n    for finder in _SPEC_FINDERS:\n        finder_instance = finder(search_path)\n        spec = finder_instance.find_module(modname, module_parts, processed, submodule_path)\n        if spec is None:\n            continue\n        return (finder_instance, spec)\n    for meta_finder in sys.meta_path:\n        meta_finder_name = meta_finder.__class__.__name__\n        if meta_finder_name not in _MetaPathFinderModuleTypes:\n            try:\n                meta_finder_name = meta_finder.__name__\n            except AttributeError:\n                continue\n            if meta_finder_name not in _MetaPathFinderModuleTypes:\n                continue\n        module_type = _MetaPathFinderModuleTypes[meta_finder_name]\n        if not hasattr(meta_finder, 'find_spec'):\n            continue\n        spec = meta_finder.find_spec(modname, submodule_path)\n        if spec:\n            return (meta_finder, ModuleSpec(spec.name, module_type, spec.origin, spec.origin, spec.submodule_search_locations))\n    raise ImportError(f\"No module named {'.'.join(module_parts)}\")",
    "label": true
  },
  {
    "code": "def alabel(label: str) -> bytes:\n    try:\n        label_bytes = label.encode('ascii')\n        ulabel(label_bytes)\n        if not valid_label_length(label_bytes):\n            raise IDNAError('Label too long')\n        return label_bytes\n    except UnicodeEncodeError:\n        pass\n    if not label:\n        raise IDNAError('No Input')\n    label = str(label)\n    check_label(label)\n    label_bytes = _punycode(label)\n    label_bytes = _alabel_prefix + label_bytes\n    if not valid_label_length(label_bytes):\n        raise IDNAError('Label too long')\n    return label_bytes",
    "label": true
  },
  {
    "code": "def _is_relative_to(self: Path, *other: Path) -> bool:\n    try:\n        self.relative_to(*other)\n        return True\n    except ValueError:\n        return False",
    "label": true
  },
  {
    "code": "def write_stub(resource, pyfile):\n    _stub_template = textwrap.dedent('\\n        def __bootstrap__():\\n            global __bootstrap__, __loader__, __file__\\n            import sys, pkg_resources, importlib.util\\n            __file__ = pkg_resources.resource_filename(__name__, %r)\\n            __loader__ = None; del __bootstrap__, __loader__\\n            spec = importlib.util.spec_from_file_location(__name__,__file__)\\n            mod = importlib.util.module_from_spec(spec)\\n            spec.loader.exec_module(mod)\\n        __bootstrap__()\\n        ').lstrip()\n    with open(pyfile, 'w') as f:\n        f.write(_stub_template % resource)",
    "label": true
  },
  {
    "code": "def split_into(iterable, sizes):\n    it = iter(iterable)\n    for size in sizes:\n        if size is None:\n            yield list(it)\n            return\n        else:\n            yield list(islice(it, size))",
    "label": true
  },
  {
    "code": "def resolve(module_name, dotted_path):\n    if module_name in sys.modules:\n        mod = sys.modules[module_name]\n    else:\n        mod = __import__(module_name)\n    if dotted_path is None:\n        result = mod\n    else:\n        parts = dotted_path.split('.')\n        result = getattr(mod, parts.pop(0))\n        for p in parts:\n            result = getattr(result, p)\n    return result",
    "label": true
  },
  {
    "code": "def write_json(obj, path, **kwargs):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(obj, f, **kwargs)",
    "label": true
  },
  {
    "code": "def rlocate(iterable, pred=bool, window_size=None):\n    if window_size is None:\n        try:\n            len_iter = len(iterable)\n            return (len_iter - i - 1 for i in locate(reversed(iterable), pred))\n        except TypeError:\n            pass\n    return reversed(list(locate(iterable, pred, window_size)))",
    "label": true
  },
  {
    "code": "def encode_multipart_formdata(fields, boundary=None):\n    body = BytesIO()\n    if boundary is None:\n        boundary = choose_boundary()\n    for field in iter_field_objects(fields):\n        body.write(b('--%s\\r\\n' % boundary))\n        writer(body).write(field.render_headers())\n        data = field.data\n        if isinstance(data, int):\n            data = str(data)\n        if isinstance(data, six.text_type):\n            writer(body).write(data)\n        else:\n            body.write(data)\n        body.write(b'\\r\\n')\n    body.write(b('--%s--\\r\\n' % boundary))\n    content_type = str('multipart/form-data; boundary=%s' % boundary)\n    return (body.getvalue(), content_type)",
    "label": true
  },
  {
    "code": "def register_arguments(func: nodes.FunctionDef, args: list | None=None) -> None:\n    if args is None:\n        if func.args.vararg:\n            func.set_local(func.args.vararg, func.args)\n        if func.args.kwarg:\n            func.set_local(func.args.kwarg, func.args)\n        args = func.args.args\n        if args is None:\n            return\n    for arg in args:\n        if isinstance(arg, nodes.AssignName):\n            func.set_local(arg.name, arg)\n        else:\n            register_arguments(func, arg.elts)",
    "label": true
  },
  {
    "code": "def modpath_from_file_with_callback(filename: str, path: Sequence[str] | None=None, is_package_cb: Callable[[str, list[str]], bool] | None=None) -> list[str]:\n    filename = os.path.expanduser(_path_from_filename(filename))\n    paths_to_check = sys.path.copy()\n    if path:\n        paths_to_check += path\n    for pathname in itertools.chain(paths_to_check, map(_cache_normalize_path, paths_to_check)):\n        if not pathname:\n            continue\n        modpath = _get_relative_base_path(filename, pathname)\n        if not modpath:\n            continue\n        assert is_package_cb is not None\n        if is_package_cb(pathname, modpath[:-1]):\n            return modpath\n    raise ImportError('Unable to find module for {} in {}'.format(filename, ', \\n'.join(sys.path)))",
    "label": true
  },
  {
    "code": "def zip_equal(*iterables):\n    if hexversion >= 50987174:\n        warnings.warn('zip_equal will be removed in a future version of more-itertools. Use the builtin zip function with strict=True instead.', DeprecationWarning)\n    try:\n        first_size = len(iterables[0])\n        for i, it in enumerate(iterables[1:], 1):\n            size = len(it)\n            if size != first_size:\n                break\n        else:\n            return zip(*iterables)\n        raise UnequalIterablesError(details=(first_size, i, size))\n    except TypeError:\n        return _zip_equal_generator(iterables)",
    "label": true
  },
  {
    "code": "def _hash_of_file(path: str, algorithm: str) -> str:\n    with open(path, 'rb') as archive:\n        hash = hashlib.new(algorithm)\n        for chunk in read_chunks(archive):\n            hash.update(chunk)\n    return hash.hexdigest()",
    "label": true
  },
  {
    "code": "def _is_key_file_encrypted(key_file: str) -> bool:\n    with open(key_file) as f:\n        for line in f:\n            if 'ENCRYPTED' in line:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def create_customers(log: dict[str, list[dict]]) -> list[Customer]:\n    customer_list = []\n    for cust in log['customers']:\n        customer = Customer(cust['id'])\n        for line in cust['lines']:\n            contract = Contract(datetime.datetime.now())\n            contract.new_month = lambda *args: None\n            contract.bill_call = lambda *args: None\n            contract = None\n            if line['contract'] == 'prepaid':\n                contract = PrepaidContract(datetime.date(2017, 12, 25), 100)\n            elif line['contract'] == 'mtm':\n                contract = MTMContract(datetime.date(2017, 12, 25))\n            elif line['contract'] == 'term':\n                contract = TermContract(datetime.date(2017, 12, 25), datetime.date(2019, 6, 25))\n            else:\n                print('ERROR: unknown contract type')\n            line = PhoneLine(line['number'], contract)\n            customer.add_phone_line(line)\n        customer_list.append(customer)\n    return customer_list",
    "label": true
  },
  {
    "code": "def find_spec(module, paths):\n    finder = importlib.machinery.PathFinder().find_spec if isinstance(paths, list) else importlib.util.find_spec\n    return finder(module, paths)",
    "label": true
  },
  {
    "code": "def init_register_ending_setters(source_code):\n    ending_transformer = TransformVisitor()\n    for node_class in nodes.ALL_NODE_CLASSES:\n        ending_transformer.register_transform(node_class, fix_start_attributes, lambda node: getattr(node, 'fromlineno', None) is None or getattr(node, 'col_offset', None) is None)\n    ending_transformer.register_transform(nodes.BinOp, _set_start_from_first_child)\n    ending_transformer.register_transform(nodes.ClassDef, _set_start_from_first_decorator)\n    ending_transformer.register_transform(nodes.FunctionDef, _set_start_from_first_decorator)\n    ending_transformer.register_transform(nodes.Tuple, _set_start_from_first_child)\n    ending_transformer.register_transform(nodes.Arguments, fix_arguments(source_code))\n    ending_transformer.register_transform(nodes.Slice, fix_slice(source_code))\n    for node_class in NODES_WITHOUT_CHILDREN:\n        ending_transformer.register_transform(node_class, set_without_children)\n    for node_class in NODES_WITH_CHILDREN:\n        ending_transformer.register_transform(node_class, set_from_last_child)\n    ending_transformer.register_transform(nodes.Subscript, fix_subscript(source_code))\n    for node_class, start_pred, end_pred in NODES_REQUIRING_SOURCE:\n        if start_pred is not None:\n            ending_transformer.register_transform(node_class, start_setter_from_source(source_code, start_pred))\n        if end_pred is not None:\n            if node_class is nodes.Tuple:\n                ending_transformer.register_transform(node_class, end_setter_from_source(source_code, end_pred, True))\n            else:\n                ending_transformer.register_transform(node_class, end_setter_from_source(source_code, end_pred))\n    ending_transformer.register_transform(nodes.BinOp, add_parens(source_code))\n    ending_transformer.register_transform(nodes.Const, add_parens(source_code))\n    ending_transformer.register_transform(nodes.Tuple, add_parens(source_code))\n    return ending_transformer",
    "label": true
  },
  {
    "code": "def _infer_object__new__decorator_check(node) -> bool:\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, nodes.Attribute):\n            if decorator.as_string() == OBJECT_DUNDER_NEW:\n                return True\n    return False",
    "label": true
  },
  {
    "code": "def _prepareconfig(args: Optional[Union[List[str], 'os.PathLike[str]']]=None, plugins: Optional[Sequence[Union[str, _PluggyPlugin]]]=None) -> 'Config':\n    if args is None:\n        args = sys.argv[1:]\n    elif isinstance(args, os.PathLike):\n        args = [os.fspath(args)]\n    elif not isinstance(args, list):\n        msg = '`args` parameter expected to be a list of strings, got: {!r} (type: {})'\n        raise TypeError(msg.format(args, type(args)))\n    config = get_config(args, plugins)\n    pluginmanager = config.pluginmanager\n    try:\n        if plugins:\n            for plugin in plugins:\n                if isinstance(plugin, str):\n                    pluginmanager.consider_pluginarg(plugin)\n                else:\n                    pluginmanager.register(plugin)\n        config = pluginmanager.hook.pytest_cmdline_parse(pluginmanager=pluginmanager, args=args)\n        return config\n    except BaseException:\n        config._ensure_unconfigure()\n        raise",
    "label": true
  },
  {
    "code": "def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]:\n    if report.when in ('setup', 'teardown'):\n        if report.failed:\n            return ('error', 'E', 'ERROR')\n        elif report.skipped:\n            return ('skipped', 's', 'SKIPPED')\n        else:\n            return ('', '', '')\n    return None",
    "label": true
  },
  {
    "code": "def edit_config(filename, settings, dry_run=False):\n    log.debug('Reading configuration from %s', filename)\n    opts = configparser.RawConfigParser()\n    opts.optionxform = lambda x: x\n    opts.read([filename])\n    for section, options in settings.items():\n        if options is None:\n            log.info('Deleting section [%s] from %s', section, filename)\n            opts.remove_section(section)\n        else:\n            if not opts.has_section(section):\n                log.debug('Adding new section [%s] to %s', section, filename)\n                opts.add_section(section)\n            for option, value in options.items():\n                if value is None:\n                    log.debug('Deleting %s.%s from %s', section, option, filename)\n                    opts.remove_option(section, option)\n                    if not opts.options(section):\n                        log.info('Deleting empty [%s] section from %s', section, filename)\n                        opts.remove_section(section)\n                else:\n                    log.debug('Setting %s.%s to %r in %s', section, option, value, filename)\n                    opts.set(section, option, value)\n    log.info('Writing %s', filename)\n    if not dry_run:\n        with open(filename, 'w') as f:\n            opts.write(f)",
    "label": true
  },
  {
    "code": "def test_load_module_asdict():\n    with TestNamespace():\n        session_buffer = BytesIO()\n        dill.dump_module(session_buffer)\n        global empty, names, x, y\n        x = y = 0\n        del empty\n        globals_state = globals().copy()\n        session_buffer.seek(0)\n        main_vars = dill.load_module_asdict(session_buffer)\n        assert main_vars is not globals()\n        assert globals() == globals_state\n        assert main_vars['__name__'] == '__main__'\n        assert main_vars['names'] == names\n        assert main_vars['names'] is not names\n        assert main_vars['x'] != x\n        assert 'y' not in main_vars\n        assert 'empty' in main_vars",
    "label": true
  },
  {
    "code": "def get_filter_by_name(filtername, **options):\n    cls = find_filter_class(filtername)\n    if cls:\n        return cls(**options)\n    else:\n        raise ClassNotFound('filter %r not found' % filtername)",
    "label": true
  },
  {
    "code": "def teardown_module():\n    if os.path.exists(fname):\n        os.remove(fname)",
    "label": true
  },
  {
    "code": "def padded(iterable, fillvalue=None, n=None, next_multiple=False):\n    it = iter(iterable)\n    if n is None:\n        yield from chain(it, repeat(fillvalue))\n    elif n < 1:\n        raise ValueError('n must be at least 1')\n    else:\n        item_count = 0\n        for item in it:\n            yield item\n            item_count += 1\n        remaining = (n - item_count) % n if next_multiple else n - item_count\n        for _ in range(remaining):\n            yield fillvalue",
    "label": true
  },
  {
    "code": "def canonicalize_version(version: Union[Version, str]) -> str:\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            return version\n    else:\n        parsed = version\n    parts = []\n    if parsed.epoch != 0:\n        parts.append(f'{parsed.epoch}!')\n    parts.append(re.sub('(\\\\.0)+$', '', '.'.join((str(x) for x in parsed.release))))\n    if parsed.pre is not None:\n        parts.append(''.join((str(x) for x in parsed.pre)))\n    if parsed.post is not None:\n        parts.append(f'.post{parsed.post}')\n    if parsed.dev is not None:\n        parts.append(f'.dev{parsed.dev}')\n    if parsed.local is not None:\n        parts.append(f'+{parsed.local}')\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def _builtin_filter_predicate(node, builtin_name) -> bool:\n    if builtin_name == 'type' and node.root().name == 're' and isinstance(node.func, nodes.Name) and (node.func.name == 'type') and isinstance(node.parent, nodes.Assign) and (len(node.parent.targets) == 1) and isinstance(node.parent.targets[0], nodes.AssignName) and (node.parent.targets[0].name in {'Pattern', 'Match'}):\n        return False\n    if isinstance(node.func, nodes.Name) and node.func.name == builtin_name:\n        return True\n    if isinstance(node.func, nodes.Attribute):\n        return node.func.attrname == 'fromkeys' and isinstance(node.func.expr, nodes.Name) and (node.func.expr.name == 'dict')\n    return False",
    "label": true
  },
  {
    "code": "def _is_attr_name(s, index, node):\n    target_len = len(node.attrname)\n    if index < target_len:\n        return False\n    return s[index - target_len + 1:index + 1] == node.attrname",
    "label": true
  },
  {
    "code": "def _transform_url(url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]) -> Tuple[str, NetlocTuple]:\n    purl = urllib.parse.urlsplit(url)\n    netloc_tuple = transform_netloc(purl.netloc)\n    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)\n    surl = urllib.parse.urlunsplit(url_pieces)\n    return (surl, cast('NetlocTuple', netloc_tuple))",
    "label": true
  },
  {
    "code": "def rich_cast(renderable: object) -> 'RenderableType':\n    from pip._vendor.rich.console import RenderableType\n    rich_visited_set: Set[type] = set()\n    while hasattr(renderable, '__rich__') and (not isclass(renderable)):\n        if hasattr(renderable, _GIBBERISH):\n            return repr(renderable)\n        cast_method = getattr(renderable, '__rich__')\n        renderable = cast_method()\n        renderable_type = type(renderable)\n        if renderable_type in rich_visited_set:\n            break\n        rich_visited_set.add(renderable_type)\n    return cast(RenderableType, renderable)",
    "label": true
  },
  {
    "code": "def _remove_shim():\n    try:\n        sys.meta_path.remove(DISTUTILS_FINDER)\n    except ValueError:\n        pass",
    "label": true
  },
  {
    "code": "def _unescape(v):\n    i = 0\n    backslash = False\n    while i < len(v):\n        if backslash:\n            backslash = False\n            if v[i] in _escapes:\n                v = v[:i - 1] + _escape_to_escapedchars[v[i]] + v[i + 1:]\n            elif v[i] == '\\\\':\n                v = v[:i - 1] + v[i:]\n            elif v[i] == 'u' or v[i] == 'U':\n                i += 1\n            else:\n                raise ValueError('Reserved escape sequence used')\n            continue\n        elif v[i] == '\\\\':\n            backslash = True\n        i += 1\n    return v",
    "label": true
  },
  {
    "code": "def distributions_from_metadata(path):\n    root = os.path.dirname(path)\n    if os.path.isdir(path):\n        if len(os.listdir(path)) == 0:\n            return\n        metadata = PathMetadata(root, path)\n    else:\n        metadata = FileMetadata(path)\n    entry = os.path.basename(path)\n    yield Distribution.from_location(root, entry, metadata, precedence=DEVELOP_DIST)",
    "label": true
  },
  {
    "code": "def get_all_filters():\n    yield from FILTERS\n    for name, _ in find_plugin_filters():\n        yield name",
    "label": true
  },
  {
    "code": "def compress_bytes(text: bytes, codes: dict[int, str]) -> bytes:\n    bits = ''.join([codes[byte] for byte in text])\n    return bytes([int(bits[a:a + 8].ljust(8, '0'), 2) for a in range(0, len(bits), 8)])",
    "label": true
  },
  {
    "code": "def save_results(results: LinterStats, base: str) -> None:\n    from pylint.lint.caching import save_results as _real_save_results\n    warnings.warn(\"'pylint.config.save_results' is deprecated, please use 'pylint.lint.save_results' instead. This will be removed in 3.0.\", DeprecationWarning, stacklevel=2)\n    return _real_save_results(results, base, PYLINT_HOME)",
    "label": true
  },
  {
    "code": "def get_encoding_from_headers(headers):\n    content_type = headers.get('content-type')\n    if not content_type:\n        return None\n    content_type, params = _parse_content_type_header(content_type)\n    if 'charset' in params:\n        return params['charset'].strip('\\'\"')\n    if 'text' in content_type:\n        return 'ISO-8859-1'\n    if 'application/json' in content_type:\n        return 'utf-8'",
    "label": true
  },
  {
    "code": "def _get_editable_info(dist: BaseDistribution) -> _EditableInfo:\n    editable_project_location = dist.editable_project_location\n    assert editable_project_location\n    location = os.path.normcase(os.path.abspath(editable_project_location))\n    from pip._internal.vcs import RemoteNotFoundError, RemoteNotValidError, vcs\n    vcs_backend = vcs.get_backend_for_dir(location)\n    if vcs_backend is None:\n        display = _format_as_name_version(dist)\n        logger.debug('No VCS found for editable requirement \"%s\" in: %r', display, location)\n        return _EditableInfo(requirement=location, comments=[f'# Editable install with no version control ({display})'])\n    vcs_name = type(vcs_backend).__name__\n    try:\n        req = vcs_backend.get_src_requirement(location, dist.raw_name)\n    except RemoteNotFoundError:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(requirement=location, comments=[f'# Editable {vcs_name} install with no remote ({display})'])\n    except RemoteNotValidError as ex:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(requirement=location, comments=[f'# Editable {vcs_name} install ({display}) with either a deleted local remote or invalid URI:', f\"# '{ex.url}'\"])\n    except BadCommand:\n        logger.warning('cannot determine version of editable source in %s (%s command not found in path)', location, vcs_backend.name)\n        return _EditableInfo(requirement=location, comments=[])\n    except InstallationError as exc:\n        logger.warning('Error when trying to get requirement for VCS system %s', exc)\n    else:\n        return _EditableInfo(requirement=req, comments=[])\n    logger.warning('Could not determine repository location of %s', location)\n    return _EditableInfo(requirement=location, comments=['## !! Could not determine repository location'])",
    "label": true
  },
  {
    "code": "def binary_op_hints(op, args):\n    if op == '+':\n        if 'int' in args and 'str' in args:\n            return 'Perhaps you wanted to cast the integer into a string or vice versa?'",
    "label": true
  },
  {
    "code": "def supported_hashes(hashes: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:\n    if hashes is None:\n        return None\n    hashes = {n: v for n, v in hashes.items() if n in _SUPPORTED_HASHES}\n    if not hashes:\n        return None\n    return hashes",
    "label": true
  },
  {
    "code": "def _make_synonym_function(compat_name: str, fn: C) -> C:\n    fn = getattr(fn, '__func__', fn)\n    if 'self' == list(inspect.signature(fn).parameters)[0]:\n\n        @wraps(fn)\n        def _inner(self, *args, **kwargs):\n            return fn(self, *args, **kwargs)\n    else:\n\n        @wraps(fn)\n        def _inner(*args, **kwargs):\n            return fn(*args, **kwargs)\n    _inner.__doc__ = f'Deprecated - use :class:`{fn.__name__}`'\n    _inner.__name__ = compat_name\n    _inner.__annotations__ = fn.__annotations__\n    if isinstance(fn, types.FunctionType):\n        _inner.__kwdefaults__ = fn.__kwdefaults__\n    elif isinstance(fn, type) and hasattr(fn, '__init__'):\n        _inner.__kwdefaults__ = fn.__init__.__kwdefaults__\n    else:\n        _inner.__kwdefaults__ = None\n    _inner.__qualname__ = fn.__qualname__\n    return cast(C, _inner)",
    "label": true
  },
  {
    "code": "def _ascii_escaped_by_config(val: Union[str, bytes], config: Optional[Config]) -> str:\n    if config is None:\n        escape_option = False\n    else:\n        escape_option = config.getini('disable_test_id_escaping_and_forfeit_all_rights_to_community_support')\n    return val if escape_option else ascii_escaped(val)",
    "label": true
  },
  {
    "code": "def group(fit: bool=True) -> Callable[..., Callable[..., Group]]:\n\n    def decorator(method: Callable[..., Iterable[RenderableType]]) -> Callable[..., Group]:\n        \"\"\"Convert a method that returns an iterable of renderables in to a Group.\"\"\"\n\n        @wraps(method)\n        def _replace(*args: Any, **kwargs: Any) -> Group:\n            renderables = method(*args, **kwargs)\n            return Group(*renderables, fit=fit)\n        return _replace\n    return decorator",
    "label": true
  },
  {
    "code": "def _parse_glibc_version(version_str: str) -> Tuple[int, int]:\n    m = re.match('(?P<major>[0-9]+)\\\\.(?P<minor>[0-9]+)', version_str)\n    if not m:\n        warnings.warn(f'Expected glibc version with 2 components major.minor, got: {version_str}', RuntimeWarning)\n        return (-1, -1)\n    return (int(m.group('major')), int(m.group('minor')))",
    "label": true
  },
  {
    "code": "def get_loop_node(frame: types.FrameType) -> Union[astroid.For, astroid.While]:\n    func_string = inspect.cleandoc(inspect.getsource(frame))\n    with_stmt_index = inspect.getlineno(frame) - frame.f_code.co_firstlineno\n    lst_str_lines = func_string.splitlines()\n    lst_from_with_stmt = lst_str_lines[with_stmt_index + 1:]\n    num_whitespace = num_whitespaces(lst_str_lines[with_stmt_index])\n    with_lines = get_with_lines(lst_from_with_stmt, num_whitespace)\n    with_module = astroid.parse(with_lines)\n    for statement in with_module.nodes_of_class((astroid.For, astroid.While)):\n        if isinstance(statement, (astroid.For, astroid.While)):\n            return statement",
    "label": true
  },
  {
    "code": "def apply(transform):\n\n    def wrap(func):\n        return functools.wraps(func)(compose(transform, func))\n    return wrap",
    "label": true
  },
  {
    "code": "def expression(s: Scanner) -> ast.Expression:\n    if s.accept(TokenType.EOF):\n        ret: ast.expr = astNameConstant(False)\n    else:\n        ret = expr(s)\n        s.accept(TokenType.EOF, reject=True)\n    return ast.fix_missing_locations(ast.Expression(ret))",
    "label": true
  },
  {
    "code": "def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split('.')\n    assert urllib3_version != ['dev']\n    if len(urllib3_version) == 2:\n        urllib3_version.append('0')\n    major, minor, patch = urllib3_version\n    major, minor, patch = (int(major), int(minor), int(patch))\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n    if chardet_version:\n        major, minor, patch = chardet_version.split('.')[:3]\n        major, minor, patch = (int(major), int(minor), int(patch))\n        assert (3, 0, 2) <= (major, minor, patch) < (6, 0, 0)\n    elif charset_normalizer_version:\n        major, minor, patch = charset_normalizer_version.split('.')[:3]\n        major, minor, patch = (int(major), int(minor), int(patch))\n        assert (2, 0, 0) <= (major, minor, patch) < (4, 0, 0)\n    else:\n        raise Exception('You need either charset_normalizer or chardet installed')",
    "label": true
  },
  {
    "code": "def _self_version_check_logic(*, state: SelfCheckState, current_time: datetime.datetime, local_version: DistributionVersion, get_remote_version: Callable[[], Optional[str]]) -> Optional[UpgradePrompt]:\n    remote_version_str = state.get(current_time)\n    if remote_version_str is None:\n        remote_version_str = get_remote_version()\n        if remote_version_str is None:\n            logger.debug('No remote pip version found')\n            return None\n        state.set(remote_version_str, current_time)\n    remote_version = parse_version(remote_version_str)\n    logger.debug('Remote version of pip: %s', remote_version)\n    logger.debug('Local version of pip:  %s', local_version)\n    pip_installed_by_pip = was_installed_by_pip('pip')\n    logger.debug('Was pip installed by pip? %s', pip_installed_by_pip)\n    if not pip_installed_by_pip:\n        return None\n    local_version_is_older = local_version < remote_version and local_version.base_version != remote_version.base_version\n    if local_version_is_older:\n        return UpgradePrompt(old=str(local_version), new=remote_version_str)\n    return None",
    "label": true
  },
  {
    "code": "def remove_draw_parameter_from_composite_strategy(node):\n    del node.args.args[0]\n    del node.args.annotations[0]\n    del node.args.type_comment_args[0]\n    return node",
    "label": true
  },
  {
    "code": "def _normalize_host(host: str | None, scheme: str | None) -> str | None:\n    if host:\n        if scheme in _NORMALIZABLE_SCHEMES:\n            is_ipv6 = _IPV6_ADDRZ_RE.match(host)\n            if is_ipv6:\n                match = _ZONE_ID_RE.search(host)\n                if match:\n                    start, end = match.span(1)\n                    zone_id = host[start:end]\n                    if zone_id.startswith('%25') and zone_id != '%25':\n                        zone_id = zone_id[3:]\n                    else:\n                        zone_id = zone_id[1:]\n                    zone_id = _encode_invalid_chars(zone_id, _UNRESERVED_CHARS)\n                    return f'{host[:start].lower()}%{zone_id}{host[end:]}'\n                else:\n                    return host.lower()\n            elif not _IPV4_RE.match(host):\n                return to_str(b'.'.join([_idna_encode(label) for label in host.split('.')]), 'ascii')\n    return host",
    "label": true
  },
  {
    "code": "def patch_all(messages_config: dict):\n    patch_checkers()\n    patch_ast_transforms()\n    patch_messages()\n    patch_error_messages(messages_config)",
    "label": true
  },
  {
    "code": "def _fn_matches(fn, glob):\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)",
    "label": true
  },
  {
    "code": "def linux_distribution(full_distribution_name: bool=True) -> Tuple[str, str, str]:\n    warnings.warn(\"distro.linux_distribution() is deprecated. It should only be used as a compatibility shim with Python's platform.linux_distribution(). Please use distro.id(), distro.version() and distro.name() instead.\", DeprecationWarning, stacklevel=2)\n    return _distro.linux_distribution(full_distribution_name)",
    "label": true
  },
  {
    "code": "def from_path(path: Union[str, bytes, PathLike], steps: int=5, chunk_size: int=512, threshold: float=0.2, cp_isolation: Optional[List[str]]=None, cp_exclusion: Optional[List[str]]=None, preemptive_behaviour: bool=True, explain: bool=False, language_threshold: float=0.1, enable_fallback: bool=True) -> CharsetMatches:\n    with open(path, 'rb') as fp:\n        return from_fp(fp, steps, chunk_size, threshold, cp_isolation, cp_exclusion, preemptive_behaviour, explain, language_threshold, enable_fallback)",
    "label": true
  },
  {
    "code": "def _no_global_under_venv() -> bool:\n    cfg_lines = _get_pyvenv_cfg_lines()\n    if cfg_lines is None:\n        logger.warning(\"Could not access 'pyvenv.cfg' despite a virtual environment being active. Assuming global site-packages is not accessible in this environment.\")\n        return True\n    for line in cfg_lines:\n        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)\n        if match is not None and match.group('value') == 'false':\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def find_executable(executable, path=None):\n    _, ext = os.path.splitext(executable)\n    if sys.platform == 'win32' and ext != '.exe':\n        executable = executable + '.exe'\n    if os.path.isfile(executable):\n        return executable\n    if path is None:\n        path = os.environ.get('PATH', None)\n        if path is None:\n            try:\n                path = os.confstr('CS_PATH')\n            except (AttributeError, ValueError):\n                path = os.defpath\n    if not path:\n        return None\n    paths = path.split(os.pathsep)\n    for p in paths:\n        f = os.path.join(p, executable)\n        if os.path.isfile(f):\n            return f\n    return None",
    "label": true
  },
  {
    "code": "def match_to_datetime(match: re.Match) -> datetime | date:\n    year_str, month_str, day_str, hour_str, minute_str, sec_str, micros_str, zulu_time, offset_sign_str, offset_hour_str, offset_minute_str = match.groups()\n    year, month, day = (int(year_str), int(month_str), int(day_str))\n    if hour_str is None:\n        return date(year, month, day)\n    hour, minute, sec = (int(hour_str), int(minute_str), int(sec_str))\n    micros = int(micros_str.ljust(6, '0')) if micros_str else 0\n    if offset_sign_str:\n        tz: tzinfo | None = cached_tz(offset_hour_str, offset_minute_str, offset_sign_str)\n    elif zulu_time:\n        tz = timezone.utc\n    else:\n        tz = None\n    return datetime(year, month, day, hour, minute, sec, micros, tzinfo=tz)",
    "label": true
  },
  {
    "code": "def deselect_by_mark(items: 'List[Item]', config: Config) -> None:\n    matchexpr = config.option.markexpr\n    if not matchexpr:\n        return\n    expr = _parse_expression(matchexpr, \"Wrong expression passed to '-m'\")\n    remaining: List[Item] = []\n    deselected: List[Item] = []\n    for item in items:\n        if expr.evaluate(MarkMatcher.from_item(item)):\n            remaining.append(item)\n        else:\n            deselected.append(item)\n    if deselected:\n        config.hook.pytest_deselected(items=deselected)\n        items[:] = remaining",
    "label": true
  },
  {
    "code": "def _parse_glibc_version(version_str: str) -> Tuple[int, int]:\n    m = re.match('(?P<major>[0-9]+)\\\\.(?P<minor>[0-9]+)', version_str)\n    if not m:\n        warnings.warn('Expected glibc version with 2 components major.minor, got: %s' % version_str, RuntimeWarning)\n        return (-1, -1)\n    return (int(m.group('major')), int(m.group('minor')))",
    "label": true
  },
  {
    "code": "def _get_gid(name):\n    if getgrnam is None or name is None:\n        return None\n    try:\n        result = getgrnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None",
    "label": true
  },
  {
    "code": "def search_packages_info(query: List[str]) -> Generator[_PackageInfo, None, None]:\n    env = get_default_environment()\n    installed = {dist.canonical_name: dist for dist in env.iter_all_distributions()}\n    query_names = [canonicalize_name(name) for name in query]\n    missing = sorted([name for name, pkg in zip(query, query_names) if pkg not in installed])\n    if missing:\n        logger.warning('Package(s) not found: %s', ', '.join(missing))\n\n    def _get_requiring_packages(current_dist: BaseDistribution) -> Iterator[str]:\n        return (dist.metadata['Name'] or 'UNKNOWN' for dist in installed.values() if current_dist.canonical_name in {canonicalize_name(d.name) for d in dist.iter_dependencies()})\n    for query_name in query_names:\n        try:\n            dist = installed[query_name]\n        except KeyError:\n            continue\n        requires = sorted((req.name for req in dist.iter_dependencies()), key=str.lower)\n        required_by = sorted(_get_requiring_packages(dist), key=str.lower)\n        try:\n            entry_points_text = dist.read_text('entry_points.txt')\n            entry_points = entry_points_text.splitlines(keepends=False)\n        except FileNotFoundError:\n            entry_points = []\n        files_iter = dist.iter_declared_entries()\n        if files_iter is None:\n            files: Optional[List[str]] = None\n        else:\n            files = sorted(files_iter)\n        metadata = dist.metadata\n        yield _PackageInfo(name=dist.raw_name, version=str(dist.version), location=dist.location or '', editable_project_location=dist.editable_project_location, requires=requires, required_by=required_by, installer=dist.installer, metadata_version=dist.metadata_version or '', classifiers=metadata.get_all('Classifier', []), summary=metadata.get('Summary', ''), homepage=metadata.get('Home-page', ''), project_urls=metadata.get_all('Project-URL', []), author=metadata.get('Author', ''), author_email=metadata.get('Author-email', ''), license=metadata.get('License', ''), entry_points=entry_points, files=files)",
    "label": true
  },
  {
    "code": "def _copy_cookie_jar(jar):\n    if jar is None:\n        return None\n    if hasattr(jar, 'copy'):\n        return jar.copy()\n    new_jar = copy.copy(jar)\n    new_jar.clear()\n    for cookie in jar:\n        new_jar.set_cookie(copy.copy(cookie))\n    return new_jar",
    "label": true
  },
  {
    "code": "def glob2(dirname, pattern):\n    assert _isrecursive(pattern)\n    yield pattern[:0]\n    for x in _rlistdir(dirname):\n        yield x",
    "label": true
  },
  {
    "code": "def unicode_is_ascii(u_string):\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode('ascii')\n        return True\n    except UnicodeEncodeError:\n        return False",
    "label": true
  },
  {
    "code": "def builtin_lookup(name: str) -> tuple[nodes.Module, Sequence[nodes.NodeNG]]:\n    manager = AstroidManager()\n    try:\n        _builtin_astroid = manager.builtins_module\n    except KeyError:\n        manager.clear_cache()\n        _builtin_astroid = manager.builtins_module\n    if name == '__dict__':\n        return (_builtin_astroid, ())\n    try:\n        stmts: Sequence[nodes.NodeNG] = _builtin_astroid.locals[name]\n    except KeyError:\n        stmts = ()\n    return (_builtin_astroid, stmts)",
    "label": true
  },
  {
    "code": "def _warn_for_function(warning: Warning, function: Callable[..., object]) -> None:\n    func = cast(types.FunctionType, function)\n    warnings.warn_explicit(warning, type(warning), lineno=func.__code__.co_firstlineno, filename=func.__code__.co_filename)",
    "label": true
  },
  {
    "code": "def call_runtest_hook(item: Item, when: \"Literal['setup', 'call', 'teardown']\", **kwds) -> 'CallInfo[None]':\n    if when == 'setup':\n        ihook: Callable[..., None] = item.ihook.pytest_runtest_setup\n    elif when == 'call':\n        ihook = item.ihook.pytest_runtest_call\n    elif when == 'teardown':\n        ihook = item.ihook.pytest_runtest_teardown\n    else:\n        assert False, f'Unhandled runtest hook case: {when}'\n    reraise: Tuple[Type[BaseException], ...] = (Exit,)\n    if not item.config.getoption('usepdb', False):\n        reraise += (KeyboardInterrupt,)\n    return CallInfo.from_call(lambda: ihook(item=item, **kwds), when=when, reraise=reraise)",
    "label": true
  },
  {
    "code": "def f(func):\n\n    def w(*args):\n        return f(*args)\n    return w",
    "label": true
  },
  {
    "code": "def load_lexer_from_file(filename, lexername='CustomLexer', **options):\n    try:\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        if lexername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' % (lexername, filename))\n        lexer_class = custom_namespace[lexername]\n        return lexer_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom lexer: %s' % err)",
    "label": true
  },
  {
    "code": "def pass_context(f: F) -> F:\n    f.jinja_pass_arg = _PassArg.context\n    return f",
    "label": true
  },
  {
    "code": "def is_dill(pickler, child=None):\n    if child is False or not hasattr(pickler.__class__, 'mro'):\n        return 'dill' in pickler.__module__\n    return Pickler in pickler.__class__.mro()",
    "label": true
  },
  {
    "code": "def string_to_tokentype(s):\n    if isinstance(s, _TokenType):\n        return s\n    if not s:\n        return Token\n    node = Token\n    for item in s.split('.'):\n        node = getattr(node, item)\n    return node",
    "label": true
  },
  {
    "code": "def load_cdll(name, macos10_16_path):\n    try:\n        if version_info >= (10, 16):\n            path = macos10_16_path\n        else:\n            path = find_library(name)\n        if not path:\n            raise OSError\n        return CDLL(path, use_errno=True)\n    except OSError:\n        raise_from(ImportError('The library %s failed to load' % name), None)",
    "label": true
  },
  {
    "code": "def _mac_platforms(arch: str) -> List[str]:\n    match = _osx_arch_pat.match(arch)\n    if match:\n        name, major, minor, actual_arch = match.groups()\n        mac_version = (int(major), int(minor))\n        arches = ['{}_{}'.format(name, arch[len('macosx_'):]) for arch in mac_platforms(mac_version, actual_arch)]\n    else:\n        arches = [arch]\n    return arches",
    "label": true
  },
  {
    "code": "def get_win_folder_from_registry(csidl_name: str) -> str:\n    shell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': 'Common AppData', 'CSIDL_LOCAL_APPDATA': 'Local AppData', 'CSIDL_PERSONAL': 'Personal', 'CSIDL_DOWNLOADS': '{374DE290-123F-4565-9164-39C4925E467B}', 'CSIDL_MYPICTURES': 'My Pictures', 'CSIDL_MYVIDEO': 'My Video', 'CSIDL_MYMUSIC': 'My Music'}.get(csidl_name)\n    if shell_folder_name is None:\n        msg = f'Unknown CSIDL name: {csidl_name}'\n        raise ValueError(msg)\n    if sys.platform != 'win32':\n        raise NotImplementedError\n    import winreg\n    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, 'Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Explorer\\\\Shell Folders')\n    directory, _ = winreg.QueryValueEx(key, shell_folder_name)\n    return str(directory)",
    "label": true
  },
  {
    "code": "def resolve_proxies(request, proxies, trust_env=True):\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get('no_proxy')\n    new_proxies = proxies.copy()\n    if trust_env and (not should_bypass_proxies(url, no_proxy=no_proxy)):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n        proxy = environ_proxies.get(scheme, environ_proxies.get('all'))\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies",
    "label": true
  },
  {
    "code": "def parse_inline_table(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, dict]:\n    pos += 1\n    nested_dict = NestedDict()\n    flags = Flags()\n    pos = skip_chars(src, pos, TOML_WS)\n    if src.startswith('}', pos):\n        return (pos + 1, nested_dict.dict)\n    while True:\n        pos, key, value = parse_key_value_pair(src, pos, parse_float)\n        key_parent, key_stem = (key[:-1], key[-1])\n        if flags.is_(key, Flags.FROZEN):\n            raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {key}')\n        try:\n            nest = nested_dict.get_or_create_nest(key_parent, access_lists=False)\n        except KeyError:\n            raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n        if key_stem in nest:\n            raise suffixed_err(src, pos, f'Duplicate inline table key {key_stem!r}')\n        nest[key_stem] = value\n        pos = skip_chars(src, pos, TOML_WS)\n        c = src[pos:pos + 1]\n        if c == '}':\n            return (pos + 1, nested_dict.dict)\n        if c != ',':\n            raise suffixed_err(src, pos, 'Unclosed inline table')\n        if isinstance(value, (dict, list)):\n            flags.set(key, Flags.FROZEN, recursive=True)\n        pos += 1\n        pos = skip_chars(src, pos, TOML_WS)",
    "label": true
  },
  {
    "code": "def ichunked(iterable, n):\n    source = peekable(iter(iterable))\n    ichunk_marker = object()\n    while True:\n        item = source.peek(ichunk_marker)\n        if item is ichunk_marker:\n            return\n        chunk = _IChunk(source, n)\n        yield chunk\n        chunk.fill_cache()",
    "label": true
  },
  {
    "code": "def binary_search(comparable, target):\n    left = 0\n    right = len(comparable) - 1\n    while left <= right:\n        middle = (left + right) // 2\n        if comparable[middle] == target:\n            return middle\n        elif comparable[middle] < target:\n            left = middle\n        else:\n            right = middle\n    return -1",
    "label": true
  },
  {
    "code": "def _remove_bom(encoded: bytes, encoding: str) -> bytes:\n    if encoding not in UNICODE_BOMS:\n        return encoded\n    bom = UNICODE_BOMS[encoding]\n    if encoded.startswith(bom):\n        return encoded[len(bom):]\n    return encoded",
    "label": true
  },
  {
    "code": "def get_bin_prefix() -> str:\n    prefix = os.path.normpath(sys.prefix)\n    if WINDOWS:\n        bin_py = os.path.join(prefix, 'Scripts')\n        if not os.path.exists(bin_py):\n            bin_py = os.path.join(prefix, 'bin')\n        return bin_py\n    if sys.platform[:6] == 'darwin' and prefix[:16] == '/System/Library/':\n        return '/usr/local/bin'\n    return os.path.join(prefix, 'bin')",
    "label": true
  },
  {
    "code": "def product_star(integer_queue: Queue) -> int:\n    product = 1\n    nqueue = Queue()\n    while not integer_queue.is_empty():\n        a = integer_queue.dequeue()\n        product *= a\n        nqueue.enqueue(a)\n    while not nqueue.is_empty():\n        integer_queue.enqueue(nqueue.dequeue())\n    return product",
    "label": true
  },
  {
    "code": "def get_build_platform():\n    from sysconfig import get_platform\n    plat = get_platform()\n    if sys.platform == 'darwin' and (not plat.startswith('macosx-')):\n        try:\n            version = _macos_vers()\n            machine = os.uname()[4].replace(' ', '_')\n            return 'macosx-%d.%d-%s' % (int(version[0]), int(version[1]), _macos_arch(machine))\n        except ValueError:\n            pass\n    return plat",
    "label": true
  },
  {
    "code": "def issue_warning(*args, **kw):\n    level = 1\n    g = globals()\n    try:\n        while sys._getframe(level).f_globals is g:\n            level += 1\n    except ValueError:\n        pass\n    warnings.warn(*args, stacklevel=level + 1, **kw)",
    "label": true
  },
  {
    "code": "def first(iterable, default=_marker):\n    try:\n        return next(iter(iterable))\n    except StopIteration as e:\n        if default is _marker:\n            raise ValueError('first() was called on an empty iterable, and no default value was provided.') from e\n        return default",
    "label": true
  },
  {
    "code": "def pytest_cmdline_main(config: Config) -> Optional[Union[int, ExitCode]]:\n    if config.option.showfixtures:\n        showfixtures(config)\n        return 0\n    if config.option.show_fixtures_per_test:\n        show_fixtures_per_test(config)\n        return 0\n    return None",
    "label": true
  },
  {
    "code": "def _cpython_abis(py_version: PythonVersion, warn: bool=False) -> List[str]:\n    py_version = tuple(py_version)\n    abis = []\n    version = _version_nodot(py_version[:2])\n    debug = pymalloc = ucs4 = ''\n    with_debug = _get_config_var('Py_DEBUG', warn)\n    has_refcount = hasattr(sys, 'gettotalrefcount')\n    has_ext = '_d.pyd' in EXTENSION_SUFFIXES\n    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n        debug = 'd'\n    if py_version < (3, 8):\n        with_pymalloc = _get_config_var('WITH_PYMALLOC', warn)\n        if with_pymalloc or with_pymalloc is None:\n            pymalloc = 'm'\n        if py_version < (3, 3):\n            unicode_size = _get_config_var('Py_UNICODE_SIZE', warn)\n            if unicode_size == 4 or (unicode_size is None and sys.maxunicode == 1114111):\n                ucs4 = 'u'\n    elif debug:\n        abis.append(f'cp{version}')\n    abis.insert(0, 'cp{version}{debug}{pymalloc}{ucs4}'.format(version=version, debug=debug, pymalloc=pymalloc, ucs4=ucs4))\n    return abis",
    "label": true
  },
  {
    "code": "def guess_decode_from_terminal(text, term):\n    if getattr(term, 'encoding', None):\n        try:\n            text = text.decode(term.encoding)\n        except UnicodeDecodeError:\n            pass\n        else:\n            return (text, term.encoding)\n    return guess_decode(text)",
    "label": true
  },
  {
    "code": "def _get_codes_helper(tree: HuffmanTree, path: str) -> dict[int, str]:\n    if tree.is_leaf():\n        return {tree.symbol: path}\n    else:\n        left = _get_codes_helper(tree.left, path + '0')\n        right = _get_codes_helper(tree.right, path + '1')\n        left.update(right)\n        return left",
    "label": true
  },
  {
    "code": "def is_defined(name: str, node: nodes.NodeNG) -> bool:\n    is_defined_so_far = False\n    if isinstance(node, nodes.NamedExpr):\n        is_defined_so_far = node.target.name == name\n    if isinstance(node, (nodes.Import, nodes.ImportFrom)):\n        is_defined_so_far = any((node_name[0] == name for node_name in node.names))\n    if isinstance(node, nodes.With):\n        is_defined_so_far = any((isinstance(item[1], nodes.AssignName) and item[1].name == name for item in node.items))\n    if isinstance(node, (nodes.ClassDef, nodes.FunctionDef)):\n        is_defined_so_far = node.name == name\n    if isinstance(node, nodes.AnnAssign):\n        is_defined_so_far = node.value and isinstance(node.target, nodes.AssignName) and (node.target.name == name)\n    if isinstance(node, nodes.Assign):\n        is_defined_so_far = any((any((isinstance(elt, nodes.Starred) and isinstance(elt.value, nodes.AssignName) and (elt.value.name == name) or (isinstance(elt, nodes.AssignName) and elt.name == name) for elt in get_all_elements(target))) for target in node.targets))\n    return is_defined_so_far or any((is_defined(name, child) for child in node.get_children()))",
    "label": true
  },
  {
    "code": "def get_user() -> Optional[str]:\n    try:\n        import getpass\n        return getpass.getuser()\n    except (ImportError, KeyError):\n        return None",
    "label": true
  },
  {
    "code": "def add_metaclass(metaclass):\n\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper",
    "label": true
  },
  {
    "code": "def create_cleanup_lock(p: Path) -> Path:\n    lock_path = get_lock_path(p)\n    try:\n        fd = os.open(str(lock_path), os.O_WRONLY | os.O_CREAT | os.O_EXCL, 420)\n    except FileExistsError as e:\n        raise OSError(f'cannot create lockfile in {p}') from e\n    else:\n        pid = os.getpid()\n        spid = str(pid).encode()\n        os.write(fd, spid)\n        os.close(fd)\n        if not lock_path.is_file():\n            raise OSError('lock path got renamed after successful creation')\n        return lock_path",
    "label": true
  },
  {
    "code": "def get_provider(moduleOrReq):\n    if isinstance(moduleOrReq, Requirement):\n        return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    try:\n        module = sys.modules[moduleOrReq]\n    except KeyError:\n        __import__(moduleOrReq)\n        module = sys.modules[moduleOrReq]\n    loader = getattr(module, '__loader__', None)\n    return _find_adapter(_provider_factories, loader)(module)",
    "label": true
  },
  {
    "code": "def _fixup_find_links(find_links):\n    if isinstance(find_links, str):\n        return find_links.split()\n    assert isinstance(find_links, (tuple, list))\n    return find_links",
    "label": true
  },
  {
    "code": "def consume(iterator, n=None):\n    if n is None:\n        deque(iterator, maxlen=0)\n    else:\n        next(islice(iterator, n, n), None)",
    "label": true
  },
  {
    "code": "def patch_for_msvc_specialized_compiler():\n    msvc = import_module('setuptools.msvc')\n    if platform.system() != 'Windows':\n        return\n\n    def patch_params(mod_name, func_name):\n        \"\"\"\n        Prepare the parameters for patch_func to patch indicated function.\n        \"\"\"\n        repl_prefix = 'msvc14_'\n        repl_name = repl_prefix + func_name.lstrip('_')\n        repl = getattr(msvc, repl_name)\n        mod = import_module(mod_name)\n        if not hasattr(mod, func_name):\n            raise ImportError(func_name)\n        return (repl, mod, func_name)\n    msvc14 = functools.partial(patch_params, 'distutils._msvccompiler')\n    try:\n        patch_func(*msvc14('_get_vc_env'))\n    except ImportError:\n        pass",
    "label": true
  },
  {
    "code": "def assert_relative(path):\n    if not os.path.isabs(path):\n        return path\n    from distutils.errors import DistutilsSetupError\n    msg = textwrap.dedent('\\n        Error: setup script specifies an absolute path:\\n\\n            %s\\n\\n        setup() arguments must *always* be /-separated paths relative to the\\n        setup.py directory, *never* absolute paths.\\n        ').lstrip() % path\n    raise DistutilsSetupError(msg)",
    "label": true
  },
  {
    "code": "def interpreter_name() -> str:\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name",
    "label": true
  },
  {
    "code": "def _should_repr_global_name(obj: object) -> bool:\n    if callable(obj):\n        return False\n    try:\n        return not hasattr(obj, '__name__')\n    except Exception:\n        return True",
    "label": true
  },
  {
    "code": "def interpret(marker, execution_context=None):\n    try:\n        expr, rest = parse_marker(marker)\n    except Exception as e:\n        raise SyntaxError('Unable to interpret marker syntax: %s: %s' % (marker, e))\n    if rest and rest[0] != '#':\n        raise SyntaxError('unexpected trailing data in marker: %s: %s' % (marker, rest))\n    context = dict(DEFAULT_CONTEXT)\n    if execution_context:\n        context.update(execution_context)\n    return evaluator.evaluate(expr, context)",
    "label": true
  },
  {
    "code": "def allowed_gai_family() -> socket.AddressFamily:\n    family = socket.AF_INET\n    if HAS_IPV6:\n        family = socket.AF_UNSPEC\n    return family",
    "label": true
  },
  {
    "code": "def dist_from_wheel_url(name: str, url: str, session: PipSession) -> BaseDistribution:\n    with LazyZipOverHTTP(url, session) as zf:\n        wheel = MemoryWheel(zf.name, zf)\n        return get_wheel_distribution(wheel, canonicalize_name(name))",
    "label": true
  },
  {
    "code": "def _macos_vers(_cache=[]):\n    if not _cache:\n        version = platform.mac_ver()[0]\n        if version == '':\n            plist = '/System/Library/CoreServices/SystemVersion.plist'\n            if os.path.exists(plist):\n                if hasattr(plistlib, 'readPlist'):\n                    plist_content = plistlib.readPlist(plist)\n                    if 'ProductVersion' in plist_content:\n                        version = plist_content['ProductVersion']\n        _cache.append(version.split('.'))\n    return _cache[0]",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def set_build_number(wheel_file_content: bytes, build_number: str | None) -> bytes:\n    replacement = ('Build: %s\\r\\n' % build_number).encode('ascii') if build_number else b''\n    wheel_file_content, num_replaced = BUILD_NUM_RE.subn(replacement, wheel_file_content)\n    if not num_replaced:\n        wheel_file_content += replacement\n    return wheel_file_content",
    "label": true
  },
  {
    "code": "def _parse_rich_type_value(value: Any) -> str:\n    if isinstance(value, (list, tuple)):\n        return ','.join((_parse_rich_type_value(i) for i in value))\n    if isinstance(value, re.Pattern):\n        return str(value.pattern)\n    if isinstance(value, dict):\n        return ','.join((f'{k}:{v}' for k, v in value.items()))\n    return str(value)",
    "label": true
  },
  {
    "code": "def _toml_has_config(path: Path | str) -> bool:\n    with open(path, mode='rb') as toml_handle:\n        try:\n            content = tomllib.load(toml_handle)\n        except tomllib.TOMLDecodeError as error:\n            print(f\"Failed to load '{path}': {error}\")\n            return False\n    return 'pylint' in content.get('tool', [])",
    "label": true
  },
  {
    "code": "def dumps(data: Mapping, sort_keys: bool=False) -> str:\n    if not isinstance(data, Container) and isinstance(data, Mapping):\n        data = item(dict(data), _sort_keys=sort_keys)\n    try:\n        return data.as_string()\n    except AttributeError as ex:\n        msg = f'Expecting Mapping or TOML Container, {type(data)} given'\n        raise TypeError(msg) from ex",
    "label": true
  },
  {
    "code": "def in_venv():\n    if hasattr(sys, 'real_prefix'):\n        result = True\n    else:\n        result = sys.prefix != getattr(sys, 'base_prefix', sys.prefix)\n    return result",
    "label": true
  },
  {
    "code": "def _infer_getattr_args(node, context):\n    if len(node.args) not in (2, 3):\n        raise UseInferenceDefault\n    try:\n        obj = next(node.args[0].infer(context=context))\n        attr = next(node.args[1].infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if isinstance(obj, util.UninferableBase) or isinstance(attr, util.UninferableBase):\n        return (util.Uninferable, util.Uninferable)\n    is_string = isinstance(attr, nodes.Const) and isinstance(attr.value, str)\n    if not is_string:\n        raise UseInferenceDefault\n    return (obj, attr.value)",
    "label": true
  },
  {
    "code": "def safe_getattr(object: Any, name: str, default: Any) -> Any:\n    from _pytest.outcomes import TEST_OUTCOME\n    try:\n        return getattr(object, name, default)\n    except TEST_OUTCOME:\n        return default",
    "label": true
  },
  {
    "code": "def unregister_encoder(encoder: Encoder) -> None:\n    with contextlib.suppress(ValueError):\n        CUSTOM_ENCODERS.remove(encoder)",
    "label": true
  },
  {
    "code": "def skip_until(src: str, pos: Pos, expect: str, *, error_on: frozenset[str], error_on_eof: bool) -> Pos:\n    try:\n        new_pos = src.index(expect, pos)\n    except ValueError:\n        new_pos = len(src)\n        if error_on_eof:\n            raise suffixed_err(src, new_pos, f'Expected {expect!r}') from None\n    if not error_on.isdisjoint(src[pos:new_pos]):\n        while src[pos] not in error_on:\n            pos += 1\n        raise suffixed_err(src, pos, f'Found invalid character {src[pos]!r}')\n    return new_pos",
    "label": true
  },
  {
    "code": "def inject_into_urllib3():\n    util.SSLContext = SecureTransportContext\n    util.ssl_.SSLContext = SecureTransportContext\n    util.HAS_SNI = HAS_SNI\n    util.ssl_.HAS_SNI = HAS_SNI\n    util.IS_SECURETRANSPORT = True\n    util.ssl_.IS_SECURETRANSPORT = True",
    "label": true
  },
  {
    "code": "def check_legacy_setup_py_options(options: Values, reqs: List[InstallRequirement]) -> None:\n    has_build_options = _has_option(options, reqs, 'build_options')\n    has_global_options = _has_option(options, reqs, 'global_options')\n    if has_build_options or has_global_options:\n        deprecated(reason='--build-option and --global-option are deprecated.', issue=11859, replacement='to use --config-settings', gone_in='23.3')\n        logger.warning('Implying --no-binary=:all: due to the presence of --build-option / --global-option. ')\n        options.format_control.disallow_binaries()",
    "label": true
  },
  {
    "code": "def canonicalize_version(version: Union[Version, str]) -> str:\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            return version\n    else:\n        parsed = version\n    parts = []\n    if parsed.epoch != 0:\n        parts.append(f'{parsed.epoch}!')\n    parts.append(re.sub('(\\\\.0)+$', '', '.'.join((str(x) for x in parsed.release))))\n    if parsed.pre is not None:\n        parts.append(''.join((str(x) for x in parsed.pre)))\n    if parsed.post is not None:\n        parts.append(f'.post{parsed.post}')\n    if parsed.dev is not None:\n        parts.append(f'.dev{parsed.dev}')\n    if parsed.local is not None:\n        parts.append(f'+{parsed.local}')\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def compatible_tags(python_version: Optional[PythonVersion]=None, interpreter: Optional[str]=None, platforms: Optional[Iterable[str]]=None) -> Iterator[Tag]:\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, 'none', platform_)\n    if interpreter:\n        yield Tag(interpreter, 'none', 'any')\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, 'none', 'any')",
    "label": true
  },
  {
    "code": "def python_2_unicode_compatible(klass):\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied to %s because it doesn't define __str__().\" % klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass",
    "label": true
  },
  {
    "code": "def set_verbosity(v):\n    if v <= 0:\n        set_threshold(logging.WARN)\n    elif v == 1:\n        set_threshold(logging.INFO)\n    elif v >= 2:\n        set_threshold(logging.DEBUG)",
    "label": true
  },
  {
    "code": "def __setstate__(state):\n    g = globals()\n    for k, v in state.items():\n        g['_sset_' + _state_vars[k]](k, g[k], v)\n    return state",
    "label": true
  },
  {
    "code": "def remove_suffix(text, suffix):\n    rest, suffix, null = text.partition(suffix)\n    return rest",
    "label": true
  },
  {
    "code": "def _create_rlock(count, owner, *args):\n    lock = RLockType()\n    if owner is not None:\n        lock._acquire_restore((count, owner))\n    if owner and (not lock._is_owned()):\n        raise UnpicklingError('Cannot acquire lock')\n    return lock",
    "label": true
  },
  {
    "code": "def srange(s: str) -> str:\n    _expanded = lambda p: p if not isinstance(p, ParseResults) else ''.join((chr(c) for c in range(ord(p[0]), ord(p[1]) + 1)))\n    try:\n        return ''.join((_expanded(part) for part in _reBracketExpr.parse_string(s).body))\n    except Exception as e:\n        return ''",
    "label": true
  },
  {
    "code": "def _special_method_cache(method, cache_wrapper):\n    name = method.__name__\n    special_names = ('__getattr__', '__getitem__')\n    if name not in special_names:\n        return\n    wrapper_name = '__cached' + name\n\n    def proxy(self, *args, **kwargs):\n        if wrapper_name not in vars(self):\n            bound = types.MethodType(method, self)\n            cache = cache_wrapper(bound)\n            setattr(self, wrapper_name, cache)\n        else:\n            cache = getattr(self, wrapper_name)\n        return cache(*args, **kwargs)\n    return proxy",
    "label": true
  },
  {
    "code": "def _mkstemp(*args, **kw):\n    old_open = os.open\n    try:\n        os.open = os_open\n        return tempfile.mkstemp(*args, **kw)\n    finally:\n        os.open = old_open",
    "label": true
  },
  {
    "code": "def is_url(name: str) -> bool:\n    scheme = get_url_scheme(name)\n    if scheme is None:\n        return False\n    return scheme in ['http', 'https', 'file', 'ftp'] + vcs.all_schemes",
    "label": true
  },
  {
    "code": "def no_binary() -> Option:\n    format_control = FormatControl(set(), set())\n    return Option('--no-binary', dest='format_control', action='callback', callback=_handle_no_binary, type='str', default=format_control, help='Do not use binary packages. Can be supplied multiple times, and each time adds to the existing value. Accepts either \":all:\" to disable all binary packages, \":none:\" to empty the set (notice the colons), or one or more package names with commas between them (no colons). Note that some packages are tricky to compile and may fail to install when this option is used on them.')",
    "label": true
  },
  {
    "code": "def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:\n    tokenizer.consume('WS')\n    marker_var_left = _parse_marker_var(tokenizer)\n    tokenizer.consume('WS')\n    marker_op = _parse_marker_op(tokenizer)\n    tokenizer.consume('WS')\n    marker_var_right = _parse_marker_var(tokenizer)\n    tokenizer.consume('WS')\n    return (marker_var_left, marker_op, marker_var_right)",
    "label": true
  },
  {
    "code": "def _lookup_module(modmap, name, obj, main_module):\n    for modobj, modname in modmap.by_name[name]:\n        if modobj is obj and sys.modules[modname] is not main_module:\n            return (modname, name)\n    __module__ = getattr(obj, '__module__', None)\n    if isinstance(obj, IMPORTED_AS_TYPES) or (__module__ is not None and any((regex.fullmatch(__module__) for regex in IMPORTED_AS_MODULES))):\n        for modobj, objname, modname in modmap.by_id[id(obj)]:\n            if sys.modules[modname] is not main_module:\n                return (modname, objname)\n    return (None, None)",
    "label": true
  },
  {
    "code": "def _latex_row(cell_values, colwidths, colaligns, escrules=LATEX_ESCAPE_RULES):\n\n    def escape_char(c):\n        return escrules.get(c, c)\n    escaped_values = [''.join(map(escape_char, cell)) for cell in cell_values]\n    rowfmt = DataRow('', '&', '\\\\\\\\')\n    return _build_simple_row(escaped_values, rowfmt)",
    "label": true
  },
  {
    "code": "def array(raw: str=None) -> Array:\n    if raw is None:\n        raw = '[]'\n    return value(raw)",
    "label": true
  },
  {
    "code": "def load_extensions(environment: 'Environment', extensions: t.Sequence[t.Union[str, t.Type['Extension']]]) -> t.Dict[str, 'Extension']:\n    result = {}\n    for extension in extensions:\n        if isinstance(extension, str):\n            extension = t.cast(t.Type['Extension'], import_string(extension))\n        result[extension.identifier] = extension(environment)\n    return result",
    "label": true
  },
  {
    "code": "def unquote_unreserved(uri):\n    parts = uri.split('%')\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]\n            else:\n                parts[i] = f'%{parts[i]}'\n        else:\n            parts[i] = f'%{parts[i]}'\n    return ''.join(parts)",
    "label": true
  },
  {
    "code": "def _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n            if isinstance(lhs, Variable):\n                environment_key = lhs.value\n                lhs_value = environment[environment_key]\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                environment_key = rhs.value\n                rhs_value = environment[environment_key]\n            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in ['and', 'or']\n            if marker == 'or':\n                groups.append([])\n    return any((all(item) for item in groups))",
    "label": true
  },
  {
    "code": "def do_format(value: str, *args: t.Any, **kwargs: t.Any) -> str:\n    if args and kwargs:\n        raise FilterArgumentError(\"can't handle positional and keyword arguments at the same time\")\n    return soft_str(value) % (kwargs or args)",
    "label": true
  },
  {
    "code": "def key_value_rule(src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat) -> Pos:\n    pos, key, value = parse_key_value_pair(src, pos, parse_float)\n    key_parent, key_stem = (key[:-1], key[-1])\n    abs_key_parent = header + key_parent\n    relative_path_cont_keys = (header + key[:i] for i in range(1, len(key)))\n    for cont_key in relative_path_cont_keys:\n        if out.flags.is_(cont_key, Flags.EXPLICIT_NEST):\n            raise suffixed_err(src, pos, f'Cannot redefine namespace {cont_key}')\n        out.flags.add_pending(cont_key, Flags.EXPLICIT_NEST)\n    if out.flags.is_(abs_key_parent, Flags.FROZEN):\n        raise suffixed_err(src, pos, f'Cannot mutate immutable namespace {abs_key_parent}')\n    try:\n        nest = out.data.get_or_create_nest(abs_key_parent)\n    except KeyError:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value') from None\n    if key_stem in nest:\n        raise suffixed_err(src, pos, 'Cannot overwrite a value')\n    if isinstance(value, (dict, list)):\n        out.flags.set(header + key, Flags.FROZEN, recursive=True)\n    nest[key_stem] = value\n    return pos",
    "label": true
  },
  {
    "code": "def _has_option(options: Values, reqs: List[InstallRequirement], option: str) -> bool:\n    if getattr(options, option, None):\n        return True\n    for req in reqs:\n        if getattr(req, option, None):\n            return True\n    return False",
    "label": true
  },
  {
    "code": "def _license(dist: 'Distribution', val: dict, root_dir: _Path):\n    from setuptools.config import expand\n    if 'file' in val:\n        _set_config(dist, 'license', expand.read_files([val['file']], root_dir))\n        dist._referenced_files.add(val['file'])\n    else:\n        _set_config(dist, 'license', val['text'])",
    "label": true
  },
  {
    "code": "def today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n    return today > given",
    "label": true
  },
  {
    "code": "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    tokenizer.consume('WS')\n    name_token = tokenizer.expect('IDENTIFIER', expected='package name at the start of dependency specifier')\n    name = name_token.text\n    tokenizer.consume('WS')\n    extras = _parse_extras(tokenizer)\n    tokenizer.consume('WS')\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n    tokenizer.expect('END', expected='end of dependency specifier')\n    return ParsedRequirement(name, url, extras, specifier, marker)",
    "label": true
  },
  {
    "code": "def main(args: Optional[List[str]]=None) -> int:\n    if args is None:\n        args = sys.argv[1:]\n    warnings.filterwarnings(action='ignore', category=DeprecationWarning, module='.*pkg_resources')\n    deprecation.install_warning_logger()\n    autocomplete()\n    try:\n        cmd_name, cmd_args = parse_command(args)\n    except PipError as exc:\n        sys.stderr.write(f'ERROR: {exc}')\n        sys.stderr.write(os.linesep)\n        sys.exit(1)\n    try:\n        locale.setlocale(locale.LC_ALL, '')\n    except locale.Error as e:\n        logger.debug('Ignoring error %s when setting locale', e)\n    command = create_command(cmd_name, isolated='--isolated' in cmd_args)\n    return command.main(cmd_args)",
    "label": true
  },
  {
    "code": "def inject_into_urllib3():\n    _validate_dependencies_met()\n    util.SSLContext = PyOpenSSLContext\n    util.ssl_.SSLContext = PyOpenSSLContext\n    util.HAS_SNI = HAS_SNI\n    util.ssl_.HAS_SNI = HAS_SNI\n    util.IS_PYOPENSSL = True\n    util.ssl_.IS_PYOPENSSL = True",
    "label": true
  },
  {
    "code": "def _confidence_transformer(value: str) -> Sequence[str]:\n    if not value:\n        return interfaces.CONFIDENCE_LEVEL_NAMES\n    values = pylint_utils._check_csv(value)\n    for confidence in values:\n        if confidence not in interfaces.CONFIDENCE_LEVEL_NAMES:\n            raise argparse.ArgumentTypeError(f'{value} should be in {(*interfaces.CONFIDENCE_LEVEL_NAMES,)}')\n    return values",
    "label": true
  },
  {
    "code": "def pytest_addoption(parser: Parser) -> None:\n    group = parser.getgroup('debugconfig')\n    group.addoption('--setupplan', '--setup-plan', action='store_true', help=\"Show what fixtures and tests would be executed but don't execute anything\")",
    "label": true
  },
  {
    "code": "def get_style_by_name(name):\n    if name in STYLE_MAP:\n        mod, cls = STYLE_MAP[name].split('::')\n        builtin = 'yes'\n    else:\n        for found_name, style in find_plugin_styles():\n            if name == found_name:\n                return style\n        builtin = ''\n        mod = name\n        cls = name.title() + 'Style'\n    try:\n        mod = __import__('pygments.styles.' + mod, None, None, [cls])\n    except ImportError:\n        raise ClassNotFound('Could not find style module %r' % mod + (builtin and ', though it should be builtin') + '.')\n    try:\n        return getattr(mod, cls)\n    except AttributeError:\n        raise ClassNotFound('Could not find style class %r in style module.' % cls)",
    "label": true
  },
  {
    "code": "def _declare_state(vartype, **kw):\n    globals().update(kw)\n    _state_vars.update(dict.fromkeys(kw, vartype))",
    "label": true
  },
  {
    "code": "def function_wrapper(wrapper):\n\n    def _wrapper(wrapped, instance, args, kwargs):\n        target_wrapped = args[0]\n        if instance is None:\n            target_wrapper = wrapper\n        elif inspect.isclass(instance):\n            target_wrapper = wrapper.__get__(None, instance)\n        else:\n            target_wrapper = wrapper.__get__(instance, type(instance))\n        return FunctionWrapper(target_wrapped, target_wrapper)\n    return FunctionWrapper(wrapper, _wrapper)",
    "label": true
  },
  {
    "code": "def init(autoreset=False, convert=None, strip=None, wrap=True):\n    if not wrap and any([autoreset, convert, strip]):\n        raise ValueError('wrap=False conflicts with any other arg=True')\n    global wrapped_stdout, wrapped_stderr\n    global orig_stdout, orig_stderr\n    orig_stdout = sys.stdout\n    orig_stderr = sys.stderr\n    if sys.stdout is None:\n        wrapped_stdout = None\n    else:\n        sys.stdout = wrapped_stdout = wrap_stream(orig_stdout, convert, strip, autoreset, wrap)\n    if sys.stderr is None:\n        wrapped_stderr = None\n    else:\n        sys.stderr = wrapped_stderr = wrap_stream(orig_stderr, convert, strip, autoreset, wrap)\n    global atexit_done\n    if not atexit_done:\n        atexit.register(reset_all)\n        atexit_done = True",
    "label": true
  },
  {
    "code": "def _run_pylint_config(argv: Sequence[str] | None=None) -> None:\n    from pylint.lint.run import _PylintConfigRun\n    _PylintConfigRun(argv or sys.argv[1:])",
    "label": true
  },
  {
    "code": "def display_path(path: str) -> str:\n    path = os.path.normcase(os.path.abspath(path))\n    if path.startswith(os.getcwd() + os.path.sep):\n        path = '.' + path[len(os.getcwd()):]\n    return path",
    "label": true
  },
  {
    "code": "def _version_split(version: str) -> List[str]:\n    result: List[str] = []\n    for item in version.split('.'):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result",
    "label": true
  },
  {
    "code": "def _is_load_subscript(index_node: nodes.Name, loop_node: Union[nodes.For, nodes.Comprehension]) -> bool:\n    iterable = _iterable_if_range(loop_node.iter)\n    return isinstance(index_node.parent, nodes.Subscript) and isinstance(index_node.parent.value, nodes.Name) and (index_node.parent.value.name == iterable) and (index_node.parent.ctx == astroid.Load)",
    "label": true
  },
  {
    "code": "def unpackb(packed, **kwargs):\n    unpacker = Unpacker(None, max_buffer_size=len(packed), **kwargs)\n    unpacker.feed(packed)\n    try:\n        ret = unpacker._unpack()\n    except OutOfData:\n        raise ValueError('Unpack failed: incomplete input')\n    except RecursionError as e:\n        if _is_recursionerror(e):\n            raise StackError\n        raise\n    if unpacker._got_extradata():\n        raise ExtraData(ret, unpacker._get_extradata())\n    return ret",
    "label": true
  },
  {
    "code": "def sliding_window(iterable, n):\n    it = iter(iterable)\n    window = deque(islice(it, n), maxlen=n)\n    if len(window) == n:\n        yield tuple(window)\n    for x in it:\n        window.append(x)\n        yield tuple(window)",
    "label": true
  },
  {
    "code": "def newer_pairwise_group(sources_groups, targets):\n    if len(sources_groups) != len(targets):\n        raise ValueError(\"'sources_group' and 'targets' must be the same length\")\n    n_sources = []\n    n_targets = []\n    for i in range(len(sources_groups)):\n        if newer_group(sources_groups[i], targets[i]):\n            n_sources.append(sources_groups[i])\n            n_targets.append(targets[i])\n    return (n_sources, n_targets)",
    "label": true
  },
  {
    "code": "def _config(path: Optional[Path]=None, config: Config=DEFAULT_CONFIG, **config_kwargs: Any) -> Config:\n    if path and (config is DEFAULT_CONFIG and 'settings_path' not in config_kwargs and ('settings_file' not in config_kwargs)):\n        config_kwargs['settings_path'] = path\n    if config_kwargs:\n        if config is not DEFAULT_CONFIG:\n            raise ValueError('You can either specify custom configuration options using kwargs or passing in a Config object. Not Both!')\n        config = Config(**config_kwargs)\n    return config",
    "label": true
  },
  {
    "code": "def _build_one_inside_env(req: InstallRequirement, output_dir: str, build_options: List[str], global_options: List[str], editable: bool) -> Optional[str]:\n    with TempDirectory(kind='wheel') as temp_dir:\n        assert req.name\n        if req.use_pep517:\n            assert req.metadata_directory\n            assert req.pep517_backend\n            if global_options:\n                logger.warning('Ignoring --global-option when building %s using PEP 517', req.name)\n            if build_options:\n                logger.warning('Ignoring --build-option when building %s using PEP 517', req.name)\n            if editable:\n                wheel_path = build_wheel_editable(name=req.name, backend=req.pep517_backend, metadata_directory=req.metadata_directory, tempd=temp_dir.path)\n            else:\n                wheel_path = build_wheel_pep517(name=req.name, backend=req.pep517_backend, metadata_directory=req.metadata_directory, tempd=temp_dir.path)\n        else:\n            wheel_path = build_wheel_legacy(name=req.name, setup_py_path=req.setup_py_path, source_dir=req.unpacked_source_directory, global_options=global_options, build_options=build_options, tempd=temp_dir.path)\n        if wheel_path is not None:\n            wheel_name = os.path.basename(wheel_path)\n            dest_path = os.path.join(output_dir, wheel_name)\n            try:\n                wheel_hash, length = hash_file(wheel_path)\n                shutil.move(wheel_path, dest_path)\n                logger.info('Created wheel for %s: filename=%s size=%d sha256=%s', req.name, wheel_name, length, wheel_hash.hexdigest())\n                logger.info('Stored in directory: %s', output_dir)\n                return dest_path\n            except Exception as e:\n                logger.warning('Building wheel for %s failed: %s', req.name, e)\n        if not req.use_pep517:\n            _clean_one_legacy(req, global_options)\n        return None",
    "label": true
  },
  {
    "code": "def _match_hostname(cert: _TYPE_PEER_CERT_RET_DICT | None, asserted_hostname: str, hostname_checks_common_name: bool=False) -> None:\n    stripped_hostname = asserted_hostname.strip('[]')\n    if is_ipaddress(stripped_hostname):\n        asserted_hostname = stripped_hostname\n    try:\n        match_hostname(cert, asserted_hostname, hostname_checks_common_name)\n    except CertificateError as e:\n        log.warning('Certificate did not match expected hostname: %s. Certificate: %s', asserted_hostname, cert)\n        e._peer_cert = cert\n        raise",
    "label": true
  }
]